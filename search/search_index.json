{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Pydantic2Django","text":"<p>Generate Django models from Pydantic models, Python dataclasses, or plain typed classes \u2014 and convert data back and forth reliably.</p> <p>[!IMPORTANT] Namespace rename and deprecation: The distribution is now <code>django-typed2django</code> and the new import namespace is <code>typed2django</code>. The old <code>pydantic2django</code> namespace is deprecated and will be removed in version 1.1.0. Please migrate imports to <code>typed2django.*</code>.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Automatic Django model generation (<code>models.py</code>) from your Python types</li> <li>Bidirectional mapping: use <code>.from_pydantic()</code> and <code>.to_pydantic()</code> on generated models</li> <li>Relationship handling: <code>ForeignKey</code> and <code>ManyToManyField</code> inferred from your types</li> <li>Type-hint aware: supports <code>Optional</code>, <code>Union</code>, <code>Literal</code>, and common collections</li> <li>Modular design: separate pipelines for Pydantic, Dataclasses, and Typed classes</li> </ul>"},{"location":"#quickstart","title":"Quickstart","text":"<ol> <li>Install</li> </ol> <pre><code>pip install django-typed2django\n</code></pre> <ol> <li>Define your Pydantic models</li> </ol> <pre><code>import uuid\nfrom pydantic import BaseModel, Field\n\nclass User(BaseModel):\n    id: uuid.UUID = Field(default_factory=uuid.uuid4)\n    name: str\n    email: str\n\nclass Product(BaseModel):\n    id: uuid.UUID = Field(default_factory=uuid.uuid4)\n    name: str\n    price: float\n    owner: User\n</code></pre> <ol> <li>Generate a <code>models.py</code> file</li> </ol> <pre><code>from typed2django.pydantic.generator import StaticPydanticModelGenerator\n\ngenerator = StaticPydanticModelGenerator(\n    output_path=\"my_app/models.py\",\n    packages=[\"my_app.pydantic_models\"],\n    app_label=\"my_app\",\n    verbose=True,\n)\n\n# Write my_app/models.py\ngenerator.generate()\n</code></pre> <ol> <li>Use the generated models</li> </ol> <pre><code>from my_app.models import User as DjangoUser\nfrom my_app.pydantic_models import User as PydanticUser\n\np_user = PydanticUser(name=\"Jane Doe\", email=\"jane.doe@example.com\")\n\n# Create Django instance from Pydantic\ndj_user = DjangoUser.from_pydantic(p_user)\ndj_user.save()\n\n# Convert back to Pydantic\nassert dj_user.to_pydantic().name == \"Jane Doe\"\n</code></pre>"},{"location":"#how-it-works","title":"How it works","text":"<ol> <li>Discovery: scan packages to find your source models</li> <li>Factory: analyze fields, type hints, and relationships</li> <li>Generator: render a complete <code>models.py</code> with imports, classes, and fields</li> </ol>"},{"location":"#explore-next","title":"Explore next","text":"<ul> <li>Architecture: high-level design and internals</li> <li>Docs: documentation files colocated with source code</li> <li>API: full reference generated from the code</li> </ul> <p>You can navigate these sections from the left sidebar.</p>"},{"location":"Architecture/CONTEXT_HANDLING/","title":"Context Handling in Pydantic2Django","text":""},{"location":"Architecture/CONTEXT_HANDLING/#overview","title":"Overview","text":"<p>Pydantic2Django handles non-serializable fields through a context system. When a Pydantic model contains fields that cannot be directly serialized to the database (like custom classes, functions, or complex objects), these fields are marked as \"context fields\" and require special handling during conversion back to Pydantic objects.</p>"},{"location":"Architecture/CONTEXT_HANDLING/#how-it-works","title":"How It Works","text":""},{"location":"Architecture/CONTEXT_HANDLING/#1-field-detection","title":"1. Field Detection","text":"<p>During model generation, fields are analyzed for serializability. A field is considered non-serializable if it: - Is a custom class without JSON serialization methods - Contains callable objects - Contains complex objects that can't be stored in the database - Is explicitly marked as non-serializable</p>"},{"location":"Architecture/CONTEXT_HANDLING/#2-storage","title":"2. Storage","text":"<p>Non-serializable fields are stored in the database as: <pre><code>models.TextField(is_relationship=True)\n</code></pre></p> <p>This special flag indicates that the field requires context when converting back to a Pydantic object.</p>"},{"location":"Architecture/CONTEXT_HANDLING/#3-type-safety","title":"3. Type Safety","text":"<p>For each Django model with context fields, a corresponding <code>TypedDict</code> is generated:</p> <pre><code>class MyModelContext(TypedDict):\n    \"\"\"Required context fields for converting MyModel back to MyPydanticModel.\"\"\"\n    field1: Any\n    field2: Any\n</code></pre>"},{"location":"Architecture/CONTEXT_HANDLING/#4-usage-example","title":"4. Usage Example","text":"<pre><code>from my_app.models import MyModel, MyModelContext\nfrom my_app.pydantic_models import MyPydanticModel\n\n# Creating from Pydantic\npydantic_obj = MyPydanticModel(\n    field1=ComplexObject(),\n    field2=lambda x: x + 1\n)\ndjango_obj = MyModel.from_pydantic(pydantic_obj)\n\n# Converting back to Pydantic\ncontext: MyModelContext = {\n    \"field1\": ComplexObject(),  # Must provide the non-serializable objects\n    \"field2\": lambda x: x + 1\n}\npydantic_obj = django_obj.to_pydantic(context=context)\n</code></pre>"},{"location":"Architecture/CONTEXT_HANDLING/#error-handling","title":"Error Handling","text":"<p>If you attempt to convert a Django model back to Pydantic without providing the required context, you'll get a clear error message:</p> <pre><code>ValueError: This model has non-serializable fields that require context: field1, field2.\nPlease provide the context dictionary when calling to_pydantic().\n</code></pre> <p>If you provide context but miss some required fields:</p> <pre><code>ValueError: Missing required context fields: field2\n</code></pre>"},{"location":"Architecture/CONTEXT_HANDLING/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Type Hints: Always use the generated Context type for better IDE support:    <pre><code>context: MyModelContext = {...}\n</code></pre></p> </li> <li> <p>Documentation: The generated Django models include docstrings listing all required context fields.</p> </li> <li> <p>Validation: Consider implementing validation for context values before passing them to <code>to_pydantic()</code>.</p> </li> <li> <p>State Management: Keep context values organized and easily accessible in your application.</p> </li> </ol>"},{"location":"Architecture/CONTEXT_HANDLING/#technical-details","title":"Technical Details","text":""},{"location":"Architecture/CONTEXT_HANDLING/#field-type-resolution","title":"Field Type Resolution","text":"<p>The <code>is_serializable_type()</code> function in <code>field_utils.py</code> determines if a type is serializable by checking: - Basic Python types (str, int, float, bool, etc.) - Collections (list, dict, set) with serializable elements - Pydantic models - Enums - Classes with JSON serialization methods</p>"},{"location":"Architecture/CONTEXT_HANDLING/#context-field-detection","title":"Context Field Detection","text":"<p>Fields are marked as context fields when: 1. The type is not serializable 2. The field is explicitly marked with <code>is_relationship=True</code> 3. The field requires custom serialization logic</p>"},{"location":"Architecture/CONTEXT_HANDLING/#generated-code","title":"Generated Code","text":"<p>For each model with context fields, the following is generated: 1. A TypedDict for context validation 2. A modified <code>to_pydantic()</code> method requiring context 3. Documentation explaining the context requirements 4. Proper imports and exports in the models file</p>"},{"location":"Architecture/CONTEXT_HANDLING/#example-implementation","title":"Example Implementation","text":"<pre><code># Generated Django Model\nfrom typing import TypedDict\n\nclass UserContext(TypedDict):\n    avatar_processor: Any\n    permissions_handler: Any\n\nclass User(Pydantic2DjangoBaseClass[UserPydantic]):\n    name = models.CharField(max_length=255)\n    avatar_processor = models.TextField(is_relationship=True)\n    permissions_handler = models.TextField(is_relationship=True)\n\n    def to_pydantic(self, context: UserContext) -&gt; UserPydantic:\n        return super().to_pydantic(context=context)\n\n# Usage\nuser = User.objects.get(id=1)\ncontext: UserContext = {\n    \"avatar_processor\": AvatarProcessor(),\n    \"permissions_handler\": PermissionsHandler()\n}\npydantic_user = user.to_pydantic(context=context)\n</code></pre>"},{"location":"Architecture/OVERVIEW/","title":"OVERVIEW","text":""},{"location":"Architecture/OVERVIEW/#architecture-overview","title":"Architecture overview","text":"<p>This document explains how <code>pydantic2django</code> is organized, what the shared core is responsible for, and what each implementation (Pydantic, Dataclass, TypedClass) has in common.</p> <p>For inlined API links below, we use mkdocstrings\u2019 Python handler syntax; see \u201cmkdocstrings usage\u201d for details. Reference: mkdocstrings usage.</p>"},{"location":"Architecture/OVERVIEW/#big-picture","title":"Big picture","text":"<ul> <li>Goal: Convert typed Python models into Django models and make them interoperable in both directions.</li> <li>Layers:</li> <li>Core: Shared building blocks (discovery, factories, bidirectional type mapping, typing utils, import aggregation, context handling, code generation base).</li> <li>Implementations: Source-specific adapters that plug into the core (Pydantic, Dataclass, TypedClass).</li> <li>Django base models: Ready-to-extend base classes that store or map typed objects inside Django models.</li> </ul> <p>High-level flow (static code generation path):</p> <ol> <li>Discover source models in Python packages.</li> <li>For each model, convert its fields to Django fields via the bidirectional type mapper.</li> <li>Create an in-memory Django model class and collect rendered definitions using Jinja templates.</li> <li>Write a <code>models.py</code> file including imports, model classes, and optional context classes.</li> </ol> <p>Templates live in <code>src/pydantic2django/django/templates</code> and are used by the generator base.</p>"},{"location":"Architecture/OVERVIEW/#core-responsibilities","title":"Core responsibilities","text":"<ul> <li>Static generation orchestration</li> <li> <p>               Bases: <code>ABC</code>, <code>Generic[SourceModelType, FieldInfoType]</code></p> <p>Abstract base class for generating static Django models from source models (like Pydantic or Dataclasses).</p> Source code in <code>src/pydantic2django/core/base_generator.py</code> <pre><code>class BaseStaticGenerator(ABC, Generic[SourceModelType, FieldInfoType]):\n    \"\"\"\n    Abstract base class for generating static Django models from source models (like Pydantic or Dataclasses).\n    \"\"\"\n\n    def __init__(\n        self,\n        output_path: str,\n        app_label: str,\n        filter_function: Callable[[type[SourceModelType]], bool] | None,\n        verbose: bool,\n        discovery_instance: BaseDiscovery[SourceModelType],\n        model_factory_instance: BaseModelFactory[SourceModelType, FieldInfoType],\n        module_mappings: dict[str, str] | None,\n        base_model_class: type[models.Model],\n        packages: list[str] | None = None,\n        class_name_prefix: str = \"Django\",\n        enable_timescale: bool = True,\n        # --- GFK flags ---\n        enable_gfk: bool = True,\n        gfk_policy: str | None = \"all_nested\",\n        gfk_threshold_children: int | None = 4,\n        gfk_value_mode: str | None = \"typed_columns\",\n        gfk_normalize_common_attrs: bool = False,\n    ):\n        \"\"\"\n        Initialize the base generator.\n\n        Args:\n            output_path: Path to output the generated models.py file.\n            packages: List of packages to scan for source models.\n            app_label: Django app label to use for the models.\n            filter_function: Optional function to filter which source models to include.\n            verbose: Print verbose output.\n            discovery_instance: An instance of a BaseDiscovery subclass.\n            model_factory_instance: An instance of a BaseModelFactory subclass.\n            module_mappings: Optional mapping of modules to remap imports.\n            base_model_class: The base Django model class to inherit from.\n            class_name_prefix: Prefix for generated Django model class names.\n            enable_timescale: Whether to enable TimescaleDB support for hypertables.\n        \"\"\"\n        self.output_path = output_path\n        self.packages = packages\n        self.app_label = app_label\n        self.filter_function = filter_function\n        self.verbose = verbose\n        self.discovery_instance = discovery_instance\n        self.model_factory_instance = model_factory_instance\n        # Base model class must be provided explicitly by subclass at call site.\n        self.base_model_class = base_model_class\n        self.class_name_prefix = class_name_prefix\n        self.carriers: list[ConversionCarrier[SourceModelType]] = []  # Stores results from model factory\n        # Feature flags\n        self.enable_timescale: bool = bool(enable_timescale)\n        # GFK feature flags\n        self.enable_gfk: bool = bool(enable_gfk)\n        self.gfk_policy: str | None = gfk_policy\n        self.gfk_threshold_children: int | None = gfk_threshold_children\n        self.gfk_value_mode: str | None = gfk_value_mode\n        self.gfk_normalize_common_attrs: bool = bool(gfk_normalize_common_attrs)\n\n        self.import_handler = ImportHandler(module_mappings=module_mappings)\n\n        # Initialize Jinja2 environment\n        # Look for templates in the django/templates subdirectory\n        # package_templates_dir = os.path.join(os.path.dirname(__file__), \"..\", \"templates\") # Old path\n        package_templates_dir = os.path.join(os.path.dirname(__file__), \"..\", \"django\", \"templates\")  # Corrected path\n\n        # If templates don't exist in the package, use the ones relative to the execution?\n        # This might need adjustment based on packaging/distribution.\n        # For now, assume templates are relative to the package structure.\n        if not os.path.exists(package_templates_dir):\n            # Fallback or raise error might be needed\n            package_templates_dir = os.path.join(pathlib.Path(__file__).parent.parent.absolute(), \"templates\")\n            if not os.path.exists(package_templates_dir):\n                logger.warning(\n                    f\"Templates directory not found at expected location: {package_templates_dir}. Jinja might fail.\"\n                )\n\n        self.jinja_env = jinja2.Environment(\n            loader=jinja2.FileSystemLoader(package_templates_dir),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        )\n\n        # Register common custom filters\n        self.jinja_env.filters[\"format_type_string\"] = TypeHandler.format_type_string\n        # Provide an escaping filter for embedding strings safely in generated Python code\n        from ..core.utils.strings import sanitize_string as _escape_py_str  # Local import to avoid cycles\n\n        self.jinja_env.filters[\"escape_py_str\"] = _escape_py_str\n        # Add more common filters if needed\n\n        # Add base model import\n        self.import_handler._add_type_import(self.base_model_class)\n\n    # --- Abstract Methods to be Implemented by Subclasses ---\n\n    @abstractmethod\n    def _get_source_model_name(self, carrier: ConversionCarrier[SourceModelType]) -&gt; str:\n        \"\"\"Get the name of the original source model from the carrier.\"\"\"\n        pass\n\n    @abstractmethod\n    def _add_source_model_import(self, carrier: ConversionCarrier[SourceModelType]):\n        \"\"\"Add the necessary import for the original source model.\"\"\"\n        pass\n\n    @abstractmethod\n    def _prepare_template_context(\n        self, unique_model_definitions: list[str], django_model_names: list[str], imports: dict\n    ) -&gt; dict:\n        \"\"\"Prepare the subclass-specific context for the main models_file.py.j2 template.\"\"\"\n        pass\n\n    @abstractmethod\n    def _get_models_in_processing_order(self) -&gt; list[SourceModelType]:\n        \"\"\"Return source models in the correct processing (dependency) order.\"\"\"\n        pass\n\n    @abstractmethod\n    def _get_model_definition_extra_context(self, carrier: ConversionCarrier[SourceModelType]) -&gt; dict:\n        \"\"\"Provide extra context specific to the source type for model_definition.py.j2.\"\"\"\n        pass\n\n    @abstractmethod\n    def _get_default_base_model_class(self) -&gt; type[models.Model]:\n        \"\"\"Return the required base Django model class for this generator.\n\n        Subclasses implement this so they can easily and explicitly resolve\n        the correct base and pass it into the base initializer.\n        \"\"\"\n        pass\n\n    # --- Common Methods ---\n\n    def generate(self) -&gt; str:\n        \"\"\"\n        Main entry point: Generate and write the models file.\n\n        Returns:\n            The path to the generated models file.\n        \"\"\"\n        try:\n            content = self.generate_models_file()\n            self._write_models_file(content)\n            logger.info(f\"Successfully generated models file at {self.output_path}\")\n            return self.output_path\n        except Exception as e:\n            logger.exception(f\"Error generating models file: {e}\", exc_info=True)  # Use exc_info for traceback\n            raise\n\n    def _write_models_file(self, content: str) -&gt; None:\n        \"\"\"Write the generated content to the output file.\"\"\"\n        if self.verbose:\n            logger.info(f\"Writing models to {self.output_path}\")\n\n        output_dir = os.path.dirname(self.output_path)\n        if output_dir and not os.path.exists(output_dir):\n            try:\n                os.makedirs(output_dir)\n                if self.verbose:\n                    logger.info(f\"Created output directory: {output_dir}\")\n            except OSError as e:\n                logger.error(f\"Failed to create output directory {output_dir}: {e}\")\n                raise  # Re-raise after logging\n\n        try:\n            with open(self.output_path, \"w\", encoding=\"utf-8\") as f:  # Specify encoding\n                f.write(content)\n            if self.verbose:\n                logger.info(f\"Successfully wrote models to {self.output_path}\")\n        except OSError as e:\n            logger.error(f\"Failed to write to output file {self.output_path}: {e}\")\n            raise  # Re-raise after logging\n\n    def discover_models(self) -&gt; None:\n        \"\"\"Discover source models using the configured discovery instance.\"\"\"\n        if self.verbose:\n            logger.info(f\"Discovering models from packages: {self.packages}\")\n\n        # Corrected call matching BaseDiscovery signature\n        self.discovery_instance.discover_models(\n            self.packages or [],  # Pass empty list if None\n            app_label=self.app_label,\n            user_filters=self.filter_function,  # Keep as is for now\n        )\n\n        # Analyze dependencies after discovery\n        self.discovery_instance.analyze_dependencies()\n\n        if self.verbose:\n            logger.info(f\"Discovered {len(self.discovery_instance.filtered_models)} models after filtering.\")\n            if self.discovery_instance.filtered_models:\n                for name in self.discovery_instance.filtered_models.keys():\n                    logger.info(f\"  - {name}\")\n            else:\n                logger.info(\"  (No models found or passed filter)\")\n            logger.info(\"Dependency analysis complete.\")\n\n    def setup_django_model(self, source_model: SourceModelType) -&gt; ConversionCarrier[SourceModelType] | None:\n        \"\"\"\n        Uses the model factory to create a Django model representation from a source model.\n\n        Args:\n            source_model: The source model instance (e.g., Pydantic class, Dataclass).\n\n        Returns:\n            A ConversionCarrier containing the results, or None if creation failed.\n        \"\"\"\n        source_model_name = getattr(source_model, \"__name__\", str(source_model))\n        if self.verbose:\n            logger.info(f\"Setting up Django model for {source_model_name}\")\n\n        # Instantiate the carrier here\n        carrier = ConversionCarrier(\n            source_model=cast(type[SourceModelType], source_model),\n            meta_app_label=self.app_label,\n            base_django_model=self.base_model_class,\n            class_name_prefix=self.class_name_prefix,\n            # Add other defaults/configs if needed, e.g., strict mode\n            strict=False,  # Example default\n            # GFK flags\n            enable_gfk=self.enable_gfk,\n            gfk_policy=self.gfk_policy,\n            gfk_threshold_children=self.gfk_threshold_children,\n            gfk_value_mode=self.gfk_value_mode,\n            gfk_normalize_common_attrs=self.gfk_normalize_common_attrs,\n        )\n\n        try:\n            # Use the factory to process the source model and populate the carrier\n            self.model_factory_instance.make_django_model(carrier)  # Pass carrier to factory\n\n            if carrier.django_model:\n                self.carriers.append(carrier)\n                if self.verbose:\n                    logger.info(f\"Successfully processed {source_model_name} -&gt; {carrier.django_model.__name__}\")\n                return carrier\n            else:\n                # Log if model creation resulted in None (e.g., only context fields)\n                # Check carrier.context_fields or carrier.invalid_fields for details\n                if carrier.context_fields and not carrier.django_fields and not carrier.relationship_fields:\n                    logger.info(f\"Skipped Django model class for {source_model_name} - only context fields found.\")\n                elif carrier.invalid_fields:\n                    logger.warning(\n                        f\"Skipped Django model class for {source_model_name} due to invalid fields: {carrier.invalid_fields}\"\n                    )\n                else:\n                    logger.warning(f\"Django model was not generated for {source_model_name} for unknown reasons.\")\n                return None  # Return None if no Django model was created\n\n        except Exception as e:\n            logger.error(f\"Error processing {source_model_name} with factory: {e}\", exc_info=True)\n            return None\n\n    def generate_model_definition(self, carrier: ConversionCarrier[SourceModelType]) -&gt; str:\n        \"\"\"\n        Generates a string definition for a single Django model using a template.\n\n        Args:\n            carrier: The ConversionCarrier containing the generated Django model and context.\n\n        Returns:\n            The string representation of the Django model definition.\n        \"\"\"\n        if not carrier.django_model:\n            # It's possible a carrier exists only for context, handle gracefully.\n            source_name = self._get_source_model_name(carrier)\n            if carrier.model_context and carrier.model_context.context_fields:\n                logger.info(f\"Skipping Django model definition for {source_name} (likely context-only).\")\n                return \"\"\n            else:\n                logger.warning(\n                    f\"Cannot generate model definition for {source_name}: django_model is missing in carrier.\"\n                )\n                return \"\"\n\n        django_model_name = self._clean_generic_type(carrier.django_model.__name__)\n        source_model_name = self._get_source_model_name(carrier)  # Get original name via abstract method\n\n        # --- Prepare Fields ---\n        fields_info = []\n        # Combine regular and relationship fields from the carrier\n        all_django_fields = {**carrier.django_fields, **carrier.relationship_fields}\n\n        for field_name, field_object in all_django_fields.items():\n            # The field_object is already an instantiated Django field\n            # Add (name, object) tuple directly for the template\n            fields_info.append((field_name, field_object))\n\n        # --- Prepare Meta ---\n        meta_options = {}\n        if hasattr(carrier.django_model, \"_meta\"):\n            model_meta = carrier.django_model._meta\n            meta_options = {\n                \"db_table\": getattr(model_meta, \"db_table\", f\"{self.app_label}_{django_model_name.lower()}\"),\n                \"app_label\": self.app_label,\n                \"verbose_name\": getattr(model_meta, \"verbose_name\", django_model_name),\n                \"verbose_name_plural\": getattr(model_meta, \"verbose_name_plural\", f\"{django_model_name}s\"),\n                # Add other meta options if needed\n            }\n\n        # --- Prepare Base Class Info ---\n        base_model_name = self.base_model_class.__name__\n        if carrier.django_model.__bases__ and carrier.django_model.__bases__[0] != models.Model:\n            # Use the immediate parent if it's not the absolute base 'models.Model'\n            # Assumes single inheritance for the generated model besides the ultimate base\n            parent_class = carrier.django_model.__bases__[0]\n            # Check if the parent is our intended base_model_class or something else\n            # This logic might need refinement depending on how complex the inheritance gets\n            if issubclass(parent_class, models.Model) and parent_class != models.Model:\n                base_model_name = parent_class.__name__\n                # Add import for the parent if it's not the configured base_model_class\n                if parent_class != self.base_model_class:\n                    self.import_handler._add_type_import(parent_class)\n\n        # --- Get Subclass Specific Context ---\n        extra_context = self._get_model_definition_extra_context(carrier)\n\n        # --- Process Pending Multi-FK Unions and add to definitions dict ---\n        multi_fk_field_names = []  # Keep track for validation hint\n        validation_needed = False\n        if carrier.pending_multi_fk_unions:\n            validation_needed = True\n            for original_field_name, union_details in carrier.pending_multi_fk_unions:\n                pydantic_models = union_details.get(\"models\", [])\n                for pydantic_model in pydantic_models:\n                    # Construct field name (e.g., original_name_relatedmodel)\n                    fk_field_name = f\"{original_field_name}_{pydantic_model.__name__.lower()}\"\n                    multi_fk_field_names.append(fk_field_name)\n                    # Get corresponding Django model\n                    pydantic_factory = cast(PydanticModelFactory, self.model_factory_instance)\n                    django_model_rel = pydantic_factory.relationship_accessor.get_django_model_for_pydantic(\n                        pydantic_model\n                    )\n                    if not django_model_rel:\n                        logger.error(\n                            f\"Could not find Django model for Pydantic model {pydantic_model.__name__} referenced in multi-FK union for {original_field_name}. Skipping FK field.\"\n                        )\n                        continue\n                    # Use string for model ref in kwargs\n                    target_model_str = f\"'{django_model_rel._meta.app_label}.{django_model_rel.__name__}'\"\n                    # Add import for the related Django model\n                    self.import_handler._add_type_import(django_model_rel)\n\n                    # Define FK kwargs (always null=True, blank=True)\n                    # Use strings for values that need to be represented in code\n                    fk_kwargs = {\n                        \"to\": target_model_str,\n                        \"on_delete\": \"models.SET_NULL\",  # Use string for template\n                        \"null\": True,\n                        \"blank\": True,\n                        # Generate related_name to avoid clashes\n                        \"related_name\": f\"'{carrier.django_model.__name__.lower()}_{fk_field_name}_set'\",  # Ensure related_name is quoted string\n                    }\n                    # Generate the definition string\n                    fk_def_string = generate_field_definition_string(models.ForeignKey, fk_kwargs, self.app_label)\n                    # Add to the main definitions dictionary\n                    carrier.django_field_definitions[fk_field_name] = fk_def_string\n\n        # --- Prepare Final Context --- #\n        # Ensure the context uses the potentially updated definitions dict from the carrier\n        # Subclass _get_model_definition_extra_context should already provide this\n        # via `field_definitions=carrier.django_field_definitions`\n        template_context = {\n            \"model_name\": django_model_name,\n            \"pydantic_model_name\": source_model_name,\n            \"base_model_name\": base_model_name,\n            \"is_timescale_model\": bool(str(base_model_name).endswith(\"TimescaleBase\")),\n            \"meta\": meta_options,\n            \"app_label\": self.app_label,\n            \"multi_fk_field_names\": multi_fk_field_names,  # Pass names for validation hint\n            \"validation_needed\": validation_needed,  # Signal if validation needed\n            # Include extra context from subclass (should include field_definitions)\n            **extra_context,\n        }\n\n        # --- Render Template --- #\n        template = self.jinja_env.get_template(\"model_definition.py.j2\")\n        definition_str = template.render(**template_context)\n\n        # Add import for the original source model\n        self._add_source_model_import(carrier)\n\n        return definition_str\n\n    def _deduplicate_definitions(self, definitions: list[str]) -&gt; list[str]:\n        \"\"\"Remove duplicate model definitions based on class name.\"\"\"\n        unique_definitions = []\n        seen_class_names = set()\n        for definition in definitions:\n            # Basic regex to find 'class ClassName(' - might need adjustment for complex cases\n            match = re.search(r\"^\\s*class\\s+(\\w+)\\(\", definition, re.MULTILINE)\n            if match:\n                class_name = match.group(1)\n                if class_name not in seen_class_names:\n                    unique_definitions.append(definition)\n                    seen_class_names.add(class_name)\n                # else: logger.debug(f\"Skipping duplicate definition for class: {class_name}\")\n            else:\n                # If no class definition found (e.g., comments, imports), keep it? Or discard?\n                # For now, keep non-class definitions assuming they might be needed context/comments.\n                unique_definitions.append(definition)\n                logger.warning(\"Could not extract class name from definition block for deduplication.\")\n\n        return unique_definitions\n\n    def _clean_generic_type(self, name: str) -&gt; str:\n        \"\"\"Remove generic parameters like [T] or &lt;T&gt; from a type name.\"\"\"\n        # Handles Class[Param] or Class&lt;Param&gt;\n        cleaned_name = re.sub(r\"[\\[&lt;].*?[\\]&gt;]\", \"\", name)\n        # Also handle cases like 'ModelName.T' if typevars are used this way\n        cleaned_name = cleaned_name.split(\".\")[-1]\n        return cleaned_name\n\n    def generate_models_file(self) -&gt; str:\n        \"\"\"\n        Generates the complete content for the models.py file.\n        This method orchestrates discovery, model setup, definition generation,\n        import collection, and template rendering.\n        Subclasses might override this to add specific steps (like context class generation).\n        \"\"\"\n        self.discover_models()  # Populates discovery instance\n        models_to_process = self._get_models_in_processing_order()  # Abstract method\n\n        # Reset state for this run\n        self.carriers = []\n        self.import_handler.extra_type_imports.clear()\n        self.import_handler.pydantic_imports.clear()\n        self.import_handler.context_class_imports.clear()\n        self.import_handler.imported_names.clear()\n        self.import_handler.processed_field_types.clear()\n\n        # Re-add base model import after clearing\n        self.import_handler._add_type_import(self.base_model_class)\n\n        model_definitions = []\n        django_model_names = []  # For __all__\n\n        # Setup Django models first (populates self.carriers)\n        for source_model in models_to_process:\n            self.setup_django_model(source_model)  # Calls factory, populates carrier\n\n        # Generate definitions from carriers\n        for carrier in self.carriers:\n            # Generate Django model definition if model exists\n            if carrier.django_model:\n                try:\n                    model_def = self.generate_model_definition(carrier)  # Uses template\n                    if model_def:  # Only add if definition was generated\n                        model_definitions.append(model_def)\n                        django_model_name = self._clean_generic_type(carrier.django_model.__name__)\n                        django_model_names.append(f\"'{django_model_name}'\")\n                except Exception as e:\n                    source_name = self._get_source_model_name(carrier)\n                    logger.error(f\"Error generating definition for source model {source_name}: {e}\", exc_info=True)\n\n            # Subclasses might add context class generation here by overriding this method\n            # or by generate_model_definition adding context-related imports.\n\n        # Deduplicate definitions\n        unique_model_definitions = self._deduplicate_definitions(model_definitions)\n\n        # Deduplicate imports gathered during the process\n        imports = self.import_handler.deduplicate_imports()\n\n        # Prepare context using subclass method (_prepare_template_context)\n        template_context = self._prepare_template_context(unique_model_definitions, django_model_names, imports)\n\n        # Add common context items\n        template_context.update(\n            {\n                \"generation_timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n                \"base_model_module\": self.base_model_class.__module__,\n                \"base_model_name\": self.base_model_class.__name__,\n                \"extra_type_imports\": sorted(self.import_handler.extra_type_imports),\n                # Add other common items as needed\n            }\n        )\n\n        # Render the main template\n        template = self.jinja_env.get_template(\"models_file.py.j2\")\n        return template.render(**template_context)\n</code></pre> </li> <li> <p>Model discovery (abstract + per-source dependency graph)</p> </li> <li> <p>               Bases: <code>ABC</code>, <code>Generic[TModel]</code></p> <p>Abstract base class for discovering models (e.g., Pydantic, Dataclasses).</p> Source code in <code>src/pydantic2django/core/discovery.py</code> <pre><code>class BaseDiscovery(abc.ABC, Generic[TModel]):\n    \"\"\"Abstract base class for discovering models (e.g., Pydantic, Dataclasses).\"\"\"\n\n    def __init__(self):\n        self.all_models: dict[str, type[TModel]] = {}  # All discovered models before any filtering\n        self.filtered_models: dict[str, type[TModel]] = {}  # Models after all filters\n        self.dependencies: dict[type[TModel], set[type[TModel]]] = {}  # Dependencies between filtered models\n\n    @abc.abstractmethod\n    def _is_target_model(self, obj: Any) -&gt; bool:\n        \"\"\"Check if an object is the type of model this discovery class handles.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def _default_eligibility_filter(self, model: type[TModel]) -&gt; bool:\n        \"\"\"\n        Apply default filtering logic inherent to the model type (e.g., exclude abstract classes).\n        Return True if the model is eligible, False otherwise.\n        \"\"\"\n        pass\n\n    def discover_models(\n        self,\n        packages: list[str],\n        app_label: str,  # Keep for potential use in filters or subclasses\n        user_filters: Optional[Union[Callable[[type[TModel]], bool], list[Callable[[type[TModel]], bool]]]] = None,\n    ) -&gt; None:\n        \"\"\"Discover target models in the specified packages, applying default and user filters.\"\"\"\n        self.all_models = {}\n        self.filtered_models = {}\n        self.dependencies = {}\n\n        # Normalize user_filters to always be a list\n        if user_filters is None:\n            filters = []\n        elif isinstance(user_filters, list):\n            filters = user_filters\n        else:  # It's a single callable\n            filters = [user_filters]\n\n        model_type_name = getattr(self, \"__name__\", \"TargetModel\")  # Get class name for logging\n\n        logger.info(f\"Starting {model_type_name} discovery in packages: {packages}\")\n        for package_name in packages:\n            try:\n                package = importlib.import_module(package_name)\n                logger.debug(f\"Scanning package: {package_name}\")\n\n                for _importer, modname, _ispkg in pkgutil.walk_packages(\n                    path=package.__path__ if hasattr(package, \"__path__\") else None,\n                    prefix=package.__name__ + \".\",\n                    onerror=lambda name: logger.warning(f\"Error accessing module {name}\"),\n                ):\n                    try:\n                        module = importlib.import_module(modname)\n                        for name, obj in inspect.getmembers(module):\n                            # Use the subclass implementation to check if it's the right model type\n                            if self._is_target_model(obj):\n                                model_qualname = f\"{modname}.{name}\"\n                                if model_qualname not in self.all_models:\n                                    self.all_models[model_qualname] = obj\n                                    logger.debug(f\"Discovered potential {model_type_name}: {model_qualname}\")\n\n                                    # Apply filters sequentially using subclass implementation\n                                    is_eligible = self._default_eligibility_filter(obj)\n                                    if is_eligible:\n                                        for user_filter in filters:\n                                            try:\n                                                if not user_filter(obj):\n                                                    is_eligible = False\n                                                    logger.debug(\n                                                        f\"Filtered out {model_type_name} by user filter: {model_qualname}\"\n                                                    )\n                                                    break  # No need to check other filters\n                                            except Exception as filter_exc:\n                                                # Attempt to get filter name, default to repr\n                                                filter_name = getattr(user_filter, \"__name__\", repr(user_filter))\n                                                logger.error(\n                                                    f\"Error applying user filter {filter_name} to {model_qualname}: {filter_exc}\",\n                                                    exc_info=True,\n                                                )\n                                                is_eligible = False  # Exclude on filter error\n                                                break\n\n                                    if is_eligible:\n                                        self.filtered_models[model_qualname] = obj\n                                        logger.debug(f\"Added eligible {model_type_name}: {model_qualname}\")\n\n                    except ImportError as e:\n                        logger.warning(f\"Could not import module {modname}: {e}\")\n                    except Exception as e:\n                        logger.error(f\"Error inspecting module {modname} for {model_type_name}s: {e}\", exc_info=True)\n\n            except ImportError:\n                logger.error(f\"Package {package_name} not found.\")\n            except Exception as e:\n                logger.error(f\"Error discovering {model_type_name}s in package {package_name}: {e}\", exc_info=True)\n\n        logger.info(\n            f\"{model_type_name} discovery complete. Found {len(self.all_models)} total models, {len(self.filtered_models)} after filtering.\"\n        )\n\n        # Hooks for subclass-specific post-processing if needed\n        self._post_discovery_hook()\n\n        # Resolve forward references if applicable (might be subclass specific)\n        self._resolve_forward_refs()\n\n        # Build dependency graph for filtered models\n        self.analyze_dependencies()\n\n    @abc.abstractmethod\n    def analyze_dependencies(self) -&gt; None:\n        \"\"\"Analyze dependencies between the filtered models.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def get_models_in_registration_order(self) -&gt; list[type[TModel]]:\n        \"\"\"Return filtered models sorted topologically based on dependencies.\"\"\"\n        pass\n\n    # Optional hook for subclasses to run code after discovery loop but before analyze\n    def _post_discovery_hook(self) -&gt; None:\n        pass\n\n    # Keep placeholder, subclasses might override if needed\n    def _resolve_forward_refs(self) -&gt; None:\n        \"\"\"Placeholder for resolving forward references if needed.\"\"\"\n        logger.debug(\"Base _resolve_forward_refs called (if applicable).\")\n        pass\n</code></pre> </li> <li> <p>Model/field factories and the conversion carrier</p> </li> <li> <p>               Bases: <code>ABC</code>, <code>Generic[SourceModelType, SourceFieldType]</code></p> <p>Abstract base class for model factories.</p> Source code in <code>src/pydantic2django/core/factories.py</code> <pre><code>class BaseModelFactory(ABC, Generic[SourceModelType, SourceFieldType]):\n    \"\"\"Abstract base class for model factories.\"\"\"\n\n    field_factory: BaseFieldFactory[SourceFieldType]\n\n    def __init__(self, field_factory: BaseFieldFactory[SourceFieldType], *args, **kwargs):\n        \"\"\"\n        Initializes the model factory with a compatible field factory.\n        Allows subclasses to accept additional dependencies.\n        \"\"\"\n        self.field_factory = field_factory\n\n    @abstractmethod\n    def _process_source_fields(self, carrier: ConversionCarrier[SourceModelType]):\n        \"\"\"Abstract method for subclasses to implement field processing.\"\"\"\n        pass\n\n    # Common logic moved from subclasses\n    def _handle_field_collisions(self, carrier: ConversionCarrier[SourceModelType]):\n        \"\"\"Check for field name collisions with the base Django model.\"\"\"\n        base_model = carrier.base_django_model\n        if not base_model or not hasattr(base_model, \"_meta\"):\n            return\n\n        try:\n            base_fields = base_model._meta.get_fields(include_parents=True, include_hidden=False)\n            base_field_names = {f.name for f in base_fields if not f.name.endswith(\"+\")}\n        except Exception as e:\n            logger.warning(f\"Could not get fields from base model {base_model.__name__} for collision check: {e}\")\n            return\n\n        all_new_fields = set(carrier.django_fields.keys()) | set(carrier.relationship_fields.keys())\n        collision_fields = all_new_fields &amp; base_field_names\n\n        if collision_fields:\n            source_name = getattr(carrier.source_model, \"__name__\", \"?\")\n            msg = f\"Field collision detected between {source_name} and base model {base_model.__name__}: {collision_fields}.\"\n            if carrier.strict:\n                logger.error(msg + \" Raising error due to strict=True.\")\n                raise ValueError(msg + \" Use strict=False or rename fields.\")\n            else:\n                logger.warning(msg + \" Removing colliding fields from generated model (strict=False).\")\n                for field_name in collision_fields:\n                    carrier.django_fields.pop(field_name, None)\n                    carrier.relationship_fields.pop(field_name, None)\n\n    def _create_django_meta(self, carrier: ConversionCarrier[SourceModelType]):\n        \"\"\"Create the Meta class for the generated Django model.\"\"\"\n        source_name = getattr(carrier.source_model, \"__name__\", \"UnknownSourceModel\")\n        source_model_name_cleaned = source_name.replace(\"_\", \" \")\n        meta_attrs = {\n            \"app_label\": carrier.meta_app_label,\n            \"db_table\": f\"{carrier.meta_app_label}_{source_name.lower()}\",\n            # Keep dynamic models abstract so Django does not register them in the\n            # global app registry. This avoids conflicts with concrete generated models\n            # imported later (e.g., during makemigrations in tests).\n            \"abstract\": True,\n            \"managed\": True,\n            \"verbose_name\": source_model_name_cleaned,\n            \"verbose_name_plural\": source_model_name_cleaned + \"s\",\n            \"ordering\": [\"pk\"],\n        }\n\n        # Create Meta, not inheriting from base Meta to ensure abstract stays True\n        logger.debug(\"Creating new Meta class for dynamic model (abstract=True)\")\n        carrier.django_meta_class = type(\"Meta\", (), meta_attrs)\n\n    def _assemble_django_model_class(self, carrier: ConversionCarrier[SourceModelType]):\n        \"\"\"Assemble the final Django model class using type().\"\"\"\n        source_name = getattr(carrier.source_model, \"__name__\", \"UnknownSourceModel\")\n        source_module = getattr(carrier.source_model, \"__module__\", None)\n\n        model_attrs: dict[str, Any] = {\n            **carrier.django_fields,\n            **carrier.relationship_fields,\n            # Set __module__ for where the model appears to live\n            \"__module__\": source_module or f\"{carrier.meta_app_label}.models\",\n        }\n        if carrier.django_meta_class:\n            model_attrs[\"Meta\"] = carrier.django_meta_class\n\n        # Add a reference back to the source model (generic attribute name)\n        model_attrs[\"_pydantic2django_source\"] = carrier.source_model\n\n        bases = (carrier.base_django_model,) if carrier.base_django_model else (models.Model,)\n\n        # Even if no fields were generated (e.g., collisions with base removed them),\n        # we still assemble the model class so that later phases (e.g., relationship finalization)\n        # can inject fields and meta indexes onto the carrier.\n        if not carrier.django_fields and not carrier.relationship_fields:\n            logger.info(\n                f\"No Django fields generated for {source_name}, assembling bare model class to allow later injections.\"\n            )\n\n        model_name = f\"{carrier.class_name_prefix}{source_name}\"\n        logger.debug(f\"Assembling model class '{model_name}' with bases {bases} and attrs: {list(model_attrs.keys())}\")\n\n        try:\n            # Use type() to dynamically create the class\n            carrier.django_model = cast(type[models.Model], type(model_name, bases, model_attrs))\n            logger.info(f\"Successfully assembled Django model class: {model_name}\")\n        except Exception as e:\n            logger.error(f\"Failed to assemble Django model class {model_name}: {e}\", exc_info=True)\n            carrier.invalid_fields.append((\"_assembly\", f\"Failed to create model type: {e}\"))\n            carrier.django_model = None\n\n    @abstractmethod\n    def _build_model_context(self, carrier: ConversionCarrier[SourceModelType]):\n        \"\"\"Abstract method for subclasses to build the specific ModelContext.\"\"\"\n        pass\n\n    # Main orchestration method\n    def make_django_model(self, carrier: ConversionCarrier[SourceModelType]) -&gt; None:\n        \"\"\"\n        Orchestrates the Django model creation process.\n        Subclasses implement _process_source_fields and _build_model_context.\n        Handles caching.\n        \"\"\"\n        model_key = carrier.model_key\n        logger.debug(f\"Attempting to create Django model for {model_key}\")\n\n        # TODO: Cache handling needs refinement - how to access subclass cache?\n        # For now, skipping cache check in base class.\n        # if model_key in self._converted_models and not carrier.existing_model:\n        #     # ... update carrier from cache ...\n        #     return\n\n        # Reset results on carrier\n        carrier.django_fields = {}\n        carrier.relationship_fields = {}\n        carrier.context_fields = {}\n        carrier.invalid_fields = []\n        carrier.django_meta_class = None\n        carrier.django_model = None\n        carrier.model_context = None\n        carrier.django_field_definitions = {}  # Reset definitions dict\n\n        # Core Steps\n        self._process_source_fields(carrier)\n        self._handle_field_collisions(carrier)\n        self._create_django_meta(carrier)\n        self._assemble_django_model_class(carrier)\n\n        # Build context only if model assembly succeeded\n        if carrier.django_model:\n            self._build_model_context(carrier)\n</code></pre> </li> <li> <p>               Bases: <code>ABC</code>, <code>Generic[SourceFieldType]</code></p> <p>Abstract base class for field factories.</p> Source code in <code>src/pydantic2django/core/factories.py</code> <pre><code>class BaseFieldFactory(ABC, Generic[SourceFieldType]):\n    \"\"\"Abstract base class for field factories.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        # Allow subclasses to accept necessary dependencies (e.g., relationship accessors)\n        pass\n\n    @abstractmethod\n    def create_field(\n        self, field_info: SourceFieldType, model_name: str, carrier: ConversionCarrier\n    ) -&gt; FieldConversionResult:\n        \"\"\"\n        Convert a source field type into a Django Field.\n\n        Args:\n            field_info: The field information object from the source (Pydantic/Dataclass).\n            model_name: The name of the source model containing the field.\n            carrier: The conversion carrier for context (e.g., app_label, relationships).\n\n        Returns:\n            A FieldConversionResult containing the generated Django field or context/error info.\n        \"\"\"\n        pass\n</code></pre> </li> <li> <p>               Bases: <code>Generic[SourceModelType]</code></p> <p>Carrier class for converting a source model (Pydantic/Dataclass) to a Django model. Holds configuration and accumulates results during the conversion process. Generalized from the original DjangoModelFactoryCarrier.</p> Source code in <code>src/pydantic2django/core/factories.py</code> <pre><code>@dataclass\nclass ConversionCarrier(Generic[SourceModelType]):\n    \"\"\"\n    Carrier class for converting a source model (Pydantic/Dataclass) to a Django model.\n    Holds configuration and accumulates results during the conversion process.\n    Generalized from the original DjangoModelFactoryCarrier.\n    \"\"\"\n\n    source_model: type[SourceModelType]\n    meta_app_label: str\n    base_django_model: type[models.Model]  # Base Django model to inherit from\n    existing_model: Optional[type[models.Model]] = None  # For updating existing models\n    class_name_prefix: str = \"Django\"  # Prefix for generated Django model name\n    strict: bool = False  # Strict mode for field collisions\n    used_related_names_per_target: dict[str, set[str]] = field(default_factory=dict)\n    django_field_definitions: dict[str, str] = field(default_factory=dict)  # Added field defs\n\n    # --- Result fields (populated during conversion) ---\n    django_fields: dict[str, models.Field] = field(default_factory=dict)\n    relationship_fields: dict[str, models.Field] = field(default_factory=dict)\n    context_fields: dict[str, Any] = field(default_factory=dict)  # Store original source field info\n    context_data: dict[str, Any] = field(default_factory=dict)\n    # Stores (original_field_name, union_details_dict) for multi-FK unions\n    pending_multi_fk_unions: list[tuple[str, dict]] = field(default_factory=list)\n    # --- GFK mode support ---\n    enable_gfk: bool = False\n    gfk_policy: str | None = None\n    gfk_threshold_children: int | None = None\n    gfk_value_mode: str | None = None\n    gfk_normalize_common_attrs: bool = False\n    # Mark children that should be represented as GenericEntry rows\n    pending_gfk_children: list[dict[str, Any]] = field(default_factory=list)\n    invalid_fields: list[tuple[str, str]] = field(default_factory=list)\n    django_meta_class: Optional[type] = None\n    django_model: Optional[type[models.Model]] = None  # Changed from DjangoModelType\n    model_context: Optional[ModelContext] = None\n    # Removed import_handler from carrier\n\n    def model_key(self) -&gt; str:\n        \"\"\"Generate a unique key for the source model.\"\"\"\n        module = getattr(self.source_model, \"__module__\", \"?\")\n        name = getattr(self.source_model, \"__name__\", \"UnknownModel\")\n        return f\"{module}.{name}\"\n\n    def __str__(self):\n        source_name = getattr(self.source_model, \"__name__\", \"UnknownSource\")\n        django_name = getattr(self.django_model, \"__name__\", \"None\") if self.django_model else \"None\"\n        return f\"{source_name} -&gt; {django_name}\"\n</code></pre> </li> <li> <p>Data structure holding the result of attempting to convert a single source field.</p> Source code in <code>src/pydantic2django/core/factories.py</code> <pre><code>@dataclass\nclass FieldConversionResult:\n    \"\"\"Data structure holding the result of attempting to convert a single source field.\"\"\"\n\n    field_info: Any  # Original source field info (FieldInfo, dataclasses.Field)\n    field_name: str\n    # type_mapping_definition: Optional[TypeMappingDefinition] = None # Keep mapping info internal?\n    field_kwargs: dict[str, Any] = field(default_factory=dict)\n    django_field: Optional[models.Field] = None\n    context_field: Optional[Any] = None  # Holds original field_info if handled by context\n    error_str: Optional[str] = None\n    field_definition_str: Optional[str] = None  # Added field definition string\n    # Added required_imports dictionary\n    required_imports: dict[str, list[str]] = field(default_factory=dict)\n    # Store the raw kwargs returned by the mapper\n    raw_mapper_kwargs: dict[str, Any] = field(default_factory=dict)\n\n    def add_import(self, module: str, name: str):\n        \"\"\"Helper to add an import to this result.\"\"\"\n        if not module or module == \"builtins\":\n            return\n        current_names = self.required_imports.setdefault(module, [])\n        if name not in current_names:\n            current_names.append(name)\n\n    def add_import_for_obj(self, obj: Any):\n        \"\"\"Helper to add an import for a given object (class, function, etc.).\"\"\"\n        if hasattr(obj, \"__module__\") and hasattr(obj, \"__name__\"):\n            module = obj.__module__\n            name = obj.__name__\n            self.add_import(module, name)\n        else:\n            logger.warning(f\"Could not determine import for object: {obj!r}\")\n\n    def __str__(self):\n        status = \"Success\" if self.django_field else (\"Context\" if self.context_field else f\"Error: {self.error_str}\")\n        field_type = type(self.django_field).__name__ if self.django_field else \"N/A\"\n        return f\"FieldConversionResult(field={self.field_name}, status={status}, django_type={field_type})\"\n</code></pre> </li> <li> <p>Bidirectional type mapping (Python/Pydantic \u2194 Django.Field)</p> </li> <li> <p>Registry and entry point for bidirectional type mapping.</p> Source code in <code>src/pydantic2django/core/bidirectional_mapper.py</code> <pre><code>class BidirectionalTypeMapper:\n    \"\"\"Registry and entry point for bidirectional type mapping.\"\"\"\n\n    def __init__(self, relationship_accessor: Optional[RelationshipConversionAccessor] = None):\n        self.relationship_accessor = relationship_accessor or RelationshipConversionAccessor()\n        self._registry: list[type[TypeMappingUnit]] = self._build_registry()\n        # Caches\n        self._pydantic_cache: dict[Any, Optional[type[TypeMappingUnit]]] = {}\n        self._django_cache: dict[type[models.Field], Optional[type[TypeMappingUnit]]] = {}\n\n    def _build_registry(self) -&gt; list[type[TypeMappingUnit]]:\n        \"\"\"Discover and order TypeMappingUnit subclasses.\"\"\"\n        # Order matters less for selection now, but still useful for tie-breaking?\n        # References mapping units imported from .mapping_units\n        ordered_units = [\n            # Specific PKs first (subclass of IntField)\n            BigAutoFieldMapping,\n            SmallAutoFieldMapping,\n            AutoFieldMapping,\n            # Specific Numerics (subclass of IntField/FloatField/DecimalField)\n            PositiveBigIntFieldMapping,\n            PositiveSmallIntFieldMapping,\n            PositiveIntFieldMapping,\n            # Specific Strings (subclass of CharField/TextField)\n            EmailFieldMapping,\n            URLFieldMapping,\n            SlugFieldMapping,\n            IPAddressFieldMapping,\n            FilePathFieldMapping,  # Needs Path, but Django field is specific\n            # File Fields (map Path/str, Django fields are specific)\n            ImageFieldMapping,  # Subclass of FileField\n            FileFieldMapping,\n            # Other specific types before bases\n            UUIDFieldMapping,\n            JsonFieldMapping,  # Before generic collections/Any might map elsewhere\n            # Base Relationship types (before fields they might inherit from like FK &lt; Field)\n            ManyToManyFieldMapping,\n            OneToOneFieldMapping,\n            ForeignKeyMapping,\n            # General Base Types LAST\n            DecimalFieldMapping,\n            DateTimeFieldMapping,\n            DateFieldMapping,\n            TimeFieldMapping,\n            DurationFieldMapping,\n            BinaryFieldMapping,\n            FloatFieldMapping,\n            BoolFieldMapping,\n            # Str/Text: Order now primarily determined by `matches` score overrides\n            TextFieldMapping,\n            StrFieldMapping,\n            # Specific Int types first\n            BigIntFieldMapping,  # Map int to BigInt before Int\n            SmallIntFieldMapping,\n            IntFieldMapping,\n            # Enum handled dynamically by find method\n            EnumFieldMapping,  # Include EnumFieldMapping here for the loop\n        ]\n        # Remove duplicates just in case\n        seen = set()\n        unique_units = []\n        for unit in ordered_units:\n            if unit not in seen:\n                unique_units.append(unit)\n                seen.add(unit)\n        return unique_units\n\n    def _find_unit_for_pydantic_type(\n        self, py_type: Any, field_info: Optional[FieldInfo] = None\n    ) -&gt; Optional[type[TypeMappingUnit]]:\n        \"\"\"\n        Find the best mapping unit for a given Pydantic type and FieldInfo.\n        Uses a scoring system based on the `matches` classmethod of each unit.\n        Handles Optional unwrapping and caching.\n        \"\"\"\n        original_type_for_cache = py_type  # Use the original type as the cache key\n\n        # --- Unwrap Optional ---\n        origin = get_origin(py_type)\n        if origin is Optional:\n            args = get_args(py_type)\n            # Get the first non-None type argument\n            type_to_match = next((arg for arg in args if arg is not type(None)), Any)\n            logger.debug(f\"Unwrapped Optional[{type_to_match.__name__}] to {type_to_match.__name__}\")\n        # Handle X | None syntax (UnionType)\n        elif origin is UnionType:\n            args = get_args(py_type)\n            non_none_args = [arg for arg in args if arg is not type(None)]\n            if len(non_none_args) == 1:  # If it's just `T | None`\n                type_to_match = non_none_args[0]\n                logger.debug(f\"Unwrapped Union[{py_type}] with None to {type_to_match}\")\n            else:  # Keep the original UnionType if it's Union[A, B, ...]\n                type_to_match = py_type\n                logger.debug(f\"Keeping UnionType {py_type} as is for matching.\")\n        else:\n            type_to_match = py_type  # Use the original type if not Optional or simple T | None\n\n        logger.debug(\n            f\"Final type_to_match for scoring: {type_to_match} (origin: {get_origin(type_to_match)}, args: {get_args(type_to_match)})\"\n        )\n\n        # --- Cache Check ---\n        # Re-enable caching\n        cache_key = (original_type_for_cache, field_info)\n        if cache_key in self._pydantic_cache:\n            # logger.debug(f\"Cache hit for {cache_key}\")\n            return self._pydantic_cache[cache_key]\n        # logger.debug(f\"Cache miss for {cache_key}\")\n\n        # --- Literal Type Check (using original type) --- #\n        original_origin = get_origin(original_type_for_cache)\n        if original_origin is Literal:\n            logger.debug(f\"Type {original_type_for_cache} is Literal. Selecting EnumFieldMapping directly.\")\n            best_unit = EnumFieldMapping\n            self._pydantic_cache[cache_key] = best_unit\n            return best_unit\n\n        # --- Prioritize Collection Types -&gt; JSON --- #\n        # Use the unwrapped origin for this check\n        # unwrapped_origin = get_origin(type_to_match)\n        # if unwrapped_origin in (list, dict, set, tuple):\n        #     logger.debug(f\"Type {type_to_match} is a collection. Selecting JsonFieldMapping directly.\")\n        #     best_unit = JsonFieldMapping\n        #     self._pydantic_cache[cache_key] = best_unit\n        #     return best_unit\n\n        # --- Initialization --- #\n        best_unit: Optional[type[TypeMappingUnit]] = None\n        highest_score = 0.0\n        scores: dict[str, float | str] = {}  # Store scores for debugging\n\n        # --- Relationship Check (Specific Model Types and Lists of Models) BEFORE Scoring --- #\n        # Check if the type_to_match itself is a known model\n        try:\n            is_direct_known_model = (\n                inspect.isclass(type_to_match)\n                and (issubclass(type_to_match, BaseModel) or dataclasses.is_dataclass(type_to_match))\n                and self.relationship_accessor.is_source_model_known(type_to_match)\n            )\n        except TypeError:\n            is_direct_known_model = False\n\n        if is_direct_known_model:\n            logger.debug(\n                f\"Type {type_to_match.__name__} is a known related model. Selecting ForeignKeyMapping directly.\"\n            )\n            best_unit = ForeignKeyMapping\n            self._pydantic_cache[cache_key] = best_unit\n            return best_unit\n\n        # Check if it's a list/set of known models (potential M2M)\n        unwrapped_origin = get_origin(type_to_match)\n        unwrapped_args = get_args(type_to_match)\n        if unwrapped_origin in (list, set) and unwrapped_args:  # Check for list or set\n            inner_type = unwrapped_args[0]\n            try:\n                is_list_of_known_models = (\n                    inspect.isclass(inner_type)\n                    and (issubclass(inner_type, BaseModel) or dataclasses.is_dataclass(inner_type))\n                    and self.relationship_accessor.is_source_model_known(inner_type)\n                )\n            except TypeError:\n                is_list_of_known_models = False\n                logger.error(f\"TypeError checking if {inner_type} is a known model list item.\", exc_info=True)\n\n            logger.debug(\n                f\"Checking list/set: unwrapped_origin={unwrapped_origin}, inner_type={inner_type}, is_list_of_known_models={is_list_of_known_models}\"\n            )\n            if is_list_of_known_models:\n                logger.debug(\n                    f\"Type {type_to_match} is a list/set of known models ({inner_type.__name__}). Selecting ManyToManyFieldMapping directly.\"\n                )\n                best_unit = ManyToManyFieldMapping\n                self._pydantic_cache[cache_key] = best_unit\n                return best_unit\n            else:\n                logger.debug(\n                    f\"Type {type_to_match} is a list/set, but inner type {inner_type} is not a known model. Proceeding.\"\n                )\n\n        # --- Specific Union Handling BEFORE Scoring --- #\n        unwrapped_args = get_args(type_to_match)\n        # Check for non-model Unions (Model unions handled in get_django_mapping signal)\n        if unwrapped_origin in (Union, UnionType) and unwrapped_args:\n            logger.debug(f\"Evaluating specific Union type {type_to_match} args: {unwrapped_args} before scoring.\")\n            has_str = any(arg is str for arg in unwrapped_args)\n            has_collection_or_any = any(\n                get_origin(arg) in (dict, list, set, tuple) or arg is Any\n                for arg in unwrapped_args\n                if arg is not type(None)\n            )\n            # Don't handle Union[ModelA, ModelB] here, that needs the signal mechanism\n            is_model_union = any(\n                inspect.isclass(arg) and (issubclass(arg, BaseModel) or dataclasses.is_dataclass(arg))\n                for arg in unwrapped_args\n                if arg is not type(None)\n            )\n\n            if not is_model_union:\n                if has_str and not has_collection_or_any:\n                    logger.debug(f\"Union {type_to_match} contains str, selecting TextFieldMapping directly.\")\n                    best_unit = TextFieldMapping\n                    self._pydantic_cache[cache_key] = best_unit\n                    return best_unit\n                elif has_collection_or_any:\n                    logger.debug(f\"Union {type_to_match} contains complex types, selecting JsonFieldMapping directly.\")\n                    best_unit = JsonFieldMapping\n                    self._pydantic_cache[cache_key] = best_unit\n                    return best_unit\n                # Else: Union of simple types (e.g., int | float) - let scoring handle it.\n                else:\n                    logger.debug(f\"Union {type_to_match} is non-model, non-str/complex. Proceeding to scoring.\")\n            else:\n                logger.debug(\n                    f\"Union {type_to_match} contains models. Proceeding to scoring (expecting JsonField fallback).\"\n                )\n\n        # --- Scoring Loop (Only if not a known related model or specific Union handled above) --- #\n        # Use type_to_match (unwrapped) for matching\n        # --- EDIT: Removed redundant check `if best_unit is None:` --- #\n        # This loop now runs only if no direct selection happened above.\n        for unit_cls in self._registry:\n            try:  # Add try-except around matches call for robustness\n                # Pass the unwrapped type to matches\n                score = unit_cls.matches(type_to_match, field_info)\n                if score &gt; 0:  # Log all positive scores\n                    scores[unit_cls.__name__] = score  # Store score regardless of whether it's the highest\n                    logger.debug(\n                        f\"Scoring {unit_cls.__name__}.matches({type_to_match}, {field_info=}) -&gt; {score}\"\n                    )  # Added logging\n                if score &gt; highest_score:\n                    highest_score = score\n                    best_unit = unit_cls\n                    # Store the winning score as well - Moved above\n                    # scores[unit_cls.__name__] = score  # Overwrite if it was a lower score before\n                # elif score &gt; 0:  # Log non-winning positive scores too - Moved above\n                # Only add if not already present (first positive score encountered)\n                # scores.setdefault(unit_cls.__name__, score)\n            except Exception as e:\n                logger.error(f\"Error calling {unit_cls.__name__}.matches for {type_to_match}: {e}\", exc_info=True)\n                scores[unit_cls.__name__] = f\"ERROR: {e}\"  # Log error in scores dict\n\n        # Sort scores for clearer logging (highest first)\n        sorted_scores = dict(\n            sorted(scores.items(), key=lambda item: item[1] if isinstance(item[1], (int, float)) else -1, reverse=True)\n        )\n        logger.debug(\n            f\"Scores for {original_type_for_cache} (unwrapped: {type_to_match}, {field_info=}): {sorted_scores}\"\n        )\n        if best_unit:  # Added logging\n            logger.debug(f\"Selected best unit: {best_unit.__name__} with score {highest_score}\")  # Added logging\n        else:  # Added logging\n            logger.debug(\"No best unit found based on scoring.\")  # Added logging\n\n        # --- Handle Fallbacks (Collections/Any) --- #\n        if best_unit is None and highest_score == 0.0:\n            logger.debug(f\"Re-evaluating fallback/handling for {type_to_match}\")\n            unwrapped_origin = get_origin(type_to_match)\n            unwrapped_args = get_args(type_to_match)\n\n            # 1. Check standard collections first (MOVED FROM TOP)\n            if unwrapped_origin in (dict, list, set, tuple) or type_to_match in (dict, list, set, tuple):\n                # Re-check list/set here to ensure it wasn't a list of known models handled above\n                if unwrapped_origin in (list, set) and unwrapped_args:\n                    inner_type = unwrapped_args[0]\n                    try:\n                        is_list_of_known_models_fallback = (\n                            inspect.isclass(inner_type)\n                            and (issubclass(inner_type, BaseModel) or dataclasses.is_dataclass(inner_type))\n                            and self.relationship_accessor.is_source_model_known(inner_type)\n                        )\n                    except TypeError:\n                        is_list_of_known_models_fallback = False\n\n                    if not is_list_of_known_models_fallback:\n                        logger.debug(f\"Type {type_to_match} is a non-model collection, selecting JsonFieldMapping.\")\n                        best_unit = JsonFieldMapping\n                    # else: It was a list of known models, should have been handled earlier. Log warning?\n                    else:\n                        logger.warning(f\"List of known models {type_to_match} reached fallback logic unexpectedly.\")\n                        # Default to M2M as a safe bet?\n                        best_unit = ManyToManyFieldMapping\n                # Handle dict/tuple\n                elif unwrapped_origin in (dict, tuple) or type_to_match in (dict, tuple):\n                    logger.debug(f\"Type {type_to_match} is a dict/tuple collection, selecting JsonFieldMapping.\")\n                    best_unit = JsonFieldMapping\n            # 2. Check for Any\n            elif type_to_match is Any:\n                logger.debug(\"Type is Any, selecting JsonFieldMapping.\")\n                best_unit = JsonFieldMapping\n\n        # Final Logging\n        if best_unit is None:\n            logger.warning(\n                f\"No specific mapping unit found for Python type: {original_type_for_cache} (unwrapped to {type_to_match}) with field_info: {field_info}\"\n            )\n            # Log cache state before potential fallback write\n            logger.debug(f\"Cache keys before fallback write: {list(self._pydantic_cache.keys())}\")\n\n        # Re-enable cache write\n        self._pydantic_cache[cache_key] = best_unit  # Cache using original key\n        return best_unit\n\n    def _find_unit_for_django_field(self, dj_field_type: type[models.Field]) -&gt; Optional[type[TypeMappingUnit]]:\n        \"\"\"Find the most specific mapping unit based on Django field type MRO and registry order.\"\"\"\n        # Revert to simpler single pass using refined registry order.\n        if dj_field_type in self._django_cache:\n            return self._django_cache[dj_field_type]\n\n        # Filter registry to exclude EnumFieldMapping unless it's specifically needed? No, registry order handles it.\n        # Ensure EnumFieldMapping isn't incorrectly picked before Str/Int if choices are present.\n        # The registry order should have Str/Int base mappings *after* EnumFieldMapping if EnumFieldMapping\n        # only maps Enum/Literal python types. But dj_field_type matching is different.\n        # If a CharField has choices, we want EnumFieldMapping logic, not StrFieldMapping.\n        registry_for_django = self._registry  # Use the full registry for now\n\n        for unit_cls in registry_for_django:\n            # Special check: If field has choices, prioritize EnumFieldMapping if applicable type\n            # This is handled by get_pydantic_mapping logic already, not needed here.\n\n            if issubclass(dj_field_type, unit_cls.django_field_type):\n                # Found the first, most specific match based on registry order\n                # Example: PositiveIntegerField is subclass of IntegerField. If PositiveIntFieldMapping\n                # comes first in registry, it will be matched correctly.\n                self._django_cache[dj_field_type] = unit_cls\n                return unit_cls\n\n        # Fallback if no unit explicitly handles it (should be rare)\n        logger.warning(\n            f\"No specific mapping unit found for Django field type: {dj_field_type.__name__}, check registry order.\"\n        )\n        self._django_cache[dj_field_type] = None\n        return None\n\n    def get_django_mapping(\n        self,\n        python_type: Any,\n        field_info: Optional[FieldInfo] = None,\n        parent_pydantic_model: Optional[type[BaseModel]] = None,  # Add parent model for self-ref check\n    ) -&gt; tuple[type[models.Field], dict[str, Any]]:\n        \"\"\"Get the corresponding Django Field type and constructor kwargs for a Python type.\"\"\"\n        processed_type_info = TypeHandler.process_field_type(python_type)\n        original_py_type = python_type\n        is_optional = processed_type_info[\"is_optional\"]\n        is_list = processed_type_info[\"is_list\"]\n\n        unit_cls = None  # Initialize unit_cls\n        base_py_type = original_py_type  # Start with original\n        union_details = None  # Store details if it's a Union[BaseModel,...]\n        gfk_details = None\n\n        # --- Check for M2M case FIRST ---\n        if is_list:\n            # Get the type inside the list, handling Optional[List[T]]\n            list_inner_type = original_py_type\n            if is_optional:\n                args_check = get_args(list_inner_type)\n                list_inner_type = next((arg for arg in args_check if arg is not type(None)), Any)\n\n            # Now get the type *inside* the list\n            list_args = get_args(list_inner_type)  # Should be List[T]\n            inner_type = list_args[0] if list_args else Any\n\n            # --- GFK Check: Is the inner type a Union of known models? ---\n            inner_origin = get_origin(inner_type)\n            inner_args = get_args(inner_type)\n            if inner_origin in (Union, UnionType) and inner_args:\n                union_models = []\n                other_types = [\n                    arg\n                    for arg in inner_args\n                    if not (\n                        inspect.isclass(arg)\n                        and (issubclass(arg, BaseModel) or dataclasses.is_dataclass(arg))\n                        and self.relationship_accessor.is_source_model_known(arg)\n                    )\n                ]\n                union_models = [arg for arg in inner_args if arg not in other_types]\n\n                if union_models and not other_types:\n                    logger.debug(f\"Detected GFK List[Union[...]] with models: {union_models}\")\n                    gfk_details = {\n                        \"type\": \"gfk\",\n                        \"models\": union_models,\n                        \"is_optional\": is_optional,\n                    }\n                    unit_cls = JsonFieldMapping\n                    base_py_type = original_py_type\n\n            if unit_cls is None:\n                # --- M2M Check: Is the inner type a known related BaseModel OR Dataclass? ---\n                if (\n                    inspect.isclass(inner_type)\n                    and (issubclass(inner_type, BaseModel) or dataclasses.is_dataclass(inner_type))\n                    and self.relationship_accessor.is_source_model_known(inner_type)\n                ):\n                    unit_cls = ManyToManyFieldMapping\n                    base_py_type = inner_type\n                    logger.debug(f\"Detected List[RelatedModel] ({inner_type.__name__}), mapping to ManyToManyField.\")\n                else:\n                    # --- Fallback for other lists ---\n                    unit_cls = JsonFieldMapping\n                    base_py_type = original_py_type\n                    logger.debug(f\"Detected List of non-models ({original_py_type}), mapping directly to JSONField.\")\n\n        # --- If not a list, find unit for the base (non-list) type ---\n        if unit_cls is None:\n            # --- Handle Union[BaseModel,...] Signaling FIRST --- #\n            simplified_base_type = processed_type_info[\"type_obj\"]\n            simplified_origin = get_origin(simplified_base_type)\n            simplified_args = get_args(simplified_base_type)\n\n            logger.debug(\n                f\"Checking simplified type for Union[Model,...]: {simplified_base_type!r} (Origin: {simplified_origin})\"\n            )\n            # Log the is_optional flag determined by TypeHandler\n            logger.debug(f\"TypeHandler returned is_optional: {is_optional} for original type: {original_py_type!r}\")\n\n            # Check if the simplified origin is Union[...] or T | U\n            if simplified_origin in (Union, UnionType) and simplified_args:\n                union_models = []\n                other_types_in_union = []\n\n                for arg in simplified_args:\n                    # We already unwrapped Optional, so no need to check for NoneType here\n                    logger.debug(f\"-- Checking simplified Union arg: {arg!r}\")\n\n                    # Check if arg is a known BaseModel or Dataclass\n                    is_class = inspect.isclass(arg)\n                    # Need try-except for issubclass with non-class types\n                    is_pyd_model = False\n                    is_dc = False\n                    is_known_by_accessor = False\n                    if is_class:\n                        try:\n                            is_pyd_model = issubclass(arg, BaseModel)\n                            is_dc = dataclasses.is_dataclass(arg)\n                            # Only check accessor if it's a model type\n                            if is_pyd_model or is_dc:\n                                is_known_by_accessor = self.relationship_accessor.is_source_model_known(arg)\n                        except TypeError:\n                            # issubclass might fail if arg is not a class (e.g., a type alias)\n                            pass  # Keep flags as False\n\n                    logger.debug(\n                        f\"    is_class: {is_class}, is_pyd_model: {is_pyd_model}, is_dc: {is_dc}, is_known_by_accessor: {is_known_by_accessor}\"\n                    )\n\n                    is_known_model_or_dc = is_class and (is_pyd_model or is_dc) and is_known_by_accessor\n\n                    if is_known_model_or_dc:\n                        logger.debug(f\"    -&gt; Added {arg.__name__} to union_models\")  # More specific logging\n                        union_models.append(arg)\n                    else:\n                        # Make sure we don't add NoneType here if Optional wasn't fully handled upstream somehow\n                        if arg is not type(None):\n                            logger.debug(f\"    -&gt; Added {arg!r} to other_types_in_union\")  # More specific logging\n                            other_types_in_union.append(arg)\n\n                # --- EDIT: Only set union_details IF ONLY models were found ---\n                # Add logging just before the check\n                logger.debug(\n                    f\"Finished Union arg loop. union_models: {[m.__name__ for m in union_models]}, other_types: {other_types_in_union}\"\n                )\n                if union_models and not other_types_in_union:\n                    logger.debug(\n                        f\"Detected Union containing ONLY known models: {union_models}. Generating _union_details signal.\"\n                    )\n                    union_details = {\n                        \"type\": \"multi_fk\",\n                        \"models\": union_models,\n                        \"is_optional\": is_optional,  # Use the flag determined earlier\n                    }\n                    # Log the created union_details\n                    logger.debug(f\"Generated union_details: {union_details!r}\")\n                    # Set unit_cls to JsonFieldMapping for model unions\n                    unit_cls = JsonFieldMapping\n                    base_py_type = original_py_type\n                    logger.debug(\"Setting unit_cls to JsonFieldMapping for model union\")\n\n            # --- Now, find the unit for the (potentially complex) base type --- #\n            # Only find unit if not already set (e.g. by model union handling)\n            if unit_cls is None:\n                # Determine the type to use for finding the unit.\n                # If it was M2M or handled List, unit_cls is already set.\n                # Otherwise, use the processed type_obj which handles Optional/Annotated.\n                type_for_unit_finding = processed_type_info[\"type_obj\"]\n                logger.debug(f\"Type used for finding unit (after Union check): {type_for_unit_finding!r}\")\n\n                # Use the simplified base type after processing Optional/Annotated\n                base_py_type = type_for_unit_finding\n                logger.debug(f\"Finding unit for base type: {base_py_type!r} with field_info: {field_info}\")\n                unit_cls = self._find_unit_for_pydantic_type(base_py_type, field_info)\n\n        # --- Check if a unit was found --- #\n        if not unit_cls:\n            # If _find_unit_for_pydantic_type returned None, fallback to JSON\n            logger.warning(\n                f\"No mapping unit found by scoring for base type {base_py_type} \"\n                f\"(derived from {original_py_type}), falling back to JSONField.\"\n            )\n            unit_cls = JsonFieldMapping\n            # Consider raising MappingError if even JSON doesn't fit?\n            # raise MappingError(f\"Could not find mapping unit for Python type: {base_py_type}\")\n\n        # &gt;&gt; Add logging to check selected unit &lt;&lt;\n        logger.info(f\"Selected Unit for {original_py_type}: {unit_cls.__name__ if unit_cls else 'None'}\")\n\n        instance_unit = unit_cls()  # Instantiate to call methods\n\n        # --- Determine Django Field Type ---\n        # Start with the type defined on the selected unit class\n        django_field_type = instance_unit.django_field_type\n\n        # --- Get Kwargs (before potentially overriding field type for Enums) ---\n        kwargs = instance_unit.pydantic_to_django_kwargs(base_py_type, field_info)\n\n        # --- Add Union or GFK Details if applicable --- #\n        if union_details:\n            logger.info(\"Adding _union_details to kwargs.\")\n            kwargs[\"_union_details\"] = union_details\n            kwargs[\"null\"] = union_details.get(\"is_optional\", False)\n            kwargs[\"blank\"] = union_details.get(\"is_optional\", False)\n        elif gfk_details:\n            logger.info(\"Adding _gfk_details to kwargs.\")\n            kwargs[\"_gfk_details\"] = gfk_details\n            # GFK fields are placeholder JSONFields, nullability is based on Optional status\n            kwargs[\"null\"] = is_optional\n            kwargs[\"blank\"] = is_optional\n        else:\n            logger.debug(\"union_details and gfk_details are None, skipping addition to kwargs.\")\n            # --- Special Handling for Enums/Literals (Only if not multi-FK/GFK union) --- #\n            if unit_cls is EnumFieldMapping:\n                field_type_hint = kwargs.pop(\"_field_type_hint\", None)\n                if field_type_hint and isinstance(field_type_hint, type) and issubclass(field_type_hint, models.Field):\n                    # Directly use the hinted field type if valid\n                    logger.debug(\n                        f\"Using hinted field type {field_type_hint.__name__} from EnumFieldMapping for {base_py_type}.\"\n                    )\n                    django_field_type = field_type_hint\n                    # Ensure max_length is removed if type becomes IntegerField\n                    if django_field_type == models.IntegerField:\n                        kwargs.pop(\"max_length\", None)\n                else:\n                    logger.warning(\"EnumFieldMapping selected but failed to get valid field type hint from kwargs.\")\n\n            # --- Handle Relationships (Only if not multi-FK union) --- #\n            # This section needs to run *after* unit selection but *before* final nullability checks\n            if unit_cls in (ForeignKeyMapping, OneToOneFieldMapping, ManyToManyFieldMapping):\n                # Ensure base_py_type is the related model (set during M2M check or found by find_unit for FK/O2O)\n                related_py_model = base_py_type\n\n                # Check if it's a known Pydantic BaseModel OR a known Dataclass\n                is_pyd_or_dc = inspect.isclass(related_py_model) and (\n                    issubclass(related_py_model, BaseModel) or dataclasses.is_dataclass(related_py_model)\n                )\n                if not is_pyd_or_dc:\n                    raise MappingError(\n                        f\"Relationship mapping unit {unit_cls.__name__} selected, but base type {related_py_model} is not a known Pydantic model or Dataclass.\"\n                    )\n\n                # Check for self-reference BEFORE trying to get the Django model\n                is_self_ref = parent_pydantic_model is not None and related_py_model == parent_pydantic_model\n\n                if is_self_ref:\n                    model_ref = \"self\"\n                    # Get the target Django model name for logging/consistency if possible, but use 'self'\n                    # Check if the related model is a Pydantic BaseModel or a dataclass\n                    if inspect.isclass(related_py_model) and issubclass(related_py_model, BaseModel):\n                        target_django_model = self.relationship_accessor.get_django_model_for_pydantic(\n                            cast(type[BaseModel], related_py_model)\n                        )\n                    elif dataclasses.is_dataclass(related_py_model):\n                        target_django_model = self.relationship_accessor.get_django_model_for_dataclass(\n                            related_py_model\n                        )\n                    else:\n                        # This case should ideally not be reached due to earlier checks, but handle defensively\n                        target_django_model = None\n                        logger.warning(\n                            f\"Self-reference check: related_py_model '{related_py_model}' is neither BaseModel nor dataclass.\"\n                        )\n\n                    logger.debug(\n                        f\"Detected self-reference for {related_py_model.__name__ if inspect.isclass(related_py_model) else related_py_model} \"\n                        f\"(Django: {getattr(target_django_model, '__name__', 'N/A')}), using 'self'.\"\n                    )\n                else:\n                    # Get target Django model based on source type (Pydantic or Dataclass)\n                    target_django_model = None\n                    # Ensure related_py_model is actually a type before issubclass check\n                    if inspect.isclass(related_py_model) and issubclass(related_py_model, BaseModel):\n                        # Cast to satisfy type checker, as we've confirmed it's a BaseModel subclass here\n                        target_django_model = self.relationship_accessor.get_django_model_for_pydantic(\n                            cast(type[BaseModel], related_py_model)\n                        )\n                    elif dataclasses.is_dataclass(related_py_model):\n                        target_django_model = self.relationship_accessor.get_django_model_for_dataclass(\n                            related_py_model\n                        )\n\n                    if not target_django_model:\n                        raise MappingError(\n                            f\"Cannot map relationship: No corresponding Django model found for source model \"\n                            f\"{related_py_model.__name__} in RelationshipConversionAccessor.\"\n                        )\n                    # Use lowercase label for internal consistency with existing expectations\n                    model_ref = getattr(target_django_model._meta, \"label_lower\", target_django_model.__name__)\n\n                kwargs[\"to\"] = model_ref\n                django_field_type = unit_cls.django_field_type  # Re-confirm M2MField, FK, O2O type\n                # Set on_delete for FK/O2O based on Optional status\n                if unit_cls in (ForeignKeyMapping, OneToOneFieldMapping):\n                    # Default to CASCADE for non-optional, SET_NULL for optional (matching test expectation)\n                    kwargs[\"on_delete\"] = (\n                        models.SET_NULL if is_optional else models.CASCADE\n                    )  # Changed PROTECT to CASCADE\n\n        # --- Final Adjustments (Nullability, etc.) --- #\n        # Apply nullability. M2M fields cannot be null in Django.\n        # Do not override nullability if it was already forced by a multi-FK union\n        if django_field_type != models.ManyToManyField and not union_details:\n            kwargs[\"null\"] = is_optional\n            # Explicitly set blank based on optionality.\n            # Simplified logic: Mirror the null assignment directly\n            kwargs[\"blank\"] = is_optional\n\n        logger.debug(\n            f\"FINAL RETURN from get_django_mapping: Type={django_field_type}, Kwargs={kwargs}\"\n        )  # Added final state logging\n        return django_field_type, kwargs\n\n    def get_pydantic_mapping(self, dj_field: models.Field) -&gt; tuple[Any, dict[str, Any]]:\n        \"\"\"Get the corresponding Pydantic type hint and FieldInfo kwargs for a Django Field.\"\"\"\n        dj_field_type = type(dj_field)\n        is_optional = dj_field.null\n        is_choices = bool(dj_field.choices)\n\n        # --- Find base unit (ignoring choices for now) ---\n        # Find the mapping unit based on the specific Django field type MRO\n        # This gives us the correct underlying Python type (str, int, etc.)\n        base_unit_cls = self._find_unit_for_django_field(dj_field_type)\n\n        if not base_unit_cls:\n            logger.warning(f\"No base mapping unit for {dj_field_type.__name__}, falling back to Any.\")\n            pydantic_type = Optional[Any] if is_optional else Any\n            return pydantic_type, {}\n\n        base_instance_unit = base_unit_cls()\n        # Get the base Pydantic type from this unit\n        final_pydantic_type = base_instance_unit.python_type\n\n        # --- Determine Final Pydantic Type Adjustments --- #\n        # (Relationships, AutoPK, Optional wrapper)\n\n        # Handle choices FIRST to determine the core type before Optional wrapping\n        if is_choices:\n            # Default to base type, override if valid choices found\n            final_pydantic_type = base_instance_unit.python_type\n            if dj_field.choices:  # Explicit check before iteration\n                try:\n                    choice_values = tuple(choice[0] for choice in dj_field.choices)\n                    if choice_values:  # Ensure the tuple is not empty\n                        final_pydantic_type = Literal[choice_values]  # type: ignore\n                        logger.debug(f\"Mapped choices for '{dj_field.name}' to Pydantic type: {final_pydantic_type}\")\n                    else:\n                        logger.warning(\n                            f\"Field '{dj_field.name}' has choices defined, but extracted values are empty. Falling back.\"\n                        )\n                        # Keep final_pydantic_type as base type\n                except Exception as e:\n                    logger.warning(f\"Failed to extract choices for field '{dj_field.name}'. Error: {e}. Falling back.\")\n                    # Keep final_pydantic_type as base type\n            # If dj_field.choices was None/empty initially, final_pydantic_type remains the base type\n        else:\n            # Get the base Pydantic type from this unit if not choices\n            final_pydantic_type = base_instance_unit.python_type\n\n        # 1. Handle Relationships first, as they determine the core type\n        if base_unit_cls in (ForeignKeyMapping, OneToOneFieldMapping, ManyToManyFieldMapping):\n            related_dj_model = getattr(dj_field, \"related_model\", None)\n            if not related_dj_model:\n                raise MappingError(f\"Cannot determine related Django model for field '{dj_field.name}'\")\n\n            # Resolve 'self' reference\n            if related_dj_model == \"self\":\n                # We need the Django model class that dj_field belongs to.\n                # This info isn't directly passed, so this approach might be limited.\n                # Assuming self-reference points to the same type hierarchy for now.\n                # A better solution might need the model context passed down.\n                logger.warning(\n                    f\"Handling 'self' reference for field '{dj_field.name}'. Mapping might be incomplete without parent model context.\"\n                )\n                # Attempt to get Pydantic model mapped to the field's owner model if possible (heuristically)\n                # This is complex and potentially fragile.\n                # For now, let's use a placeholder or raise an error if needed strictly.\n                # Sticking with the base type (e.g., Any or int for PK) might be safer without context.\n                # Use the base type (likely PK int/uuid) as the fallback type here\n                target_pydantic_model = base_instance_unit.python_type\n                logger.debug(f\"Using Any as placeholder for 'self' reference '{dj_field.name}'\")\n            else:\n                target_pydantic_model = self.relationship_accessor.get_pydantic_model_for_django(related_dj_model)\n\n            if not target_pydantic_model or target_pydantic_model is Any:\n                if related_dj_model != \"self\":  # Avoid redundant warning for self\n                    logger.warning(\n                        f\"Cannot map relationship: No corresponding Pydantic model found for Django model \"\n                        f\"'{related_dj_model._meta.label if hasattr(related_dj_model, '_meta') else related_dj_model.__name__}'. \"\n                        f\"Using placeholder '{final_pydantic_type}'.\"\n                    )\n                # Keep final_pydantic_type as the base unit's python_type (e.g., int for FK)\n            else:\n                if base_unit_cls == ManyToManyFieldMapping:\n                    final_pydantic_type = list[target_pydantic_model]\n                else:  # FK or O2O\n                    # Keep the PK type (e.g., int) if target model not found,\n                    # otherwise use the target Pydantic model type.\n                    final_pydantic_type = target_pydantic_model  # This should now be the related model type\n                logger.debug(f\"Mapped relationship field '{dj_field.name}' to Pydantic type: {final_pydantic_type}\")\n\n        # 2. AutoPK override (after relationship resolution)\n        is_auto_pk = dj_field.primary_key and isinstance(\n            dj_field, (models.AutoField, models.BigAutoField, models.SmallAutoField)\n        )\n        if is_auto_pk:\n            final_pydantic_type = Optional[int]\n            logger.debug(f\"Mapped AutoPK field '{dj_field.name}' to {final_pydantic_type}\")\n            is_optional = True  # AutoPKs are always optional in Pydantic input\n\n        # 3. Apply Optional[...] wrapper if necessary (AFTER relationship/AutoPK)\n        # Do not wrap M2M lists or already Optional AutoPKs in Optional[] again.\n        # Also, don't wrap if the type is already Literal (choices handled Optionality) - NO, wrap Literal too if null=True\n        if is_optional and not is_auto_pk:  # Check if is_choices? No, optional applies to literal too.\n            origin = get_origin(final_pydantic_type)\n            args = get_args(final_pydantic_type)\n            is_already_optional = origin is Optional or origin is UnionType and type(None) in args\n\n            if not is_already_optional:\n                final_pydantic_type = Optional[final_pydantic_type]\n                logger.debug(f\"Wrapped type for '{dj_field.name}' in Optional: {final_pydantic_type}\")\n\n        # --- Generate FieldInfo Kwargs --- #\n        # Use EnumFieldMapping logic for kwargs ONLY if choices exist,\n        # otherwise use the base unit determined earlier. # --&gt; NO, always use base unit for kwargs now. Literal type handles choices.\n        # kwargs_unit_cls = EnumFieldMapping if is_choices else base_unit_cls # OLD logic\n        instance_unit = base_unit_cls()  # Use the base unit (e.g., StrFieldMapping) for base kwargs\n\n        field_info_kwargs = instance_unit.django_to_pydantic_field_info_kwargs(dj_field)\n\n        # --- Explicitly cast title (verbose_name) and description (help_text) --- #\n        if field_info_kwargs.get(\"title\") is not None:\n            field_info_kwargs[\"title\"] = str(field_info_kwargs[\"title\"])\n            logger.debug(f\"Ensured title is str for '{dj_field.name}': {field_info_kwargs['title']}\")\n        if field_info_kwargs.get(\"description\") is not None:\n            field_info_kwargs[\"description\"] = str(field_info_kwargs[\"description\"])\n            logger.debug(f\"Ensured description is str for '{dj_field.name}': {field_info_kwargs['description']}\")\n        # --- End Casting --- #\n\n        # --- Keep choices in json_schema_extra even when using Literal ---\n        # This preserves the (value, label) mapping as metadata alongside the Literal type.\n        if (\n            is_choices\n            and \"json_schema_extra\" in field_info_kwargs\n            and \"choices\" in field_info_kwargs[\"json_schema_extra\"]\n        ):\n            logger.debug(f\"Kept choices in json_schema_extra for Literal field '{dj_field.name}'\")\n        elif is_choices:\n            logger.debug(\n                f\"Field '{dj_field.name}' has choices, but they weren't added to json_schema_extra by the mapping unit.\"\n            )\n\n        # Clean up redundant `default=None` for Optional fields handled by Pydantic v2.\n        # Do not force-add default=None; only keep explicit defaults (e.g., for AutoPK).\n        if is_optional:\n            if field_info_kwargs.get(\"default\") is None and not is_auto_pk:\n                # Remove implicit default=None for Optional fields\n                field_info_kwargs.pop(\"default\", None)\n                logger.debug(f\"Removed redundant default=None for Optional field '{dj_field.name}'\")\n            elif is_auto_pk and \"default\" not in field_info_kwargs:\n                # Keep default=None for AutoPK if not already set\n                field_info_kwargs[\"default\"] = None\n                logger.debug(f\"Set default=None for AutoPK Optional field '{dj_field.name}'\")\n\n        logger.debug(\n            f\"Final Pydantic mapping for '{dj_field.name}': Type={final_pydantic_type}, Kwargs={field_info_kwargs}\"\n        )\n        return final_pydantic_type, field_info_kwargs\n</code></pre> </li> <li> <p>Base class defining a bidirectional mapping between a Python type and a Django Field.</p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>class TypeMappingUnit:\n    \"\"\"Base class defining a bidirectional mapping between a Python type and a Django Field.\"\"\"\n\n    python_type: type[T_PydanticType]\n    django_field_type: type[models.Field]  # Use base class here\n\n    def __init_subclass__(cls, **kwargs):\n        \"\"\"Ensure subclasses define the required types.\"\"\"\n        super().__init_subclass__(**kwargs)\n        if not hasattr(cls, \"python_type\") or not hasattr(cls, \"django_field_type\"):\n            raise NotImplementedError(\n                \"Subclasses of TypeMappingUnit must define 'python_type' and 'django_field_type' class attributes.\"\n            )\n\n    @classmethod\n    def matches(cls, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; float:\n        \"\"\"\n        Calculate a score indicating how well this unit matches the given Python type and FieldInfo.\n\n        Args:\n            py_type: The Python type to match against.\n            field_info: Optional Pydantic FieldInfo for context.\n\n        Returns:\n            A float score (0.0 = no match, higher = better match).\n            Base implementation scores:\n            - 1.0 for exact type match (cls.python_type == py_type)\n            - 0.5 for subclass match (issubclass(py_type, cls.python_type))\n            - 0.0 otherwise\n        \"\"\"\n        target_py_type = cls.python_type\n        if py_type == target_py_type:\n            return 1.0\n        try:\n            # Check issubclass only if both are actual classes and py_type is not Any\n            if (\n                py_type is not Any\n                and inspect.isclass(py_type)\n                and inspect.isclass(target_py_type)\n                and issubclass(py_type, target_py_type)\n            ):\n                # Don't match if it's the same type (already handled by exact match)\n                if py_type is not target_py_type:\n                    return 0.5\n        except TypeError:\n            # issubclass fails on non-classes (like Any, List[int], etc.)\n            pass\n        return 0.0\n\n    def pydantic_to_django_kwargs(self, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; dict[str, Any]:\n        \"\"\"Generate Django field constructor kwargs from Pydantic FieldInfo.\"\"\"\n        kwargs = {}\n        if field_info:\n            # Map common attributes\n            if field_info.title:\n                kwargs[\"verbose_name\"] = field_info.title\n            if field_info.description:\n                kwargs[\"help_text\"] = field_info.description\n\n            # Only consider `default` if `default_factory` is None\n            if field_info.default_factory is None:\n                if field_info.default is not PydanticUndefined and field_info.default is not None:\n                    # Django doesn't handle callable defaults easily here\n                    if not callable(field_info.default):\n                        kwargs[\"default\"] = field_info.default\n                elif field_info.default is None:  # Explicitly check for None default\n                    kwargs[\"default\"] = None  # Add default=None if present in FieldInfo\n            # else: If default_factory is present, do not add a 'default' kwarg.\n            # No warning needed as this is now expected behavior.\n\n            # Note: Frozen, ge, le etc. are validation rules, map separately if needed\n        return kwargs\n\n    def django_to_pydantic_field_info_kwargs(self, dj_field: models.Field) -&gt; dict[str, Any]:\n        \"\"\"Generate Pydantic FieldInfo kwargs from a Django field instance.\"\"\"\n        kwargs = {}\n        field_name = getattr(dj_field, \"name\", \"unknown_field\")  # Get field name for logging\n\n        # Title: Use verbose_name or generate from field name\n        verbose_name = getattr(dj_field, \"verbose_name\", None)\n        logger.debug(f\"Processing field '{field_name}': verbose_name='{verbose_name}'\")\n        if verbose_name:\n            # Ensure verbose_name is a string, handling lazy proxies\n            kwargs[\"title\"] = force_str(verbose_name).capitalize()\n        elif field_name != \"unknown_field\" and isinstance(field_name, str):\n            # Generate title from name if verbose_name is missing and name is a string\n            generated_title = field_name.replace(\"_\", \" \").capitalize()\n            kwargs[\"title\"] = generated_title\n            logger.debug(f\"Generated title for '{field_name}': '{generated_title}'\")\n        # else: field name is None or 'unknown_field', no title generated by default\n\n        # Description\n        if dj_field.help_text:\n            # Ensure help_text is a string, handling lazy proxies\n            kwargs[\"description\"] = force_str(dj_field.help_text)\n\n        # Default value/factory handling\n        if dj_field.has_default():\n            dj_default = dj_field.get_default()\n            if dj_default is not models.fields.NOT_PROVIDED:\n                if callable(dj_default):\n                    factory_set = False\n                    if dj_default is dict:\n                        kwargs[\"default_factory\"] = dict\n                        factory_set = True\n                    elif dj_default is list:\n                        kwargs[\"default_factory\"] = list\n                        factory_set = True\n                    # Add other known callable mappings if needed\n                    else:\n                        logger.debug(\n                            f\"Django field '{dj_field.name}' has an unmapped callable default ({dj_default}), \"\n                            \"not mapping to Pydantic default/default_factory.\"\n                        )\n                    if factory_set:\n                        kwargs.pop(\"default\", None)\n                # Handle non-callable defaults\n                # Map default={} back to default_factory=dict for JSONField\n                elif dj_default == {}:\n                    kwargs[\"default_factory\"] = dict\n                    kwargs.pop(\"default\", None)\n                elif dj_default == []:\n                    kwargs[\"default_factory\"] = list\n                    kwargs.pop(\"default\", None)\n                elif dj_default is not None:\n                    # Add non-None, non-callable, non-empty-collection defaults\n                    logger.debug(\n                        f\"Processing non-callable default for '{field_name}'. Type: {type(dj_default)}, Value: {dj_default!r}\"\n                    )\n                    # Apply force_str ONLY if the default value's type suggests it might be a lazy proxy string.\n                    # A simple check is if 'proxy' is in the type name.\n                    processed_default = dj_default\n                    if \"proxy\" in type(dj_default).__name__:\n                        try:\n                            processed_default = force_str(dj_default)\n                            logger.debug(\n                                f\"Applied force_str to potential lazy default for '{field_name}'. New value: {processed_default!r}\"\n                            )\n                        except Exception as e:\n                            logger.error(\n                                f\"Failed to apply force_str to default value for '{field_name}': {e}. Assigning raw default.\"\n                            )\n                            processed_default = dj_default  # Keep original on error\n\n                    kwargs[\"default\"] = processed_default\n                    logger.debug(f\"Assigned final default for '{field_name}': {kwargs.get('default')!r}\")\n\n        # Handle AutoField PKs -&gt; frozen=True, default=None\n        is_auto_pk = dj_field.primary_key and isinstance(\n            dj_field, (models.AutoField, models.BigAutoField, models.SmallAutoField)\n        )\n        if is_auto_pk:\n            kwargs[\"frozen\"] = True\n            kwargs[\"default\"] = None\n\n        # Handle choices (including processing labels and limiting)\n        # Log choices *before* calling handle_choices\n        if hasattr(dj_field, \"choices\") and dj_field.choices:\n            try:\n                # Log the raw choices from the Django field\n                raw_choices_repr = repr(list(dj_field.choices))  # Materialize and get repr\n                logger.debug(f\"Field '{field_name}': Raw choices before handle_choices: {raw_choices_repr}\")\n            except Exception as log_err:\n                logger.warning(f\"Field '{field_name}': Error logging raw choices: {log_err}\")\n\n            self.handle_choices(dj_field, kwargs)\n            # Log choices *after* handle_choices modified kwargs\n            processed_choices_repr = repr(kwargs.get(\"json_schema_extra\", {}).get(\"choices\"))\n            logger.debug(f\"Field '{field_name}': Choices in kwargs after handle_choices: {processed_choices_repr}\")\n\n        # Handle non-choice max_length only if choices were NOT processed\n        elif dj_field.max_length is not None:\n            # Only add max_length if not choices - specific units can override\n            kwargs[\"max_length\"] = dj_field.max_length\n\n        logger.debug(f\"Base kwargs generated for '{field_name}': {kwargs}\")\n        return kwargs\n\n    def handle_choices(self, dj_field: models.Field, kwargs: dict[str, Any]) -&gt; None:\n        \"\"\"\n        Handles Django field choices, ensuring lazy translation proxies are resolved.\n\n        It processes the choices, forces string conversion on labels within an\n        active translation context, limits the number of choices added to the schema,\n        and stores them in `json_schema_extra`.\n        \"\"\"\n        field_name = getattr(dj_field, \"name\", \"unknown_field\")\n        processed_choices = []\n        MAX_CHOICES_IN_SCHEMA = 30  # TODO: Make configurable\n        limited_choices = []\n        default_value = kwargs.get(\"default\")  # Use potentially processed default\n        default_included = False\n\n        # --- Ensure Translation Context --- #\n        active_translation = None\n        if translation:\n            try:\n                # Get the currently active language to restore later\n                current_language = translation.get_language()\n                # Activate the default language (or a specific one like 'en')\n                # This forces lazy objects to resolve using a consistent language.\n                # Using settings.LANGUAGE_CODE assumes it's set correctly.\n                default_language = getattr(settings, \"LANGUAGE_CODE\", \"en\")  # Fallback to 'en'\n                active_translation = translation.override(default_language)\n                logger.debug(\n                    f\"Activated translation override ('{default_language}') for processing choices of '{field_name}'\"\n                )\n                active_translation.__enter__()  # Manually enter context\n            except Exception as trans_err:\n                logger.warning(f\"Failed to activate translation context for '{field_name}': {trans_err}\")\n                active_translation = None  # Ensure it's None if activation failed\n        else:\n            logger.warning(\"Django translation module not available. Lazy choices might not resolve correctly.\")\n\n        # --- Process Choices (within potential translation context) --- #\n        try:\n            all_choices = list(dj_field.choices or [])\n            for value, label in all_choices:\n                logger.debug(\n                    f\"  Processing choice for '{field_name}': Value={value!r}, Label={label!r} (Type: {type(label)})\"\n                )\n                try:\n                    # Apply force_str defensively; should resolve lazy proxies if context is active\n                    processed_label = force_str(label)\n                    logger.debug(\n                        f\"  Processed label for '{field_name}': Value={value!r}, Label={processed_label!r} (Type: {type(processed_label)})\"\n                    )\n                except Exception as force_str_err:\n                    logger.error(\n                        f\"Error using force_str on label for '{field_name}' (value: {value!r}): {force_str_err}\"\n                    )\n                    # Fallback: use repr or a placeholder if force_str fails completely\n                    processed_label = f\"&lt;unresolved: {repr(label)}&gt;\"\n                processed_choices.append((value, processed_label))\n\n            # --- Limit Choices --- #\n            if len(processed_choices) &gt; MAX_CHOICES_IN_SCHEMA:\n                logger.warning(\n                    f\"Limiting choices for '{field_name}' from {len(processed_choices)} to {MAX_CHOICES_IN_SCHEMA}\"\n                )\n                if default_value is not None:\n                    for val, lbl in processed_choices:\n                        if val == default_value:\n                            limited_choices.append((val, lbl))\n                            default_included = True\n                            break\n                remaining_slots = MAX_CHOICES_IN_SCHEMA - len(limited_choices)\n                if remaining_slots &gt; 0:\n                    for val, lbl in processed_choices:\n                        if len(limited_choices) &gt;= MAX_CHOICES_IN_SCHEMA:\n                            break\n                        if not (default_included and val == default_value):\n                            limited_choices.append((val, lbl))\n                final_choices_list = limited_choices\n            else:\n                final_choices_list = processed_choices\n\n            # --- Store Choices --- #\n            kwargs.setdefault(\"json_schema_extra\", {})[\"choices\"] = final_choices_list\n            kwargs.pop(\"max_length\", None)  # Remove max_length if choices are present\n            logger.debug(f\"Stored final choices in json_schema_extra for '{field_name}'\")\n\n        except Exception as e:\n            logger.error(f\"Error processing or limiting choices for field '{field_name}': {e}\", exc_info=True)\n            kwargs.pop(\"json_schema_extra\", None)\n\n        finally:\n            # --- Deactivate Translation Context --- #\n            if active_translation:\n                try:\n                    active_translation.__exit__(None, None, None)  # Manually exit context\n                    logger.debug(f\"Deactivated translation override for '{field_name}'\")\n                except Exception as trans_exit_err:\n                    logger.warning(f\"Error deactivating translation context for '{field_name}': {trans_exit_err}\")\n</code></pre> </li> <li> <p>Relationships and cross-model resolution</p> </li> <li> Source code in <code>src/pydantic2django/core/relationships.py</code> <pre><code>@dataclass\nclass RelationshipConversionAccessor:\n    available_relationships: list[RelationshipMapper] = field(default_factory=list)\n    # dependencies: Optional[dict[str, set[str]]] = field(default=None) # Keep if used\n\n    @classmethod\n    def from_dict(cls, relationship_mapping_dict: dict) -&gt; \"RelationshipConversionAccessor\":\n        \"\"\"\n        Convert a dictionary of strings representing model qualified names to a RelationshipConversionAccessor\n\n        The dictionary should be of the form:\n        {\n            \"pydantic_model_qualified_name\": \"django_model_qualified_name\",\n            ...\n        }\n        \"\"\"\n        available_relationships = []\n        for pydantic_mqn, django_mqn in relationship_mapping_dict.items():\n            try:\n                # Split the module path and class name\n                pydantic_module_path, pydantic_class_name = pydantic_mqn.rsplit(\".\", 1)\n                django_module_path, django_class_name = django_mqn.rsplit(\".\", 1)\n\n                # Import the modules\n                pydantic_module = importlib.import_module(pydantic_module_path)\n                django_module = importlib.import_module(django_module_path)\n\n                # Get the actual class objects\n                pydantic_model = getattr(pydantic_module, pydantic_class_name)\n                django_model = getattr(django_module, django_class_name)\n\n                available_relationships.append(RelationshipMapper(pydantic_model, django_model, context=None))\n            except Exception as e:\n                logger.warning(f\"Error importing model {pydantic_mqn} or {django_mqn}: {e}\")\n                continue\n        return cls(available_relationships)\n\n    def to_dict(self) -&gt; dict:\n        \"\"\"\n        Convert the relationships to a dictionary of strings representing\n        model qualified names for bidirectional conversion.\n\n        Can be stored in a JSON field, and used to reconstruct the relationships.\n        \"\"\"\n        relationship_mapping_dict = {}\n        for relationship in self.available_relationships:\n            # Skip relationships where either model is None\n            if relationship.pydantic_model is None or relationship.django_model is None:\n                continue\n\n            pydantic_mqn = self._get_pydantic_model_qualified_name(relationship.pydantic_model)\n            django_mqn = self._get_django_model_qualified_name(relationship.django_model)\n            relationship_mapping_dict[pydantic_mqn] = django_mqn\n\n        return relationship_mapping_dict\n\n    def _get_pydantic_model_qualified_name(self, model: type[BaseModel] | None) -&gt; str:\n        \"\"\"Get the fully qualified name of a Pydantic model as module.class_name\"\"\"\n        if model is None:\n            return \"\"\n        return f\"{model.__module__}.{model.__name__}\"\n\n    def _get_django_model_qualified_name(self, model: type[models.Model] | None) -&gt; str:\n        \"\"\"Get the fully qualified name of a Django model as app_label.model_name\"\"\"\n        if model is None:\n            return \"\"\n        return f\"{model._meta.app_label}.{model.__name__}\"\n\n    @property\n    def available_source_models(self) -&gt; list[type]:\n        \"\"\"Get a list of all source models (Pydantic or Dataclass).\"\"\"\n        models = []\n        for r in self.available_relationships:\n            if r.pydantic_model:\n                models.append(r.pydantic_model)\n            if r.dataclass_model:\n                models.append(r.dataclass_model)\n        return models\n\n    @property\n    def available_django_models(self) -&gt; list[type[models.Model]]:\n        \"\"\"Get a list of all Django models in the relationship accessor\"\"\"\n        return [r.django_model for r in self.available_relationships if r.django_model is not None]\n\n    def add_pydantic_model(self, model: type[BaseModel]) -&gt; None:\n        \"\"\"Add a Pydantic model to the relationship accessor\"\"\"\n        # Check if the model is already in available_pydantic_models by comparing class names\n        model_name = model.__name__\n        existing_models = [m.__name__ for m in self.available_source_models]\n\n        if model_name not in existing_models:\n            self.available_relationships.append(RelationshipMapper(model, None, context=None))\n\n    def add_dataclass_model(self, model: type) -&gt; None:\n        \"\"\"Add a Dataclass model to the relationship accessor\"\"\"\n        # Check if the model is already mapped\n        if any(r.dataclass_model == model for r in self.available_relationships):\n            return  # Already exists\n        # Check if a Pydantic model with the same name is already mapped (potential conflict)\n        if any(r.pydantic_model and r.pydantic_model.__name__ == model.__name__ for r in self.available_relationships):\n            logger.warning(f\"Adding dataclass {model.__name__}, but a Pydantic model with the same name exists.\")\n\n        self.available_relationships.append(RelationshipMapper(dataclass_model=model))\n\n    def add_django_model(self, model: type[models.Model]) -&gt; None:\n        \"\"\"Add a Django model to the relationship accessor\"\"\"\n        # Check if the model is already in available_django_models by comparing class names\n        model_name = model.__name__\n        existing_models = [m.__name__ for m in self.available_django_models]\n\n        if model_name not in existing_models:\n            self.available_relationships.append(RelationshipMapper(None, None, model, context=None))\n\n    def get_django_model_for_pydantic(self, pydantic_model: type[BaseModel]) -&gt; Optional[type[models.Model]]:\n        \"\"\"\n        Find the corresponding Django model for a given Pydantic model\n\n        Returns None if no matching Django model is found\n        \"\"\"\n        for relationship in self.available_relationships:\n            if relationship.pydantic_model == pydantic_model and relationship.django_model is not None:\n                return relationship.django_model\n        return None\n\n    def get_pydantic_model_for_django(self, django_model: type[models.Model]) -&gt; Optional[type[BaseModel]]:\n        \"\"\"\n        Find the corresponding Pydantic model for a given Django model\n\n        Returns None if no matching Pydantic model is found\n        \"\"\"\n        for relationship in self.available_relationships:\n            if relationship.django_model == django_model and relationship.pydantic_model is not None:\n                return relationship.pydantic_model\n        return None\n\n    def get_django_model_for_dataclass(self, dataclass_model: type) -&gt; Optional[type[models.Model]]:\n        \"\"\"Find the corresponding Django model for a given Dataclass model.\"\"\"\n        logger.debug(f\"Searching for Django model matching dataclass: {dataclass_model.__name__}\")\n        for relationship in self.available_relationships:\n            # Check if this mapper holds the target dataclass and has a linked Django model\n            if relationship.dataclass_model == dataclass_model and relationship.django_model is not None:\n                logger.debug(f\"  Found match: {relationship.django_model.__name__}\")\n                return relationship.django_model\n        logger.debug(f\"  No match found for dataclass {dataclass_model.__name__}\")\n        return None\n\n    def map_relationship(self, source_model: type, django_model: type[models.Model]) -&gt; None:\n        \"\"\"\n        Create or update a mapping between a source model (Pydantic/Dataclass) and a Django model.\n        \"\"\"\n        source_type = (\n            \"pydantic\"\n            if isinstance(source_model, type) and issubclass(source_model, BaseModel)\n            else \"dataclass\"\n            if dataclasses.is_dataclass(source_model)\n            else \"unknown\"\n        )\n\n        if source_type == \"unknown\":\n            logger.warning(f\"Cannot map relationship for unknown source type: {source_model}\")\n            return\n\n        # Check if either model already exists in a relationship\n        for relationship in self.available_relationships:\n            if source_type == \"pydantic\" and relationship.pydantic_model == source_model:\n                relationship.django_model = django_model\n                # Ensure dataclass_model is None if we map pydantic\n                relationship.dataclass_model = None\n                logger.debug(f\"Updated mapping: Pydantic {source_model.__name__} -&gt; Django {django_model.__name__}\")\n                return\n            if source_type == \"dataclass\" and relationship.dataclass_model == source_model:\n                relationship.django_model = django_model\n                # Ensure pydantic_model is None\n                relationship.pydantic_model = None\n                logger.debug(f\"Updated mapping: Dataclass {source_model.__name__} -&gt; Django {django_model.__name__}\")\n                return\n            if relationship.django_model == django_model:\n                # Map the source model based on its type\n                if source_type == \"pydantic\":\n                    relationship.pydantic_model = cast(type[BaseModel], source_model)\n                    relationship.dataclass_model = None\n                    logger.debug(\n                        f\"Updated mapping: Pydantic {source_model.__name__} -&gt; Django {django_model.__name__} (found via Django model)\"\n                    )\n                elif source_type == \"dataclass\":\n                    relationship.dataclass_model = cast(type, source_model)\n                    relationship.pydantic_model = None\n                    logger.debug(\n                        f\"Updated mapping: Dataclass {source_model.__name__} -&gt; Django {django_model.__name__} (found via Django model)\"\n                    )\n                return\n\n        # If no existing relationship found, create a new one\n        logger.debug(\n            f\"Creating new mapping: {source_type.capitalize()} {source_model.__name__} -&gt; Django {django_model.__name__}\"\n        )\n        if source_type == \"pydantic\":\n            self.available_relationships.append(\n                RelationshipMapper(pydantic_model=cast(type[BaseModel], source_model), django_model=django_model)\n            )\n        elif source_type == \"dataclass\":\n            self.available_relationships.append(\n                RelationshipMapper(dataclass_model=cast(type, source_model), django_model=django_model)\n            )\n\n    def is_source_model_known(self, model: type) -&gt; bool:\n        \"\"\"Check if a specific source model (Pydantic or Dataclass) is known.\"\"\"\n        is_pydantic = isinstance(model, type) and issubclass(model, BaseModel)\n        is_dataclass = dataclasses.is_dataclass(model)\n\n        for r in self.available_relationships:\n            if is_pydantic and r.pydantic_model == model:\n                return True\n            if is_dataclass and r.dataclass_model == model:\n                return True\n        return False\n\n    # Add a method to lookup source type by name\n    def get_source_model_by_name(self, model_name: str) -&gt; Optional[type]:\n        \"\"\"Find a known source model (Pydantic or Dataclass) by its class name.\"\"\"\n        for r in self.available_relationships:\n            if r.pydantic_model and r.pydantic_model.__name__ == model_name:\n                return r.pydantic_model\n            if r.dataclass_model and r.dataclass_model.__name__ == model_name:\n                return r.dataclass_model\n        return None\n</code></pre> </li> <li> <p>Bidirectional mapper between source models (Pydantic/Dataclass) and Django models.</p> Source code in <code>src/pydantic2django/core/relationships.py</code> <pre><code>@dataclass\nclass RelationshipMapper:\n    \"\"\"\n    Bidirectional mapper between source models (Pydantic/Dataclass) and Django models.\n    \"\"\"\n\n    # Allow storing either source type\n    pydantic_model: Optional[type[BaseModel]] = None\n    dataclass_model: Optional[type] = None\n    django_model: Optional[type[models.Model]] = None\n    context: Optional[ModelContext] = None  # Keep context if needed later\n\n    @property\n    def source_model(self) -&gt; Optional[type]:\n        \"\"\"Return the source model (either Pydantic or Dataclass).\"\"\"\n        return self.pydantic_model or self.dataclass_model\n</code></pre> </li> <li> <p>Typing utilities</p> </li> <li> Source code in <code>src/pydantic2django/core/typing.py</code> <pre><code>class TypeHandler:\n    PATTERNS = {\n        \"angle_bracket_class\": re.compile(r\"&lt;class '([^']+)'&gt;\"),\n    }\n\n    @staticmethod\n    def _add_import(imports: dict[str, list[str]], module: str, name: str):\n        \"\"\"Safely add an import to the dictionary.\"\"\"\n        if not module or module == \"builtins\":\n            return\n        # Avoid adding the module itself if name matches module (e.g., import datetime)\n        # if name == module.split('.')[-1]:\n        #     name = module # This logic might be too simplistic, revert for now\n        current_names = imports.setdefault(module, [])\n        if name not in current_names:\n            current_names.append(name)\n\n    @staticmethod\n    def _merge_imports(dict1: dict, dict2: dict) -&gt; dict:\n        \"\"\"Merge two import dictionaries.\"\"\"\n        merged = dict1.copy()\n        for module, names in dict2.items():\n            current_names = merged.setdefault(module, [])\n            for name in names:\n                if name not in current_names:\n                    current_names.append(name)\n        # Sort names within each module for consistency\n        for module in merged:\n            merged[module].sort()\n        return merged\n\n    @staticmethod\n    def get_class_name(type_obj: Any) -&gt; str:\n        \"\"\"Extract a simple, usable class name from a type object.\"\"\"\n        origin = get_origin(type_obj)\n        args = get_args(type_obj)\n\n        # Check for Optional[T] specifically first (Union[T, NoneType])\n        if origin in (Union, UnionType) and len(args) == 2 and type(None) in args:\n            return \"Optional\"\n\n        if origin:\n            # Now check for other origins\n            if origin in (Union, UnionType):  # Handles Union[A, B, ...]\n                return \"Union\"\n            if origin is list:\n                return \"List\"  # Use capital L consistently\n            if origin is dict:\n                return \"Dict\"  # Use capital D consistently\n            if origin is tuple:\n                return \"Tuple\"  # Use capital T consistently\n            if origin is set:\n                return \"Set\"  # Use capital S consistently\n            if origin is Callable:\n                return \"Callable\"\n            if origin is type:\n                return \"Type\"\n            # Fallback for other generic types\n            return getattr(origin, \"__name__\", str(origin))\n\n        # Handle non-generic types\n        if hasattr(type_obj, \"__name__\"):\n            return type_obj.__name__\n\n        type_str = str(type_obj)\n        match = TypeHandler.PATTERNS[\"angle_bracket_class\"].match(type_str)\n        if match:\n            return match.group(1).split(\".\")[-1]\n\n        return str(type_obj)\n\n    @staticmethod\n    def get_required_imports(type_obj: Any) -&gt; dict[str, list[str]]:\n        \"\"\"Determine necessary imports by traversing a type object.\"\"\"\n        imports: dict[str, list[str]] = {}\n        processed_types = set()\n\n        # Define modules for known Pydantic types that might need explicit import\n        pydantic_module_map = {\n            \"EmailStr\": \"pydantic\",\n            \"IPvAnyAddress\": \"pydantic\",\n            \"Json\": \"pydantic\",\n            \"BaseModel\": \"pydantic\",\n            # Add others if needed (e.g., SecretStr, UrlStr)\n        }\n\n        def _traverse(current_type: Any):\n            nonlocal imports\n            try:\n                type_repr = repr(current_type)\n                if type_repr in processed_types:\n                    return\n                processed_types.add(type_repr)\n            except TypeError:\n                # Handle unhashable types if necessary, e.g., log a warning\n                pass\n\n            origin = get_origin(current_type)\n            args = get_args(current_type)\n\n            if origin:\n                # Handle Generic Alias (List, Dict, Union, Optional, Callable, Type)\n                origin_module = getattr(origin, \"__module__\", \"\")\n                origin_name = getattr(origin, \"__name__\", \"\")\n\n                # Determine the canonical name used in 'typing' imports (e.g., List, Dict, Callable)\n                typing_name = None\n                if origin is list:\n                    typing_name = \"List\"\n                elif origin is dict:\n                    typing_name = \"Dict\"\n                elif origin is tuple:\n                    typing_name = \"Tuple\"\n                elif origin is set:\n                    typing_name = \"Set\"\n                elif origin in (Union, UnionType):  # Handle types.UnionType for Python 3.10+\n                    # We don't need to add Union or Optional imports anymore with | syntax\n                    typing_name = None\n                elif origin is type:\n                    typing_name = \"Type\"\n                # Check both typing.Callable and collections.abc.Callable\n                elif origin_module == \"typing\" and origin_name == \"Callable\":\n                    typing_name = \"Callable\"\n                elif origin_module == \"collections.abc\" and origin_name == \"Callable\":\n                    typing_name = \"Callable\"\n                # Add more specific checks if needed (e.g., Sequence, Mapping)\n\n                # Add import if we identified a standard typing construct\n                if typing_name:\n                    TypeHandler._add_import(imports, \"typing\", typing_name)\n\n                # Traverse arguments regardless of origin's module\n                for arg in args:\n                    if arg is not type(None):  # Skip NoneType in Optional/Union\n                        if isinstance(arg, TypeVar):\n                            # Handle TypeVar by traversing its constraints/bound\n                            constraints = getattr(arg, \"__constraints__\", ())\n                            bound = getattr(arg, \"__bound__\", None)\n                            if bound:\n                                _traverse(bound)\n                            for constraint in constraints:\n                                _traverse(constraint)\n                        else:\n                            _traverse(arg)  # Recursively traverse arguments\n            # Handle Base Types or Classes (int, str, MyClass, etc.)\n            elif isinstance(current_type, type):\n                module_name = getattr(current_type, \"__module__\", \"\")\n                type_name = getattr(current_type, \"__name__\", \"\")\n\n                if not type_name or module_name == \"builtins\":\n                    pass  # Skip builtins or types without names\n                elif module_name == \"typing\" and type_name not in (\"NoneType\", \"Generic\"):\n                    # Catch Any, etc. used directly\n                    TypeHandler._add_import(imports, \"typing\", type_name)\n                # Check for dataclasses and Pydantic models specifically\n                elif is_dataclass(current_type) or (\n                    inspect.isclass(current_type) and issubclass(current_type, BaseModel)\n                ):\n                    actual_module = inspect.getmodule(current_type)\n                    if actual_module and actual_module.__name__ != \"__main__\":\n                        TypeHandler._add_import(imports, actual_module.__name__, type_name)\n                    # Add specific imports if needed (e.g., dataclasses.dataclass, pydantic.BaseModel)\n                    if is_dataclass(current_type):\n                        TypeHandler._add_import(imports, \"dataclasses\", \"dataclass\")\n                    # No need to add BaseModel here usually, handled by pydantic_module_map or direct usage\n                elif module_name:\n                    # Handle known standard library modules explicitly\n                    known_stdlib = {\"datetime\", \"decimal\", \"uuid\", \"pathlib\"}\n                    if module_name in known_stdlib:\n                        TypeHandler._add_import(imports, module_name, type_name)\n                    # Handle known Pydantic types explicitly (redundant with BaseModel check?)\n                    elif type_name in pydantic_module_map:\n                        TypeHandler._add_import(imports, pydantic_module_map[type_name], type_name)\n                    # Assume other types defined in modules need importing\n                    elif module_name != \"__main__\":  # Avoid importing from main script context\n                        TypeHandler._add_import(imports, module_name, type_name)\n\n            elif current_type is Any:\n                TypeHandler._add_import(imports, \"typing\", \"Any\")\n            elif isinstance(current_type, TypeVar):\n                # Handle TypeVar used directly\n                constraints = getattr(current_type, \"__constraints__\", ())\n                bound = getattr(current_type, \"__bound__\", None)\n                if bound:\n                    _traverse(bound)\n                for c in constraints:\n                    _traverse(c)\n            # Consider adding ForwardRef handling if needed:\n            # elif isinstance(current_type, typing.ForwardRef):\n            #     # Potentially add logic to resolve/import forward refs\n            #     pass\n\n        _traverse(type_obj)\n\n        # Clean up imports (unique, sorted)\n        final_imports = {}\n        for module, names in imports.items():\n            unique_names = sorted(set(names))\n            if unique_names:\n                final_imports[module] = unique_names\n        return final_imports\n\n    @staticmethod\n    def process_field_type(field_type: Any) -&gt; dict[str, Any]:\n        \"\"\"Process a field type to get name, flags, imports, and contained dataclasses.\"\"\"\n        logger.debug(f\"[TypeHandler] Processing type: {field_type!r}\")\n        is_optional = False\n        is_list = False\n        metadata: tuple[Any, ...] | None = None  # Initialize metadata with type hint\n        imports = set()\n        contained_dataclasses = set()\n        current_type = field_type  # Keep track of the potentially unwrapped type\n\n        # Helper function (remains the same)\n        def _is_potential_dataclass(t: Any) -&gt; bool:\n            return inspect.isclass(t) and is_dataclass(t)\n\n        def _find_contained_dataclasses(current_type: Any):\n            origin = get_origin(current_type)\n            args = get_args(current_type)\n            if origin:\n                for arg in args:\n                    if arg is not type(None):\n                        _find_contained_dataclasses(arg)\n            elif _is_potential_dataclass(current_type):\n                contained_dataclasses.add(current_type)\n\n        _find_contained_dataclasses(field_type)\n        if contained_dataclasses:\n            logger.debug(f\"  Found potential contained dataclasses: {[dc.__name__ for dc in contained_dataclasses]}\")\n\n        # --- Simplification Loop ---\n        # Repeatedly unwrap until we hit a base type or Any\n        processed = True\n        while processed:\n            processed = False\n            origin = get_origin(current_type)\n            args = get_args(current_type)\n\n            # 0. Unwrap Annotated[T, ...]\n            # Check if the origin exists and has the name 'Annotated'\n            # This check is more robust than `origin is Annotated` across Python versions\n            if origin is Annotated:\n                if args:\n                    core_type = args[0]\n                    metadata = args[1:]\n                    current_type = core_type\n                    logger.debug(f\"  Unwrapped Annotated, current type: {current_type!r}, metadata: {metadata!r}\")\n                    processed = True\n                    continue  # Restart loop with unwrapped type\n                else:\n                    logger.warning(\"  Found Annotated without arguments? Treating as Any.\")\n                    current_type = Any\n                    processed = True\n                    continue\n\n            # 1. Unwrap Optional[T] (Union[T, NoneType])\n            if origin in (Union, UnionType) and type(None) in args:\n                is_optional = True  # Flag it\n                # Rebuild the Union without NoneType\n                non_none_args = tuple(arg for arg in args if arg is not type(None))\n                if len(non_none_args) == 1:\n                    current_type = non_none_args[0]  # Simplify Union[T, None] to T\n                elif len(non_none_args) &gt; 1:\n                    # Use UnionType to rebuild\n                    current_type = reduce(lambda x, y: x | y, non_none_args)\n                else:  # pragma: no cover\n                    # Should not happen if NoneType was in args\n                    current_type = Any\n                logger.debug(f\"  Unwrapped Union with None, current type: {current_type!r}\")\n                processed = True\n                continue  # Restart loop with the non-optional type\n\n            # 2. Unwrap List[T] or Sequence[T]\n            if origin in (list, Sequence):\n                is_list = True  # Flag it\n                if args:\n                    current_type = args[0]\n                    logger.debug(f\"  Unwrapped List/Sequence, current element type: {current_type!r}\")\n                else:\n                    current_type = Any  # List without args -&gt; List[Any]\n                    logger.debug(\"  Unwrapped List/Sequence without args, assuming Any\")\n                processed = True\n                continue  # Restart loop with unwrapped element type\n\n            # 3. Unwrap Literal[...]\n            if origin is Literal:\n                # Keep the Literal origin, but simplify args if possible?\n                # No, the mapper needs the original Literal to extract choices.\n                # Just log and break the loop for Literal.\n                logger.debug(\"  Hit Literal origin, stopping simplification loop.\")\n                break  # Stop simplification here, keep Literal type\n\n        # --- Post-Loop Handling ---\n        # At this point, current_type should be the base type (int, str, datetime, Any, etc.)\n        # or a complex type we don't simplify further (like a raw Union or a specific class)\n        base_type_obj = current_type\n\n        # --- FIX: If the original type was a list, ensure base_type_obj reflects the *List* --- #\n        # The simplification loop above sets current_type to the *inner* type of the list.\n        # We need the actual List type for the mapper logic.\n        if is_list:\n            # Determine the simplified inner type from the end of the loop\n            simplified_inner_type = base_type_obj\n\n            # Check if the original type involved Optional wrapping the list\n            # A simple check: was is_optional also flagged?\n            if is_optional:\n                # Reconstruct Optional[List[SimplifiedInner]]\n                reconstructed_type = list[simplified_inner_type] | None\n                logger.debug(\n                    f\"  Original was Optional[List-like]. Reconstructing List[...] | None \"\n                    f\"around simplified inner type {simplified_inner_type!r} -&gt; {reconstructed_type!r}\"\n                )\n            else:\n                # Reconstruct List[SimplifiedInner]\n                reconstructed_type = list[simplified_inner_type]\n                logger.debug(\n                    f\"  Original was List-like (non-optional). Reconstructing List[...] \"\n                    f\"around simplified inner type {simplified_inner_type!r} -&gt; {reconstructed_type!r}\"\n                )\n\n            # Check against original type structure (might be more robust but complex?)\n            # original_origin = get_origin(field_type)\n            # if original_origin is Optional and get_origin(get_args(field_type)[0]) in (list, Sequence):\n            #     # Handle Optional[List[...]] structure\n            # elif original_origin in (list, Sequence):\n            #     # Handle List[...] structure\n            # else:\n            #     # Handle complex cases like Annotated[Optional[List[...]]]\n\n            base_type_obj = reconstructed_type\n\n        # --- End FIX --- #\n\n        # Add check for Callable simplification\n        origin = get_origin(base_type_obj)\n        if origin is Callable or (\n            hasattr(base_type_obj, \"__module__\")\n            and base_type_obj.__module__ == \"collections.abc\"\n            and base_type_obj.__name__ == \"Callable\"\n        ):\n            logger.debug(\n                f\"  Final type is complex Callable {base_type_obj!r}, simplifying base object to Callable origin.\"\n            )\n            base_type_obj = Callable\n\n        # --- Result Assembly ---\n        imports = TypeHandler.get_required_imports(field_type)  # Imports based on original\n        type_string = TypeHandler.format_type_string(field_type)  # Formatting based on original\n\n        result = {\n            \"type_str\": type_string,\n            \"type_obj\": base_type_obj,  # THIS is the crucial simplified type object\n            \"is_optional\": is_optional,\n            \"is_list\": is_list,\n            \"imports\": imports,\n            \"contained_dataclasses\": contained_dataclasses,\n            \"metadata\": metadata,\n        }\n        logger.debug(f\"[TypeHandler] Processed result: {result!r}\")\n        return result\n\n    @staticmethod\n    def format_type_string(type_obj: Any) -&gt; str:\n        \"\"\"Return a string representation suitable for generated code.\"\"\"\n        # --- Simplified version to break recursion ---\n        # Get the raw string representation first\n        raw_repr = TypeHandler._get_raw_type_string(type_obj)\n\n        # Basic cleanup for common typing constructs\n        base_name = raw_repr.replace(\"typing.\", \"\")\n\n        # Attempt to refine based on origin/args if needed (optional)\n        origin = get_origin(type_obj)\n        args = get_args(type_obj)\n\n        if origin in (Union, UnionType) and len(args) == 2 and type(None) in args:\n            # Handle Optional[T]\n            inner_type_str = TypeHandler.format_type_string(next(arg for arg in args if arg is not type(None)))\n            return f\"{inner_type_str} | None\"\n        elif origin in (list, Sequence):\n            # Handle List[T] / Sequence[T]\n            if args:\n                inner_type_str = TypeHandler.format_type_string(args[0])\n                return f\"List[{inner_type_str}]\"  # Prefer List for generated code\n            else:\n                return \"List[Any]\"\n        elif origin is dict:\n            if args and len(args) == 2:\n                key_type_str = TypeHandler.format_type_string(args[0])\n                value_type_str = TypeHandler.format_type_string(args[1])\n                return f\"Dict[{key_type_str}, {value_type_str}]\"\n            else:\n                return \"dict\"\n        elif origin is Callable:\n            if args:\n                # For Callable[[A, B], R], args is ([A, B], R) in Py3.9+\n                # For Callable[A, R], args is (A, R)\n                # For Callable[[], R], args is ([], R)\n                param_part = args[0]\n                return_part = args[-1]\n\n                if param_part is ...:\n                    param_str = \"...\"\n                elif isinstance(param_part, list):\n                    param_types = [TypeHandler.format_type_string(p) for p in param_part]\n                    param_str = f'[{\", \".join(param_types)}]'\n                else:  # Single argument\n                    param_str = f\"[{TypeHandler.format_type_string(param_part)}]\"\n\n                return_type_str = TypeHandler.format_type_string(return_part)\n                return f\"Callable[{param_str}, {return_type_str}]\"\n            else:\n                return \"Callable\"\n        elif origin in (Union, UnionType):  # Non-optional Union\n            inner_types = [TypeHandler.format_type_string(arg) for arg in args]\n            return \" | \".join(inner_types)\n        elif origin is Literal:\n            inner_values = [repr(arg) for arg in args]\n            return f\"Literal[{', '.join(inner_values)}]\"\n        # Add other origins like Dict, Tuple, Callable if needed\n\n        # Fallback to the cleaned raw representation\n        return base_name.replace(\"collections.abc.\", \"\")\n\n    @staticmethod\n    def _get_raw_type_string(type_obj: Any) -&gt; str:\n        module = getattr(type_obj, \"__module__\", \"\")\n        if module == \"typing\":\n            return repr(type_obj).replace(\"typing.\", \"\")\n        # Use name for classes/dataclasses\n        if hasattr(type_obj, \"__name__\") and isinstance(type_obj, type):\n            return type_obj.__name__\n        # Fallback to str\n        return str(type_obj)\n</code></pre> </li> <li> <p>Import aggregation for generated code</p> </li> <li> <p>Handles import statements for generated Django models and their context classes. Tracks and deduplicates imports from multiple sources while ensuring all necessary dependencies are included.</p> Source code in <code>src/pydantic2django/core/imports.py</code> <pre><code>class ImportHandler:\n    \"\"\"\n    Handles import statements for generated Django models and their context classes.\n    Tracks and deduplicates imports from multiple sources while ensuring all necessary\n    dependencies are included.\n    \"\"\"\n\n    def __init__(self, module_mappings: Optional[dict[str, str]] = None):\n        \"\"\"\n        Initialize empty collections for different types of imports.\n\n        Args:\n            module_mappings: Optional mapping of modules to remap (e.g. {\"__main__\": \"my_app.models\"})\n        \"\"\"\n        # Track imports by category\n        self.extra_type_imports: set[str] = set()  # For typing and other utility imports\n        self.pydantic_imports: set[str] = set()  # For Pydantic model imports\n        self.context_class_imports: set[str] = set()  # For context class and field type imports\n\n        # For tracking imported names to avoid duplicates\n        self.imported_names: dict[str, str] = {}  # Maps type name to its module\n\n        # For tracking field type dependencies we've already processed\n        self.processed_field_types: set[str] = set()\n\n        # Module mappings to remap imports (e.g. \"__main__\" -&gt; \"my_app.models\")\n        self.module_mappings = module_mappings or {}\n\n        logger.info(\"ImportHandler initialized\")\n        if self.module_mappings:\n            logger.info(f\"Using module mappings: {self.module_mappings}\")\n\n    def add_import(self, module: str, name: str):\n        \"\"\"Adds a single import based on module and name strings.\"\"\"\n        if not module or module == \"builtins\":\n            return\n\n        # Apply module mappings\n        if module in self.module_mappings:\n            module = self.module_mappings[module]\n\n        # Clean name (e.g., remove generics for import statement)\n        clean_name = self._clean_generic_type(name)\n\n        # Check if already imported\n        if name in self.imported_names:\n            # Could verify module matches, but usually name is unique enough\n            logger.debug(f\"Skipping already imported name: {name} (from module {module})\")\n            return\n        if clean_name != name and clean_name in self.imported_names:\n            logger.debug(f\"Skipping already imported clean name: {clean_name} (from module {module})\")\n            return\n\n        # Determine category\n        # Simplistic: If module is known Pydantic, Django, or common stdlib -&gt; context\n        # Otherwise, if it's 'typing' -&gt; extra_type\n        # TODO: Refine categorization if needed (e.g., dedicated django_imports set)\n        import_statement = f\"from {module} import {clean_name}\"\n        if module == \"typing\":\n            self.extra_type_imports.add(clean_name)  # Add only name to typing imports set\n            logger.debug(f\"Adding typing import: {clean_name}\")\n        # elif module.startswith(\"django.\"):\n        # Add to a dedicated django set if we create one\n        #    self.context_class_imports.add(import_statement)\n        #    logger.info(f\"Adding Django import: {import_statement}\")\n        else:\n            # Default to context imports for non-typing\n            self.context_class_imports.add(import_statement)\n            logger.info(f\"Adding context class import: {import_statement}\")\n\n        # Mark as imported\n        self.imported_names[name] = module\n        if clean_name != name:\n            self.imported_names[clean_name] = module\n\n    def add_pydantic_model_import(self, model_class: type) -&gt; None:\n        \"\"\"\n        Add an import statement for a Pydantic model.\n\n        Args:\n            model_class: The Pydantic model class to import\n        \"\"\"\n        if not hasattr(model_class, \"__module__\") or not hasattr(model_class, \"__name__\"):\n            logger.warning(f\"Cannot add import for {model_class}: missing __module__ or __name__\")\n            return\n\n        module_path = model_class.__module__\n        model_name = self._clean_generic_type(model_class.__name__)\n\n        # Apply module mappings if needed\n        if module_path in self.module_mappings:\n            actual_module = self.module_mappings[module_path]\n            logger.debug(f\"Remapping module import: {module_path} -&gt; {actual_module}\")\n            module_path = actual_module\n\n        logger.debug(f\"Processing Pydantic model import: {model_name} from {module_path}\")\n\n        # Skip if already imported\n        if model_name in self.imported_names:\n            logger.debug(f\"Skipping already imported model: {model_name}\")\n            return\n\n        import_statement = f\"from {module_path} import {model_name}\"\n        logger.info(f\"Adding Pydantic import: {import_statement}\")\n        self.pydantic_imports.add(import_statement)\n        self.imported_names[model_name] = module_path\n\n    def add_context_field_type_import(self, field_type: Any) -&gt; None:\n        \"\"\"\n        Add an import statement for a context field type with recursive dependency detection.\n\n        Args:\n            field_type: The field type to import\n        \"\"\"\n        # Skip if we've already processed this field type\n        field_type_str = str(field_type)\n        if field_type_str in self.processed_field_types:\n            logger.debug(f\"Skipping already processed field type: {field_type_str}\")\n            return\n\n        logger.info(f\"Processing context field type: {field_type_str}\")\n        self.processed_field_types.add(field_type_str)\n\n        # Try to add direct import for the field type if it's a class\n        self._add_type_import(field_type)\n\n        # Handle nested types in generics, unions, etc.\n        self._process_nested_types(field_type)\n\n        # Add typing imports based on the field type string\n        self._add_typing_imports(field_type_str)\n\n    def _add_type_import(self, field_type: Any) -&gt; None:\n        \"\"\"\n        Add an import for a single type object if it has module and name attributes.\n\n        Args:\n            field_type: The type to import\n        \"\"\"\n        try:\n            if hasattr(field_type, \"__module__\") and hasattr(field_type, \"__name__\"):\n                type_module = field_type.__module__\n                type_name = field_type.__name__\n\n                # Apply module mappings if needed\n                if type_module in self.module_mappings:\n                    actual_module = self.module_mappings[type_module]\n                    logger.debug(f\"Remapping module import: {type_module} -&gt; {actual_module}\")\n                    type_module = actual_module\n\n                logger.debug(f\"Examining type: {type_name} from module {type_module}\")\n\n                # Skip built-in types and typing module types\n                if (\n                    type_module.startswith(\"typing\")\n                    or type_module == \"builtins\"\n                    or type_name in [\"str\", \"int\", \"float\", \"bool\", \"dict\", \"list\"]\n                ):\n                    logger.debug(f\"Skipping built-in or typing type: {type_name}\")\n                    return\n\n                # Skip TypeVar definitions to avoid conflicts\n                if type_name == \"T\" or type_name == \"TypeVar\":\n                    logger.debug(f\"Skipping TypeVar definition: {type_name} - will be defined locally\")\n                    return\n\n                # Clean up any parametrized generic types for the import statement\n                clean_type_name = self._clean_generic_type(type_name)\n\n                # Use the original type_name (potentially with generics) for the imported_names check\n                if type_name in self.imported_names:\n                    logger.debug(f\"Skipping already imported type: {type_name}\")\n                    return\n\n                # Add to context class imports *before* marking as imported\n                # Use the clean name for the import statement itself\n                import_statement = f\"from {type_module} import {clean_type_name}\"\n                logger.info(f\"Adding context class import: {import_statement}\")\n                self.context_class_imports.add(import_statement)\n\n                # Add the original type name to imported_names to prevent re-processing\n                self.imported_names[type_name] = type_module\n                # Also add the cleaned name in case it's encountered separately\n                if clean_type_name != type_name:\n                    self.imported_names[clean_type_name] = type_module\n\n        except (AttributeError, TypeError) as e:\n            logger.warning(f\"Error processing type import for {field_type}: {e}\")\n\n    def _process_nested_types(self, field_type: Any) -&gt; None:\n        \"\"\"\n        Recursively process nested types in generics, unions, etc.\n\n        Args:\n            field_type: The type that might contain nested types\n        \"\"\"\n        # Handle __args__ for generic types, unions, etc.\n        if hasattr(field_type, \"__args__\"):\n            logger.debug(f\"Processing nested types for {field_type}\")\n            for arg_type in field_type.__args__:\n                logger.debug(f\"Found nested type argument: {arg_type}\")\n                # Recursively process each argument type\n                self.add_context_field_type_import(arg_type)\n\n        # Handle __origin__ for generic types (like List, Dict, etc.)\n        if hasattr(field_type, \"__origin__\"):\n            logger.debug(f\"Processing origin type for {field_type}: {field_type.__origin__}\")\n            self.add_context_field_type_import(field_type.__origin__)\n\n    def _add_typing_imports(self, field_type_str: str) -&gt; None:\n        \"\"\"\n        Add required typing imports based on the string representation of the field type.\n\n        Args:\n            field_type_str: String representation of the field type\n        \"\"\"\n        # Check for common typing constructs\n        if \"List[\" in field_type_str or \"list[\" in field_type_str:\n            logger.debug(f\"Adding List import from {field_type_str}\")\n            self.extra_type_imports.add(\"List\")\n\n        if \"Dict[\" in field_type_str or \"dict[\" in field_type_str:\n            logger.debug(f\"Adding Dict import from {field_type_str}\")\n            self.extra_type_imports.add(\"Dict\")\n\n        if \"Tuple[\" in field_type_str or \"tuple[\" in field_type_str:\n            logger.debug(f\"Adding Tuple import from {field_type_str}\")\n            self.extra_type_imports.add(\"Tuple\")\n\n        if \"Optional[\" in field_type_str or \"Union[\" in field_type_str or \"None\" in field_type_str:\n            logger.debug(f\"Adding Optional import from {field_type_str}\")\n            self.extra_type_imports.add(\"Optional\")\n\n        if \"Union[\" in field_type_str:\n            logger.debug(f\"Adding Union import from {field_type_str}\")\n            self.extra_type_imports.add(\"Union\")\n\n        if \"Callable[\" in field_type_str:\n            logger.debug(f\"Adding Callable import from {field_type_str}\")\n            self.extra_type_imports.add(\"Callable\")\n\n        if \"Any\" in field_type_str:\n            logger.debug(f\"Adding Any import from {field_type_str}\")\n            self.extra_type_imports.add(\"Any\")\n\n        # Extract custom types from the field type string\n        self._extract_custom_types_from_string(field_type_str)\n\n    def _extract_custom_types_from_string(self, field_type_str: str) -&gt; None:\n        \"\"\"\n        Extract custom type names from a string representation of a field type.\n\n        Args:\n            field_type_str: String representation of the field type\n        \"\"\"\n        # Extract potential type names from the string\n        # This regex looks for capitalized words that might be type names\n        type_names = re.findall(r\"[A-Z][a-zA-Z0-9]*\", field_type_str)\n\n        logger.debug(f\"Extracted potential type names from string {field_type_str}: {type_names}\")\n\n        for type_name in type_names:\n            # Skip common type names that are already handled\n            if type_name in [\"List\", \"Dict\", \"Optional\", \"Union\", \"Tuple\", \"Callable\", \"Any\"]:\n                logger.debug(f\"Skipping common typing name: {type_name}\")\n                continue\n\n            # Skip if already in imported names\n            if type_name in self.imported_names:\n                logger.debug(f\"Skipping already imported name: {type_name}\")\n                continue\n\n            # Log potential custom type\n            logger.info(f\"Adding potential custom type to extra_type_imports: {type_name}\")\n\n            # Add to extra type imports - these are types that we couldn't resolve to a module\n            # They'll need to be imported elsewhere or we might generate an error\n            self.extra_type_imports.add(type_name)\n\n    def get_required_imports(self, field_type_str: str) -&gt; dict[str, list[str]]:\n        \"\"\"\n        Get typing and custom type imports required for a field type.\n\n        Args:\n            field_type_str: String representation of a field type\n\n        Returns:\n            Dictionary with \"typing\" and \"custom\" import lists\n        \"\"\"\n        logger.debug(f\"Getting required imports for: {field_type_str}\")\n        self._add_typing_imports(field_type_str)\n\n        # Get custom types (non-typing types)\n        custom_types = [\n            name\n            for name in self.extra_type_imports\n            if name not in [\"List\", \"Dict\", \"Tuple\", \"Set\", \"Optional\", \"Union\", \"Any\", \"Callable\"]\n        ]\n\n        logger.debug(f\"Found custom types: {custom_types}\")\n\n        # Return the latest state of imports\n        return {\n            \"typing\": list(self.extra_type_imports),\n            \"custom\": custom_types,\n        }\n\n    def deduplicate_imports(self) -&gt; dict[str, set[str]]:\n        \"\"\"\n        De-duplicate imports between Pydantic models and context field types.\n\n        Returns:\n            Dict with de-duplicated import sets\n        \"\"\"\n        logger.info(\"Deduplicating imports\")\n        logger.debug(f\"Current pydantic imports: {self.pydantic_imports}\")\n        logger.debug(f\"Current context imports: {self.context_class_imports}\")\n\n        # Extract class names and modules from import statements\n        pydantic_classes = {}\n        context_classes = {}\n\n        # Handle special case for TypeVar imports\n        typevars = set()\n\n        for import_stmt in self.pydantic_imports:\n            if import_stmt.startswith(\"from \") and \" import \" in import_stmt:\n                module, classes = import_stmt.split(\" import \")\n                module = module.replace(\"from \", \"\")\n\n                # Skip __main__ and rewrite to real module paths if possible\n                if module == \"__main__\":\n                    logger.warning(f\"Skipping __main__ import: {import_stmt} - these won't work when imported\")\n                    continue\n\n                for cls in classes.split(\", \"):\n                    # Check if it's a TypeVar to handle duplicate definitions\n                    if cls == \"T\" or cls == \"TypeVar\":\n                        typevars.add(cls)\n                        continue\n\n                    # Clean up any parameterized generic types in class names\n                    cls = self._clean_generic_type(cls)\n                    pydantic_classes[cls] = module\n\n        for import_stmt in self.context_class_imports:\n            if import_stmt.startswith(\"from \") and \" import \" in import_stmt:\n                module, classes = import_stmt.split(\" import \")\n                module = module.replace(\"from \", \"\")\n\n                # Skip __main__ imports or rewrite to real module paths if possible\n                if module == \"__main__\":\n                    logger.warning(f\"Skipping __main__ import: {import_stmt} - these won't work when imported\")\n                    continue\n\n                for cls in classes.split(\", \"):\n                    # Check if it's a TypeVar to handle duplicate definitions\n                    if cls == \"T\" or cls == \"TypeVar\":\n                        typevars.add(cls)\n                        continue\n\n                    # Clean up any parameterized generic types in class names\n                    cls = self._clean_generic_type(cls)\n                    # If this class is already imported in pydantic imports, skip it\n                    if cls in pydantic_classes:\n                        logger.debug(f\"Skipping duplicate context import for {cls}, already in pydantic imports\")\n                        continue\n                    context_classes[cls] = module\n\n        # Rebuild import statements\n        module_to_classes = {}\n        for cls, module in pydantic_classes.items():\n            if module not in module_to_classes:\n                module_to_classes[module] = []\n            module_to_classes[module].append(cls)\n\n        deduplicated_pydantic_imports = set()\n        for module, classes in module_to_classes.items():\n            deduplicated_pydantic_imports.add(f\"from {module} import {', '.join(sorted(classes))}\")\n\n        # Same for context imports\n        module_to_classes = {}\n        for cls, module in context_classes.items():\n            if module not in module_to_classes:\n                module_to_classes[module] = []\n            module_to_classes[module].append(cls)\n\n        deduplicated_context_imports = set()\n        for module, classes in module_to_classes.items():\n            deduplicated_context_imports.add(f\"from {module} import {', '.join(sorted(classes))}\")\n\n        logger.info(f\"Final pydantic imports: {deduplicated_pydantic_imports}\")\n        logger.info(f\"Final context imports: {deduplicated_context_imports}\")\n\n        # Log any TypeVar names we're skipping\n        if typevars:\n            logger.info(f\"Skipping TypeVar imports: {typevars} - these will be defined locally\")\n\n        return {\"pydantic\": deduplicated_pydantic_imports, \"context\": deduplicated_context_imports}\n\n    def _clean_generic_type(self, name: str) -&gt; str:\n        \"\"\"\n        Clean generic parameters from a type name.\n\n        Args:\n            name: The type name to clean\n\n        Returns:\n            The cleaned type name without generic parameters\n        \"\"\"\n        if \"[\" in name or \"&lt;\" in name:\n            cleaned = re.sub(r\"\\[.*\\]\", \"\", name)\n            logger.debug(f\"Cleaned generic type {name} to {cleaned}\")\n            return cleaned\n        return name\n</code></pre> </li> <li> <p>Context handling for non-serializable fields</p> </li> <li> <p>               Bases: <code>Generic[SourceModelType]</code></p> <p>Base class for model context classes. Stores context information for a Django model's fields that require special handling during conversion back to the source object (Pydantic/Dataclass).</p> Source code in <code>src/pydantic2django/core/context.py</code> <pre><code>@dataclass\nclass ModelContext(Generic[SourceModelType]):  # Make ModelContext generic\n    \"\"\"\n    Base class for model context classes.\n    Stores context information for a Django model's fields that require special handling\n    during conversion back to the source object (Pydantic/Dataclass).\n    \"\"\"\n\n    django_model: type[models.Model]\n    source_class: type[SourceModelType]  # Changed from pydantic_class\n    context_fields: dict[str, FieldContext] = field(default_factory=dict)\n    context_data: dict[str, Any] = field(default_factory=dict)\n\n    @property\n    def required_context_keys(self) -&gt; set[str]:\n        required_fields = {fc.field_name for fc in self.context_fields.values() if not fc.is_optional}\n        return required_fields\n\n    def add_field(\n        self,\n        field_name: str,\n        field_type_str: str,\n        is_optional: bool = False,\n        is_list: bool = False,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Add a field to the context storage.\n\n        Args:\n            field_name: Name of the field.\n            field_type_str: String representation of the field's type.\n            is_optional: Whether the field is optional.\n            is_list: Whether the field is a list.\n            **kwargs: Additional metadata for the field.\n        \"\"\"\n        # Pass is_optional, is_list explicitly\n        field_context = FieldContext(\n            field_name=field_name,\n            field_type_str=field_type_str,\n            is_optional=is_optional,\n            is_list=is_list,\n            additional_metadata=kwargs,\n        )\n        self.context_fields[field_name] = field_context\n\n    def validate_context(self, context: dict[str, Any]) -&gt; None:\n        \"\"\"\n        Validate that all required context fields are present.\n\n        Args:\n            context: The context dictionary to validate\n\n        Raises:\n            ValueError: If required context fields are missing\n        \"\"\"\n\n        missing_fields = self.required_context_keys - set(context.keys())\n        if missing_fields:\n            raise ValueError(f\"Missing required context fields: {', '.join(missing_fields)}\")\n\n    def get_field_type_str(self, field_name: str) -&gt; Optional[str]:\n        \"\"\"Get the string representation type of a context field.\"\"\"\n        field_context = self.context_fields.get(field_name)\n        return field_context.field_type_str if field_context else None\n\n    def get_field_by_name(self, field_name: str) -&gt; Optional[FieldContext]:\n        \"\"\"\n        Get a field context by name.\n\n        Args:\n            field_name: Name of the field to find\n\n        Returns:\n            The FieldContext if found, None otherwise\n        \"\"\"\n        return self.context_fields.get(field_name)\n\n    def to_conversion_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Convert context to a dictionary format suitable for conversion back to source object.\"\"\"\n        # Renamed from to_dict to be more generic\n        return {\n            field_name: field_context.value\n            for field_name, field_context in self.context_fields.items()\n            if field_context.value is not None\n        }\n\n    def set_value(self, field_name: str, value: Any) -&gt; None:\n        \"\"\"\n        Set the value for a context field.\n\n        Args:\n            field_name: Name of the field\n            value: Value to set\n\n        Raises:\n            ValueError: If the field doesn't exist in the context\n        \"\"\"\n        field = self.get_field_by_name(field_name)\n        if field is None:\n            raise ValueError(f\"Field {field_name} not found in context\")\n        field.value = value\n\n    def get_value(self, field_name: str) -&gt; Optional[Any]:\n        \"\"\"\n        Get the value of a context field.\n\n        Args:\n            field_name: Name of the field\n\n        Returns:\n            The field value if it exists and has been set, None otherwise\n        \"\"\"\n        field = self.get_field_by_name(field_name)\n        if field is not None:\n            return field.value\n        return None\n\n    def get_required_imports(self) -&gt; dict[str, set[str]]:  # Return sets for auto-dedup\n        \"\"\"\n        Get all required imports for the context class fields using TypeHandler.\n        \"\"\"\n        imports: dict[str, set[str]] = {\"typing\": set(), \"custom\": set()}\n\n        # Process each field\n        for _, field_context in self.context_fields.items():\n            # Use TypeHandler with the stored type string\n            type_imports = TypeHandler.get_required_imports(field_context.field_type_str)\n\n            # Add to our overall imports\n            imports[\"typing\"].update(type_imports.get(\"typing\", []))\n            imports[\"custom\"].update(type_imports.get(\"datetime\", []))  # Example specific types\n            imports[\"custom\"].update(type_imports.get(\"decimal\", []))\n            imports[\"custom\"].update(type_imports.get(\"uuid\", []))\n            # Add any other known modules TypeHandler might return\n\n            # Add Optional/List based on flags\n            if field_context.is_optional:\n                imports[\"typing\"].add(\"Optional\")\n            if field_context.is_list:\n                imports[\"typing\"].add(\"List\")\n\n        # Add base source model import\n        source_module = getattr(self.source_class, \"__module__\", None)\n        source_name = getattr(self.source_class, \"__name__\", None)\n        if source_module and source_name and source_module != \"builtins\":\n            imports[\"custom\"].add(f\"from {source_module} import {source_name}\")\n\n        # Add BaseModel or dataclass import\n        if isinstance(self.source_class, type) and issubclass(self.source_class, BaseModel):\n            imports[\"custom\"].add(\"from pydantic import BaseModel\")\n        elif dataclasses.is_dataclass(self.source_class):\n            imports[\"custom\"].add(\"from dataclasses import dataclass\")\n\n        # Add Any import if needed\n        if any(\"Any\" in fc.field_type_str for fc in self.context_fields.values()):\n            imports[\"typing\"].add(\"Any\")\n\n        return imports\n\n    @classmethod\n    def generate_context_class_code(cls, model_context: \"ModelContext\", jinja_env: Any | None = None) -&gt; str:\n        \"\"\"\n        Generate a string representation of the context class.\n\n        Args:\n            model_context: The ModelContext to generate a class for\n            jinja_env: Optional Jinja2 environment to use for rendering\n\n        Returns:\n            String representation of the context class\n        \"\"\"\n        # Create a ContextClassGenerator and use it to generate the class\n        generator = ContextClassGenerator(jinja_env=jinja_env)\n        return generator.generate_context_class(model_context)\n</code></pre> </li> <li> <p>Utility class for generating context class code from ModelContext objects.</p> Source code in <code>src/pydantic2django/core/context.py</code> <pre><code>class ContextClassGenerator:\n    \"\"\"\n    Utility class for generating context class code from ModelContext objects.\n    \"\"\"\n\n    def __init__(self, jinja_env: Any | None = None) -&gt; None:\n        \"\"\"\n        Initialize the ContextClassGenerator.\n\n        Args:\n            jinja_env: Optional Jinja2 environment to use for template rendering.\n        \"\"\"\n        self.jinja_env = jinja_env\n        # Initialize imports needed for the context class generation\n        self.imports: dict[str, set[str]] = {\"typing\": set(), \"custom\": set()}\n\n    def _load_template(self, template_name: str) -&gt; Any:\n        \"\"\"Load Jinja2 template.\"\"\"\n        if self.jinja_env:\n            return self.jinja_env.get_template(template_name)\n        else:\n            # Fallback to basic string formatting if Jinja2 is not available\n            # Note: This is a simplified fallback and might not handle complex templates\n            # Load template content from file or define as string here\n            # Example using basic string formatting:\n            # template_content = \"... {model_name} ... {field_definitions} ...\"\n            # return template_content\n            raise ImportError(\"Jinja2 environment not provided for template loading.\")\n\n    def _simplify_type_string(self, type_str: str) -&gt; str:\n        \"\"\"\n        Simplifies complex type strings for cleaner code generation.\n        Removes module paths like 'typing.' or full paths for common types.\n        \"\"\"\n        # Basic simplification: remove typing module path\n        simplified = type_str.replace(\"typing.\", \"\")\n\n        # Use TypeHandler to potentially clean further if needed\n        # simplified = TypeHandler.clean_type_string(simplified)\n\n        # Regex to remove full paths for nested standard types like list, dict, etc.\n        # Define common standard types that might appear with full paths\n        standard_types = [\"list\", \"dict\", \"tuple\", \"set\", \"Optional\", \"Union\"]\n\n        def replacer_class(match):\n            full_path = match.group(0)\n            # Extract the class name after the last dot\n            class_name = full_path.split(\".\")[-1]\n            # Check if the extracted class name is a standard type we want to simplify\n            if class_name in standard_types:\n                # If it is, return just the class name\n                return class_name\n            else:\n                # Otherwise, keep the full path (or handle custom imports)\n                # For now, keeping full path for non-standard types\n                # self._maybe_add_type_to_imports(full_path) # Add import for custom type\n                return full_path\n\n        # Pattern to find qualified names (e.g., some.module.ClassName)\n        # This needs careful crafting to avoid unintended replacements\n        # Example: r'\\b([a-zA-Z_][\\w\\.]*\\.)?([A-Z][a-zA-Z0-9_]*)\\b' might be too broad\n        # Focusing on paths likely coming from TypeHandler.get_required_imports might be safer\n        # For now, rely on basic replace and potential TypeHandler cleaning\n\n        return simplified\n\n    def generate_context_class(self, model_context: ModelContext) -&gt; str:\n        \"\"\"\n        Generates the Python code string for a context dataclass.\n        \"\"\"\n        template = self._load_template(\"context_class.py.j2\")\n        self.imports = model_context.get_required_imports()  # Get imports first\n\n        field_definitions = []\n        for field_name, field_context in model_context.context_fields.items():\n            field_type_str = field_context.field_type_str  # field_type is now the string representation\n\n            # Use TypeHandler._get_raw_type_string to get the clean, unquoted type string\n            # --- Corrected import path for TypeHandler ---\n            from .typing import TypeHandler\n\n            clean_type = TypeHandler._get_raw_type_string(field_type_str)\n\n            # Simplify the type string for display\n            simplified_type = self._simplify_type_string(clean_type)\n\n            # Add necessary imports based on the simplified type\n            # (Assuming _simplify_type_string and get_required_imports handle this)\n\n            # Format default value if present\n            default_repr = repr(field_context.value) if field_context.value is not None else \"None\"\n\n            field_def = f\"    {field_name}: {simplified_type} = field(default={default_repr})\"\n            field_definitions.append(field_def)\n\n        # Prepare imports for the template\n        typing_imports_str = \", \".join(sorted(self.imports[\"typing\"]))\n        custom_imports_list = sorted(self.imports[\"custom\"])  # Keep as list of strings\n\n        model_name = self._clean_generic_type(model_context.django_model.__name__)\n        source_class_name = self._clean_generic_type(model_context.source_class.__name__)\n\n        return template.render(\n            model_name=model_name,\n            # Use source_class_name instead of pydantic_class\n            source_class_name=source_class_name,\n            source_module=model_context.source_class.__module__,\n            field_definitions=\"\\n\".join(field_definitions),\n            typing_imports=typing_imports_str,\n            custom_imports=custom_imports_list,\n        )\n\n    def _clean_generic_type(self, name: str) -&gt; str:\n        \"\"\"Remove generic parameters like [T] from class names.\"\"\"\n        return name.split(\"[\")[0]\n</code></pre> </li> <li> <p>Serialization helpers</p> </li> <li> <p>Serialize a value using the most appropriate method.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The value to serialize</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The serialized value</p> Source code in <code>src/pydantic2django/core/serialization.py</code> <pre><code>def serialize_value(value: Any) -&gt; Any:\n    \"\"\"\n    Serialize a value using the most appropriate method.\n\n    Args:\n        value: The value to serialize\n\n    Returns:\n        The serialized value\n    \"\"\"\n    method, serializer = get_serialization_method(value)\n\n    if method == SerializationMethod.NONE:\n        # If no serialization method is found, return the value as is\n        return value\n\n    if serializer is None:\n        return value\n\n    try:\n        return serializer()\n    except Exception:\n        # If serialization fails, return the string representation\n        return str(value)\n</code></pre> </li> <li> <p>Get the appropriate serialization method for an object.</p> <p>This function checks for various serialization methods in order of preference: 1. model_dump (Pydantic) 2. to_json 3. to_dict 4. str (if overridden) 5. dict</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>The object to check for serialization methods</p> required <p>Returns:</p> Type Description <code>tuple[SerializationMethod, Optional[Callable[[], Any]]]</code> <p>A tuple of (SerializationMethod, Optional[Callable]). The callable is None if no method is found.</p> Source code in <code>src/pydantic2django/core/serialization.py</code> <pre><code>def get_serialization_method(\n    obj: Any,\n) -&gt; tuple[SerializationMethod, Optional[Callable[[], Any]]]:\n    \"\"\"\n    Get the appropriate serialization method for an object.\n\n    This function checks for various serialization methods in order of preference:\n    1. model_dump (Pydantic)\n    2. to_json\n    3. to_dict\n    4. __str__ (if overridden)\n    5. __dict__\n\n    Args:\n        obj: The object to check for serialization methods\n\n    Returns:\n        A tuple of (SerializationMethod, Optional[Callable]). The callable is None if no method is found.\n    \"\"\"\n    # Check for Pydantic model_dump\n    if isinstance(obj, BaseModel):\n        return SerializationMethod.MODEL_DUMP, obj.model_dump\n\n    # Check for to_json method\n    if hasattr(obj, \"to_json\"):\n        return SerializationMethod.TO_JSON, obj.to_json\n\n    # Check for to_dict method\n    if hasattr(obj, \"to_dict\"):\n        return SerializationMethod.TO_DICT, obj.to_dict\n\n    # Check for overridden __str__ method\n    if hasattr(obj, \"__str__\") and obj.__class__.__str__ is not object.__str__:\n        return SerializationMethod.STR, obj.__str__\n\n    # Check for __dict__ attribute\n    if hasattr(obj, \"__dict__\"):\n\n        def dict_serializer():\n            return {\n                \"__class__\": obj.__class__.__name__,\n                \"__module__\": obj.__class__.__module__,\n                \"data\": obj.__dict__,\n            }\n\n        return SerializationMethod.DICT, dict_serializer\n\n    return SerializationMethod.NONE, None\n</code></pre> </li> </ul>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.base_generator.BaseStaticGenerator.__init__","title":"<code>__init__(output_path, app_label, filter_function, verbose, discovery_instance, model_factory_instance, module_mappings, base_model_class, packages=None, class_name_prefix='Django', enable_timescale=True, enable_gfk=True, gfk_policy='all_nested', gfk_threshold_children=4, gfk_value_mode='typed_columns', gfk_normalize_common_attrs=False)</code>","text":"<p>Initialize the base generator.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>str</code> <p>Path to output the generated models.py file.</p> required <code>packages</code> <code>list[str] | None</code> <p>List of packages to scan for source models.</p> <code>None</code> <code>app_label</code> <code>str</code> <p>Django app label to use for the models.</p> required <code>filter_function</code> <code>Callable[[type[SourceModelType]], bool] | None</code> <p>Optional function to filter which source models to include.</p> required <code>verbose</code> <code>bool</code> <p>Print verbose output.</p> required <code>discovery_instance</code> <code>BaseDiscovery[SourceModelType]</code> <p>An instance of a BaseDiscovery subclass.</p> required <code>model_factory_instance</code> <code>BaseModelFactory[SourceModelType, FieldInfoType]</code> <p>An instance of a BaseModelFactory subclass.</p> required <code>module_mappings</code> <code>dict[str, str] | None</code> <p>Optional mapping of modules to remap imports.</p> required <code>base_model_class</code> <code>type[Model]</code> <p>The base Django model class to inherit from.</p> required <code>class_name_prefix</code> <code>str</code> <p>Prefix for generated Django model class names.</p> <code>'Django'</code> <code>enable_timescale</code> <code>bool</code> <p>Whether to enable TimescaleDB support for hypertables.</p> <code>True</code> Source code in <code>src/pydantic2django/core/base_generator.py</code> <pre><code>def __init__(\n    self,\n    output_path: str,\n    app_label: str,\n    filter_function: Callable[[type[SourceModelType]], bool] | None,\n    verbose: bool,\n    discovery_instance: BaseDiscovery[SourceModelType],\n    model_factory_instance: BaseModelFactory[SourceModelType, FieldInfoType],\n    module_mappings: dict[str, str] | None,\n    base_model_class: type[models.Model],\n    packages: list[str] | None = None,\n    class_name_prefix: str = \"Django\",\n    enable_timescale: bool = True,\n    # --- GFK flags ---\n    enable_gfk: bool = True,\n    gfk_policy: str | None = \"all_nested\",\n    gfk_threshold_children: int | None = 4,\n    gfk_value_mode: str | None = \"typed_columns\",\n    gfk_normalize_common_attrs: bool = False,\n):\n    \"\"\"\n    Initialize the base generator.\n\n    Args:\n        output_path: Path to output the generated models.py file.\n        packages: List of packages to scan for source models.\n        app_label: Django app label to use for the models.\n        filter_function: Optional function to filter which source models to include.\n        verbose: Print verbose output.\n        discovery_instance: An instance of a BaseDiscovery subclass.\n        model_factory_instance: An instance of a BaseModelFactory subclass.\n        module_mappings: Optional mapping of modules to remap imports.\n        base_model_class: The base Django model class to inherit from.\n        class_name_prefix: Prefix for generated Django model class names.\n        enable_timescale: Whether to enable TimescaleDB support for hypertables.\n    \"\"\"\n    self.output_path = output_path\n    self.packages = packages\n    self.app_label = app_label\n    self.filter_function = filter_function\n    self.verbose = verbose\n    self.discovery_instance = discovery_instance\n    self.model_factory_instance = model_factory_instance\n    # Base model class must be provided explicitly by subclass at call site.\n    self.base_model_class = base_model_class\n    self.class_name_prefix = class_name_prefix\n    self.carriers: list[ConversionCarrier[SourceModelType]] = []  # Stores results from model factory\n    # Feature flags\n    self.enable_timescale: bool = bool(enable_timescale)\n    # GFK feature flags\n    self.enable_gfk: bool = bool(enable_gfk)\n    self.gfk_policy: str | None = gfk_policy\n    self.gfk_threshold_children: int | None = gfk_threshold_children\n    self.gfk_value_mode: str | None = gfk_value_mode\n    self.gfk_normalize_common_attrs: bool = bool(gfk_normalize_common_attrs)\n\n    self.import_handler = ImportHandler(module_mappings=module_mappings)\n\n    # Initialize Jinja2 environment\n    # Look for templates in the django/templates subdirectory\n    # package_templates_dir = os.path.join(os.path.dirname(__file__), \"..\", \"templates\") # Old path\n    package_templates_dir = os.path.join(os.path.dirname(__file__), \"..\", \"django\", \"templates\")  # Corrected path\n\n    # If templates don't exist in the package, use the ones relative to the execution?\n    # This might need adjustment based on packaging/distribution.\n    # For now, assume templates are relative to the package structure.\n    if not os.path.exists(package_templates_dir):\n        # Fallback or raise error might be needed\n        package_templates_dir = os.path.join(pathlib.Path(__file__).parent.parent.absolute(), \"templates\")\n        if not os.path.exists(package_templates_dir):\n            logger.warning(\n                f\"Templates directory not found at expected location: {package_templates_dir}. Jinja might fail.\"\n            )\n\n    self.jinja_env = jinja2.Environment(\n        loader=jinja2.FileSystemLoader(package_templates_dir),\n        trim_blocks=True,\n        lstrip_blocks=True,\n    )\n\n    # Register common custom filters\n    self.jinja_env.filters[\"format_type_string\"] = TypeHandler.format_type_string\n    # Provide an escaping filter for embedding strings safely in generated Python code\n    from ..core.utils.strings import sanitize_string as _escape_py_str  # Local import to avoid cycles\n\n    self.jinja_env.filters[\"escape_py_str\"] = _escape_py_str\n    # Add more common filters if needed\n\n    # Add base model import\n    self.import_handler._add_type_import(self.base_model_class)\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.base_generator.BaseStaticGenerator.discover_models","title":"<code>discover_models()</code>","text":"<p>Discover source models using the configured discovery instance.</p> Source code in <code>src/pydantic2django/core/base_generator.py</code> <pre><code>def discover_models(self) -&gt; None:\n    \"\"\"Discover source models using the configured discovery instance.\"\"\"\n    if self.verbose:\n        logger.info(f\"Discovering models from packages: {self.packages}\")\n\n    # Corrected call matching BaseDiscovery signature\n    self.discovery_instance.discover_models(\n        self.packages or [],  # Pass empty list if None\n        app_label=self.app_label,\n        user_filters=self.filter_function,  # Keep as is for now\n    )\n\n    # Analyze dependencies after discovery\n    self.discovery_instance.analyze_dependencies()\n\n    if self.verbose:\n        logger.info(f\"Discovered {len(self.discovery_instance.filtered_models)} models after filtering.\")\n        if self.discovery_instance.filtered_models:\n            for name in self.discovery_instance.filtered_models.keys():\n                logger.info(f\"  - {name}\")\n        else:\n            logger.info(\"  (No models found or passed filter)\")\n        logger.info(\"Dependency analysis complete.\")\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.base_generator.BaseStaticGenerator.generate","title":"<code>generate()</code>","text":"<p>Main entry point: Generate and write the models file.</p> <p>Returns:</p> Type Description <code>str</code> <p>The path to the generated models file.</p> Source code in <code>src/pydantic2django/core/base_generator.py</code> <pre><code>def generate(self) -&gt; str:\n    \"\"\"\n    Main entry point: Generate and write the models file.\n\n    Returns:\n        The path to the generated models file.\n    \"\"\"\n    try:\n        content = self.generate_models_file()\n        self._write_models_file(content)\n        logger.info(f\"Successfully generated models file at {self.output_path}\")\n        return self.output_path\n    except Exception as e:\n        logger.exception(f\"Error generating models file: {e}\", exc_info=True)  # Use exc_info for traceback\n        raise\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.base_generator.BaseStaticGenerator.generate_model_definition","title":"<code>generate_model_definition(carrier)</code>","text":"<p>Generates a string definition for a single Django model using a template.</p> <p>Parameters:</p> Name Type Description Default <code>carrier</code> <code>ConversionCarrier[SourceModelType]</code> <p>The ConversionCarrier containing the generated Django model and context.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The string representation of the Django model definition.</p> Source code in <code>src/pydantic2django/core/base_generator.py</code> <pre><code>def generate_model_definition(self, carrier: ConversionCarrier[SourceModelType]) -&gt; str:\n    \"\"\"\n    Generates a string definition for a single Django model using a template.\n\n    Args:\n        carrier: The ConversionCarrier containing the generated Django model and context.\n\n    Returns:\n        The string representation of the Django model definition.\n    \"\"\"\n    if not carrier.django_model:\n        # It's possible a carrier exists only for context, handle gracefully.\n        source_name = self._get_source_model_name(carrier)\n        if carrier.model_context and carrier.model_context.context_fields:\n            logger.info(f\"Skipping Django model definition for {source_name} (likely context-only).\")\n            return \"\"\n        else:\n            logger.warning(\n                f\"Cannot generate model definition for {source_name}: django_model is missing in carrier.\"\n            )\n            return \"\"\n\n    django_model_name = self._clean_generic_type(carrier.django_model.__name__)\n    source_model_name = self._get_source_model_name(carrier)  # Get original name via abstract method\n\n    # --- Prepare Fields ---\n    fields_info = []\n    # Combine regular and relationship fields from the carrier\n    all_django_fields = {**carrier.django_fields, **carrier.relationship_fields}\n\n    for field_name, field_object in all_django_fields.items():\n        # The field_object is already an instantiated Django field\n        # Add (name, object) tuple directly for the template\n        fields_info.append((field_name, field_object))\n\n    # --- Prepare Meta ---\n    meta_options = {}\n    if hasattr(carrier.django_model, \"_meta\"):\n        model_meta = carrier.django_model._meta\n        meta_options = {\n            \"db_table\": getattr(model_meta, \"db_table\", f\"{self.app_label}_{django_model_name.lower()}\"),\n            \"app_label\": self.app_label,\n            \"verbose_name\": getattr(model_meta, \"verbose_name\", django_model_name),\n            \"verbose_name_plural\": getattr(model_meta, \"verbose_name_plural\", f\"{django_model_name}s\"),\n            # Add other meta options if needed\n        }\n\n    # --- Prepare Base Class Info ---\n    base_model_name = self.base_model_class.__name__\n    if carrier.django_model.__bases__ and carrier.django_model.__bases__[0] != models.Model:\n        # Use the immediate parent if it's not the absolute base 'models.Model'\n        # Assumes single inheritance for the generated model besides the ultimate base\n        parent_class = carrier.django_model.__bases__[0]\n        # Check if the parent is our intended base_model_class or something else\n        # This logic might need refinement depending on how complex the inheritance gets\n        if issubclass(parent_class, models.Model) and parent_class != models.Model:\n            base_model_name = parent_class.__name__\n            # Add import for the parent if it's not the configured base_model_class\n            if parent_class != self.base_model_class:\n                self.import_handler._add_type_import(parent_class)\n\n    # --- Get Subclass Specific Context ---\n    extra_context = self._get_model_definition_extra_context(carrier)\n\n    # --- Process Pending Multi-FK Unions and add to definitions dict ---\n    multi_fk_field_names = []  # Keep track for validation hint\n    validation_needed = False\n    if carrier.pending_multi_fk_unions:\n        validation_needed = True\n        for original_field_name, union_details in carrier.pending_multi_fk_unions:\n            pydantic_models = union_details.get(\"models\", [])\n            for pydantic_model in pydantic_models:\n                # Construct field name (e.g., original_name_relatedmodel)\n                fk_field_name = f\"{original_field_name}_{pydantic_model.__name__.lower()}\"\n                multi_fk_field_names.append(fk_field_name)\n                # Get corresponding Django model\n                pydantic_factory = cast(PydanticModelFactory, self.model_factory_instance)\n                django_model_rel = pydantic_factory.relationship_accessor.get_django_model_for_pydantic(\n                    pydantic_model\n                )\n                if not django_model_rel:\n                    logger.error(\n                        f\"Could not find Django model for Pydantic model {pydantic_model.__name__} referenced in multi-FK union for {original_field_name}. Skipping FK field.\"\n                    )\n                    continue\n                # Use string for model ref in kwargs\n                target_model_str = f\"'{django_model_rel._meta.app_label}.{django_model_rel.__name__}'\"\n                # Add import for the related Django model\n                self.import_handler._add_type_import(django_model_rel)\n\n                # Define FK kwargs (always null=True, blank=True)\n                # Use strings for values that need to be represented in code\n                fk_kwargs = {\n                    \"to\": target_model_str,\n                    \"on_delete\": \"models.SET_NULL\",  # Use string for template\n                    \"null\": True,\n                    \"blank\": True,\n                    # Generate related_name to avoid clashes\n                    \"related_name\": f\"'{carrier.django_model.__name__.lower()}_{fk_field_name}_set'\",  # Ensure related_name is quoted string\n                }\n                # Generate the definition string\n                fk_def_string = generate_field_definition_string(models.ForeignKey, fk_kwargs, self.app_label)\n                # Add to the main definitions dictionary\n                carrier.django_field_definitions[fk_field_name] = fk_def_string\n\n    # --- Prepare Final Context --- #\n    # Ensure the context uses the potentially updated definitions dict from the carrier\n    # Subclass _get_model_definition_extra_context should already provide this\n    # via `field_definitions=carrier.django_field_definitions`\n    template_context = {\n        \"model_name\": django_model_name,\n        \"pydantic_model_name\": source_model_name,\n        \"base_model_name\": base_model_name,\n        \"is_timescale_model\": bool(str(base_model_name).endswith(\"TimescaleBase\")),\n        \"meta\": meta_options,\n        \"app_label\": self.app_label,\n        \"multi_fk_field_names\": multi_fk_field_names,  # Pass names for validation hint\n        \"validation_needed\": validation_needed,  # Signal if validation needed\n        # Include extra context from subclass (should include field_definitions)\n        **extra_context,\n    }\n\n    # --- Render Template --- #\n    template = self.jinja_env.get_template(\"model_definition.py.j2\")\n    definition_str = template.render(**template_context)\n\n    # Add import for the original source model\n    self._add_source_model_import(carrier)\n\n    return definition_str\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.base_generator.BaseStaticGenerator.generate_models_file","title":"<code>generate_models_file()</code>","text":"<p>Generates the complete content for the models.py file. This method orchestrates discovery, model setup, definition generation, import collection, and template rendering. Subclasses might override this to add specific steps (like context class generation).</p> Source code in <code>src/pydantic2django/core/base_generator.py</code> <pre><code>def generate_models_file(self) -&gt; str:\n    \"\"\"\n    Generates the complete content for the models.py file.\n    This method orchestrates discovery, model setup, definition generation,\n    import collection, and template rendering.\n    Subclasses might override this to add specific steps (like context class generation).\n    \"\"\"\n    self.discover_models()  # Populates discovery instance\n    models_to_process = self._get_models_in_processing_order()  # Abstract method\n\n    # Reset state for this run\n    self.carriers = []\n    self.import_handler.extra_type_imports.clear()\n    self.import_handler.pydantic_imports.clear()\n    self.import_handler.context_class_imports.clear()\n    self.import_handler.imported_names.clear()\n    self.import_handler.processed_field_types.clear()\n\n    # Re-add base model import after clearing\n    self.import_handler._add_type_import(self.base_model_class)\n\n    model_definitions = []\n    django_model_names = []  # For __all__\n\n    # Setup Django models first (populates self.carriers)\n    for source_model in models_to_process:\n        self.setup_django_model(source_model)  # Calls factory, populates carrier\n\n    # Generate definitions from carriers\n    for carrier in self.carriers:\n        # Generate Django model definition if model exists\n        if carrier.django_model:\n            try:\n                model_def = self.generate_model_definition(carrier)  # Uses template\n                if model_def:  # Only add if definition was generated\n                    model_definitions.append(model_def)\n                    django_model_name = self._clean_generic_type(carrier.django_model.__name__)\n                    django_model_names.append(f\"'{django_model_name}'\")\n            except Exception as e:\n                source_name = self._get_source_model_name(carrier)\n                logger.error(f\"Error generating definition for source model {source_name}: {e}\", exc_info=True)\n\n        # Subclasses might add context class generation here by overriding this method\n        # or by generate_model_definition adding context-related imports.\n\n    # Deduplicate definitions\n    unique_model_definitions = self._deduplicate_definitions(model_definitions)\n\n    # Deduplicate imports gathered during the process\n    imports = self.import_handler.deduplicate_imports()\n\n    # Prepare context using subclass method (_prepare_template_context)\n    template_context = self._prepare_template_context(unique_model_definitions, django_model_names, imports)\n\n    # Add common context items\n    template_context.update(\n        {\n            \"generation_timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n            \"base_model_module\": self.base_model_class.__module__,\n            \"base_model_name\": self.base_model_class.__name__,\n            \"extra_type_imports\": sorted(self.import_handler.extra_type_imports),\n            # Add other common items as needed\n        }\n    )\n\n    # Render the main template\n    template = self.jinja_env.get_template(\"models_file.py.j2\")\n    return template.render(**template_context)\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.base_generator.BaseStaticGenerator.setup_django_model","title":"<code>setup_django_model(source_model)</code>","text":"<p>Uses the model factory to create a Django model representation from a source model.</p> <p>Parameters:</p> Name Type Description Default <code>source_model</code> <code>SourceModelType</code> <p>The source model instance (e.g., Pydantic class, Dataclass).</p> required <p>Returns:</p> Type Description <code>ConversionCarrier[SourceModelType] | None</code> <p>A ConversionCarrier containing the results, or None if creation failed.</p> Source code in <code>src/pydantic2django/core/base_generator.py</code> <pre><code>def setup_django_model(self, source_model: SourceModelType) -&gt; ConversionCarrier[SourceModelType] | None:\n    \"\"\"\n    Uses the model factory to create a Django model representation from a source model.\n\n    Args:\n        source_model: The source model instance (e.g., Pydantic class, Dataclass).\n\n    Returns:\n        A ConversionCarrier containing the results, or None if creation failed.\n    \"\"\"\n    source_model_name = getattr(source_model, \"__name__\", str(source_model))\n    if self.verbose:\n        logger.info(f\"Setting up Django model for {source_model_name}\")\n\n    # Instantiate the carrier here\n    carrier = ConversionCarrier(\n        source_model=cast(type[SourceModelType], source_model),\n        meta_app_label=self.app_label,\n        base_django_model=self.base_model_class,\n        class_name_prefix=self.class_name_prefix,\n        # Add other defaults/configs if needed, e.g., strict mode\n        strict=False,  # Example default\n        # GFK flags\n        enable_gfk=self.enable_gfk,\n        gfk_policy=self.gfk_policy,\n        gfk_threshold_children=self.gfk_threshold_children,\n        gfk_value_mode=self.gfk_value_mode,\n        gfk_normalize_common_attrs=self.gfk_normalize_common_attrs,\n    )\n\n    try:\n        # Use the factory to process the source model and populate the carrier\n        self.model_factory_instance.make_django_model(carrier)  # Pass carrier to factory\n\n        if carrier.django_model:\n            self.carriers.append(carrier)\n            if self.verbose:\n                logger.info(f\"Successfully processed {source_model_name} -&gt; {carrier.django_model.__name__}\")\n            return carrier\n        else:\n            # Log if model creation resulted in None (e.g., only context fields)\n            # Check carrier.context_fields or carrier.invalid_fields for details\n            if carrier.context_fields and not carrier.django_fields and not carrier.relationship_fields:\n                logger.info(f\"Skipped Django model class for {source_model_name} - only context fields found.\")\n            elif carrier.invalid_fields:\n                logger.warning(\n                    f\"Skipped Django model class for {source_model_name} due to invalid fields: {carrier.invalid_fields}\"\n                )\n            else:\n                logger.warning(f\"Django model was not generated for {source_model_name} for unknown reasons.\")\n            return None  # Return None if no Django model was created\n\n    except Exception as e:\n        logger.error(f\"Error processing {source_model_name} with factory: {e}\", exc_info=True)\n        return None\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.discovery.BaseDiscovery.analyze_dependencies","title":"<code>analyze_dependencies()</code>  <code>abstractmethod</code>","text":"<p>Analyze dependencies between the filtered models.</p> Source code in <code>src/pydantic2django/core/discovery.py</code> <pre><code>@abc.abstractmethod\ndef analyze_dependencies(self) -&gt; None:\n    \"\"\"Analyze dependencies between the filtered models.\"\"\"\n    pass\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.discovery.BaseDiscovery.discover_models","title":"<code>discover_models(packages, app_label, user_filters=None)</code>","text":"<p>Discover target models in the specified packages, applying default and user filters.</p> Source code in <code>src/pydantic2django/core/discovery.py</code> <pre><code>def discover_models(\n    self,\n    packages: list[str],\n    app_label: str,  # Keep for potential use in filters or subclasses\n    user_filters: Optional[Union[Callable[[type[TModel]], bool], list[Callable[[type[TModel]], bool]]]] = None,\n) -&gt; None:\n    \"\"\"Discover target models in the specified packages, applying default and user filters.\"\"\"\n    self.all_models = {}\n    self.filtered_models = {}\n    self.dependencies = {}\n\n    # Normalize user_filters to always be a list\n    if user_filters is None:\n        filters = []\n    elif isinstance(user_filters, list):\n        filters = user_filters\n    else:  # It's a single callable\n        filters = [user_filters]\n\n    model_type_name = getattr(self, \"__name__\", \"TargetModel\")  # Get class name for logging\n\n    logger.info(f\"Starting {model_type_name} discovery in packages: {packages}\")\n    for package_name in packages:\n        try:\n            package = importlib.import_module(package_name)\n            logger.debug(f\"Scanning package: {package_name}\")\n\n            for _importer, modname, _ispkg in pkgutil.walk_packages(\n                path=package.__path__ if hasattr(package, \"__path__\") else None,\n                prefix=package.__name__ + \".\",\n                onerror=lambda name: logger.warning(f\"Error accessing module {name}\"),\n            ):\n                try:\n                    module = importlib.import_module(modname)\n                    for name, obj in inspect.getmembers(module):\n                        # Use the subclass implementation to check if it's the right model type\n                        if self._is_target_model(obj):\n                            model_qualname = f\"{modname}.{name}\"\n                            if model_qualname not in self.all_models:\n                                self.all_models[model_qualname] = obj\n                                logger.debug(f\"Discovered potential {model_type_name}: {model_qualname}\")\n\n                                # Apply filters sequentially using subclass implementation\n                                is_eligible = self._default_eligibility_filter(obj)\n                                if is_eligible:\n                                    for user_filter in filters:\n                                        try:\n                                            if not user_filter(obj):\n                                                is_eligible = False\n                                                logger.debug(\n                                                    f\"Filtered out {model_type_name} by user filter: {model_qualname}\"\n                                                )\n                                                break  # No need to check other filters\n                                        except Exception as filter_exc:\n                                            # Attempt to get filter name, default to repr\n                                            filter_name = getattr(user_filter, \"__name__\", repr(user_filter))\n                                            logger.error(\n                                                f\"Error applying user filter {filter_name} to {model_qualname}: {filter_exc}\",\n                                                exc_info=True,\n                                            )\n                                            is_eligible = False  # Exclude on filter error\n                                            break\n\n                                if is_eligible:\n                                    self.filtered_models[model_qualname] = obj\n                                    logger.debug(f\"Added eligible {model_type_name}: {model_qualname}\")\n\n                except ImportError as e:\n                    logger.warning(f\"Could not import module {modname}: {e}\")\n                except Exception as e:\n                    logger.error(f\"Error inspecting module {modname} for {model_type_name}s: {e}\", exc_info=True)\n\n        except ImportError:\n            logger.error(f\"Package {package_name} not found.\")\n        except Exception as e:\n            logger.error(f\"Error discovering {model_type_name}s in package {package_name}: {e}\", exc_info=True)\n\n    logger.info(\n        f\"{model_type_name} discovery complete. Found {len(self.all_models)} total models, {len(self.filtered_models)} after filtering.\"\n    )\n\n    # Hooks for subclass-specific post-processing if needed\n    self._post_discovery_hook()\n\n    # Resolve forward references if applicable (might be subclass specific)\n    self._resolve_forward_refs()\n\n    # Build dependency graph for filtered models\n    self.analyze_dependencies()\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.discovery.BaseDiscovery.get_models_in_registration_order","title":"<code>get_models_in_registration_order()</code>  <code>abstractmethod</code>","text":"<p>Return filtered models sorted topologically based on dependencies.</p> Source code in <code>src/pydantic2django/core/discovery.py</code> <pre><code>@abc.abstractmethod\ndef get_models_in_registration_order(self) -&gt; list[type[TModel]]:\n    \"\"\"Return filtered models sorted topologically based on dependencies.\"\"\"\n    pass\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.factories.BaseModelFactory.__init__","title":"<code>__init__(field_factory, *args, **kwargs)</code>","text":"<p>Initializes the model factory with a compatible field factory. Allows subclasses to accept additional dependencies.</p> Source code in <code>src/pydantic2django/core/factories.py</code> <pre><code>def __init__(self, field_factory: BaseFieldFactory[SourceFieldType], *args, **kwargs):\n    \"\"\"\n    Initializes the model factory with a compatible field factory.\n    Allows subclasses to accept additional dependencies.\n    \"\"\"\n    self.field_factory = field_factory\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.factories.BaseModelFactory.make_django_model","title":"<code>make_django_model(carrier)</code>","text":"<p>Orchestrates the Django model creation process. Subclasses implement _process_source_fields and _build_model_context. Handles caching.</p> Source code in <code>src/pydantic2django/core/factories.py</code> <pre><code>def make_django_model(self, carrier: ConversionCarrier[SourceModelType]) -&gt; None:\n    \"\"\"\n    Orchestrates the Django model creation process.\n    Subclasses implement _process_source_fields and _build_model_context.\n    Handles caching.\n    \"\"\"\n    model_key = carrier.model_key\n    logger.debug(f\"Attempting to create Django model for {model_key}\")\n\n    # TODO: Cache handling needs refinement - how to access subclass cache?\n    # For now, skipping cache check in base class.\n    # if model_key in self._converted_models and not carrier.existing_model:\n    #     # ... update carrier from cache ...\n    #     return\n\n    # Reset results on carrier\n    carrier.django_fields = {}\n    carrier.relationship_fields = {}\n    carrier.context_fields = {}\n    carrier.invalid_fields = []\n    carrier.django_meta_class = None\n    carrier.django_model = None\n    carrier.model_context = None\n    carrier.django_field_definitions = {}  # Reset definitions dict\n\n    # Core Steps\n    self._process_source_fields(carrier)\n    self._handle_field_collisions(carrier)\n    self._create_django_meta(carrier)\n    self._assemble_django_model_class(carrier)\n\n    # Build context only if model assembly succeeded\n    if carrier.django_model:\n        self._build_model_context(carrier)\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.factories.BaseFieldFactory.create_field","title":"<code>create_field(field_info, model_name, carrier)</code>  <code>abstractmethod</code>","text":"<p>Convert a source field type into a Django Field.</p> <p>Parameters:</p> Name Type Description Default <code>field_info</code> <code>SourceFieldType</code> <p>The field information object from the source (Pydantic/Dataclass).</p> required <code>model_name</code> <code>str</code> <p>The name of the source model containing the field.</p> required <code>carrier</code> <code>ConversionCarrier</code> <p>The conversion carrier for context (e.g., app_label, relationships).</p> required <p>Returns:</p> Type Description <code>FieldConversionResult</code> <p>A FieldConversionResult containing the generated Django field or context/error info.</p> Source code in <code>src/pydantic2django/core/factories.py</code> <pre><code>@abstractmethod\ndef create_field(\n    self, field_info: SourceFieldType, model_name: str, carrier: ConversionCarrier\n) -&gt; FieldConversionResult:\n    \"\"\"\n    Convert a source field type into a Django Field.\n\n    Args:\n        field_info: The field information object from the source (Pydantic/Dataclass).\n        model_name: The name of the source model containing the field.\n        carrier: The conversion carrier for context (e.g., app_label, relationships).\n\n    Returns:\n        A FieldConversionResult containing the generated Django field or context/error info.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.factories.ConversionCarrier.model_key","title":"<code>model_key()</code>","text":"<p>Generate a unique key for the source model.</p> Source code in <code>src/pydantic2django/core/factories.py</code> <pre><code>def model_key(self) -&gt; str:\n    \"\"\"Generate a unique key for the source model.\"\"\"\n    module = getattr(self.source_model, \"__module__\", \"?\")\n    name = getattr(self.source_model, \"__name__\", \"UnknownModel\")\n    return f\"{module}.{name}\"\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.factories.FieldConversionResult.add_import","title":"<code>add_import(module, name)</code>","text":"<p>Helper to add an import to this result.</p> Source code in <code>src/pydantic2django/core/factories.py</code> <pre><code>def add_import(self, module: str, name: str):\n    \"\"\"Helper to add an import to this result.\"\"\"\n    if not module or module == \"builtins\":\n        return\n    current_names = self.required_imports.setdefault(module, [])\n    if name not in current_names:\n        current_names.append(name)\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.factories.FieldConversionResult.add_import_for_obj","title":"<code>add_import_for_obj(obj)</code>","text":"<p>Helper to add an import for a given object (class, function, etc.).</p> Source code in <code>src/pydantic2django/core/factories.py</code> <pre><code>def add_import_for_obj(self, obj: Any):\n    \"\"\"Helper to add an import for a given object (class, function, etc.).\"\"\"\n    if hasattr(obj, \"__module__\") and hasattr(obj, \"__name__\"):\n        module = obj.__module__\n        name = obj.__name__\n        self.add_import(module, name)\n    else:\n        logger.warning(f\"Could not determine import for object: {obj!r}\")\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.bidirectional_mapper.BidirectionalTypeMapper.get_django_mapping","title":"<code>get_django_mapping(python_type, field_info=None, parent_pydantic_model=None)</code>","text":"<p>Get the corresponding Django Field type and constructor kwargs for a Python type.</p> Source code in <code>src/pydantic2django/core/bidirectional_mapper.py</code> <pre><code>def get_django_mapping(\n    self,\n    python_type: Any,\n    field_info: Optional[FieldInfo] = None,\n    parent_pydantic_model: Optional[type[BaseModel]] = None,  # Add parent model for self-ref check\n) -&gt; tuple[type[models.Field], dict[str, Any]]:\n    \"\"\"Get the corresponding Django Field type and constructor kwargs for a Python type.\"\"\"\n    processed_type_info = TypeHandler.process_field_type(python_type)\n    original_py_type = python_type\n    is_optional = processed_type_info[\"is_optional\"]\n    is_list = processed_type_info[\"is_list\"]\n\n    unit_cls = None  # Initialize unit_cls\n    base_py_type = original_py_type  # Start with original\n    union_details = None  # Store details if it's a Union[BaseModel,...]\n    gfk_details = None\n\n    # --- Check for M2M case FIRST ---\n    if is_list:\n        # Get the type inside the list, handling Optional[List[T]]\n        list_inner_type = original_py_type\n        if is_optional:\n            args_check = get_args(list_inner_type)\n            list_inner_type = next((arg for arg in args_check if arg is not type(None)), Any)\n\n        # Now get the type *inside* the list\n        list_args = get_args(list_inner_type)  # Should be List[T]\n        inner_type = list_args[0] if list_args else Any\n\n        # --- GFK Check: Is the inner type a Union of known models? ---\n        inner_origin = get_origin(inner_type)\n        inner_args = get_args(inner_type)\n        if inner_origin in (Union, UnionType) and inner_args:\n            union_models = []\n            other_types = [\n                arg\n                for arg in inner_args\n                if not (\n                    inspect.isclass(arg)\n                    and (issubclass(arg, BaseModel) or dataclasses.is_dataclass(arg))\n                    and self.relationship_accessor.is_source_model_known(arg)\n                )\n            ]\n            union_models = [arg for arg in inner_args if arg not in other_types]\n\n            if union_models and not other_types:\n                logger.debug(f\"Detected GFK List[Union[...]] with models: {union_models}\")\n                gfk_details = {\n                    \"type\": \"gfk\",\n                    \"models\": union_models,\n                    \"is_optional\": is_optional,\n                }\n                unit_cls = JsonFieldMapping\n                base_py_type = original_py_type\n\n        if unit_cls is None:\n            # --- M2M Check: Is the inner type a known related BaseModel OR Dataclass? ---\n            if (\n                inspect.isclass(inner_type)\n                and (issubclass(inner_type, BaseModel) or dataclasses.is_dataclass(inner_type))\n                and self.relationship_accessor.is_source_model_known(inner_type)\n            ):\n                unit_cls = ManyToManyFieldMapping\n                base_py_type = inner_type\n                logger.debug(f\"Detected List[RelatedModel] ({inner_type.__name__}), mapping to ManyToManyField.\")\n            else:\n                # --- Fallback for other lists ---\n                unit_cls = JsonFieldMapping\n                base_py_type = original_py_type\n                logger.debug(f\"Detected List of non-models ({original_py_type}), mapping directly to JSONField.\")\n\n    # --- If not a list, find unit for the base (non-list) type ---\n    if unit_cls is None:\n        # --- Handle Union[BaseModel,...] Signaling FIRST --- #\n        simplified_base_type = processed_type_info[\"type_obj\"]\n        simplified_origin = get_origin(simplified_base_type)\n        simplified_args = get_args(simplified_base_type)\n\n        logger.debug(\n            f\"Checking simplified type for Union[Model,...]: {simplified_base_type!r} (Origin: {simplified_origin})\"\n        )\n        # Log the is_optional flag determined by TypeHandler\n        logger.debug(f\"TypeHandler returned is_optional: {is_optional} for original type: {original_py_type!r}\")\n\n        # Check if the simplified origin is Union[...] or T | U\n        if simplified_origin in (Union, UnionType) and simplified_args:\n            union_models = []\n            other_types_in_union = []\n\n            for arg in simplified_args:\n                # We already unwrapped Optional, so no need to check for NoneType here\n                logger.debug(f\"-- Checking simplified Union arg: {arg!r}\")\n\n                # Check if arg is a known BaseModel or Dataclass\n                is_class = inspect.isclass(arg)\n                # Need try-except for issubclass with non-class types\n                is_pyd_model = False\n                is_dc = False\n                is_known_by_accessor = False\n                if is_class:\n                    try:\n                        is_pyd_model = issubclass(arg, BaseModel)\n                        is_dc = dataclasses.is_dataclass(arg)\n                        # Only check accessor if it's a model type\n                        if is_pyd_model or is_dc:\n                            is_known_by_accessor = self.relationship_accessor.is_source_model_known(arg)\n                    except TypeError:\n                        # issubclass might fail if arg is not a class (e.g., a type alias)\n                        pass  # Keep flags as False\n\n                logger.debug(\n                    f\"    is_class: {is_class}, is_pyd_model: {is_pyd_model}, is_dc: {is_dc}, is_known_by_accessor: {is_known_by_accessor}\"\n                )\n\n                is_known_model_or_dc = is_class and (is_pyd_model or is_dc) and is_known_by_accessor\n\n                if is_known_model_or_dc:\n                    logger.debug(f\"    -&gt; Added {arg.__name__} to union_models\")  # More specific logging\n                    union_models.append(arg)\n                else:\n                    # Make sure we don't add NoneType here if Optional wasn't fully handled upstream somehow\n                    if arg is not type(None):\n                        logger.debug(f\"    -&gt; Added {arg!r} to other_types_in_union\")  # More specific logging\n                        other_types_in_union.append(arg)\n\n            # --- EDIT: Only set union_details IF ONLY models were found ---\n            # Add logging just before the check\n            logger.debug(\n                f\"Finished Union arg loop. union_models: {[m.__name__ for m in union_models]}, other_types: {other_types_in_union}\"\n            )\n            if union_models and not other_types_in_union:\n                logger.debug(\n                    f\"Detected Union containing ONLY known models: {union_models}. Generating _union_details signal.\"\n                )\n                union_details = {\n                    \"type\": \"multi_fk\",\n                    \"models\": union_models,\n                    \"is_optional\": is_optional,  # Use the flag determined earlier\n                }\n                # Log the created union_details\n                logger.debug(f\"Generated union_details: {union_details!r}\")\n                # Set unit_cls to JsonFieldMapping for model unions\n                unit_cls = JsonFieldMapping\n                base_py_type = original_py_type\n                logger.debug(\"Setting unit_cls to JsonFieldMapping for model union\")\n\n        # --- Now, find the unit for the (potentially complex) base type --- #\n        # Only find unit if not already set (e.g. by model union handling)\n        if unit_cls is None:\n            # Determine the type to use for finding the unit.\n            # If it was M2M or handled List, unit_cls is already set.\n            # Otherwise, use the processed type_obj which handles Optional/Annotated.\n            type_for_unit_finding = processed_type_info[\"type_obj\"]\n            logger.debug(f\"Type used for finding unit (after Union check): {type_for_unit_finding!r}\")\n\n            # Use the simplified base type after processing Optional/Annotated\n            base_py_type = type_for_unit_finding\n            logger.debug(f\"Finding unit for base type: {base_py_type!r} with field_info: {field_info}\")\n            unit_cls = self._find_unit_for_pydantic_type(base_py_type, field_info)\n\n    # --- Check if a unit was found --- #\n    if not unit_cls:\n        # If _find_unit_for_pydantic_type returned None, fallback to JSON\n        logger.warning(\n            f\"No mapping unit found by scoring for base type {base_py_type} \"\n            f\"(derived from {original_py_type}), falling back to JSONField.\"\n        )\n        unit_cls = JsonFieldMapping\n        # Consider raising MappingError if even JSON doesn't fit?\n        # raise MappingError(f\"Could not find mapping unit for Python type: {base_py_type}\")\n\n    # &gt;&gt; Add logging to check selected unit &lt;&lt;\n    logger.info(f\"Selected Unit for {original_py_type}: {unit_cls.__name__ if unit_cls else 'None'}\")\n\n    instance_unit = unit_cls()  # Instantiate to call methods\n\n    # --- Determine Django Field Type ---\n    # Start with the type defined on the selected unit class\n    django_field_type = instance_unit.django_field_type\n\n    # --- Get Kwargs (before potentially overriding field type for Enums) ---\n    kwargs = instance_unit.pydantic_to_django_kwargs(base_py_type, field_info)\n\n    # --- Add Union or GFK Details if applicable --- #\n    if union_details:\n        logger.info(\"Adding _union_details to kwargs.\")\n        kwargs[\"_union_details\"] = union_details\n        kwargs[\"null\"] = union_details.get(\"is_optional\", False)\n        kwargs[\"blank\"] = union_details.get(\"is_optional\", False)\n    elif gfk_details:\n        logger.info(\"Adding _gfk_details to kwargs.\")\n        kwargs[\"_gfk_details\"] = gfk_details\n        # GFK fields are placeholder JSONFields, nullability is based on Optional status\n        kwargs[\"null\"] = is_optional\n        kwargs[\"blank\"] = is_optional\n    else:\n        logger.debug(\"union_details and gfk_details are None, skipping addition to kwargs.\")\n        # --- Special Handling for Enums/Literals (Only if not multi-FK/GFK union) --- #\n        if unit_cls is EnumFieldMapping:\n            field_type_hint = kwargs.pop(\"_field_type_hint\", None)\n            if field_type_hint and isinstance(field_type_hint, type) and issubclass(field_type_hint, models.Field):\n                # Directly use the hinted field type if valid\n                logger.debug(\n                    f\"Using hinted field type {field_type_hint.__name__} from EnumFieldMapping for {base_py_type}.\"\n                )\n                django_field_type = field_type_hint\n                # Ensure max_length is removed if type becomes IntegerField\n                if django_field_type == models.IntegerField:\n                    kwargs.pop(\"max_length\", None)\n            else:\n                logger.warning(\"EnumFieldMapping selected but failed to get valid field type hint from kwargs.\")\n\n        # --- Handle Relationships (Only if not multi-FK union) --- #\n        # This section needs to run *after* unit selection but *before* final nullability checks\n        if unit_cls in (ForeignKeyMapping, OneToOneFieldMapping, ManyToManyFieldMapping):\n            # Ensure base_py_type is the related model (set during M2M check or found by find_unit for FK/O2O)\n            related_py_model = base_py_type\n\n            # Check if it's a known Pydantic BaseModel OR a known Dataclass\n            is_pyd_or_dc = inspect.isclass(related_py_model) and (\n                issubclass(related_py_model, BaseModel) or dataclasses.is_dataclass(related_py_model)\n            )\n            if not is_pyd_or_dc:\n                raise MappingError(\n                    f\"Relationship mapping unit {unit_cls.__name__} selected, but base type {related_py_model} is not a known Pydantic model or Dataclass.\"\n                )\n\n            # Check for self-reference BEFORE trying to get the Django model\n            is_self_ref = parent_pydantic_model is not None and related_py_model == parent_pydantic_model\n\n            if is_self_ref:\n                model_ref = \"self\"\n                # Get the target Django model name for logging/consistency if possible, but use 'self'\n                # Check if the related model is a Pydantic BaseModel or a dataclass\n                if inspect.isclass(related_py_model) and issubclass(related_py_model, BaseModel):\n                    target_django_model = self.relationship_accessor.get_django_model_for_pydantic(\n                        cast(type[BaseModel], related_py_model)\n                    )\n                elif dataclasses.is_dataclass(related_py_model):\n                    target_django_model = self.relationship_accessor.get_django_model_for_dataclass(\n                        related_py_model\n                    )\n                else:\n                    # This case should ideally not be reached due to earlier checks, but handle defensively\n                    target_django_model = None\n                    logger.warning(\n                        f\"Self-reference check: related_py_model '{related_py_model}' is neither BaseModel nor dataclass.\"\n                    )\n\n                logger.debug(\n                    f\"Detected self-reference for {related_py_model.__name__ if inspect.isclass(related_py_model) else related_py_model} \"\n                    f\"(Django: {getattr(target_django_model, '__name__', 'N/A')}), using 'self'.\"\n                )\n            else:\n                # Get target Django model based on source type (Pydantic or Dataclass)\n                target_django_model = None\n                # Ensure related_py_model is actually a type before issubclass check\n                if inspect.isclass(related_py_model) and issubclass(related_py_model, BaseModel):\n                    # Cast to satisfy type checker, as we've confirmed it's a BaseModel subclass here\n                    target_django_model = self.relationship_accessor.get_django_model_for_pydantic(\n                        cast(type[BaseModel], related_py_model)\n                    )\n                elif dataclasses.is_dataclass(related_py_model):\n                    target_django_model = self.relationship_accessor.get_django_model_for_dataclass(\n                        related_py_model\n                    )\n\n                if not target_django_model:\n                    raise MappingError(\n                        f\"Cannot map relationship: No corresponding Django model found for source model \"\n                        f\"{related_py_model.__name__} in RelationshipConversionAccessor.\"\n                    )\n                # Use lowercase label for internal consistency with existing expectations\n                model_ref = getattr(target_django_model._meta, \"label_lower\", target_django_model.__name__)\n\n            kwargs[\"to\"] = model_ref\n            django_field_type = unit_cls.django_field_type  # Re-confirm M2MField, FK, O2O type\n            # Set on_delete for FK/O2O based on Optional status\n            if unit_cls in (ForeignKeyMapping, OneToOneFieldMapping):\n                # Default to CASCADE for non-optional, SET_NULL for optional (matching test expectation)\n                kwargs[\"on_delete\"] = (\n                    models.SET_NULL if is_optional else models.CASCADE\n                )  # Changed PROTECT to CASCADE\n\n    # --- Final Adjustments (Nullability, etc.) --- #\n    # Apply nullability. M2M fields cannot be null in Django.\n    # Do not override nullability if it was already forced by a multi-FK union\n    if django_field_type != models.ManyToManyField and not union_details:\n        kwargs[\"null\"] = is_optional\n        # Explicitly set blank based on optionality.\n        # Simplified logic: Mirror the null assignment directly\n        kwargs[\"blank\"] = is_optional\n\n    logger.debug(\n        f\"FINAL RETURN from get_django_mapping: Type={django_field_type}, Kwargs={kwargs}\"\n    )  # Added final state logging\n    return django_field_type, kwargs\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.bidirectional_mapper.BidirectionalTypeMapper.get_pydantic_mapping","title":"<code>get_pydantic_mapping(dj_field)</code>","text":"<p>Get the corresponding Pydantic type hint and FieldInfo kwargs for a Django Field.</p> Source code in <code>src/pydantic2django/core/bidirectional_mapper.py</code> <pre><code>def get_pydantic_mapping(self, dj_field: models.Field) -&gt; tuple[Any, dict[str, Any]]:\n    \"\"\"Get the corresponding Pydantic type hint and FieldInfo kwargs for a Django Field.\"\"\"\n    dj_field_type = type(dj_field)\n    is_optional = dj_field.null\n    is_choices = bool(dj_field.choices)\n\n    # --- Find base unit (ignoring choices for now) ---\n    # Find the mapping unit based on the specific Django field type MRO\n    # This gives us the correct underlying Python type (str, int, etc.)\n    base_unit_cls = self._find_unit_for_django_field(dj_field_type)\n\n    if not base_unit_cls:\n        logger.warning(f\"No base mapping unit for {dj_field_type.__name__}, falling back to Any.\")\n        pydantic_type = Optional[Any] if is_optional else Any\n        return pydantic_type, {}\n\n    base_instance_unit = base_unit_cls()\n    # Get the base Pydantic type from this unit\n    final_pydantic_type = base_instance_unit.python_type\n\n    # --- Determine Final Pydantic Type Adjustments --- #\n    # (Relationships, AutoPK, Optional wrapper)\n\n    # Handle choices FIRST to determine the core type before Optional wrapping\n    if is_choices:\n        # Default to base type, override if valid choices found\n        final_pydantic_type = base_instance_unit.python_type\n        if dj_field.choices:  # Explicit check before iteration\n            try:\n                choice_values = tuple(choice[0] for choice in dj_field.choices)\n                if choice_values:  # Ensure the tuple is not empty\n                    final_pydantic_type = Literal[choice_values]  # type: ignore\n                    logger.debug(f\"Mapped choices for '{dj_field.name}' to Pydantic type: {final_pydantic_type}\")\n                else:\n                    logger.warning(\n                        f\"Field '{dj_field.name}' has choices defined, but extracted values are empty. Falling back.\"\n                    )\n                    # Keep final_pydantic_type as base type\n            except Exception as e:\n                logger.warning(f\"Failed to extract choices for field '{dj_field.name}'. Error: {e}. Falling back.\")\n                # Keep final_pydantic_type as base type\n        # If dj_field.choices was None/empty initially, final_pydantic_type remains the base type\n    else:\n        # Get the base Pydantic type from this unit if not choices\n        final_pydantic_type = base_instance_unit.python_type\n\n    # 1. Handle Relationships first, as they determine the core type\n    if base_unit_cls in (ForeignKeyMapping, OneToOneFieldMapping, ManyToManyFieldMapping):\n        related_dj_model = getattr(dj_field, \"related_model\", None)\n        if not related_dj_model:\n            raise MappingError(f\"Cannot determine related Django model for field '{dj_field.name}'\")\n\n        # Resolve 'self' reference\n        if related_dj_model == \"self\":\n            # We need the Django model class that dj_field belongs to.\n            # This info isn't directly passed, so this approach might be limited.\n            # Assuming self-reference points to the same type hierarchy for now.\n            # A better solution might need the model context passed down.\n            logger.warning(\n                f\"Handling 'self' reference for field '{dj_field.name}'. Mapping might be incomplete without parent model context.\"\n            )\n            # Attempt to get Pydantic model mapped to the field's owner model if possible (heuristically)\n            # This is complex and potentially fragile.\n            # For now, let's use a placeholder or raise an error if needed strictly.\n            # Sticking with the base type (e.g., Any or int for PK) might be safer without context.\n            # Use the base type (likely PK int/uuid) as the fallback type here\n            target_pydantic_model = base_instance_unit.python_type\n            logger.debug(f\"Using Any as placeholder for 'self' reference '{dj_field.name}'\")\n        else:\n            target_pydantic_model = self.relationship_accessor.get_pydantic_model_for_django(related_dj_model)\n\n        if not target_pydantic_model or target_pydantic_model is Any:\n            if related_dj_model != \"self\":  # Avoid redundant warning for self\n                logger.warning(\n                    f\"Cannot map relationship: No corresponding Pydantic model found for Django model \"\n                    f\"'{related_dj_model._meta.label if hasattr(related_dj_model, '_meta') else related_dj_model.__name__}'. \"\n                    f\"Using placeholder '{final_pydantic_type}'.\"\n                )\n            # Keep final_pydantic_type as the base unit's python_type (e.g., int for FK)\n        else:\n            if base_unit_cls == ManyToManyFieldMapping:\n                final_pydantic_type = list[target_pydantic_model]\n            else:  # FK or O2O\n                # Keep the PK type (e.g., int) if target model not found,\n                # otherwise use the target Pydantic model type.\n                final_pydantic_type = target_pydantic_model  # This should now be the related model type\n            logger.debug(f\"Mapped relationship field '{dj_field.name}' to Pydantic type: {final_pydantic_type}\")\n\n    # 2. AutoPK override (after relationship resolution)\n    is_auto_pk = dj_field.primary_key and isinstance(\n        dj_field, (models.AutoField, models.BigAutoField, models.SmallAutoField)\n    )\n    if is_auto_pk:\n        final_pydantic_type = Optional[int]\n        logger.debug(f\"Mapped AutoPK field '{dj_field.name}' to {final_pydantic_type}\")\n        is_optional = True  # AutoPKs are always optional in Pydantic input\n\n    # 3. Apply Optional[...] wrapper if necessary (AFTER relationship/AutoPK)\n    # Do not wrap M2M lists or already Optional AutoPKs in Optional[] again.\n    # Also, don't wrap if the type is already Literal (choices handled Optionality) - NO, wrap Literal too if null=True\n    if is_optional and not is_auto_pk:  # Check if is_choices? No, optional applies to literal too.\n        origin = get_origin(final_pydantic_type)\n        args = get_args(final_pydantic_type)\n        is_already_optional = origin is Optional or origin is UnionType and type(None) in args\n\n        if not is_already_optional:\n            final_pydantic_type = Optional[final_pydantic_type]\n            logger.debug(f\"Wrapped type for '{dj_field.name}' in Optional: {final_pydantic_type}\")\n\n    # --- Generate FieldInfo Kwargs --- #\n    # Use EnumFieldMapping logic for kwargs ONLY if choices exist,\n    # otherwise use the base unit determined earlier. # --&gt; NO, always use base unit for kwargs now. Literal type handles choices.\n    # kwargs_unit_cls = EnumFieldMapping if is_choices else base_unit_cls # OLD logic\n    instance_unit = base_unit_cls()  # Use the base unit (e.g., StrFieldMapping) for base kwargs\n\n    field_info_kwargs = instance_unit.django_to_pydantic_field_info_kwargs(dj_field)\n\n    # --- Explicitly cast title (verbose_name) and description (help_text) --- #\n    if field_info_kwargs.get(\"title\") is not None:\n        field_info_kwargs[\"title\"] = str(field_info_kwargs[\"title\"])\n        logger.debug(f\"Ensured title is str for '{dj_field.name}': {field_info_kwargs['title']}\")\n    if field_info_kwargs.get(\"description\") is not None:\n        field_info_kwargs[\"description\"] = str(field_info_kwargs[\"description\"])\n        logger.debug(f\"Ensured description is str for '{dj_field.name}': {field_info_kwargs['description']}\")\n    # --- End Casting --- #\n\n    # --- Keep choices in json_schema_extra even when using Literal ---\n    # This preserves the (value, label) mapping as metadata alongside the Literal type.\n    if (\n        is_choices\n        and \"json_schema_extra\" in field_info_kwargs\n        and \"choices\" in field_info_kwargs[\"json_schema_extra\"]\n    ):\n        logger.debug(f\"Kept choices in json_schema_extra for Literal field '{dj_field.name}'\")\n    elif is_choices:\n        logger.debug(\n            f\"Field '{dj_field.name}' has choices, but they weren't added to json_schema_extra by the mapping unit.\"\n        )\n\n    # Clean up redundant `default=None` for Optional fields handled by Pydantic v2.\n    # Do not force-add default=None; only keep explicit defaults (e.g., for AutoPK).\n    if is_optional:\n        if field_info_kwargs.get(\"default\") is None and not is_auto_pk:\n            # Remove implicit default=None for Optional fields\n            field_info_kwargs.pop(\"default\", None)\n            logger.debug(f\"Removed redundant default=None for Optional field '{dj_field.name}'\")\n        elif is_auto_pk and \"default\" not in field_info_kwargs:\n            # Keep default=None for AutoPK if not already set\n            field_info_kwargs[\"default\"] = None\n            logger.debug(f\"Set default=None for AutoPK Optional field '{dj_field.name}'\")\n\n    logger.debug(\n        f\"Final Pydantic mapping for '{dj_field.name}': Type={final_pydantic_type}, Kwargs={field_info_kwargs}\"\n    )\n    return final_pydantic_type, field_info_kwargs\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.mapping_units.TypeMappingUnit.__init_subclass__","title":"<code>__init_subclass__(**kwargs)</code>","text":"<p>Ensure subclasses define the required types.</p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>def __init_subclass__(cls, **kwargs):\n    \"\"\"Ensure subclasses define the required types.\"\"\"\n    super().__init_subclass__(**kwargs)\n    if not hasattr(cls, \"python_type\") or not hasattr(cls, \"django_field_type\"):\n        raise NotImplementedError(\n            \"Subclasses of TypeMappingUnit must define 'python_type' and 'django_field_type' class attributes.\"\n        )\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.mapping_units.TypeMappingUnit.django_to_pydantic_field_info_kwargs","title":"<code>django_to_pydantic_field_info_kwargs(dj_field)</code>","text":"<p>Generate Pydantic FieldInfo kwargs from a Django field instance.</p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>def django_to_pydantic_field_info_kwargs(self, dj_field: models.Field) -&gt; dict[str, Any]:\n    \"\"\"Generate Pydantic FieldInfo kwargs from a Django field instance.\"\"\"\n    kwargs = {}\n    field_name = getattr(dj_field, \"name\", \"unknown_field\")  # Get field name for logging\n\n    # Title: Use verbose_name or generate from field name\n    verbose_name = getattr(dj_field, \"verbose_name\", None)\n    logger.debug(f\"Processing field '{field_name}': verbose_name='{verbose_name}'\")\n    if verbose_name:\n        # Ensure verbose_name is a string, handling lazy proxies\n        kwargs[\"title\"] = force_str(verbose_name).capitalize()\n    elif field_name != \"unknown_field\" and isinstance(field_name, str):\n        # Generate title from name if verbose_name is missing and name is a string\n        generated_title = field_name.replace(\"_\", \" \").capitalize()\n        kwargs[\"title\"] = generated_title\n        logger.debug(f\"Generated title for '{field_name}': '{generated_title}'\")\n    # else: field name is None or 'unknown_field', no title generated by default\n\n    # Description\n    if dj_field.help_text:\n        # Ensure help_text is a string, handling lazy proxies\n        kwargs[\"description\"] = force_str(dj_field.help_text)\n\n    # Default value/factory handling\n    if dj_field.has_default():\n        dj_default = dj_field.get_default()\n        if dj_default is not models.fields.NOT_PROVIDED:\n            if callable(dj_default):\n                factory_set = False\n                if dj_default is dict:\n                    kwargs[\"default_factory\"] = dict\n                    factory_set = True\n                elif dj_default is list:\n                    kwargs[\"default_factory\"] = list\n                    factory_set = True\n                # Add other known callable mappings if needed\n                else:\n                    logger.debug(\n                        f\"Django field '{dj_field.name}' has an unmapped callable default ({dj_default}), \"\n                        \"not mapping to Pydantic default/default_factory.\"\n                    )\n                if factory_set:\n                    kwargs.pop(\"default\", None)\n            # Handle non-callable defaults\n            # Map default={} back to default_factory=dict for JSONField\n            elif dj_default == {}:\n                kwargs[\"default_factory\"] = dict\n                kwargs.pop(\"default\", None)\n            elif dj_default == []:\n                kwargs[\"default_factory\"] = list\n                kwargs.pop(\"default\", None)\n            elif dj_default is not None:\n                # Add non-None, non-callable, non-empty-collection defaults\n                logger.debug(\n                    f\"Processing non-callable default for '{field_name}'. Type: {type(dj_default)}, Value: {dj_default!r}\"\n                )\n                # Apply force_str ONLY if the default value's type suggests it might be a lazy proxy string.\n                # A simple check is if 'proxy' is in the type name.\n                processed_default = dj_default\n                if \"proxy\" in type(dj_default).__name__:\n                    try:\n                        processed_default = force_str(dj_default)\n                        logger.debug(\n                            f\"Applied force_str to potential lazy default for '{field_name}'. New value: {processed_default!r}\"\n                        )\n                    except Exception as e:\n                        logger.error(\n                            f\"Failed to apply force_str to default value for '{field_name}': {e}. Assigning raw default.\"\n                        )\n                        processed_default = dj_default  # Keep original on error\n\n                kwargs[\"default\"] = processed_default\n                logger.debug(f\"Assigned final default for '{field_name}': {kwargs.get('default')!r}\")\n\n    # Handle AutoField PKs -&gt; frozen=True, default=None\n    is_auto_pk = dj_field.primary_key and isinstance(\n        dj_field, (models.AutoField, models.BigAutoField, models.SmallAutoField)\n    )\n    if is_auto_pk:\n        kwargs[\"frozen\"] = True\n        kwargs[\"default\"] = None\n\n    # Handle choices (including processing labels and limiting)\n    # Log choices *before* calling handle_choices\n    if hasattr(dj_field, \"choices\") and dj_field.choices:\n        try:\n            # Log the raw choices from the Django field\n            raw_choices_repr = repr(list(dj_field.choices))  # Materialize and get repr\n            logger.debug(f\"Field '{field_name}': Raw choices before handle_choices: {raw_choices_repr}\")\n        except Exception as log_err:\n            logger.warning(f\"Field '{field_name}': Error logging raw choices: {log_err}\")\n\n        self.handle_choices(dj_field, kwargs)\n        # Log choices *after* handle_choices modified kwargs\n        processed_choices_repr = repr(kwargs.get(\"json_schema_extra\", {}).get(\"choices\"))\n        logger.debug(f\"Field '{field_name}': Choices in kwargs after handle_choices: {processed_choices_repr}\")\n\n    # Handle non-choice max_length only if choices were NOT processed\n    elif dj_field.max_length is not None:\n        # Only add max_length if not choices - specific units can override\n        kwargs[\"max_length\"] = dj_field.max_length\n\n    logger.debug(f\"Base kwargs generated for '{field_name}': {kwargs}\")\n    return kwargs\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.mapping_units.TypeMappingUnit.handle_choices","title":"<code>handle_choices(dj_field, kwargs)</code>","text":"<p>Handles Django field choices, ensuring lazy translation proxies are resolved.</p> <p>It processes the choices, forces string conversion on labels within an active translation context, limits the number of choices added to the schema, and stores them in <code>json_schema_extra</code>.</p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>def handle_choices(self, dj_field: models.Field, kwargs: dict[str, Any]) -&gt; None:\n    \"\"\"\n    Handles Django field choices, ensuring lazy translation proxies are resolved.\n\n    It processes the choices, forces string conversion on labels within an\n    active translation context, limits the number of choices added to the schema,\n    and stores them in `json_schema_extra`.\n    \"\"\"\n    field_name = getattr(dj_field, \"name\", \"unknown_field\")\n    processed_choices = []\n    MAX_CHOICES_IN_SCHEMA = 30  # TODO: Make configurable\n    limited_choices = []\n    default_value = kwargs.get(\"default\")  # Use potentially processed default\n    default_included = False\n\n    # --- Ensure Translation Context --- #\n    active_translation = None\n    if translation:\n        try:\n            # Get the currently active language to restore later\n            current_language = translation.get_language()\n            # Activate the default language (or a specific one like 'en')\n            # This forces lazy objects to resolve using a consistent language.\n            # Using settings.LANGUAGE_CODE assumes it's set correctly.\n            default_language = getattr(settings, \"LANGUAGE_CODE\", \"en\")  # Fallback to 'en'\n            active_translation = translation.override(default_language)\n            logger.debug(\n                f\"Activated translation override ('{default_language}') for processing choices of '{field_name}'\"\n            )\n            active_translation.__enter__()  # Manually enter context\n        except Exception as trans_err:\n            logger.warning(f\"Failed to activate translation context for '{field_name}': {trans_err}\")\n            active_translation = None  # Ensure it's None if activation failed\n    else:\n        logger.warning(\"Django translation module not available. Lazy choices might not resolve correctly.\")\n\n    # --- Process Choices (within potential translation context) --- #\n    try:\n        all_choices = list(dj_field.choices or [])\n        for value, label in all_choices:\n            logger.debug(\n                f\"  Processing choice for '{field_name}': Value={value!r}, Label={label!r} (Type: {type(label)})\"\n            )\n            try:\n                # Apply force_str defensively; should resolve lazy proxies if context is active\n                processed_label = force_str(label)\n                logger.debug(\n                    f\"  Processed label for '{field_name}': Value={value!r}, Label={processed_label!r} (Type: {type(processed_label)})\"\n                )\n            except Exception as force_str_err:\n                logger.error(\n                    f\"Error using force_str on label for '{field_name}' (value: {value!r}): {force_str_err}\"\n                )\n                # Fallback: use repr or a placeholder if force_str fails completely\n                processed_label = f\"&lt;unresolved: {repr(label)}&gt;\"\n            processed_choices.append((value, processed_label))\n\n        # --- Limit Choices --- #\n        if len(processed_choices) &gt; MAX_CHOICES_IN_SCHEMA:\n            logger.warning(\n                f\"Limiting choices for '{field_name}' from {len(processed_choices)} to {MAX_CHOICES_IN_SCHEMA}\"\n            )\n            if default_value is not None:\n                for val, lbl in processed_choices:\n                    if val == default_value:\n                        limited_choices.append((val, lbl))\n                        default_included = True\n                        break\n            remaining_slots = MAX_CHOICES_IN_SCHEMA - len(limited_choices)\n            if remaining_slots &gt; 0:\n                for val, lbl in processed_choices:\n                    if len(limited_choices) &gt;= MAX_CHOICES_IN_SCHEMA:\n                        break\n                    if not (default_included and val == default_value):\n                        limited_choices.append((val, lbl))\n            final_choices_list = limited_choices\n        else:\n            final_choices_list = processed_choices\n\n        # --- Store Choices --- #\n        kwargs.setdefault(\"json_schema_extra\", {})[\"choices\"] = final_choices_list\n        kwargs.pop(\"max_length\", None)  # Remove max_length if choices are present\n        logger.debug(f\"Stored final choices in json_schema_extra for '{field_name}'\")\n\n    except Exception as e:\n        logger.error(f\"Error processing or limiting choices for field '{field_name}': {e}\", exc_info=True)\n        kwargs.pop(\"json_schema_extra\", None)\n\n    finally:\n        # --- Deactivate Translation Context --- #\n        if active_translation:\n            try:\n                active_translation.__exit__(None, None, None)  # Manually exit context\n                logger.debug(f\"Deactivated translation override for '{field_name}'\")\n            except Exception as trans_exit_err:\n                logger.warning(f\"Error deactivating translation context for '{field_name}': {trans_exit_err}\")\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.mapping_units.TypeMappingUnit.matches","title":"<code>matches(py_type, field_info=None)</code>  <code>classmethod</code>","text":"<p>Calculate a score indicating how well this unit matches the given Python type and FieldInfo.</p> <p>Parameters:</p> Name Type Description Default <code>py_type</code> <code>Any</code> <p>The Python type to match against.</p> required <code>field_info</code> <code>Optional[FieldInfo]</code> <p>Optional Pydantic FieldInfo for context.</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>A float score (0.0 = no match, higher = better match).</p> <code>float</code> <p>Base implementation scores:</p> <code>float</code> <ul> <li>1.0 for exact type match (cls.python_type == py_type)</li> </ul> <code>float</code> <ul> <li>0.5 for subclass match (issubclass(py_type, cls.python_type))</li> </ul> <code>float</code> <ul> <li>0.0 otherwise</li> </ul> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>@classmethod\ndef matches(cls, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; float:\n    \"\"\"\n    Calculate a score indicating how well this unit matches the given Python type and FieldInfo.\n\n    Args:\n        py_type: The Python type to match against.\n        field_info: Optional Pydantic FieldInfo for context.\n\n    Returns:\n        A float score (0.0 = no match, higher = better match).\n        Base implementation scores:\n        - 1.0 for exact type match (cls.python_type == py_type)\n        - 0.5 for subclass match (issubclass(py_type, cls.python_type))\n        - 0.0 otherwise\n    \"\"\"\n    target_py_type = cls.python_type\n    if py_type == target_py_type:\n        return 1.0\n    try:\n        # Check issubclass only if both are actual classes and py_type is not Any\n        if (\n            py_type is not Any\n            and inspect.isclass(py_type)\n            and inspect.isclass(target_py_type)\n            and issubclass(py_type, target_py_type)\n        ):\n            # Don't match if it's the same type (already handled by exact match)\n            if py_type is not target_py_type:\n                return 0.5\n    except TypeError:\n        # issubclass fails on non-classes (like Any, List[int], etc.)\n        pass\n    return 0.0\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.mapping_units.TypeMappingUnit.pydantic_to_django_kwargs","title":"<code>pydantic_to_django_kwargs(py_type, field_info=None)</code>","text":"<p>Generate Django field constructor kwargs from Pydantic FieldInfo.</p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>def pydantic_to_django_kwargs(self, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; dict[str, Any]:\n    \"\"\"Generate Django field constructor kwargs from Pydantic FieldInfo.\"\"\"\n    kwargs = {}\n    if field_info:\n        # Map common attributes\n        if field_info.title:\n            kwargs[\"verbose_name\"] = field_info.title\n        if field_info.description:\n            kwargs[\"help_text\"] = field_info.description\n\n        # Only consider `default` if `default_factory` is None\n        if field_info.default_factory is None:\n            if field_info.default is not PydanticUndefined and field_info.default is not None:\n                # Django doesn't handle callable defaults easily here\n                if not callable(field_info.default):\n                    kwargs[\"default\"] = field_info.default\n            elif field_info.default is None:  # Explicitly check for None default\n                kwargs[\"default\"] = None  # Add default=None if present in FieldInfo\n        # else: If default_factory is present, do not add a 'default' kwarg.\n        # No warning needed as this is now expected behavior.\n\n        # Note: Frozen, ge, le etc. are validation rules, map separately if needed\n    return kwargs\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.relationships.RelationshipConversionAccessor.available_django_models","title":"<code>available_django_models</code>  <code>property</code>","text":"<p>Get a list of all Django models in the relationship accessor</p>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.relationships.RelationshipConversionAccessor.available_source_models","title":"<code>available_source_models</code>  <code>property</code>","text":"<p>Get a list of all source models (Pydantic or Dataclass).</p>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.relationships.RelationshipConversionAccessor.add_dataclass_model","title":"<code>add_dataclass_model(model)</code>","text":"<p>Add a Dataclass model to the relationship accessor</p> Source code in <code>src/pydantic2django/core/relationships.py</code> <pre><code>def add_dataclass_model(self, model: type) -&gt; None:\n    \"\"\"Add a Dataclass model to the relationship accessor\"\"\"\n    # Check if the model is already mapped\n    if any(r.dataclass_model == model for r in self.available_relationships):\n        return  # Already exists\n    # Check if a Pydantic model with the same name is already mapped (potential conflict)\n    if any(r.pydantic_model and r.pydantic_model.__name__ == model.__name__ for r in self.available_relationships):\n        logger.warning(f\"Adding dataclass {model.__name__}, but a Pydantic model with the same name exists.\")\n\n    self.available_relationships.append(RelationshipMapper(dataclass_model=model))\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.relationships.RelationshipConversionAccessor.add_django_model","title":"<code>add_django_model(model)</code>","text":"<p>Add a Django model to the relationship accessor</p> Source code in <code>src/pydantic2django/core/relationships.py</code> <pre><code>def add_django_model(self, model: type[models.Model]) -&gt; None:\n    \"\"\"Add a Django model to the relationship accessor\"\"\"\n    # Check if the model is already in available_django_models by comparing class names\n    model_name = model.__name__\n    existing_models = [m.__name__ for m in self.available_django_models]\n\n    if model_name not in existing_models:\n        self.available_relationships.append(RelationshipMapper(None, None, model, context=None))\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.relationships.RelationshipConversionAccessor.add_pydantic_model","title":"<code>add_pydantic_model(model)</code>","text":"<p>Add a Pydantic model to the relationship accessor</p> Source code in <code>src/pydantic2django/core/relationships.py</code> <pre><code>def add_pydantic_model(self, model: type[BaseModel]) -&gt; None:\n    \"\"\"Add a Pydantic model to the relationship accessor\"\"\"\n    # Check if the model is already in available_pydantic_models by comparing class names\n    model_name = model.__name__\n    existing_models = [m.__name__ for m in self.available_source_models]\n\n    if model_name not in existing_models:\n        self.available_relationships.append(RelationshipMapper(model, None, context=None))\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.relationships.RelationshipConversionAccessor.from_dict","title":"<code>from_dict(relationship_mapping_dict)</code>  <code>classmethod</code>","text":"<p>Convert a dictionary of strings representing model qualified names to a RelationshipConversionAccessor</p> <p>The dictionary should be of the form: {     \"pydantic_model_qualified_name\": \"django_model_qualified_name\",     ... }</p> Source code in <code>src/pydantic2django/core/relationships.py</code> <pre><code>@classmethod\ndef from_dict(cls, relationship_mapping_dict: dict) -&gt; \"RelationshipConversionAccessor\":\n    \"\"\"\n    Convert a dictionary of strings representing model qualified names to a RelationshipConversionAccessor\n\n    The dictionary should be of the form:\n    {\n        \"pydantic_model_qualified_name\": \"django_model_qualified_name\",\n        ...\n    }\n    \"\"\"\n    available_relationships = []\n    for pydantic_mqn, django_mqn in relationship_mapping_dict.items():\n        try:\n            # Split the module path and class name\n            pydantic_module_path, pydantic_class_name = pydantic_mqn.rsplit(\".\", 1)\n            django_module_path, django_class_name = django_mqn.rsplit(\".\", 1)\n\n            # Import the modules\n            pydantic_module = importlib.import_module(pydantic_module_path)\n            django_module = importlib.import_module(django_module_path)\n\n            # Get the actual class objects\n            pydantic_model = getattr(pydantic_module, pydantic_class_name)\n            django_model = getattr(django_module, django_class_name)\n\n            available_relationships.append(RelationshipMapper(pydantic_model, django_model, context=None))\n        except Exception as e:\n            logger.warning(f\"Error importing model {pydantic_mqn} or {django_mqn}: {e}\")\n            continue\n    return cls(available_relationships)\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.relationships.RelationshipConversionAccessor.get_django_model_for_dataclass","title":"<code>get_django_model_for_dataclass(dataclass_model)</code>","text":"<p>Find the corresponding Django model for a given Dataclass model.</p> Source code in <code>src/pydantic2django/core/relationships.py</code> <pre><code>def get_django_model_for_dataclass(self, dataclass_model: type) -&gt; Optional[type[models.Model]]:\n    \"\"\"Find the corresponding Django model for a given Dataclass model.\"\"\"\n    logger.debug(f\"Searching for Django model matching dataclass: {dataclass_model.__name__}\")\n    for relationship in self.available_relationships:\n        # Check if this mapper holds the target dataclass and has a linked Django model\n        if relationship.dataclass_model == dataclass_model and relationship.django_model is not None:\n            logger.debug(f\"  Found match: {relationship.django_model.__name__}\")\n            return relationship.django_model\n    logger.debug(f\"  No match found for dataclass {dataclass_model.__name__}\")\n    return None\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.relationships.RelationshipConversionAccessor.get_django_model_for_pydantic","title":"<code>get_django_model_for_pydantic(pydantic_model)</code>","text":"<p>Find the corresponding Django model for a given Pydantic model</p> <p>Returns None if no matching Django model is found</p> Source code in <code>src/pydantic2django/core/relationships.py</code> <pre><code>def get_django_model_for_pydantic(self, pydantic_model: type[BaseModel]) -&gt; Optional[type[models.Model]]:\n    \"\"\"\n    Find the corresponding Django model for a given Pydantic model\n\n    Returns None if no matching Django model is found\n    \"\"\"\n    for relationship in self.available_relationships:\n        if relationship.pydantic_model == pydantic_model and relationship.django_model is not None:\n            return relationship.django_model\n    return None\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.relationships.RelationshipConversionAccessor.get_pydantic_model_for_django","title":"<code>get_pydantic_model_for_django(django_model)</code>","text":"<p>Find the corresponding Pydantic model for a given Django model</p> <p>Returns None if no matching Pydantic model is found</p> Source code in <code>src/pydantic2django/core/relationships.py</code> <pre><code>def get_pydantic_model_for_django(self, django_model: type[models.Model]) -&gt; Optional[type[BaseModel]]:\n    \"\"\"\n    Find the corresponding Pydantic model for a given Django model\n\n    Returns None if no matching Pydantic model is found\n    \"\"\"\n    for relationship in self.available_relationships:\n        if relationship.django_model == django_model and relationship.pydantic_model is not None:\n            return relationship.pydantic_model\n    return None\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.relationships.RelationshipConversionAccessor.get_source_model_by_name","title":"<code>get_source_model_by_name(model_name)</code>","text":"<p>Find a known source model (Pydantic or Dataclass) by its class name.</p> Source code in <code>src/pydantic2django/core/relationships.py</code> <pre><code>def get_source_model_by_name(self, model_name: str) -&gt; Optional[type]:\n    \"\"\"Find a known source model (Pydantic or Dataclass) by its class name.\"\"\"\n    for r in self.available_relationships:\n        if r.pydantic_model and r.pydantic_model.__name__ == model_name:\n            return r.pydantic_model\n        if r.dataclass_model and r.dataclass_model.__name__ == model_name:\n            return r.dataclass_model\n    return None\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.relationships.RelationshipConversionAccessor.is_source_model_known","title":"<code>is_source_model_known(model)</code>","text":"<p>Check if a specific source model (Pydantic or Dataclass) is known.</p> Source code in <code>src/pydantic2django/core/relationships.py</code> <pre><code>def is_source_model_known(self, model: type) -&gt; bool:\n    \"\"\"Check if a specific source model (Pydantic or Dataclass) is known.\"\"\"\n    is_pydantic = isinstance(model, type) and issubclass(model, BaseModel)\n    is_dataclass = dataclasses.is_dataclass(model)\n\n    for r in self.available_relationships:\n        if is_pydantic and r.pydantic_model == model:\n            return True\n        if is_dataclass and r.dataclass_model == model:\n            return True\n    return False\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.relationships.RelationshipConversionAccessor.map_relationship","title":"<code>map_relationship(source_model, django_model)</code>","text":"<p>Create or update a mapping between a source model (Pydantic/Dataclass) and a Django model.</p> Source code in <code>src/pydantic2django/core/relationships.py</code> <pre><code>def map_relationship(self, source_model: type, django_model: type[models.Model]) -&gt; None:\n    \"\"\"\n    Create or update a mapping between a source model (Pydantic/Dataclass) and a Django model.\n    \"\"\"\n    source_type = (\n        \"pydantic\"\n        if isinstance(source_model, type) and issubclass(source_model, BaseModel)\n        else \"dataclass\"\n        if dataclasses.is_dataclass(source_model)\n        else \"unknown\"\n    )\n\n    if source_type == \"unknown\":\n        logger.warning(f\"Cannot map relationship for unknown source type: {source_model}\")\n        return\n\n    # Check if either model already exists in a relationship\n    for relationship in self.available_relationships:\n        if source_type == \"pydantic\" and relationship.pydantic_model == source_model:\n            relationship.django_model = django_model\n            # Ensure dataclass_model is None if we map pydantic\n            relationship.dataclass_model = None\n            logger.debug(f\"Updated mapping: Pydantic {source_model.__name__} -&gt; Django {django_model.__name__}\")\n            return\n        if source_type == \"dataclass\" and relationship.dataclass_model == source_model:\n            relationship.django_model = django_model\n            # Ensure pydantic_model is None\n            relationship.pydantic_model = None\n            logger.debug(f\"Updated mapping: Dataclass {source_model.__name__} -&gt; Django {django_model.__name__}\")\n            return\n        if relationship.django_model == django_model:\n            # Map the source model based on its type\n            if source_type == \"pydantic\":\n                relationship.pydantic_model = cast(type[BaseModel], source_model)\n                relationship.dataclass_model = None\n                logger.debug(\n                    f\"Updated mapping: Pydantic {source_model.__name__} -&gt; Django {django_model.__name__} (found via Django model)\"\n                )\n            elif source_type == \"dataclass\":\n                relationship.dataclass_model = cast(type, source_model)\n                relationship.pydantic_model = None\n                logger.debug(\n                    f\"Updated mapping: Dataclass {source_model.__name__} -&gt; Django {django_model.__name__} (found via Django model)\"\n                )\n            return\n\n    # If no existing relationship found, create a new one\n    logger.debug(\n        f\"Creating new mapping: {source_type.capitalize()} {source_model.__name__} -&gt; Django {django_model.__name__}\"\n    )\n    if source_type == \"pydantic\":\n        self.available_relationships.append(\n            RelationshipMapper(pydantic_model=cast(type[BaseModel], source_model), django_model=django_model)\n        )\n    elif source_type == \"dataclass\":\n        self.available_relationships.append(\n            RelationshipMapper(dataclass_model=cast(type, source_model), django_model=django_model)\n        )\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.relationships.RelationshipConversionAccessor.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert the relationships to a dictionary of strings representing model qualified names for bidirectional conversion.</p> <p>Can be stored in a JSON field, and used to reconstruct the relationships.</p> Source code in <code>src/pydantic2django/core/relationships.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"\n    Convert the relationships to a dictionary of strings representing\n    model qualified names for bidirectional conversion.\n\n    Can be stored in a JSON field, and used to reconstruct the relationships.\n    \"\"\"\n    relationship_mapping_dict = {}\n    for relationship in self.available_relationships:\n        # Skip relationships where either model is None\n        if relationship.pydantic_model is None or relationship.django_model is None:\n            continue\n\n        pydantic_mqn = self._get_pydantic_model_qualified_name(relationship.pydantic_model)\n        django_mqn = self._get_django_model_qualified_name(relationship.django_model)\n        relationship_mapping_dict[pydantic_mqn] = django_mqn\n\n    return relationship_mapping_dict\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.relationships.RelationshipMapper.source_model","title":"<code>source_model</code>  <code>property</code>","text":"<p>Return the source model (either Pydantic or Dataclass).</p>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.typing.TypeHandler.format_type_string","title":"<code>format_type_string(type_obj)</code>  <code>staticmethod</code>","text":"<p>Return a string representation suitable for generated code.</p> Source code in <code>src/pydantic2django/core/typing.py</code> <pre><code>@staticmethod\ndef format_type_string(type_obj: Any) -&gt; str:\n    \"\"\"Return a string representation suitable for generated code.\"\"\"\n    # --- Simplified version to break recursion ---\n    # Get the raw string representation first\n    raw_repr = TypeHandler._get_raw_type_string(type_obj)\n\n    # Basic cleanup for common typing constructs\n    base_name = raw_repr.replace(\"typing.\", \"\")\n\n    # Attempt to refine based on origin/args if needed (optional)\n    origin = get_origin(type_obj)\n    args = get_args(type_obj)\n\n    if origin in (Union, UnionType) and len(args) == 2 and type(None) in args:\n        # Handle Optional[T]\n        inner_type_str = TypeHandler.format_type_string(next(arg for arg in args if arg is not type(None)))\n        return f\"{inner_type_str} | None\"\n    elif origin in (list, Sequence):\n        # Handle List[T] / Sequence[T]\n        if args:\n            inner_type_str = TypeHandler.format_type_string(args[0])\n            return f\"List[{inner_type_str}]\"  # Prefer List for generated code\n        else:\n            return \"List[Any]\"\n    elif origin is dict:\n        if args and len(args) == 2:\n            key_type_str = TypeHandler.format_type_string(args[0])\n            value_type_str = TypeHandler.format_type_string(args[1])\n            return f\"Dict[{key_type_str}, {value_type_str}]\"\n        else:\n            return \"dict\"\n    elif origin is Callable:\n        if args:\n            # For Callable[[A, B], R], args is ([A, B], R) in Py3.9+\n            # For Callable[A, R], args is (A, R)\n            # For Callable[[], R], args is ([], R)\n            param_part = args[0]\n            return_part = args[-1]\n\n            if param_part is ...:\n                param_str = \"...\"\n            elif isinstance(param_part, list):\n                param_types = [TypeHandler.format_type_string(p) for p in param_part]\n                param_str = f'[{\", \".join(param_types)}]'\n            else:  # Single argument\n                param_str = f\"[{TypeHandler.format_type_string(param_part)}]\"\n\n            return_type_str = TypeHandler.format_type_string(return_part)\n            return f\"Callable[{param_str}, {return_type_str}]\"\n        else:\n            return \"Callable\"\n    elif origin in (Union, UnionType):  # Non-optional Union\n        inner_types = [TypeHandler.format_type_string(arg) for arg in args]\n        return \" | \".join(inner_types)\n    elif origin is Literal:\n        inner_values = [repr(arg) for arg in args]\n        return f\"Literal[{', '.join(inner_values)}]\"\n    # Add other origins like Dict, Tuple, Callable if needed\n\n    # Fallback to the cleaned raw representation\n    return base_name.replace(\"collections.abc.\", \"\")\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.typing.TypeHandler.get_class_name","title":"<code>get_class_name(type_obj)</code>  <code>staticmethod</code>","text":"<p>Extract a simple, usable class name from a type object.</p> Source code in <code>src/pydantic2django/core/typing.py</code> <pre><code>@staticmethod\ndef get_class_name(type_obj: Any) -&gt; str:\n    \"\"\"Extract a simple, usable class name from a type object.\"\"\"\n    origin = get_origin(type_obj)\n    args = get_args(type_obj)\n\n    # Check for Optional[T] specifically first (Union[T, NoneType])\n    if origin in (Union, UnionType) and len(args) == 2 and type(None) in args:\n        return \"Optional\"\n\n    if origin:\n        # Now check for other origins\n        if origin in (Union, UnionType):  # Handles Union[A, B, ...]\n            return \"Union\"\n        if origin is list:\n            return \"List\"  # Use capital L consistently\n        if origin is dict:\n            return \"Dict\"  # Use capital D consistently\n        if origin is tuple:\n            return \"Tuple\"  # Use capital T consistently\n        if origin is set:\n            return \"Set\"  # Use capital S consistently\n        if origin is Callable:\n            return \"Callable\"\n        if origin is type:\n            return \"Type\"\n        # Fallback for other generic types\n        return getattr(origin, \"__name__\", str(origin))\n\n    # Handle non-generic types\n    if hasattr(type_obj, \"__name__\"):\n        return type_obj.__name__\n\n    type_str = str(type_obj)\n    match = TypeHandler.PATTERNS[\"angle_bracket_class\"].match(type_str)\n    if match:\n        return match.group(1).split(\".\")[-1]\n\n    return str(type_obj)\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.typing.TypeHandler.get_required_imports","title":"<code>get_required_imports(type_obj)</code>  <code>staticmethod</code>","text":"<p>Determine necessary imports by traversing a type object.</p> Source code in <code>src/pydantic2django/core/typing.py</code> <pre><code>@staticmethod\ndef get_required_imports(type_obj: Any) -&gt; dict[str, list[str]]:\n    \"\"\"Determine necessary imports by traversing a type object.\"\"\"\n    imports: dict[str, list[str]] = {}\n    processed_types = set()\n\n    # Define modules for known Pydantic types that might need explicit import\n    pydantic_module_map = {\n        \"EmailStr\": \"pydantic\",\n        \"IPvAnyAddress\": \"pydantic\",\n        \"Json\": \"pydantic\",\n        \"BaseModel\": \"pydantic\",\n        # Add others if needed (e.g., SecretStr, UrlStr)\n    }\n\n    def _traverse(current_type: Any):\n        nonlocal imports\n        try:\n            type_repr = repr(current_type)\n            if type_repr in processed_types:\n                return\n            processed_types.add(type_repr)\n        except TypeError:\n            # Handle unhashable types if necessary, e.g., log a warning\n            pass\n\n        origin = get_origin(current_type)\n        args = get_args(current_type)\n\n        if origin:\n            # Handle Generic Alias (List, Dict, Union, Optional, Callable, Type)\n            origin_module = getattr(origin, \"__module__\", \"\")\n            origin_name = getattr(origin, \"__name__\", \"\")\n\n            # Determine the canonical name used in 'typing' imports (e.g., List, Dict, Callable)\n            typing_name = None\n            if origin is list:\n                typing_name = \"List\"\n            elif origin is dict:\n                typing_name = \"Dict\"\n            elif origin is tuple:\n                typing_name = \"Tuple\"\n            elif origin is set:\n                typing_name = \"Set\"\n            elif origin in (Union, UnionType):  # Handle types.UnionType for Python 3.10+\n                # We don't need to add Union or Optional imports anymore with | syntax\n                typing_name = None\n            elif origin is type:\n                typing_name = \"Type\"\n            # Check both typing.Callable and collections.abc.Callable\n            elif origin_module == \"typing\" and origin_name == \"Callable\":\n                typing_name = \"Callable\"\n            elif origin_module == \"collections.abc\" and origin_name == \"Callable\":\n                typing_name = \"Callable\"\n            # Add more specific checks if needed (e.g., Sequence, Mapping)\n\n            # Add import if we identified a standard typing construct\n            if typing_name:\n                TypeHandler._add_import(imports, \"typing\", typing_name)\n\n            # Traverse arguments regardless of origin's module\n            for arg in args:\n                if arg is not type(None):  # Skip NoneType in Optional/Union\n                    if isinstance(arg, TypeVar):\n                        # Handle TypeVar by traversing its constraints/bound\n                        constraints = getattr(arg, \"__constraints__\", ())\n                        bound = getattr(arg, \"__bound__\", None)\n                        if bound:\n                            _traverse(bound)\n                        for constraint in constraints:\n                            _traverse(constraint)\n                    else:\n                        _traverse(arg)  # Recursively traverse arguments\n        # Handle Base Types or Classes (int, str, MyClass, etc.)\n        elif isinstance(current_type, type):\n            module_name = getattr(current_type, \"__module__\", \"\")\n            type_name = getattr(current_type, \"__name__\", \"\")\n\n            if not type_name or module_name == \"builtins\":\n                pass  # Skip builtins or types without names\n            elif module_name == \"typing\" and type_name not in (\"NoneType\", \"Generic\"):\n                # Catch Any, etc. used directly\n                TypeHandler._add_import(imports, \"typing\", type_name)\n            # Check for dataclasses and Pydantic models specifically\n            elif is_dataclass(current_type) or (\n                inspect.isclass(current_type) and issubclass(current_type, BaseModel)\n            ):\n                actual_module = inspect.getmodule(current_type)\n                if actual_module and actual_module.__name__ != \"__main__\":\n                    TypeHandler._add_import(imports, actual_module.__name__, type_name)\n                # Add specific imports if needed (e.g., dataclasses.dataclass, pydantic.BaseModel)\n                if is_dataclass(current_type):\n                    TypeHandler._add_import(imports, \"dataclasses\", \"dataclass\")\n                # No need to add BaseModel here usually, handled by pydantic_module_map or direct usage\n            elif module_name:\n                # Handle known standard library modules explicitly\n                known_stdlib = {\"datetime\", \"decimal\", \"uuid\", \"pathlib\"}\n                if module_name in known_stdlib:\n                    TypeHandler._add_import(imports, module_name, type_name)\n                # Handle known Pydantic types explicitly (redundant with BaseModel check?)\n                elif type_name in pydantic_module_map:\n                    TypeHandler._add_import(imports, pydantic_module_map[type_name], type_name)\n                # Assume other types defined in modules need importing\n                elif module_name != \"__main__\":  # Avoid importing from main script context\n                    TypeHandler._add_import(imports, module_name, type_name)\n\n        elif current_type is Any:\n            TypeHandler._add_import(imports, \"typing\", \"Any\")\n        elif isinstance(current_type, TypeVar):\n            # Handle TypeVar used directly\n            constraints = getattr(current_type, \"__constraints__\", ())\n            bound = getattr(current_type, \"__bound__\", None)\n            if bound:\n                _traverse(bound)\n            for c in constraints:\n                _traverse(c)\n        # Consider adding ForwardRef handling if needed:\n        # elif isinstance(current_type, typing.ForwardRef):\n        #     # Potentially add logic to resolve/import forward refs\n        #     pass\n\n    _traverse(type_obj)\n\n    # Clean up imports (unique, sorted)\n    final_imports = {}\n    for module, names in imports.items():\n        unique_names = sorted(set(names))\n        if unique_names:\n            final_imports[module] = unique_names\n    return final_imports\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.typing.TypeHandler.process_field_type","title":"<code>process_field_type(field_type)</code>  <code>staticmethod</code>","text":"<p>Process a field type to get name, flags, imports, and contained dataclasses.</p> Source code in <code>src/pydantic2django/core/typing.py</code> <pre><code>@staticmethod\ndef process_field_type(field_type: Any) -&gt; dict[str, Any]:\n    \"\"\"Process a field type to get name, flags, imports, and contained dataclasses.\"\"\"\n    logger.debug(f\"[TypeHandler] Processing type: {field_type!r}\")\n    is_optional = False\n    is_list = False\n    metadata: tuple[Any, ...] | None = None  # Initialize metadata with type hint\n    imports = set()\n    contained_dataclasses = set()\n    current_type = field_type  # Keep track of the potentially unwrapped type\n\n    # Helper function (remains the same)\n    def _is_potential_dataclass(t: Any) -&gt; bool:\n        return inspect.isclass(t) and is_dataclass(t)\n\n    def _find_contained_dataclasses(current_type: Any):\n        origin = get_origin(current_type)\n        args = get_args(current_type)\n        if origin:\n            for arg in args:\n                if arg is not type(None):\n                    _find_contained_dataclasses(arg)\n        elif _is_potential_dataclass(current_type):\n            contained_dataclasses.add(current_type)\n\n    _find_contained_dataclasses(field_type)\n    if contained_dataclasses:\n        logger.debug(f\"  Found potential contained dataclasses: {[dc.__name__ for dc in contained_dataclasses]}\")\n\n    # --- Simplification Loop ---\n    # Repeatedly unwrap until we hit a base type or Any\n    processed = True\n    while processed:\n        processed = False\n        origin = get_origin(current_type)\n        args = get_args(current_type)\n\n        # 0. Unwrap Annotated[T, ...]\n        # Check if the origin exists and has the name 'Annotated'\n        # This check is more robust than `origin is Annotated` across Python versions\n        if origin is Annotated:\n            if args:\n                core_type = args[0]\n                metadata = args[1:]\n                current_type = core_type\n                logger.debug(f\"  Unwrapped Annotated, current type: {current_type!r}, metadata: {metadata!r}\")\n                processed = True\n                continue  # Restart loop with unwrapped type\n            else:\n                logger.warning(\"  Found Annotated without arguments? Treating as Any.\")\n                current_type = Any\n                processed = True\n                continue\n\n        # 1. Unwrap Optional[T] (Union[T, NoneType])\n        if origin in (Union, UnionType) and type(None) in args:\n            is_optional = True  # Flag it\n            # Rebuild the Union without NoneType\n            non_none_args = tuple(arg for arg in args if arg is not type(None))\n            if len(non_none_args) == 1:\n                current_type = non_none_args[0]  # Simplify Union[T, None] to T\n            elif len(non_none_args) &gt; 1:\n                # Use UnionType to rebuild\n                current_type = reduce(lambda x, y: x | y, non_none_args)\n            else:  # pragma: no cover\n                # Should not happen if NoneType was in args\n                current_type = Any\n            logger.debug(f\"  Unwrapped Union with None, current type: {current_type!r}\")\n            processed = True\n            continue  # Restart loop with the non-optional type\n\n        # 2. Unwrap List[T] or Sequence[T]\n        if origin in (list, Sequence):\n            is_list = True  # Flag it\n            if args:\n                current_type = args[0]\n                logger.debug(f\"  Unwrapped List/Sequence, current element type: {current_type!r}\")\n            else:\n                current_type = Any  # List without args -&gt; List[Any]\n                logger.debug(\"  Unwrapped List/Sequence without args, assuming Any\")\n            processed = True\n            continue  # Restart loop with unwrapped element type\n\n        # 3. Unwrap Literal[...]\n        if origin is Literal:\n            # Keep the Literal origin, but simplify args if possible?\n            # No, the mapper needs the original Literal to extract choices.\n            # Just log and break the loop for Literal.\n            logger.debug(\"  Hit Literal origin, stopping simplification loop.\")\n            break  # Stop simplification here, keep Literal type\n\n    # --- Post-Loop Handling ---\n    # At this point, current_type should be the base type (int, str, datetime, Any, etc.)\n    # or a complex type we don't simplify further (like a raw Union or a specific class)\n    base_type_obj = current_type\n\n    # --- FIX: If the original type was a list, ensure base_type_obj reflects the *List* --- #\n    # The simplification loop above sets current_type to the *inner* type of the list.\n    # We need the actual List type for the mapper logic.\n    if is_list:\n        # Determine the simplified inner type from the end of the loop\n        simplified_inner_type = base_type_obj\n\n        # Check if the original type involved Optional wrapping the list\n        # A simple check: was is_optional also flagged?\n        if is_optional:\n            # Reconstruct Optional[List[SimplifiedInner]]\n            reconstructed_type = list[simplified_inner_type] | None\n            logger.debug(\n                f\"  Original was Optional[List-like]. Reconstructing List[...] | None \"\n                f\"around simplified inner type {simplified_inner_type!r} -&gt; {reconstructed_type!r}\"\n            )\n        else:\n            # Reconstruct List[SimplifiedInner]\n            reconstructed_type = list[simplified_inner_type]\n            logger.debug(\n                f\"  Original was List-like (non-optional). Reconstructing List[...] \"\n                f\"around simplified inner type {simplified_inner_type!r} -&gt; {reconstructed_type!r}\"\n            )\n\n        # Check against original type structure (might be more robust but complex?)\n        # original_origin = get_origin(field_type)\n        # if original_origin is Optional and get_origin(get_args(field_type)[0]) in (list, Sequence):\n        #     # Handle Optional[List[...]] structure\n        # elif original_origin in (list, Sequence):\n        #     # Handle List[...] structure\n        # else:\n        #     # Handle complex cases like Annotated[Optional[List[...]]]\n\n        base_type_obj = reconstructed_type\n\n    # --- End FIX --- #\n\n    # Add check for Callable simplification\n    origin = get_origin(base_type_obj)\n    if origin is Callable or (\n        hasattr(base_type_obj, \"__module__\")\n        and base_type_obj.__module__ == \"collections.abc\"\n        and base_type_obj.__name__ == \"Callable\"\n    ):\n        logger.debug(\n            f\"  Final type is complex Callable {base_type_obj!r}, simplifying base object to Callable origin.\"\n        )\n        base_type_obj = Callable\n\n    # --- Result Assembly ---\n    imports = TypeHandler.get_required_imports(field_type)  # Imports based on original\n    type_string = TypeHandler.format_type_string(field_type)  # Formatting based on original\n\n    result = {\n        \"type_str\": type_string,\n        \"type_obj\": base_type_obj,  # THIS is the crucial simplified type object\n        \"is_optional\": is_optional,\n        \"is_list\": is_list,\n        \"imports\": imports,\n        \"contained_dataclasses\": contained_dataclasses,\n        \"metadata\": metadata,\n    }\n    logger.debug(f\"[TypeHandler] Processed result: {result!r}\")\n    return result\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.imports.ImportHandler.__init__","title":"<code>__init__(module_mappings=None)</code>","text":"<p>Initialize empty collections for different types of imports.</p> <p>Parameters:</p> Name Type Description Default <code>module_mappings</code> <code>Optional[dict[str, str]]</code> <p>Optional mapping of modules to remap (e.g. {\"main\": \"my_app.models\"})</p> <code>None</code> Source code in <code>src/pydantic2django/core/imports.py</code> <pre><code>def __init__(self, module_mappings: Optional[dict[str, str]] = None):\n    \"\"\"\n    Initialize empty collections for different types of imports.\n\n    Args:\n        module_mappings: Optional mapping of modules to remap (e.g. {\"__main__\": \"my_app.models\"})\n    \"\"\"\n    # Track imports by category\n    self.extra_type_imports: set[str] = set()  # For typing and other utility imports\n    self.pydantic_imports: set[str] = set()  # For Pydantic model imports\n    self.context_class_imports: set[str] = set()  # For context class and field type imports\n\n    # For tracking imported names to avoid duplicates\n    self.imported_names: dict[str, str] = {}  # Maps type name to its module\n\n    # For tracking field type dependencies we've already processed\n    self.processed_field_types: set[str] = set()\n\n    # Module mappings to remap imports (e.g. \"__main__\" -&gt; \"my_app.models\")\n    self.module_mappings = module_mappings or {}\n\n    logger.info(\"ImportHandler initialized\")\n    if self.module_mappings:\n        logger.info(f\"Using module mappings: {self.module_mappings}\")\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.imports.ImportHandler.add_context_field_type_import","title":"<code>add_context_field_type_import(field_type)</code>","text":"<p>Add an import statement for a context field type with recursive dependency detection.</p> <p>Parameters:</p> Name Type Description Default <code>field_type</code> <code>Any</code> <p>The field type to import</p> required Source code in <code>src/pydantic2django/core/imports.py</code> <pre><code>def add_context_field_type_import(self, field_type: Any) -&gt; None:\n    \"\"\"\n    Add an import statement for a context field type with recursive dependency detection.\n\n    Args:\n        field_type: The field type to import\n    \"\"\"\n    # Skip if we've already processed this field type\n    field_type_str = str(field_type)\n    if field_type_str in self.processed_field_types:\n        logger.debug(f\"Skipping already processed field type: {field_type_str}\")\n        return\n\n    logger.info(f\"Processing context field type: {field_type_str}\")\n    self.processed_field_types.add(field_type_str)\n\n    # Try to add direct import for the field type if it's a class\n    self._add_type_import(field_type)\n\n    # Handle nested types in generics, unions, etc.\n    self._process_nested_types(field_type)\n\n    # Add typing imports based on the field type string\n    self._add_typing_imports(field_type_str)\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.imports.ImportHandler.add_import","title":"<code>add_import(module, name)</code>","text":"<p>Adds a single import based on module and name strings.</p> Source code in <code>src/pydantic2django/core/imports.py</code> <pre><code>def add_import(self, module: str, name: str):\n    \"\"\"Adds a single import based on module and name strings.\"\"\"\n    if not module or module == \"builtins\":\n        return\n\n    # Apply module mappings\n    if module in self.module_mappings:\n        module = self.module_mappings[module]\n\n    # Clean name (e.g., remove generics for import statement)\n    clean_name = self._clean_generic_type(name)\n\n    # Check if already imported\n    if name in self.imported_names:\n        # Could verify module matches, but usually name is unique enough\n        logger.debug(f\"Skipping already imported name: {name} (from module {module})\")\n        return\n    if clean_name != name and clean_name in self.imported_names:\n        logger.debug(f\"Skipping already imported clean name: {clean_name} (from module {module})\")\n        return\n\n    # Determine category\n    # Simplistic: If module is known Pydantic, Django, or common stdlib -&gt; context\n    # Otherwise, if it's 'typing' -&gt; extra_type\n    # TODO: Refine categorization if needed (e.g., dedicated django_imports set)\n    import_statement = f\"from {module} import {clean_name}\"\n    if module == \"typing\":\n        self.extra_type_imports.add(clean_name)  # Add only name to typing imports set\n        logger.debug(f\"Adding typing import: {clean_name}\")\n    # elif module.startswith(\"django.\"):\n    # Add to a dedicated django set if we create one\n    #    self.context_class_imports.add(import_statement)\n    #    logger.info(f\"Adding Django import: {import_statement}\")\n    else:\n        # Default to context imports for non-typing\n        self.context_class_imports.add(import_statement)\n        logger.info(f\"Adding context class import: {import_statement}\")\n\n    # Mark as imported\n    self.imported_names[name] = module\n    if clean_name != name:\n        self.imported_names[clean_name] = module\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.imports.ImportHandler.add_pydantic_model_import","title":"<code>add_pydantic_model_import(model_class)</code>","text":"<p>Add an import statement for a Pydantic model.</p> <p>Parameters:</p> Name Type Description Default <code>model_class</code> <code>type</code> <p>The Pydantic model class to import</p> required Source code in <code>src/pydantic2django/core/imports.py</code> <pre><code>def add_pydantic_model_import(self, model_class: type) -&gt; None:\n    \"\"\"\n    Add an import statement for a Pydantic model.\n\n    Args:\n        model_class: The Pydantic model class to import\n    \"\"\"\n    if not hasattr(model_class, \"__module__\") or not hasattr(model_class, \"__name__\"):\n        logger.warning(f\"Cannot add import for {model_class}: missing __module__ or __name__\")\n        return\n\n    module_path = model_class.__module__\n    model_name = self._clean_generic_type(model_class.__name__)\n\n    # Apply module mappings if needed\n    if module_path in self.module_mappings:\n        actual_module = self.module_mappings[module_path]\n        logger.debug(f\"Remapping module import: {module_path} -&gt; {actual_module}\")\n        module_path = actual_module\n\n    logger.debug(f\"Processing Pydantic model import: {model_name} from {module_path}\")\n\n    # Skip if already imported\n    if model_name in self.imported_names:\n        logger.debug(f\"Skipping already imported model: {model_name}\")\n        return\n\n    import_statement = f\"from {module_path} import {model_name}\"\n    logger.info(f\"Adding Pydantic import: {import_statement}\")\n    self.pydantic_imports.add(import_statement)\n    self.imported_names[model_name] = module_path\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.imports.ImportHandler.deduplicate_imports","title":"<code>deduplicate_imports()</code>","text":"<p>De-duplicate imports between Pydantic models and context field types.</p> <p>Returns:</p> Type Description <code>dict[str, set[str]]</code> <p>Dict with de-duplicated import sets</p> Source code in <code>src/pydantic2django/core/imports.py</code> <pre><code>def deduplicate_imports(self) -&gt; dict[str, set[str]]:\n    \"\"\"\n    De-duplicate imports between Pydantic models and context field types.\n\n    Returns:\n        Dict with de-duplicated import sets\n    \"\"\"\n    logger.info(\"Deduplicating imports\")\n    logger.debug(f\"Current pydantic imports: {self.pydantic_imports}\")\n    logger.debug(f\"Current context imports: {self.context_class_imports}\")\n\n    # Extract class names and modules from import statements\n    pydantic_classes = {}\n    context_classes = {}\n\n    # Handle special case for TypeVar imports\n    typevars = set()\n\n    for import_stmt in self.pydantic_imports:\n        if import_stmt.startswith(\"from \") and \" import \" in import_stmt:\n            module, classes = import_stmt.split(\" import \")\n            module = module.replace(\"from \", \"\")\n\n            # Skip __main__ and rewrite to real module paths if possible\n            if module == \"__main__\":\n                logger.warning(f\"Skipping __main__ import: {import_stmt} - these won't work when imported\")\n                continue\n\n            for cls in classes.split(\", \"):\n                # Check if it's a TypeVar to handle duplicate definitions\n                if cls == \"T\" or cls == \"TypeVar\":\n                    typevars.add(cls)\n                    continue\n\n                # Clean up any parameterized generic types in class names\n                cls = self._clean_generic_type(cls)\n                pydantic_classes[cls] = module\n\n    for import_stmt in self.context_class_imports:\n        if import_stmt.startswith(\"from \") and \" import \" in import_stmt:\n            module, classes = import_stmt.split(\" import \")\n            module = module.replace(\"from \", \"\")\n\n            # Skip __main__ imports or rewrite to real module paths if possible\n            if module == \"__main__\":\n                logger.warning(f\"Skipping __main__ import: {import_stmt} - these won't work when imported\")\n                continue\n\n            for cls in classes.split(\", \"):\n                # Check if it's a TypeVar to handle duplicate definitions\n                if cls == \"T\" or cls == \"TypeVar\":\n                    typevars.add(cls)\n                    continue\n\n                # Clean up any parameterized generic types in class names\n                cls = self._clean_generic_type(cls)\n                # If this class is already imported in pydantic imports, skip it\n                if cls in pydantic_classes:\n                    logger.debug(f\"Skipping duplicate context import for {cls}, already in pydantic imports\")\n                    continue\n                context_classes[cls] = module\n\n    # Rebuild import statements\n    module_to_classes = {}\n    for cls, module in pydantic_classes.items():\n        if module not in module_to_classes:\n            module_to_classes[module] = []\n        module_to_classes[module].append(cls)\n\n    deduplicated_pydantic_imports = set()\n    for module, classes in module_to_classes.items():\n        deduplicated_pydantic_imports.add(f\"from {module} import {', '.join(sorted(classes))}\")\n\n    # Same for context imports\n    module_to_classes = {}\n    for cls, module in context_classes.items():\n        if module not in module_to_classes:\n            module_to_classes[module] = []\n        module_to_classes[module].append(cls)\n\n    deduplicated_context_imports = set()\n    for module, classes in module_to_classes.items():\n        deduplicated_context_imports.add(f\"from {module} import {', '.join(sorted(classes))}\")\n\n    logger.info(f\"Final pydantic imports: {deduplicated_pydantic_imports}\")\n    logger.info(f\"Final context imports: {deduplicated_context_imports}\")\n\n    # Log any TypeVar names we're skipping\n    if typevars:\n        logger.info(f\"Skipping TypeVar imports: {typevars} - these will be defined locally\")\n\n    return {\"pydantic\": deduplicated_pydantic_imports, \"context\": deduplicated_context_imports}\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.imports.ImportHandler.get_required_imports","title":"<code>get_required_imports(field_type_str)</code>","text":"<p>Get typing and custom type imports required for a field type.</p> <p>Parameters:</p> Name Type Description Default <code>field_type_str</code> <code>str</code> <p>String representation of a field type</p> required <p>Returns:</p> Type Description <code>dict[str, list[str]]</code> <p>Dictionary with \"typing\" and \"custom\" import lists</p> Source code in <code>src/pydantic2django/core/imports.py</code> <pre><code>def get_required_imports(self, field_type_str: str) -&gt; dict[str, list[str]]:\n    \"\"\"\n    Get typing and custom type imports required for a field type.\n\n    Args:\n        field_type_str: String representation of a field type\n\n    Returns:\n        Dictionary with \"typing\" and \"custom\" import lists\n    \"\"\"\n    logger.debug(f\"Getting required imports for: {field_type_str}\")\n    self._add_typing_imports(field_type_str)\n\n    # Get custom types (non-typing types)\n    custom_types = [\n        name\n        for name in self.extra_type_imports\n        if name not in [\"List\", \"Dict\", \"Tuple\", \"Set\", \"Optional\", \"Union\", \"Any\", \"Callable\"]\n    ]\n\n    logger.debug(f\"Found custom types: {custom_types}\")\n\n    # Return the latest state of imports\n    return {\n        \"typing\": list(self.extra_type_imports),\n        \"custom\": custom_types,\n    }\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.context.ModelContext.add_field","title":"<code>add_field(field_name, field_type_str, is_optional=False, is_list=False, **kwargs)</code>","text":"<p>Add a field to the context storage.</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>Name of the field.</p> required <code>field_type_str</code> <code>str</code> <p>String representation of the field's type.</p> required <code>is_optional</code> <code>bool</code> <p>Whether the field is optional.</p> <code>False</code> <code>is_list</code> <code>bool</code> <p>Whether the field is a list.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional metadata for the field.</p> <code>{}</code> Source code in <code>src/pydantic2django/core/context.py</code> <pre><code>def add_field(\n    self,\n    field_name: str,\n    field_type_str: str,\n    is_optional: bool = False,\n    is_list: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Add a field to the context storage.\n\n    Args:\n        field_name: Name of the field.\n        field_type_str: String representation of the field's type.\n        is_optional: Whether the field is optional.\n        is_list: Whether the field is a list.\n        **kwargs: Additional metadata for the field.\n    \"\"\"\n    # Pass is_optional, is_list explicitly\n    field_context = FieldContext(\n        field_name=field_name,\n        field_type_str=field_type_str,\n        is_optional=is_optional,\n        is_list=is_list,\n        additional_metadata=kwargs,\n    )\n    self.context_fields[field_name] = field_context\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.context.ModelContext.generate_context_class_code","title":"<code>generate_context_class_code(model_context, jinja_env=None)</code>  <code>classmethod</code>","text":"<p>Generate a string representation of the context class.</p> <p>Parameters:</p> Name Type Description Default <code>model_context</code> <code>ModelContext</code> <p>The ModelContext to generate a class for</p> required <code>jinja_env</code> <code>Any | None</code> <p>Optional Jinja2 environment to use for rendering</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>String representation of the context class</p> Source code in <code>src/pydantic2django/core/context.py</code> <pre><code>@classmethod\ndef generate_context_class_code(cls, model_context: \"ModelContext\", jinja_env: Any | None = None) -&gt; str:\n    \"\"\"\n    Generate a string representation of the context class.\n\n    Args:\n        model_context: The ModelContext to generate a class for\n        jinja_env: Optional Jinja2 environment to use for rendering\n\n    Returns:\n        String representation of the context class\n    \"\"\"\n    # Create a ContextClassGenerator and use it to generate the class\n    generator = ContextClassGenerator(jinja_env=jinja_env)\n    return generator.generate_context_class(model_context)\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.context.ModelContext.get_field_by_name","title":"<code>get_field_by_name(field_name)</code>","text":"<p>Get a field context by name.</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>Name of the field to find</p> required <p>Returns:</p> Type Description <code>Optional[FieldContext]</code> <p>The FieldContext if found, None otherwise</p> Source code in <code>src/pydantic2django/core/context.py</code> <pre><code>def get_field_by_name(self, field_name: str) -&gt; Optional[FieldContext]:\n    \"\"\"\n    Get a field context by name.\n\n    Args:\n        field_name: Name of the field to find\n\n    Returns:\n        The FieldContext if found, None otherwise\n    \"\"\"\n    return self.context_fields.get(field_name)\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.context.ModelContext.get_field_type_str","title":"<code>get_field_type_str(field_name)</code>","text":"<p>Get the string representation type of a context field.</p> Source code in <code>src/pydantic2django/core/context.py</code> <pre><code>def get_field_type_str(self, field_name: str) -&gt; Optional[str]:\n    \"\"\"Get the string representation type of a context field.\"\"\"\n    field_context = self.context_fields.get(field_name)\n    return field_context.field_type_str if field_context else None\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.context.ModelContext.get_required_imports","title":"<code>get_required_imports()</code>","text":"<p>Get all required imports for the context class fields using TypeHandler.</p> Source code in <code>src/pydantic2django/core/context.py</code> <pre><code>def get_required_imports(self) -&gt; dict[str, set[str]]:  # Return sets for auto-dedup\n    \"\"\"\n    Get all required imports for the context class fields using TypeHandler.\n    \"\"\"\n    imports: dict[str, set[str]] = {\"typing\": set(), \"custom\": set()}\n\n    # Process each field\n    for _, field_context in self.context_fields.items():\n        # Use TypeHandler with the stored type string\n        type_imports = TypeHandler.get_required_imports(field_context.field_type_str)\n\n        # Add to our overall imports\n        imports[\"typing\"].update(type_imports.get(\"typing\", []))\n        imports[\"custom\"].update(type_imports.get(\"datetime\", []))  # Example specific types\n        imports[\"custom\"].update(type_imports.get(\"decimal\", []))\n        imports[\"custom\"].update(type_imports.get(\"uuid\", []))\n        # Add any other known modules TypeHandler might return\n\n        # Add Optional/List based on flags\n        if field_context.is_optional:\n            imports[\"typing\"].add(\"Optional\")\n        if field_context.is_list:\n            imports[\"typing\"].add(\"List\")\n\n    # Add base source model import\n    source_module = getattr(self.source_class, \"__module__\", None)\n    source_name = getattr(self.source_class, \"__name__\", None)\n    if source_module and source_name and source_module != \"builtins\":\n        imports[\"custom\"].add(f\"from {source_module} import {source_name}\")\n\n    # Add BaseModel or dataclass import\n    if isinstance(self.source_class, type) and issubclass(self.source_class, BaseModel):\n        imports[\"custom\"].add(\"from pydantic import BaseModel\")\n    elif dataclasses.is_dataclass(self.source_class):\n        imports[\"custom\"].add(\"from dataclasses import dataclass\")\n\n    # Add Any import if needed\n    if any(\"Any\" in fc.field_type_str for fc in self.context_fields.values()):\n        imports[\"typing\"].add(\"Any\")\n\n    return imports\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.context.ModelContext.get_value","title":"<code>get_value(field_name)</code>","text":"<p>Get the value of a context field.</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>Name of the field</p> required <p>Returns:</p> Type Description <code>Optional[Any]</code> <p>The field value if it exists and has been set, None otherwise</p> Source code in <code>src/pydantic2django/core/context.py</code> <pre><code>def get_value(self, field_name: str) -&gt; Optional[Any]:\n    \"\"\"\n    Get the value of a context field.\n\n    Args:\n        field_name: Name of the field\n\n    Returns:\n        The field value if it exists and has been set, None otherwise\n    \"\"\"\n    field = self.get_field_by_name(field_name)\n    if field is not None:\n        return field.value\n    return None\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.context.ModelContext.set_value","title":"<code>set_value(field_name, value)</code>","text":"<p>Set the value for a context field.</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>Name of the field</p> required <code>value</code> <code>Any</code> <p>Value to set</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the field doesn't exist in the context</p> Source code in <code>src/pydantic2django/core/context.py</code> <pre><code>def set_value(self, field_name: str, value: Any) -&gt; None:\n    \"\"\"\n    Set the value for a context field.\n\n    Args:\n        field_name: Name of the field\n        value: Value to set\n\n    Raises:\n        ValueError: If the field doesn't exist in the context\n    \"\"\"\n    field = self.get_field_by_name(field_name)\n    if field is None:\n        raise ValueError(f\"Field {field_name} not found in context\")\n    field.value = value\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.context.ModelContext.to_conversion_dict","title":"<code>to_conversion_dict()</code>","text":"<p>Convert context to a dictionary format suitable for conversion back to source object.</p> Source code in <code>src/pydantic2django/core/context.py</code> <pre><code>def to_conversion_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert context to a dictionary format suitable for conversion back to source object.\"\"\"\n    # Renamed from to_dict to be more generic\n    return {\n        field_name: field_context.value\n        for field_name, field_context in self.context_fields.items()\n        if field_context.value is not None\n    }\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.context.ModelContext.validate_context","title":"<code>validate_context(context)</code>","text":"<p>Validate that all required context fields are present.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>dict[str, Any]</code> <p>The context dictionary to validate</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If required context fields are missing</p> Source code in <code>src/pydantic2django/core/context.py</code> <pre><code>def validate_context(self, context: dict[str, Any]) -&gt; None:\n    \"\"\"\n    Validate that all required context fields are present.\n\n    Args:\n        context: The context dictionary to validate\n\n    Raises:\n        ValueError: If required context fields are missing\n    \"\"\"\n\n    missing_fields = self.required_context_keys - set(context.keys())\n    if missing_fields:\n        raise ValueError(f\"Missing required context fields: {', '.join(missing_fields)}\")\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.context.ContextClassGenerator.__init__","title":"<code>__init__(jinja_env=None)</code>","text":"<p>Initialize the ContextClassGenerator.</p> <p>Parameters:</p> Name Type Description Default <code>jinja_env</code> <code>Any | None</code> <p>Optional Jinja2 environment to use for template rendering.</p> <code>None</code> Source code in <code>src/pydantic2django/core/context.py</code> <pre><code>def __init__(self, jinja_env: Any | None = None) -&gt; None:\n    \"\"\"\n    Initialize the ContextClassGenerator.\n\n    Args:\n        jinja_env: Optional Jinja2 environment to use for template rendering.\n    \"\"\"\n    self.jinja_env = jinja_env\n    # Initialize imports needed for the context class generation\n    self.imports: dict[str, set[str]] = {\"typing\": set(), \"custom\": set()}\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.core.context.ContextClassGenerator.generate_context_class","title":"<code>generate_context_class(model_context)</code>","text":"<p>Generates the Python code string for a context dataclass.</p> Source code in <code>src/pydantic2django/core/context.py</code> <pre><code>def generate_context_class(self, model_context: ModelContext) -&gt; str:\n    \"\"\"\n    Generates the Python code string for a context dataclass.\n    \"\"\"\n    template = self._load_template(\"context_class.py.j2\")\n    self.imports = model_context.get_required_imports()  # Get imports first\n\n    field_definitions = []\n    for field_name, field_context in model_context.context_fields.items():\n        field_type_str = field_context.field_type_str  # field_type is now the string representation\n\n        # Use TypeHandler._get_raw_type_string to get the clean, unquoted type string\n        # --- Corrected import path for TypeHandler ---\n        from .typing import TypeHandler\n\n        clean_type = TypeHandler._get_raw_type_string(field_type_str)\n\n        # Simplify the type string for display\n        simplified_type = self._simplify_type_string(clean_type)\n\n        # Add necessary imports based on the simplified type\n        # (Assuming _simplify_type_string and get_required_imports handle this)\n\n        # Format default value if present\n        default_repr = repr(field_context.value) if field_context.value is not None else \"None\"\n\n        field_def = f\"    {field_name}: {simplified_type} = field(default={default_repr})\"\n        field_definitions.append(field_def)\n\n    # Prepare imports for the template\n    typing_imports_str = \", \".join(sorted(self.imports[\"typing\"]))\n    custom_imports_list = sorted(self.imports[\"custom\"])  # Keep as list of strings\n\n    model_name = self._clean_generic_type(model_context.django_model.__name__)\n    source_class_name = self._clean_generic_type(model_context.source_class.__name__)\n\n    return template.render(\n        model_name=model_name,\n        # Use source_class_name instead of pydantic_class\n        source_class_name=source_class_name,\n        source_module=model_context.source_class.__module__,\n        field_definitions=\"\\n\".join(field_definitions),\n        typing_imports=typing_imports_str,\n        custom_imports=custom_imports_list,\n    )\n</code></pre>"},{"location":"Architecture/OVERVIEW/#what-all-implementations-have-in-common","title":"What all implementations have in common","text":"<p>Each implementation integrates the same core concepts:</p> <ul> <li>A <code>Discovery</code> that lists eligible source models and computes a safe registration order.</li> <li>A <code>ModelFactory</code> and <code>FieldFactory</code> pair that build Django fields using the shared <code>BidirectionalTypeMapper</code>.</li> <li>A <code>Generator</code> that subclasses <code>BaseStaticGenerator</code>, wires up discovery/factories, and prepares template context.</li> <li>Use of <code>RelationshipConversionAccessor</code> so relationship fields (FK/O2O/M2M) can be resolved across generated models.</li> <li>Shared <code>ImportHandler</code>, <code>TypeHandler</code>, and <code>ModelContext</code> mechanisms.</li> </ul>"},{"location":"Architecture/OVERVIEW/#implementations","title":"Implementations","text":""},{"location":"Architecture/OVERVIEW/#pydantic","title":"Pydantic","text":"<ul> <li>Discovery, factory, and generator:</li> <li> <p>               Bases: <code>BaseDiscovery[type[BaseModel]]</code></p> <p>Discovers Pydantic models within specified packages.</p> Source code in <code>src/pydantic2django/pydantic/discovery.py</code> <pre><code>class PydanticDiscovery(BaseDiscovery[type[BaseModel]]):\n    \"\"\"Discovers Pydantic models within specified packages.\"\"\"\n\n    # __init__ is inherited and sufficient\n\n    def _is_target_model(self, obj: Any) -&gt; bool:\n        \"\"\"Check if an object is a Pydantic BaseModel, excluding the base itself.\"\"\"\n        return inspect.isclass(obj) and issubclass(obj, BaseModel) and obj is not BaseModel\n\n    def _default_eligibility_filter(self, model: type[BaseModel]) -&gt; bool:\n        \"\"\"Check default eligibility: not abstract and not inheriting directly from ABC.\"\"\"\n        # Skip models that directly inherit from ABC\n        if abc.ABC in model.__bases__:\n            logger.debug(f\"Filtering out {model.__name__} (inherits directly from ABC)\")\n            return False\n\n        # Skip models that are marked as abstract\n        if getattr(model, \"__abstract__\", False):\n            logger.debug(f\"Filtering out {model.__name__} (marked as __abstract__)\")\n            return False\n\n        # Example for potentially filtering Pydantic internal models (uncomment if needed)\n        # if model.__module__.startswith('pydantic._internal'):\n        #     logger.debug(f\"Filtering out internal Pydantic model: {model.__name__}\")\n        #     return False\n\n        return True  # Eligible by default\n\n    def discover_models(\n        self,\n        packages: list[str],\n        app_label: str,\n        user_filters: Optional[\n            Union[Callable[[type[BaseModel]], bool], list[Callable[[type[BaseModel]], bool]]]\n        ] = None,\n    ):\n        \"\"\"Discover Pydantic models in the specified packages, applying filters.\"\"\"\n        # Pass user_filters directly to the base class method\n        super().discover_models(packages, app_label, user_filters=user_filters)\n\n    # --- analyze_dependencies and get_models_in_registration_order remain ---\n\n    def analyze_dependencies(self) -&gt; None:\n        \"\"\"Build the dependency graph for the filtered Pydantic models.\"\"\"\n        logger.info(\"Analyzing dependencies between filtered Pydantic models...\")\n        self.dependencies: dict[type[BaseModel], set[type[BaseModel]]] = {}\n\n        filtered_model_qualnames = set(self.filtered_models.keys())\n\n        def _find_and_add_dependency(model_type: type[BaseModel], potential_dep_type: Any):\n            if not self._is_target_model(potential_dep_type):\n                return\n\n            dep_qualname = f\"{potential_dep_type.__module__}.{potential_dep_type.__name__}\"\n\n            if dep_qualname in filtered_model_qualnames and potential_dep_type is not model_type:\n                dep_model_obj = self.filtered_models.get(dep_qualname)\n                if dep_model_obj:\n                    if model_type in self.dependencies:\n                        self.dependencies[model_type].add(dep_model_obj)\n                    else:\n                        # Initialize if missing (shouldn't happen often with new base discover_models)\n                        logger.warning(\n                            f\"Model {model_type.__name__} wasn't pre-initialized in dependencies dict during analysis. Initializing now.\"\n                        )\n                        self.dependencies[model_type] = {dep_model_obj}\n                else:\n                    logger.warning(\n                        f\"Inconsistency: Dependency '{dep_qualname}' for model '{model_type.__name__}' found by name but not object in filtered set.\"\n                    )\n\n        # Initialize keys based on filtered models (important step)\n        for model_type in self.filtered_models.values():\n            self.dependencies[model_type] = set()\n\n        # Analyze fields\n        for model_type in self.filtered_models.values():\n            for field in model_type.model_fields.values():\n                annotation = field.annotation\n                if annotation is None:\n                    continue\n\n                origin = get_origin(annotation)\n                args = get_args(annotation)\n\n                if origin is Union and type(None) in args and len(args) == 2:\n                    annotation = next(arg for arg in args if arg is not type(None))\n                    origin = get_origin(annotation)\n                    args = get_args(annotation)\n\n                _find_and_add_dependency(model_type, annotation)\n\n                if origin in (list, dict, set, tuple):\n                    for arg in args:\n                        arg_origin = get_origin(arg)\n                        arg_args = get_args(arg)\n\n                        if arg_origin is Union and type(None) in arg_args and len(arg_args) == 2:\n                            nested_model_type = next(t for t in arg_args if t is not type(None))\n                            _find_and_add_dependency(model_type, nested_model_type)\n                        else:\n                            _find_and_add_dependency(model_type, arg)\n\n        logger.info(\"Dependency analysis complete.\")\n        # Debug logging moved inside BaseDiscovery\n\n    def get_models_in_registration_order(self) -&gt; list[type[BaseModel]]:\n        \"\"\"\n        Return models sorted topologically based on dependencies.\n        Models with no dependencies come first.\n        \"\"\"\n        if not self.dependencies:\n            logger.warning(\"No dependencies found or analyzed, returning Pydantic models in arbitrary order.\")\n            return list(self.filtered_models.values())\n\n        sorted_models = []\n        visited: set[type[BaseModel]] = set()\n        visiting: set[type[BaseModel]] = set()\n        filtered_model_objects = set(self.filtered_models.values())\n\n        def visit(model: type[BaseModel]):\n            if model in visited:\n                return\n            if model in visiting:\n                logger.error(f\"Circular dependency detected involving Pydantic model {model.__name__}\")\n                # Option: raise TypeError(...)\n                return  # Break cycle\n\n            visiting.add(model)\n\n            if model in self.dependencies:\n                # Use .get for safety, ensure deps are also in filtered set\n                for dep in self.dependencies.get(model, set()):\n                    if dep in filtered_model_objects:\n                        visit(dep)\n\n            visiting.remove(model)\n            visited.add(model)\n            sorted_models.append(model)\n\n        all_target_models = list(self.filtered_models.values())\n        for model in all_target_models:\n            if model not in visited:\n                visit(model)\n\n        logger.info(f\"Pydantic models sorted for registration: {[m.__name__ for m in sorted_models]}\")\n        return sorted_models\n</code></pre> </li> <li> <p>               Bases: <code>BaseModelFactory[type[BaseModel], FieldInfo]</code></p> <p>Creates Django models from Pydantic models.</p> Source code in <code>src/pydantic2django/pydantic/factory.py</code> <pre><code>class PydanticModelFactory(BaseModelFactory[type[BaseModel], FieldInfo]):\n    \"\"\"Creates Django models from Pydantic models.\"\"\"\n\n    # Cache specific to Pydantic models\n    _converted_models: dict[str, ConversionCarrier[type[BaseModel]]] = {}\n\n    relationship_accessor: RelationshipConversionAccessor\n    # No need for field_factory instance here if Base class handles it\n\n    def __init__(self, field_factory: PydanticFieldFactory, relationship_accessor: RelationshipConversionAccessor):\n        \"\"\"Initialize with field factory and relationship accessor.\"\"\"\n        self.relationship_accessor = relationship_accessor\n        # Pass the field_factory up to the base class\n        super().__init__(field_factory=field_factory)\n\n    # Overrides the base method to add caching and relationship mapping\n    def make_django_model(self, carrier: ConversionCarrier[type[BaseModel]]) -&gt; None:\n        \"\"\"Creates a Django model from Pydantic, checking cache first and mapping relationships.\"\"\"\n        model_key = carrier.model_key()\n        logger.debug(f\"PydanticFactory: Attempting to create Django model for {model_key}\")\n\n        # --- Check Cache --- #\n        if model_key in self._converted_models and not carrier.existing_model:\n            logger.debug(f\"PydanticFactory: Using cached conversion result for {model_key}\")\n            cached_carrier = self._converted_models[model_key]\n            # Update the passed-in carrier with cached results\n            carrier.__dict__.update(cached_carrier.__dict__)\n            # Ensure used_related_names is properly updated (dict update might not merge sets correctly)\n            for target, names in cached_carrier.used_related_names_per_target.items():\n                carrier.used_related_names_per_target.setdefault(target, set()).update(names)\n            return\n\n        # --- Call Base Implementation for Core Logic --- #\n        # This calls _process_source_fields, _assemble_django_model_class etc.\n        super().make_django_model(carrier)\n\n        # --- Register Relationship Mapping (if successful) --- #\n        if carrier.source_model and carrier.django_model:\n            logger.debug(\n                f\"PydanticFactory: Registering mapping for {carrier.source_model.__name__} -&gt; {carrier.django_model.__name__}\"\n            )\n            self.relationship_accessor.map_relationship(\n                source_model=carrier.source_model, django_model=carrier.django_model\n            )\n\n        # --- Cache Result --- #\n        if carrier.django_model and not carrier.existing_model:\n            logger.debug(f\"PydanticFactory: Caching conversion result for {model_key}\")\n            # Store a copy to prevent modification issues? Simple assignment for now.\n            self._converted_models[model_key] = carrier\n        elif not carrier.django_model:\n            logger.error(\n                f\"PydanticFactory: Failed to create Django model for {model_key}. Invalid fields: {carrier.invalid_fields}\"\n            )\n\n    # --- Implementation of Abstract Methods --- #\n\n    def _process_source_fields(self, carrier: ConversionCarrier[type[BaseModel]]):\n        \"\"\"Iterate through Pydantic fields and convert them using the field factory.\"\"\"\n        source_model = carrier.source_model\n        model_name = source_model.__name__\n\n        for field_name_original, field_info in get_model_fields(cast(type[BaseModel], source_model)).items():\n            field_name = field_info.alias or field_name_original\n\n            # Normalize the output Django field identifier consistently with XML/dataclass\n            try:\n                from ..core.utils.naming import sanitize_field_identifier\n\n                normalized_field_name = sanitize_field_identifier(field_name)\n            except Exception:\n                normalized_field_name = field_name\n\n            # Skip 'id' field if updating an existing model definition\n            # Note: _handle_id_field in field factory handles primary key logic\n            if field_name.lower() == \"id\" and carrier.existing_model:\n                logger.debug(f\"Skipping 'id' field for existing model update: {carrier.existing_model.__name__}\")\n                continue\n\n            # Cast needed because BaseFactory uses generic TFieldInfo\n            field_factory_typed = cast(PydanticFieldFactory, self.field_factory)\n            conversion_result = field_factory_typed.create_field(\n                field_info=field_info, model_name=model_name, carrier=carrier\n            )\n\n            # Store results in the carrier\n            if conversion_result.django_field:\n                # Store definition string first\n                if conversion_result.field_definition_str:\n                    carrier.django_field_definitions[normalized_field_name] = conversion_result.field_definition_str\n                else:\n                    logger.warning(\n                        f\"Missing field definition string for successfully created field '{normalized_field_name}'\"\n                    )\n\n                # Store the field instance itself\n                if isinstance(\n                    conversion_result.django_field, (models.ForeignKey, models.ManyToManyField, models.OneToOneField)\n                ):\n                    carrier.relationship_fields[normalized_field_name] = conversion_result.django_field\n                else:\n                    carrier.django_fields[normalized_field_name] = conversion_result.django_field\n\n            elif conversion_result.context_field:\n                carrier.context_fields[normalized_field_name] = conversion_result.context_field\n            elif conversion_result.error_str:\n                carrier.invalid_fields.append((normalized_field_name, conversion_result.error_str))\n            else:\n                # Should not happen if FieldConversionResult is used correctly\n                error = f\"Field factory returned unexpected empty result for {model_name}.{field_name_original}\"\n                logger.error(error)\n                carrier.invalid_fields.append((normalized_field_name, error))\n\n    def _build_pydantic_model_context(self, carrier: ConversionCarrier[type[BaseModel]]):\n        \"\"\"Builds the ModelContext specifically for Pydantic source models.\"\"\"\n        # Renamed to match base class expectation\n        self._build_model_context(carrier)\n\n    # Actual implementation of the abstract method\n    def _build_model_context(self, carrier: ConversionCarrier[type[BaseModel]]):\n        \"\"\"Builds the ModelContext specifically for Pydantic source models.\"\"\"\n        if not carrier.source_model or not carrier.django_model:\n            logger.debug(\"Skipping context build: missing source or django model.\")\n            return\n\n        try:\n            model_context = ModelContext(  # Removed generic type hint for base class compatibility\n                django_model=carrier.django_model,\n                source_class=carrier.source_model,\n            )\n            for field_name, field_info in carrier.context_fields.items():\n                if isinstance(field_info, FieldInfo) and field_info.annotation is not None:\n                    optional = is_pydantic_model_field_optional(field_info.annotation)\n                    # Use repr() for field_type_str as expected by ModelContext.add_field\n                    field_type_str = repr(field_info.annotation)\n                    model_context.add_field(\n                        field_name=field_name,\n                        field_type_str=field_type_str,  # Pass string representation\n                        is_optional=optional,\n                        annotation=field_info.annotation,  # Keep annotation if needed elsewhere\n                    )\n                elif isinstance(field_info, FieldInfo):\n                    logger.warning(f\"Context field '{field_name}' has no annotation, cannot add to ModelContext.\")\n                else:\n                    logger.warning(\n                        f\"Context field '{field_name}' is not a FieldInfo ({type(field_info)}), cannot add to ModelContext.\"\n                    )\n            carrier.model_context = model_context\n            logger.debug(f\"Successfully built ModelContext for {carrier.model_key()}\")  # Call model_key()\n        except Exception as e:\n            logger.error(f\"Failed to build ModelContext for {carrier.model_key()}: {e}\", exc_info=True)\n            carrier.model_context = None\n</code></pre> </li> <li> <p>               Bases: <code>BaseFieldFactory[FieldInfo]</code></p> <p>Creates Django fields from Pydantic fields (FieldInfo).</p> Source code in <code>src/pydantic2django/pydantic/factory.py</code> <pre><code>class PydanticFieldFactory(BaseFieldFactory[FieldInfo]):\n    \"\"\"Creates Django fields from Pydantic fields (FieldInfo).\"\"\"\n\n    # Dependencies injected\n    relationship_accessor: RelationshipConversionAccessor\n    bidirectional_mapper: BidirectionalTypeMapper\n\n    def __init__(\n        self, relationship_accessor: RelationshipConversionAccessor, bidirectional_mapper: BidirectionalTypeMapper\n    ):\n        \"\"\"Initializes with dependencies.\"\"\"\n        self.relationship_accessor = relationship_accessor\n        self.bidirectional_mapper = bidirectional_mapper\n        # No super().__init__() needed\n\n    def create_field(\n        self, field_info: FieldInfo, model_name: str, carrier: ConversionCarrier[type[BaseModel]]\n    ) -&gt; FieldConversionResult:\n        \"\"\"\n        Convert a Pydantic FieldInfo to a Django field instance.\n        Implements the abstract method from BaseFieldFactory.\n        Uses BidirectionalTypeMapper and local instantiation.\n        \"\"\"\n        # Use alias first, then the actual key from model_fields as name\n        field_name = field_info.alias or next(\n            (k for k, v in carrier.source_model.model_fields.items() if v is field_info), \"&lt;unknown&gt;\"\n        )\n\n        # Initialize result with the source field info and determined name\n        result = FieldConversionResult(field_info=field_info, field_name=field_name)\n\n        try:\n            # Handle potential 'id' field conflict\n            if id_field := self._handle_id_field(field_name, field_info):\n                result.django_field = id_field\n                # Need to capture kwargs for serialization if possible\n                # For now, assume default kwargs for ID fields\n                # TODO: Extract actual kwargs used in _handle_id_field\n                result.field_kwargs = {\"primary_key\": True}\n                if isinstance(id_field, models.CharField):\n                    result.field_kwargs[\"max_length\"] = getattr(id_field, \"max_length\", 255)\n                elif isinstance(id_field, models.UUIDField):\n                    pass  # No extra kwargs needed typically\n                else:  # AutoField\n                    pass  # No extra kwargs needed typically\n\n                result.field_definition_str = self._generate_field_def_string(result, carrier.meta_app_label)\n                return result  # ID field handled, return early\n\n            # Get field type from annotation\n            field_type = field_info.annotation\n            if field_type is None:\n                logger.warning(f\"Field '{model_name}.{field_name}' has no annotation, treating as context field.\")\n                result.context_field = field_info\n                return result\n\n            # --- Use BidirectionalTypeMapper --- #\n            try:\n                django_field_class, constructor_kwargs = self.bidirectional_mapper.get_django_mapping(\n                    python_type=field_type, field_info=field_info\n                )\n            except MappingError as e:\n                # Handle errors specifically from the mapper (e.g., missing relationship)\n                logger.error(f\"Mapping error for '{model_name}.{field_name}' (type: {field_type}): {e}\")\n                result.error_str = str(e)\n                result.context_field = field_info  # Treat as context on mapping error\n                return result\n            except Exception as e:\n                # Handle unexpected errors during mapping lookup\n                logger.error(\n                    f\"Unexpected error getting Django mapping for '{model_name}.{field_name}': {e}\", exc_info=True\n                )\n                result.error_str = f\"Unexpected mapping error: {e}\"\n                result.context_field = field_info\n                return result\n\n            # Store raw kwargs before modifications/checks\n            result.raw_mapper_kwargs = constructor_kwargs.copy()\n\n            # --- Check for Multi-FK Union Signal --- #\n            union_details = constructor_kwargs.pop(\"_union_details\", None)\n            if union_details and isinstance(union_details, dict):\n                # If GFK mode is enabled and policy says to use it, record as pending GFK child\n                if getattr(carrier, \"enable_gfk\", False) and self._should_route_to_gfk(union_details, carrier):\n                    logger.info(\n                        f\"[GFK] Routing union field '{field_name}' on '{model_name}' to GenericEntry (policy={carrier.gfk_policy}).\"\n                    )\n                    carrier.pending_gfk_children.append(\n                        {\n                            \"field_name\": field_name,\n                            \"union_details\": union_details,\n                            \"model_name\": model_name,\n                        }\n                    )\n                    # Do not generate a concrete field for this union\n                    return result\n                # Otherwise, fall back to existing multi-FK behavior\n                logger.info(f\"Detected multi-FK union signal for '{field_name}'. Deferring field generation.\")\n                # Store the original field name and the details for the generator\n                carrier.pending_multi_fk_unions.append((field_name, union_details))\n                return result  # Return early, deferring generation\n\n            # --- Check for GFK placeholder signal from mapper --- #\n            gfk_details = constructor_kwargs.pop(\"_gfk_details\", None)\n            if gfk_details and isinstance(gfk_details, dict):\n                if getattr(carrier, \"enable_gfk\", False):\n                    logger.info(\n                        f\"[GFK] Mapper signaled GFK for '{field_name}' on '{model_name}'. Recording as pending GFK child.\"\n                    )\n                    carrier.pending_gfk_children.append(\n                        {\n                            \"field_name\": field_name,\n                            \"gfk_details\": gfk_details,\n                            \"model_name\": model_name,\n                        }\n                    )\n                    # Do not generate a concrete field\n                    return result\n                else:\n                    logger.warning(\n                        f\"Received _gfk_details for '{field_name}' but enable_gfk is False. Falling back to JSON field.\"\n                    )\n\n            # --- Handle Relationships Specifically (Adjust Kwargs) --- #\n            # Check if it's a relationship type *after* getting mapping AND checking for union signal\n            is_relationship = issubclass(\n                django_field_class, (models.ForeignKey, models.OneToOneField, models.ManyToManyField)\n            )\n\n            if is_relationship:\n                # Apply specific relationship logic (like related_name uniqueness)\n                # The mapper should have set 'to' and basic 'on_delete'\n                if \"to\" not in constructor_kwargs:\n                    # This indicates an issue in the mapper or relationship accessor setup\n                    result.error_str = f\"Mapper failed to determine 'to' for relationship field '{field_name}'.\"\n                    logger.error(result.error_str)\n                    result.context_field = field_info\n                    return result\n\n                # Sanitize and ensure unique related_name\n                # Check Pydantic Field(..., json_schema_extra={\"related_name\": ...})\n                user_related_name = (\n                    field_info.json_schema_extra.get(\"related_name\")\n                    if isinstance(field_info.json_schema_extra, dict)\n                    else None\n                )\n                target_django_model_str = constructor_kwargs[\"to\"]  # Mapper returns string like app_label.ModelName\n\n                # Try to get the actual target model class to pass to sanitize_related_name if possible\n                # This relies on the target model being importable/available\n                target_model_cls = None\n                target_model_cls_name_only = target_django_model_str  # Default fallback\n                try:\n                    app_label, model_cls_name = target_django_model_str.split(\".\")\n                    target_model_cls = apps.get_model(app_label, model_cls_name)  # Use apps.get_model\n                    target_model_cls_name_only = model_cls_name  # Use name from split\n                except Exception:\n                    logger.warning(\n                        f\"Could not get target model class for '{target_django_model_str}' when generating related_name for '{field_name}'. Using model name string.\"\n                    )\n                    # Fallback: try splitting by dot just for name, otherwise use whole string\n                    target_model_cls_name_only = target_django_model_str.split(\".\")[-1]\n\n                related_name_base = (\n                    user_related_name\n                    if user_related_name\n                    else f\"{carrier.source_model.__name__.lower()}_{field_name}_set\"\n                )\n                final_related_name_base = sanitize_related_name(\n                    str(related_name_base),\n                    target_model_cls.__name__ if target_model_cls else target_model_cls_name_only,\n                    field_name,\n                )\n\n                # Ensure uniqueness using carrier's tracker\n                target_model_key_for_tracker = (\n                    target_model_cls.__name__ if target_model_cls else target_django_model_str\n                )\n                target_related_names = carrier.used_related_names_per_target.setdefault(\n                    target_model_key_for_tracker, set()\n                )\n                unique_related_name = final_related_name_base\n                counter = 1\n                while unique_related_name in target_related_names:\n                    unique_related_name = f\"{final_related_name_base}_{counter}\"\n                    counter += 1\n                target_related_names.add(unique_related_name)\n                constructor_kwargs[\"related_name\"] = unique_related_name\n                logger.debug(f\"[REL] Field '{field_name}': Assigning related_name='{unique_related_name}'\")\n\n                # Re-confirm on_delete (mapper should set default based on Optional)\n                if (\n                    django_field_class in (models.ForeignKey, models.OneToOneField)\n                    and \"on_delete\" not in constructor_kwargs\n                ):\n                    is_optional = is_pydantic_model_field_optional(field_type)\n                    constructor_kwargs[\"on_delete\"] = models.SET_NULL if is_optional else models.CASCADE\n                elif django_field_class == models.ManyToManyField:\n                    constructor_kwargs.pop(\"on_delete\", None)\n                    # M2M doesn't use null=True, mapper handles this\n                    constructor_kwargs.pop(\"null\", None)\n                    constructor_kwargs[\"blank\"] = constructor_kwargs.get(\"blank\", True)  # M2M usually blank=True\n\n            # --- Perform Instantiation Locally --- #\n            try:\n                logger.debug(\n                    f\"Instantiating {django_field_class.__name__} for '{field_name}' with kwargs: {constructor_kwargs}\"\n                )\n                result.django_field = django_field_class(**constructor_kwargs)\n                result.field_kwargs = constructor_kwargs  # Store final kwargs\n            except Exception as e:\n                error_msg = f\"Failed to instantiate Django field '{field_name}' (type: {django_field_class.__name__}) with kwargs {constructor_kwargs}: {e}\"\n                logger.error(error_msg, exc_info=True)\n                result.error_str = error_msg\n                result.context_field = field_info  # Fallback to context\n                return result\n\n            # --- Generate Field Definition String --- #\n            result.field_definition_str = self._generate_field_def_string(result, carrier.meta_app_label)\n\n            return result  # Success\n\n        except Exception as e:\n            # Catch-all for unexpected errors during conversion\n            error_msg = f\"Unexpected error converting field '{model_name}.{field_name}': {e}\"\n            logger.error(error_msg, exc_info=True)\n            result.error_str = error_msg\n            result.context_field = field_info  # Fallback to context\n            return result\n\n    def _should_route_to_gfk(self, union_details: dict, carrier: ConversionCarrier[type[BaseModel]]) -&gt; bool:\n        \"\"\"Return True if this union field should be handled via GFK based on carrier policy.\n\n        For now, support simple policies:\n        - \"all_nested\": always route\n        - \"threshold_by_children\": route when number of union models &gt;= gfk_threshold_children\n        Otherwise: False.\n        \"\"\"\n        try:\n            policy = (carrier.gfk_policy or \"\").strip()\n            if policy == \"all_nested\":\n                return True\n            if policy == \"threshold_by_children\":\n                threshold = carrier.gfk_threshold_children or 0\n                models_in_union = len(union_details.get(\"models\", []) or [])\n                return models_in_union &gt;= threshold if threshold &gt; 0 else False\n        except Exception:\n            pass\n        return False\n\n    def _generate_field_def_string(self, result: FieldConversionResult, app_label: str) -&gt; str:\n        \"\"\"Generates the field definition string safely.\"\"\"\n        if not result.django_field:\n            return \"# Field generation failed\"\n        try:\n            if result.field_kwargs:\n                return generate_field_definition_string(type(result.django_field), result.field_kwargs, app_label)\n            else:\n                logger.warning(\n                    f\"Could not generate definition string for '{result.field_name}': final kwargs not found in result. Using basic serialization.\"\n                )\n                return FieldSerializer.serialize_field(result.django_field)\n        except Exception as e:\n            logger.error(\n                f\"Failed to generate field definition string for '{result.field_name}': {e}\",\n                exc_info=True,\n            )\n            return f\"# Error generating definition: {e}\"\n\n    def _handle_id_field(self, field_name: str, field_info: FieldInfo) -&gt; Optional[models.Field]:\n        \"\"\"Handle potential ID field naming conflicts (logic moved from original factory).\"\"\"\n        if field_name.lower() == \"id\":\n            field_type = field_info.annotation\n            # Default to AutoField unless explicitly specified by type\n            field_class = models.AutoField\n            field_kwargs = {\"primary_key\": True, \"verbose_name\": \"ID\"}\n\n            # Use mapper to find appropriate Django PK field if type is specified\n            # But only override AutoField if it's clearly not a standard int sequence\n            pk_field_class_override = None\n            if field_type is UUID:\n                pk_field_class_override = models.UUIDField\n                field_kwargs.pop(\"verbose_name\")  # UUIDField doesn't need verbose_name='ID'\n            elif field_type is str:\n                # Default Pydantic str ID to CharField PK\n                pk_field_class_override = models.CharField\n                field_kwargs[\"max_length\"] = 255  # Default length\n            elif field_type is int:\n                pass  # Default AutoField is fine\n            elif field_type:\n                # Check if mapper finds a specific non-auto int field (e.g., BigIntegerField)\n                try:\n                    mapped_cls, mapped_kwargs = self.bidirectional_mapper.get_django_mapping(field_type, field_info)\n                    if issubclass(mapped_cls, models.IntegerField) and not issubclass(mapped_cls, models.AutoField):\n                        pk_field_class_override = mapped_cls\n                        field_kwargs.update(mapped_kwargs)\n                        # Ensure primary_key=True is set\n                        field_kwargs[\"primary_key\"] = True\n                    elif not issubclass(mapped_cls, models.AutoField):\n                        logger.warning(\n                            f\"Field 'id' has type {field_type} mapping to non-integer {mapped_cls.__name__}. Using AutoField PK.\"\n                        )\n                except MappingError:\n                    logger.warning(f\"Field 'id' has unmappable type {field_type}. Using AutoField PK.\")\n\n            if pk_field_class_override:\n                field_class = pk_field_class_override\n            else:\n                # Stick with AutoField, apply title if present\n                if field_info.title:\n                    field_kwargs[\"verbose_name\"] = field_info.title\n\n            logger.debug(f\"Handling field '{field_name}' as primary key using {field_class.__name__}\")\n            # Instantiate the ID field\n            try:\n                return field_class(**field_kwargs)\n            except Exception as e:\n                logger.error(\n                    f\"Failed to instantiate ID field {field_class.__name__} with kwargs {field_kwargs}: {e}\",\n                    exc_info=True,\n                )\n                # Fallback to basic AutoField? Or let error propagate?\n                # Let's return None and let the main create_field handle error reporting\n                return None\n        return None\n</code></pre> </li> <li> <p>               Bases: <code>BaseStaticGenerator[type[BaseModel], FieldInfo]</code></p> <p>Generates Django models and their context classes from Pydantic models. Inherits common logic from BaseStaticGenerator.</p> Source code in <code>src/pydantic2django/pydantic/generator.py</code> <pre><code>class StaticPydanticModelGenerator(\n    BaseStaticGenerator[type[BaseModel], FieldInfo]\n):  # TModel=type[BaseModel], TFieldInfo=FieldInfo\n    \"\"\"\n    Generates Django models and their context classes from Pydantic models.\n    Inherits common logic from BaseStaticGenerator.\n    \"\"\"\n\n    def __init__(\n        self,\n        output_path: str = \"generated_models.py\",  # Keep original default\n        packages: Optional[list[str]] = None,\n        app_label: str = \"django_app\",  # Keep original default\n        filter_function: Callable[..., bool] | None = None,\n        verbose: bool = False,\n        discovery_module: Optional[PydanticDiscovery] = None,\n        module_mappings: Optional[dict[str, str]] = None,\n        # Pydantic specific factories can be passed or constructed here\n        # NOTE: Injecting factory instances is less preferred now due to mapper dependency\n        # field_factory_instance: Optional[PydanticFieldFactory] = None,\n        # model_factory_instance: Optional[PydanticModelFactory] = None,\n        # Inject mapper instead?\n        bidirectional_mapper_instance: Optional[BidirectionalTypeMapper] = None,\n        enable_timescale: bool = True,\n        # --- GFK flags ---\n        enable_gfk: bool = True,\n        gfk_policy: str | None = \"threshold_by_children\",\n        gfk_threshold_children: int | None = 8,\n        gfk_value_mode: str | None = \"typed_columns\",\n        gfk_normalize_common_attrs: bool = False,\n    ):\n        # 1. Initialize Pydantic-specific discovery\n        # Use provided instance or create a default one\n        self.pydantic_discovery_instance = discovery_module or PydanticDiscovery()\n\n        # 2. Initialize RelationshipAccessor (needed by factories and mapper)\n        self.relationship_accessor = RelationshipConversionAccessor()\n\n        # 3. Initialize BidirectionalTypeMapper (pass relationship accessor)\n        self.bidirectional_mapper = bidirectional_mapper_instance or BidirectionalTypeMapper(\n            relationship_accessor=self.relationship_accessor\n        )\n\n        # 4. Initialize Pydantic-specific factories (pass mapper and accessor)\n        # Remove dependency on passed-in factory instances, create them here\n        self.pydantic_model_factory = create_pydantic_factory(\n            relationship_accessor=self.relationship_accessor, bidirectional_mapper=self.bidirectional_mapper\n        )\n\n        # 5. Call the base class __init__ with all required arguments\n        super().__init__(\n            output_path=output_path,\n            packages=packages or [\"pydantic_models\"],  # Default Pydantic package\n            app_label=app_label,\n            filter_function=filter_function,\n            verbose=verbose,\n            discovery_instance=self.pydantic_discovery_instance,  # Pass the specific discovery instance\n            model_factory_instance=self.pydantic_model_factory,  # Pass the newly created model factory\n            module_mappings=module_mappings,\n            base_model_class=self._get_default_base_model_class(),\n            # Jinja setup is handled by base class\n            enable_timescale=enable_timescale,\n            enable_gfk=enable_gfk,\n            gfk_policy=gfk_policy,\n            gfk_threshold_children=gfk_threshold_children,\n            gfk_value_mode=gfk_value_mode,\n            gfk_normalize_common_attrs=gfk_normalize_common_attrs,\n        )\n\n        # 6. Pydantic-specific Jinja setup or context generator\n        # Context generator needs the jinja_env from the base class\n        self.context_generator = ContextClassGenerator(jinja_env=self.jinja_env)\n\n        # 7. Track context-specific info during generation (reset in generate_models_file)\n        self.context_definitions: list[str] = []\n        self.model_has_context: dict[str, bool] = {}\n        self.context_class_names: list[str] = []\n        self.seen_context_classes: set[str] = set()\n        # Timescale classification results cached per run (name -&gt; role)\n        self._timescale_roles: dict[str, TimescaleRole] = {}\n\n    # --- Implement Abstract Methods from Base ---\n\n    def _get_source_model_name(self, carrier: ConversionCarrier[type[BaseModel]]) -&gt; str:\n        \"\"\"Get the name of the original Pydantic model.\"\"\"\n        # Ensure source_model is not None before accessing __name__\n        return carrier.source_model.__name__ if carrier.source_model else \"UnknownPydanticModel\"\n\n    def _add_source_model_import(self, carrier: ConversionCarrier[type[BaseModel]]):\n        \"\"\"Add import for the original Pydantic model.\"\"\"\n        if carrier.source_model:\n            # Use the correct method from ImportHandler\n            self.import_handler.add_pydantic_model_import(carrier.source_model)\n        else:\n            logger.warning(\"Cannot add source model import: source_model is missing from carrier.\")\n\n    def _get_models_in_processing_order(self) -&gt; list[type[BaseModel]]:\n        \"\"\"Return models in Pydantic dependency order.\"\"\"\n        # Discovery must have run first (called by base generate_models_file -&gt; discover_models)\n        # Cast the discovery_instance from the base class to the specific Pydantic type\n        discovery = cast(PydanticDiscovery, self.discovery_instance)\n        if not discovery.filtered_models:\n            logger.warning(\"No models discovered or passed filter, cannot determine processing order.\")\n            return []\n        # Ensure dependencies are analyzed if not already done (base class should handle this)\n        # if not discovery.dependencies:\n        #     discovery.analyze_dependencies() # Base class analyze_dependencies called in discover_models\n        return discovery.get_models_in_registration_order()\n\n    def _prepare_template_context(self, unique_model_definitions, django_model_names, imports) -&gt; dict:\n        \"\"\"Prepare the Pydantic-specific context for the main models_file.py.j2 template.\"\"\"\n        # Base context items (model_definitions, django_model_names, imports) are passed in.\n        # Add Pydantic-specific items gathered during generate_models_file override.\n        base_context = {\n            \"model_definitions\": unique_model_definitions,\n            \"django_model_names\": django_model_names,  # For __all__\n            # --- Imports (already structured by base class import_handler) ---\n            \"django_imports\": sorted(imports.get(\"django\", [])),\n            \"pydantic_imports\": sorted(imports.get(\"pydantic\", [])),  # Check if import handler categorizes these\n            \"general_imports\": sorted(imports.get(\"general\", [])),\n            \"context_imports\": sorted(imports.get(\"context\", [])),  # Check if import handler categorizes these\n            # It might be simpler to rely on the structured imports dict directly in the template\n            \"imports\": imports,  # Pass the whole structured dict\n            # --- Pydantic Specific ---\n            \"context_definitions\": self.context_definitions,  # Populated in generate_models_file override\n            \"all_models\": [  # This seems redundant if django_model_names covers __all__\n                f\"'{name}'\"\n                for name in django_model_names  # Use Django names for __all__ consistency?\n            ],\n            \"context_class_names\": self.context_class_names,  # Populated in generate_models_file override\n            \"model_has_context\": self.model_has_context,  # Populated in generate_models_file override\n            \"generation_source_type\": \"pydantic\",  # Flag for template logic\n        }\n        # Note: Common items like timestamp, base_model info, extra_type_imports\n        # are added by the base class generate_models_file method after calling this.\n        return base_context\n\n    def _get_model_definition_extra_context(self, carrier: ConversionCarrier[type[BaseModel]]) -&gt; dict:\n        \"\"\"Provide Pydantic-specific context for model_definition.py.j2.\"\"\"\n        context_fields_info = []\n        context_class_name = \"\"\n        has_context_for_this_model = False  # Track if this specific model has context\n\n        if carrier.model_context and carrier.model_context.context_fields:\n            has_context_for_this_model = True\n            django_model_name = (\n                self._clean_generic_type(carrier.django_model.__name__) if carrier.django_model else \"UnknownModel\"\n            )\n            context_class_name = f\"{django_model_name}Context\"\n\n            for field_name, field_context_info in carrier.model_context.context_fields.items():\n                field_type_attr = getattr(field_context_info, \"field_type\", None) or getattr(\n                    field_context_info, \"annotation\", None\n                )\n\n                if field_type_attr:\n                    type_name = TypeHandler.format_type_string(field_type_attr)\n                    # Add imports for context field types via import_handler\n                    # Use the correct method which handles nested types and typing imports\n                    self.import_handler.add_context_field_type_import(field_type_attr)\n\n                    # Remove explicit add_extra_import calls, handled by add_context_field_type_import\n                    # if getattr(field_context_info, 'is_optional', False):\n                    #      self.import_handler.add_extra_import(\"Optional\", \"typing\")\n                    # if getattr(field_context_info, 'is_list', False):\n                    #      self.import_handler.add_extra_import(\"List\", \"typing\")\n                else:\n                    type_name = \"Any\"  # Fallback\n                    logger.warning(\n                        f\"Could not determine context type annotation for field '{field_name}' in {django_model_name}\"\n                    )\n\n                context_fields_info.append((field_name, type_name))\n\n        return {\n            \"context_class_name\": context_class_name,\n            \"context_fields\": context_fields_info,\n            \"is_pydantic_source\": True,\n            \"is_dataclass_source\": False,\n            \"has_context\": has_context_for_this_model,\n            \"field_definitions\": carrier.django_field_definitions,\n        }\n\n    def _get_default_base_model_class(self) -&gt; type[models.Model]:\n        \"\"\"Return the default Django base model for Pydantic conversion.\n\n        Raises a clear ImportError if the base cannot be imported.\n        \"\"\"\n        if not django_apps.ready:\n            raise AppRegistryNotReady(\n                \"Django apps are not loaded. Call django.setup() or run within a configured Django context before \"\n                \"instantiating StaticPydanticModelGenerator.\"\n            )\n        try:\n            from typed2django.django.models import Pydantic2DjangoBaseClass as _Base\n\n            return _Base\n        except Exception as exc:  # pragma: no cover - defensive path\n            raise ImportError(\n                \"typed2django.django.models.Pydantic2DjangoBaseClass is required for Pydantic generation.\"\n            ) from exc\n\n    # --- Override generate_models_file to handle Pydantic context class generation ---\n\n    def generate_models_file(self) -&gt; str:\n        \"\"\"\n        Generates the complete models.py file content, including Pydantic context classes.\n        Overrides the base method to add context class handling during the generation loop.\n        \"\"\"\n        # 1. Base discovery and model ordering\n        self.discover_models()  # Calls base discovery and dependency analysis\n        models_to_process = self._get_models_in_processing_order()  # Uses overridden method\n\n        # 2. Reset state for this run (imports handled by base reset)\n        self.carriers = []\n        # Manually reset ImportHandler state instead of calling non-existent reset()\n        self.import_handler.extra_type_imports.clear()\n        self.import_handler.pydantic_imports.clear()\n        self.import_handler.context_class_imports.clear()\n        self.import_handler.imported_names.clear()\n        self.import_handler.processed_field_types.clear()\n\n        # Re-add base model import after clearing\n        # Note: add_pydantic_model_import might not be the right method here if base_model_class isn't Pydantic\n        # Need a more general import method on ImportHandler or handle it differently.\n        # For now, let's assume a general import is needed or handled by template.\n        # self.import_handler.add_import(self.base_model_class.__module__, self.base_model_class.__name__)\n        # Let's add it back using _add_type_import, although it's protected.\n        # A public add_general_import(module, name) on ImportHandler would be better.\n        try:\n            # This is a workaround - ideally ImportHandler would have a public method\n            self.import_handler._add_type_import(self.base_model_class)\n        except Exception as e:\n            logger.warning(f\"Could not add base model import via _add_type_import: {e}\")\n\n        # Reset Pydantic-specific tracking lists\n        self.context_definitions = []\n        self.model_has_context = {}  # Map of Pydantic model name -&gt; bool\n        self.context_class_names = []  # For __all__\n        self.seen_context_classes = set()  # For deduplication of definitions\n\n        # --- State tracking within the loop ---\n        model_definitions = []  # Store generated Django model definition strings\n        django_model_names = []  # Store generated Django model names for __all__\n        context_only_models = []  # Track Pydantic models yielding only context\n        gfk_used = False\n\n        # 3. Setup Django models (populates self.carriers via base method calling factory)\n        for source_model in models_to_process:\n            self.setup_django_model(source_model)  # Uses base setup_django_model\n\n        # 4. Generate definitions (Django models AND Pydantic Context classes)\n        for carrier in self.carriers:\n            model_name = self._get_source_model_name(carrier)  # Pydantic model name\n\n            try:\n                # --- GFK finalize hook: inject GenericRelation on parents ---\n                if getattr(carrier, \"enable_gfk\", False) and getattr(carrier, \"pending_gfk_children\", None):\n                    # Ensure imports for contenttypes\n                    self.import_handler.add_import(\"django.contrib.contenttypes.fields\", \"GenericRelation\")\n                    self.import_handler.add_import(\"django.contrib.contenttypes.fields\", \"GenericForeignKey\")\n                    self.import_handler.add_import(\"django.contrib.contenttypes.models\", \"ContentType\")\n\n                    # Inject a GenericRelation field into the parent model definition strings\n                    # Field name 'entries' for reverse access\n                    try:\n                        carrier.django_field_definitions[\n                            \"entries\"\n                        ] = \"GenericRelation('GenericEntry', related_query_name='entries')\"\n                    except Exception:\n                        pass\n                    gfk_used = True\n\n                django_model_def = \"\"\n                django_model_name_cleaned = \"\"\n\n                # --- A. Generate Django Model Definition (if applicable) ---\n                if carrier.django_model:\n                    # Check fields using safe getattr for many_to_many\n                    has_concrete_fields = any(not f.primary_key for f in carrier.django_model._meta.fields)\n                    # Use getattr for safety\n                    m2m_fields = getattr(carrier.django_model._meta, \"many_to_many\", [])\n                    has_m2m = bool(m2m_fields)\n                    has_fields = bool(carrier.django_model._meta.fields)\n\n                    if has_concrete_fields or has_m2m or (not has_concrete_fields and not has_m2m and has_fields):\n                        django_model_def = self.generate_model_definition(carrier)\n                        if django_model_def:\n                            model_definitions.append(django_model_def)\n                            django_model_name_cleaned = self._clean_generic_type(carrier.django_model.__name__)\n                            django_model_names.append(f\"'{django_model_name_cleaned}'\")\n                        else:\n                            logger.warning(f\"Base generate_model_definition returned empty for {model_name}, skipping.\")\n                    else:\n                        # Model exists but seems empty (no concrete fields/M2M)\n                        # Check if it *does* have context fields\n                        if carrier.model_context and carrier.model_context.context_fields:\n                            context_only_models.append(model_name)\n                            logger.info(f\"Skipping Django model definition for {model_name} - only has context fields.\")\n                        else:\n                            logger.warning(\n                                f\"Model {model_name} resulted in an empty Django model with no context fields. Skipping definition.\"\n                            )\n                            # Continue to next carrier if no Django model AND no context\n                            if not (carrier.model_context and carrier.model_context.context_fields):\n                                continue\n\n                # --- B. Generate Context Class Definition (Pydantic Specific) ---\n                has_context = False\n                if carrier.model_context and carrier.model_context.context_fields:\n                    has_context = True\n                    # Generate context class definition string using the context_generator\n                    # This also handles adding necessary imports for context fields via TypeHandler/ImportHandler calls within it\n                    context_def = self.context_generator.generate_context_class(carrier.model_context)\n\n                    # Determine context class name (needs Django model name)\n                    # Use the cleaned name if available, otherwise construct from Pydantic name?\n                    base_name_for_context = django_model_name_cleaned if django_model_name_cleaned else model_name\n                    context_class_name = f\"{base_name_for_context}Context\"\n\n                    # Add context class definition if not seen before\n                    if context_class_name not in self.seen_context_classes:\n                        self.context_definitions.append(context_def)\n                        self.context_class_names.append(f\"'{context_class_name}'\")\n                        self.seen_context_classes.add(context_class_name)\n\n                    # Add imports for context fields (should be handled by context_generator now)\n                    # self.import_handler.add_context_field_imports(carrier.model_context) # Example hypothetical method\n\n                # --- C. Update Tracking and Add Source Import ---\n                self.model_has_context[model_name] = has_context\n\n                # Add import for the original source model (Pydantic model)\n                self._add_source_model_import(carrier)\n\n            except Exception as e:\n                logger.error(f\"Error processing carrier for source model {model_name}: {e}\", exc_info=True)\n\n        # 5. Log Summary\n        if context_only_models:\n            logger.info(\n                f\"Skipped Django definitions for {len(context_only_models)} models with only context fields: {', '.join(context_only_models)}\"\n            )\n\n        # If GFK is used anywhere, emit GenericEntry model once per file\n        if gfk_used:\n            model_definitions.append(self._build_generic_entry_model_definition())\n            django_model_names.append(\"'GenericEntry'\")\n\n        # 6. Deduplicate Definitions (Django models only, context defs deduplicated by name during loop)\n        unique_model_definitions = self._deduplicate_definitions(model_definitions)  # Use base method\n\n        # 7. Get Imports (handled by base import_handler)\n        imports = self.import_handler.deduplicate_imports()\n\n        # 8. Prepare Template Context (using overridden Pydantic-specific method)\n        template_context = self._prepare_template_context(unique_model_definitions, django_model_names, imports)\n\n        # 9. Add Common Context Items (handled by base class) - Reuse base class logic\n        template_context.update(\n            {\n                \"generation_timestamp\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n                \"base_model_module\": self.base_model_class.__module__,\n                \"base_model_name\": self.base_model_class.__name__,\n                \"extra_type_imports\": sorted(self.import_handler.extra_type_imports),\n                # Ensure generation_source_type is set by _prepare_template_context\n            }\n        )\n\n        # 10. Render the main template\n        template = self.jinja_env.get_template(\"models_file.py.j2\")\n        return template.render(**template_context)\n\n    def _build_generic_entry_model_definition(self) -&gt; str:\n        \"\"\"Build the GenericEntry model definition string.\n\n        Uses contenttypes GenericForeignKey to attach to any parent model.\n        Includes common columns and optional typed value columns based on flags.\n        \"\"\"\n        # Ensure imports present\n        self.import_handler.add_import(\"django.contrib.contenttypes.fields\", \"GenericForeignKey\")\n        self.import_handler.add_import(\"django.contrib.contenttypes.fields\", \"GenericRelation\")\n        self.import_handler.add_import(\"django.contrib.contenttypes.models\", \"ContentType\")\n\n        fields: list[str] = []\n        fields.append(\"content_type = models.ForeignKey('contenttypes.ContentType', on_delete=models.CASCADE)\")\n        fields.append(\"object_id = models.PositiveIntegerField()\")\n        fields.append(\"content_object = GenericForeignKey('content_type', 'object_id')\")\n        fields.append(\"element_qname = models.CharField(max_length=255)\")\n        fields.append(\"type_qname = models.CharField(max_length=255, null=True, blank=True)\")\n        fields.append(\"attrs_json = models.JSONField(default=dict, blank=True)\")\n        # Optional typed value columns if configured\n        if getattr(self, \"gfk_value_mode\", None) == \"typed_columns\":\n            fields.append(\"text_value = models.TextField(null=True, blank=True)\")\n            fields.append(\"num_value = models.DecimalField(max_digits=20, decimal_places=6, null=True, blank=True)\")\n            fields.append(\"time_value = models.DateTimeField(null=True, blank=True)\")\n        fields.append(\"order_index = models.IntegerField(default=0)\")\n        fields.append(\"path_hint = models.CharField(max_length=255, null=True, blank=True)\")\n\n        # Build indexes list\n        indexes_lines = [\"models.Index(fields=['content_type', 'object_id'])\"]\n        if getattr(self, \"gfk_value_mode\", None) == \"typed_columns\":\n            indexes_lines.append(\"models.Index(fields=['element_qname'])\")\n            indexes_lines.append(\"models.Index(fields=['type_qname'])\")\n            indexes_lines.append(\"models.Index(fields=['time_value'])\")\n            indexes_lines.append(\"models.Index(fields=['content_type', 'object_id', '-time_value'])\")\n\n        # Compose class string\n        lines: list[str] = []\n        lines.append(f\"class GenericEntry({self.base_model_class.__name__}):\")\n        for f in fields:\n            lines.append(f\"    {f}\")\n        lines.append(\"\")\n        lines.append(\"    class Meta:\")\n        lines.append(f\"        app_label = '{self.app_label}'\")\n        lines.append(\"        abstract = False\")\n        lines.append(\"        indexes = [\")\n        for idx in indexes_lines:\n            lines.append(f\"            {idx},\")\n        lines.append(\"        ]\")\n        lines.append(\"\")\n        return \"\\n\".join(lines)\n\n    # Choose Timescale base per model (lazy roles computation)\n    def setup_django_model(self, source_model: type[BaseModel]) -&gt; ConversionCarrier | None:  # type: ignore[override]\n        try:\n            from pydantic2django.django.timescale.bases import PydanticTimescaleBase\n            from pydantic2django.django.timescale.heuristics import (\n                classify_pydantic_models,\n                should_use_timescale_base,\n            )\n        except Exception:\n            classify_pydantic_models = None  # type: ignore\n            should_use_timescale_base = None  # type: ignore\n            PydanticTimescaleBase = None  # type: ignore\n\n        # Compute roles lazily if not present\n        if self.enable_timescale and not getattr(self, \"_timescale_roles\", None):\n            roles: dict[str, TimescaleRole] = {}\n            try:\n                models_to_score = []\n                try:\n                    models_to_score = self._get_models_in_processing_order() or []\n                except Exception:\n                    pass\n                if not models_to_score:\n                    models_to_score = [source_model]\n                if classify_pydantic_models:\n                    roles = classify_pydantic_models(models_to_score)\n            except Exception:\n                roles = {}\n            self._timescale_roles = roles\n\n        # Select base class\n        base_cls: type[models.Model] = self.base_model_class\n        if self.enable_timescale:\n            try:\n                name = source_model.__name__\n                if should_use_timescale_base and PydanticTimescaleBase:\n                    if should_use_timescale_base(name, self._timescale_roles):  # type: ignore[arg-type]\n                        base_cls = PydanticTimescaleBase\n            except Exception:\n                pass\n\n        prev_base = self.base_model_class\n        self.base_model_class = base_cls\n        try:\n            carrier = super().setup_django_model(source_model)\n        finally:\n            self.base_model_class = prev_base\n\n        if carrier is not None:\n            carrier.context_data[\"_timescale_roles\"] = getattr(self, \"_timescale_roles\", {})\n        return carrier\n</code></pre> </li> <li> <p>Notes:</p> </li> <li>Relies on Pydantic <code>FieldInfo</code> for field metadata and constraints.</li> <li>Generates optional per-model context classes when non-serializable fields are detected.</li> </ul>"},{"location":"Architecture/OVERVIEW/#pydantic2django.pydantic.discovery.PydanticDiscovery.analyze_dependencies","title":"<code>analyze_dependencies()</code>","text":"<p>Build the dependency graph for the filtered Pydantic models.</p> Source code in <code>src/pydantic2django/pydantic/discovery.py</code> <pre><code>def analyze_dependencies(self) -&gt; None:\n    \"\"\"Build the dependency graph for the filtered Pydantic models.\"\"\"\n    logger.info(\"Analyzing dependencies between filtered Pydantic models...\")\n    self.dependencies: dict[type[BaseModel], set[type[BaseModel]]] = {}\n\n    filtered_model_qualnames = set(self.filtered_models.keys())\n\n    def _find_and_add_dependency(model_type: type[BaseModel], potential_dep_type: Any):\n        if not self._is_target_model(potential_dep_type):\n            return\n\n        dep_qualname = f\"{potential_dep_type.__module__}.{potential_dep_type.__name__}\"\n\n        if dep_qualname in filtered_model_qualnames and potential_dep_type is not model_type:\n            dep_model_obj = self.filtered_models.get(dep_qualname)\n            if dep_model_obj:\n                if model_type in self.dependencies:\n                    self.dependencies[model_type].add(dep_model_obj)\n                else:\n                    # Initialize if missing (shouldn't happen often with new base discover_models)\n                    logger.warning(\n                        f\"Model {model_type.__name__} wasn't pre-initialized in dependencies dict during analysis. Initializing now.\"\n                    )\n                    self.dependencies[model_type] = {dep_model_obj}\n            else:\n                logger.warning(\n                    f\"Inconsistency: Dependency '{dep_qualname}' for model '{model_type.__name__}' found by name but not object in filtered set.\"\n                )\n\n    # Initialize keys based on filtered models (important step)\n    for model_type in self.filtered_models.values():\n        self.dependencies[model_type] = set()\n\n    # Analyze fields\n    for model_type in self.filtered_models.values():\n        for field in model_type.model_fields.values():\n            annotation = field.annotation\n            if annotation is None:\n                continue\n\n            origin = get_origin(annotation)\n            args = get_args(annotation)\n\n            if origin is Union and type(None) in args and len(args) == 2:\n                annotation = next(arg for arg in args if arg is not type(None))\n                origin = get_origin(annotation)\n                args = get_args(annotation)\n\n            _find_and_add_dependency(model_type, annotation)\n\n            if origin in (list, dict, set, tuple):\n                for arg in args:\n                    arg_origin = get_origin(arg)\n                    arg_args = get_args(arg)\n\n                    if arg_origin is Union and type(None) in arg_args and len(arg_args) == 2:\n                        nested_model_type = next(t for t in arg_args if t is not type(None))\n                        _find_and_add_dependency(model_type, nested_model_type)\n                    else:\n                        _find_and_add_dependency(model_type, arg)\n\n    logger.info(\"Dependency analysis complete.\")\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.pydantic.discovery.PydanticDiscovery.discover_models","title":"<code>discover_models(packages, app_label, user_filters=None)</code>","text":"<p>Discover Pydantic models in the specified packages, applying filters.</p> Source code in <code>src/pydantic2django/pydantic/discovery.py</code> <pre><code>def discover_models(\n    self,\n    packages: list[str],\n    app_label: str,\n    user_filters: Optional[\n        Union[Callable[[type[BaseModel]], bool], list[Callable[[type[BaseModel]], bool]]]\n    ] = None,\n):\n    \"\"\"Discover Pydantic models in the specified packages, applying filters.\"\"\"\n    # Pass user_filters directly to the base class method\n    super().discover_models(packages, app_label, user_filters=user_filters)\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.pydantic.discovery.PydanticDiscovery.get_models_in_registration_order","title":"<code>get_models_in_registration_order()</code>","text":"<p>Return models sorted topologically based on dependencies. Models with no dependencies come first.</p> Source code in <code>src/pydantic2django/pydantic/discovery.py</code> <pre><code>def get_models_in_registration_order(self) -&gt; list[type[BaseModel]]:\n    \"\"\"\n    Return models sorted topologically based on dependencies.\n    Models with no dependencies come first.\n    \"\"\"\n    if not self.dependencies:\n        logger.warning(\"No dependencies found or analyzed, returning Pydantic models in arbitrary order.\")\n        return list(self.filtered_models.values())\n\n    sorted_models = []\n    visited: set[type[BaseModel]] = set()\n    visiting: set[type[BaseModel]] = set()\n    filtered_model_objects = set(self.filtered_models.values())\n\n    def visit(model: type[BaseModel]):\n        if model in visited:\n            return\n        if model in visiting:\n            logger.error(f\"Circular dependency detected involving Pydantic model {model.__name__}\")\n            # Option: raise TypeError(...)\n            return  # Break cycle\n\n        visiting.add(model)\n\n        if model in self.dependencies:\n            # Use .get for safety, ensure deps are also in filtered set\n            for dep in self.dependencies.get(model, set()):\n                if dep in filtered_model_objects:\n                    visit(dep)\n\n        visiting.remove(model)\n        visited.add(model)\n        sorted_models.append(model)\n\n    all_target_models = list(self.filtered_models.values())\n    for model in all_target_models:\n        if model not in visited:\n            visit(model)\n\n    logger.info(f\"Pydantic models sorted for registration: {[m.__name__ for m in sorted_models]}\")\n    return sorted_models\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.pydantic.factory.PydanticModelFactory.__init__","title":"<code>__init__(field_factory, relationship_accessor)</code>","text":"<p>Initialize with field factory and relationship accessor.</p> Source code in <code>src/pydantic2django/pydantic/factory.py</code> <pre><code>def __init__(self, field_factory: PydanticFieldFactory, relationship_accessor: RelationshipConversionAccessor):\n    \"\"\"Initialize with field factory and relationship accessor.\"\"\"\n    self.relationship_accessor = relationship_accessor\n    # Pass the field_factory up to the base class\n    super().__init__(field_factory=field_factory)\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.pydantic.factory.PydanticModelFactory.make_django_model","title":"<code>make_django_model(carrier)</code>","text":"<p>Creates a Django model from Pydantic, checking cache first and mapping relationships.</p> Source code in <code>src/pydantic2django/pydantic/factory.py</code> <pre><code>def make_django_model(self, carrier: ConversionCarrier[type[BaseModel]]) -&gt; None:\n    \"\"\"Creates a Django model from Pydantic, checking cache first and mapping relationships.\"\"\"\n    model_key = carrier.model_key()\n    logger.debug(f\"PydanticFactory: Attempting to create Django model for {model_key}\")\n\n    # --- Check Cache --- #\n    if model_key in self._converted_models and not carrier.existing_model:\n        logger.debug(f\"PydanticFactory: Using cached conversion result for {model_key}\")\n        cached_carrier = self._converted_models[model_key]\n        # Update the passed-in carrier with cached results\n        carrier.__dict__.update(cached_carrier.__dict__)\n        # Ensure used_related_names is properly updated (dict update might not merge sets correctly)\n        for target, names in cached_carrier.used_related_names_per_target.items():\n            carrier.used_related_names_per_target.setdefault(target, set()).update(names)\n        return\n\n    # --- Call Base Implementation for Core Logic --- #\n    # This calls _process_source_fields, _assemble_django_model_class etc.\n    super().make_django_model(carrier)\n\n    # --- Register Relationship Mapping (if successful) --- #\n    if carrier.source_model and carrier.django_model:\n        logger.debug(\n            f\"PydanticFactory: Registering mapping for {carrier.source_model.__name__} -&gt; {carrier.django_model.__name__}\"\n        )\n        self.relationship_accessor.map_relationship(\n            source_model=carrier.source_model, django_model=carrier.django_model\n        )\n\n    # --- Cache Result --- #\n    if carrier.django_model and not carrier.existing_model:\n        logger.debug(f\"PydanticFactory: Caching conversion result for {model_key}\")\n        # Store a copy to prevent modification issues? Simple assignment for now.\n        self._converted_models[model_key] = carrier\n    elif not carrier.django_model:\n        logger.error(\n            f\"PydanticFactory: Failed to create Django model for {model_key}. Invalid fields: {carrier.invalid_fields}\"\n        )\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.pydantic.factory.PydanticFieldFactory.__init__","title":"<code>__init__(relationship_accessor, bidirectional_mapper)</code>","text":"<p>Initializes with dependencies.</p> Source code in <code>src/pydantic2django/pydantic/factory.py</code> <pre><code>def __init__(\n    self, relationship_accessor: RelationshipConversionAccessor, bidirectional_mapper: BidirectionalTypeMapper\n):\n    \"\"\"Initializes with dependencies.\"\"\"\n    self.relationship_accessor = relationship_accessor\n    self.bidirectional_mapper = bidirectional_mapper\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.pydantic.factory.PydanticFieldFactory.create_field","title":"<code>create_field(field_info, model_name, carrier)</code>","text":"<p>Convert a Pydantic FieldInfo to a Django field instance. Implements the abstract method from BaseFieldFactory. Uses BidirectionalTypeMapper and local instantiation.</p> Source code in <code>src/pydantic2django/pydantic/factory.py</code> <pre><code>def create_field(\n    self, field_info: FieldInfo, model_name: str, carrier: ConversionCarrier[type[BaseModel]]\n) -&gt; FieldConversionResult:\n    \"\"\"\n    Convert a Pydantic FieldInfo to a Django field instance.\n    Implements the abstract method from BaseFieldFactory.\n    Uses BidirectionalTypeMapper and local instantiation.\n    \"\"\"\n    # Use alias first, then the actual key from model_fields as name\n    field_name = field_info.alias or next(\n        (k for k, v in carrier.source_model.model_fields.items() if v is field_info), \"&lt;unknown&gt;\"\n    )\n\n    # Initialize result with the source field info and determined name\n    result = FieldConversionResult(field_info=field_info, field_name=field_name)\n\n    try:\n        # Handle potential 'id' field conflict\n        if id_field := self._handle_id_field(field_name, field_info):\n            result.django_field = id_field\n            # Need to capture kwargs for serialization if possible\n            # For now, assume default kwargs for ID fields\n            # TODO: Extract actual kwargs used in _handle_id_field\n            result.field_kwargs = {\"primary_key\": True}\n            if isinstance(id_field, models.CharField):\n                result.field_kwargs[\"max_length\"] = getattr(id_field, \"max_length\", 255)\n            elif isinstance(id_field, models.UUIDField):\n                pass  # No extra kwargs needed typically\n            else:  # AutoField\n                pass  # No extra kwargs needed typically\n\n            result.field_definition_str = self._generate_field_def_string(result, carrier.meta_app_label)\n            return result  # ID field handled, return early\n\n        # Get field type from annotation\n        field_type = field_info.annotation\n        if field_type is None:\n            logger.warning(f\"Field '{model_name}.{field_name}' has no annotation, treating as context field.\")\n            result.context_field = field_info\n            return result\n\n        # --- Use BidirectionalTypeMapper --- #\n        try:\n            django_field_class, constructor_kwargs = self.bidirectional_mapper.get_django_mapping(\n                python_type=field_type, field_info=field_info\n            )\n        except MappingError as e:\n            # Handle errors specifically from the mapper (e.g., missing relationship)\n            logger.error(f\"Mapping error for '{model_name}.{field_name}' (type: {field_type}): {e}\")\n            result.error_str = str(e)\n            result.context_field = field_info  # Treat as context on mapping error\n            return result\n        except Exception as e:\n            # Handle unexpected errors during mapping lookup\n            logger.error(\n                f\"Unexpected error getting Django mapping for '{model_name}.{field_name}': {e}\", exc_info=True\n            )\n            result.error_str = f\"Unexpected mapping error: {e}\"\n            result.context_field = field_info\n            return result\n\n        # Store raw kwargs before modifications/checks\n        result.raw_mapper_kwargs = constructor_kwargs.copy()\n\n        # --- Check for Multi-FK Union Signal --- #\n        union_details = constructor_kwargs.pop(\"_union_details\", None)\n        if union_details and isinstance(union_details, dict):\n            # If GFK mode is enabled and policy says to use it, record as pending GFK child\n            if getattr(carrier, \"enable_gfk\", False) and self._should_route_to_gfk(union_details, carrier):\n                logger.info(\n                    f\"[GFK] Routing union field '{field_name}' on '{model_name}' to GenericEntry (policy={carrier.gfk_policy}).\"\n                )\n                carrier.pending_gfk_children.append(\n                    {\n                        \"field_name\": field_name,\n                        \"union_details\": union_details,\n                        \"model_name\": model_name,\n                    }\n                )\n                # Do not generate a concrete field for this union\n                return result\n            # Otherwise, fall back to existing multi-FK behavior\n            logger.info(f\"Detected multi-FK union signal for '{field_name}'. Deferring field generation.\")\n            # Store the original field name and the details for the generator\n            carrier.pending_multi_fk_unions.append((field_name, union_details))\n            return result  # Return early, deferring generation\n\n        # --- Check for GFK placeholder signal from mapper --- #\n        gfk_details = constructor_kwargs.pop(\"_gfk_details\", None)\n        if gfk_details and isinstance(gfk_details, dict):\n            if getattr(carrier, \"enable_gfk\", False):\n                logger.info(\n                    f\"[GFK] Mapper signaled GFK for '{field_name}' on '{model_name}'. Recording as pending GFK child.\"\n                )\n                carrier.pending_gfk_children.append(\n                    {\n                        \"field_name\": field_name,\n                        \"gfk_details\": gfk_details,\n                        \"model_name\": model_name,\n                    }\n                )\n                # Do not generate a concrete field\n                return result\n            else:\n                logger.warning(\n                    f\"Received _gfk_details for '{field_name}' but enable_gfk is False. Falling back to JSON field.\"\n                )\n\n        # --- Handle Relationships Specifically (Adjust Kwargs) --- #\n        # Check if it's a relationship type *after* getting mapping AND checking for union signal\n        is_relationship = issubclass(\n            django_field_class, (models.ForeignKey, models.OneToOneField, models.ManyToManyField)\n        )\n\n        if is_relationship:\n            # Apply specific relationship logic (like related_name uniqueness)\n            # The mapper should have set 'to' and basic 'on_delete'\n            if \"to\" not in constructor_kwargs:\n                # This indicates an issue in the mapper or relationship accessor setup\n                result.error_str = f\"Mapper failed to determine 'to' for relationship field '{field_name}'.\"\n                logger.error(result.error_str)\n                result.context_field = field_info\n                return result\n\n            # Sanitize and ensure unique related_name\n            # Check Pydantic Field(..., json_schema_extra={\"related_name\": ...})\n            user_related_name = (\n                field_info.json_schema_extra.get(\"related_name\")\n                if isinstance(field_info.json_schema_extra, dict)\n                else None\n            )\n            target_django_model_str = constructor_kwargs[\"to\"]  # Mapper returns string like app_label.ModelName\n\n            # Try to get the actual target model class to pass to sanitize_related_name if possible\n            # This relies on the target model being importable/available\n            target_model_cls = None\n            target_model_cls_name_only = target_django_model_str  # Default fallback\n            try:\n                app_label, model_cls_name = target_django_model_str.split(\".\")\n                target_model_cls = apps.get_model(app_label, model_cls_name)  # Use apps.get_model\n                target_model_cls_name_only = model_cls_name  # Use name from split\n            except Exception:\n                logger.warning(\n                    f\"Could not get target model class for '{target_django_model_str}' when generating related_name for '{field_name}'. Using model name string.\"\n                )\n                # Fallback: try splitting by dot just for name, otherwise use whole string\n                target_model_cls_name_only = target_django_model_str.split(\".\")[-1]\n\n            related_name_base = (\n                user_related_name\n                if user_related_name\n                else f\"{carrier.source_model.__name__.lower()}_{field_name}_set\"\n            )\n            final_related_name_base = sanitize_related_name(\n                str(related_name_base),\n                target_model_cls.__name__ if target_model_cls else target_model_cls_name_only,\n                field_name,\n            )\n\n            # Ensure uniqueness using carrier's tracker\n            target_model_key_for_tracker = (\n                target_model_cls.__name__ if target_model_cls else target_django_model_str\n            )\n            target_related_names = carrier.used_related_names_per_target.setdefault(\n                target_model_key_for_tracker, set()\n            )\n            unique_related_name = final_related_name_base\n            counter = 1\n            while unique_related_name in target_related_names:\n                unique_related_name = f\"{final_related_name_base}_{counter}\"\n                counter += 1\n            target_related_names.add(unique_related_name)\n            constructor_kwargs[\"related_name\"] = unique_related_name\n            logger.debug(f\"[REL] Field '{field_name}': Assigning related_name='{unique_related_name}'\")\n\n            # Re-confirm on_delete (mapper should set default based on Optional)\n            if (\n                django_field_class in (models.ForeignKey, models.OneToOneField)\n                and \"on_delete\" not in constructor_kwargs\n            ):\n                is_optional = is_pydantic_model_field_optional(field_type)\n                constructor_kwargs[\"on_delete\"] = models.SET_NULL if is_optional else models.CASCADE\n            elif django_field_class == models.ManyToManyField:\n                constructor_kwargs.pop(\"on_delete\", None)\n                # M2M doesn't use null=True, mapper handles this\n                constructor_kwargs.pop(\"null\", None)\n                constructor_kwargs[\"blank\"] = constructor_kwargs.get(\"blank\", True)  # M2M usually blank=True\n\n        # --- Perform Instantiation Locally --- #\n        try:\n            logger.debug(\n                f\"Instantiating {django_field_class.__name__} for '{field_name}' with kwargs: {constructor_kwargs}\"\n            )\n            result.django_field = django_field_class(**constructor_kwargs)\n            result.field_kwargs = constructor_kwargs  # Store final kwargs\n        except Exception as e:\n            error_msg = f\"Failed to instantiate Django field '{field_name}' (type: {django_field_class.__name__}) with kwargs {constructor_kwargs}: {e}\"\n            logger.error(error_msg, exc_info=True)\n            result.error_str = error_msg\n            result.context_field = field_info  # Fallback to context\n            return result\n\n        # --- Generate Field Definition String --- #\n        result.field_definition_str = self._generate_field_def_string(result, carrier.meta_app_label)\n\n        return result  # Success\n\n    except Exception as e:\n        # Catch-all for unexpected errors during conversion\n        error_msg = f\"Unexpected error converting field '{model_name}.{field_name}': {e}\"\n        logger.error(error_msg, exc_info=True)\n        result.error_str = error_msg\n        result.context_field = field_info  # Fallback to context\n        return result\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.pydantic.generator.StaticPydanticModelGenerator.generate_models_file","title":"<code>generate_models_file()</code>","text":"<p>Generates the complete models.py file content, including Pydantic context classes. Overrides the base method to add context class handling during the generation loop.</p> Source code in <code>src/pydantic2django/pydantic/generator.py</code> <pre><code>def generate_models_file(self) -&gt; str:\n    \"\"\"\n    Generates the complete models.py file content, including Pydantic context classes.\n    Overrides the base method to add context class handling during the generation loop.\n    \"\"\"\n    # 1. Base discovery and model ordering\n    self.discover_models()  # Calls base discovery and dependency analysis\n    models_to_process = self._get_models_in_processing_order()  # Uses overridden method\n\n    # 2. Reset state for this run (imports handled by base reset)\n    self.carriers = []\n    # Manually reset ImportHandler state instead of calling non-existent reset()\n    self.import_handler.extra_type_imports.clear()\n    self.import_handler.pydantic_imports.clear()\n    self.import_handler.context_class_imports.clear()\n    self.import_handler.imported_names.clear()\n    self.import_handler.processed_field_types.clear()\n\n    # Re-add base model import after clearing\n    # Note: add_pydantic_model_import might not be the right method here if base_model_class isn't Pydantic\n    # Need a more general import method on ImportHandler or handle it differently.\n    # For now, let's assume a general import is needed or handled by template.\n    # self.import_handler.add_import(self.base_model_class.__module__, self.base_model_class.__name__)\n    # Let's add it back using _add_type_import, although it's protected.\n    # A public add_general_import(module, name) on ImportHandler would be better.\n    try:\n        # This is a workaround - ideally ImportHandler would have a public method\n        self.import_handler._add_type_import(self.base_model_class)\n    except Exception as e:\n        logger.warning(f\"Could not add base model import via _add_type_import: {e}\")\n\n    # Reset Pydantic-specific tracking lists\n    self.context_definitions = []\n    self.model_has_context = {}  # Map of Pydantic model name -&gt; bool\n    self.context_class_names = []  # For __all__\n    self.seen_context_classes = set()  # For deduplication of definitions\n\n    # --- State tracking within the loop ---\n    model_definitions = []  # Store generated Django model definition strings\n    django_model_names = []  # Store generated Django model names for __all__\n    context_only_models = []  # Track Pydantic models yielding only context\n    gfk_used = False\n\n    # 3. Setup Django models (populates self.carriers via base method calling factory)\n    for source_model in models_to_process:\n        self.setup_django_model(source_model)  # Uses base setup_django_model\n\n    # 4. Generate definitions (Django models AND Pydantic Context classes)\n    for carrier in self.carriers:\n        model_name = self._get_source_model_name(carrier)  # Pydantic model name\n\n        try:\n            # --- GFK finalize hook: inject GenericRelation on parents ---\n            if getattr(carrier, \"enable_gfk\", False) and getattr(carrier, \"pending_gfk_children\", None):\n                # Ensure imports for contenttypes\n                self.import_handler.add_import(\"django.contrib.contenttypes.fields\", \"GenericRelation\")\n                self.import_handler.add_import(\"django.contrib.contenttypes.fields\", \"GenericForeignKey\")\n                self.import_handler.add_import(\"django.contrib.contenttypes.models\", \"ContentType\")\n\n                # Inject a GenericRelation field into the parent model definition strings\n                # Field name 'entries' for reverse access\n                try:\n                    carrier.django_field_definitions[\n                        \"entries\"\n                    ] = \"GenericRelation('GenericEntry', related_query_name='entries')\"\n                except Exception:\n                    pass\n                gfk_used = True\n\n            django_model_def = \"\"\n            django_model_name_cleaned = \"\"\n\n            # --- A. Generate Django Model Definition (if applicable) ---\n            if carrier.django_model:\n                # Check fields using safe getattr for many_to_many\n                has_concrete_fields = any(not f.primary_key for f in carrier.django_model._meta.fields)\n                # Use getattr for safety\n                m2m_fields = getattr(carrier.django_model._meta, \"many_to_many\", [])\n                has_m2m = bool(m2m_fields)\n                has_fields = bool(carrier.django_model._meta.fields)\n\n                if has_concrete_fields or has_m2m or (not has_concrete_fields and not has_m2m and has_fields):\n                    django_model_def = self.generate_model_definition(carrier)\n                    if django_model_def:\n                        model_definitions.append(django_model_def)\n                        django_model_name_cleaned = self._clean_generic_type(carrier.django_model.__name__)\n                        django_model_names.append(f\"'{django_model_name_cleaned}'\")\n                    else:\n                        logger.warning(f\"Base generate_model_definition returned empty for {model_name}, skipping.\")\n                else:\n                    # Model exists but seems empty (no concrete fields/M2M)\n                    # Check if it *does* have context fields\n                    if carrier.model_context and carrier.model_context.context_fields:\n                        context_only_models.append(model_name)\n                        logger.info(f\"Skipping Django model definition for {model_name} - only has context fields.\")\n                    else:\n                        logger.warning(\n                            f\"Model {model_name} resulted in an empty Django model with no context fields. Skipping definition.\"\n                        )\n                        # Continue to next carrier if no Django model AND no context\n                        if not (carrier.model_context and carrier.model_context.context_fields):\n                            continue\n\n            # --- B. Generate Context Class Definition (Pydantic Specific) ---\n            has_context = False\n            if carrier.model_context and carrier.model_context.context_fields:\n                has_context = True\n                # Generate context class definition string using the context_generator\n                # This also handles adding necessary imports for context fields via TypeHandler/ImportHandler calls within it\n                context_def = self.context_generator.generate_context_class(carrier.model_context)\n\n                # Determine context class name (needs Django model name)\n                # Use the cleaned name if available, otherwise construct from Pydantic name?\n                base_name_for_context = django_model_name_cleaned if django_model_name_cleaned else model_name\n                context_class_name = f\"{base_name_for_context}Context\"\n\n                # Add context class definition if not seen before\n                if context_class_name not in self.seen_context_classes:\n                    self.context_definitions.append(context_def)\n                    self.context_class_names.append(f\"'{context_class_name}'\")\n                    self.seen_context_classes.add(context_class_name)\n\n                # Add imports for context fields (should be handled by context_generator now)\n                # self.import_handler.add_context_field_imports(carrier.model_context) # Example hypothetical method\n\n            # --- C. Update Tracking and Add Source Import ---\n            self.model_has_context[model_name] = has_context\n\n            # Add import for the original source model (Pydantic model)\n            self._add_source_model_import(carrier)\n\n        except Exception as e:\n            logger.error(f\"Error processing carrier for source model {model_name}: {e}\", exc_info=True)\n\n    # 5. Log Summary\n    if context_only_models:\n        logger.info(\n            f\"Skipped Django definitions for {len(context_only_models)} models with only context fields: {', '.join(context_only_models)}\"\n        )\n\n    # If GFK is used anywhere, emit GenericEntry model once per file\n    if gfk_used:\n        model_definitions.append(self._build_generic_entry_model_definition())\n        django_model_names.append(\"'GenericEntry'\")\n\n    # 6. Deduplicate Definitions (Django models only, context defs deduplicated by name during loop)\n    unique_model_definitions = self._deduplicate_definitions(model_definitions)  # Use base method\n\n    # 7. Get Imports (handled by base import_handler)\n    imports = self.import_handler.deduplicate_imports()\n\n    # 8. Prepare Template Context (using overridden Pydantic-specific method)\n    template_context = self._prepare_template_context(unique_model_definitions, django_model_names, imports)\n\n    # 9. Add Common Context Items (handled by base class) - Reuse base class logic\n    template_context.update(\n        {\n            \"generation_timestamp\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n            \"base_model_module\": self.base_model_class.__module__,\n            \"base_model_name\": self.base_model_class.__name__,\n            \"extra_type_imports\": sorted(self.import_handler.extra_type_imports),\n            # Ensure generation_source_type is set by _prepare_template_context\n        }\n    )\n\n    # 10. Render the main template\n    template = self.jinja_env.get_template(\"models_file.py.j2\")\n    return template.render(**template_context)\n</code></pre>"},{"location":"Architecture/OVERVIEW/#dataclass","title":"Dataclass","text":"<ul> <li>Discovery, factory, and generator:</li> <li> <p>               Bases: <code>BaseDiscovery[DataclassType]</code></p> <p>Discovers Python dataclasses within specified packages.</p> Source code in <code>src/pydantic2django/dataclass/discovery.py</code> <pre><code>class DataclassDiscovery(BaseDiscovery[DataclassType]):\n    \"\"\"Discovers Python dataclasses within specified packages.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        # Dataclass specific attributes (all_models now in base)\n\n    def _is_target_model(self, obj: Any) -&gt; bool:\n        \"\"\"Check if an object is a dataclass type.\"\"\"\n        return inspect.isclass(obj) and dataclasses.is_dataclass(obj)\n\n    def _default_eligibility_filter(self, model: DataclassType) -&gt; bool:\n        \"\"\"Check default eligibility for dataclasses (e.g., not inheriting directly from ABC).\"\"\"\n        # Skip models that directly inherit from ABC\n        if abc.ABC in model.__bases__:\n            logger.debug(f\"Filtering out dataclass {model.__name__} (inherits directly from ABC)\")\n            return False\n        # Dataclasses don't have a standard __abstract__ marker like Pydantic\n        # Add other default checks if needed for dataclasses\n        return True\n\n    # discover_models is now implemented in the BaseDiscovery class\n    # It will call the _is_target_model and _default_eligibility_filter defined above.\n\n    # --- analyze_dependencies and get_models_in_registration_order remain ---\n\n    def analyze_dependencies(self) -&gt; None:\n        \"\"\"Build the dependency graph for the filtered dataclasses.\"\"\"\n        logger.info(\"Analyzing dependencies between filtered dataclasses...\")\n        self.dependencies: dict[DataclassType, set[DataclassType]] = {}\n\n        filtered_model_qualnames = set(self.filtered_models.keys())\n\n        def _find_and_add_dependency(model_type: DataclassType, potential_dep_type: Any):\n            if not self._is_target_model(potential_dep_type):\n                return\n\n            dep_qualname = f\"{potential_dep_type.__module__}.{potential_dep_type.__name__}\"\n\n            if dep_qualname in filtered_model_qualnames and potential_dep_type is not model_type:\n                dep_model_obj = self.filtered_models.get(dep_qualname)\n                if dep_model_obj:\n                    if model_type in self.dependencies:\n                        self.dependencies[model_type].add(dep_model_obj)\n                    else:\n                        logger.warning(\n                            f\"Model {model_type.__name__} wasn't pre-initialized in dependencies dict during analysis. Initializing now.\"\n                        )\n                        self.dependencies[model_type] = {dep_model_obj}\n                else:\n                    logger.warning(\n                        f\"Inconsistency: Dependency '{dep_qualname}' for dataclass '{model_type.__name__}' found by name but not object in filtered set.\"\n                    )\n\n        # Initialize keys based on filtered models\n        for model_type in self.filtered_models.values():\n            self.dependencies[model_type] = set()\n\n        # Analyze fields using dataclasses.fields\n        for model_type in self.filtered_models.values():\n            assert dataclasses.is_dataclass(model_type), f\"Expected {model_type} to be a dataclass\"\n            for field in dataclasses.fields(model_type):\n                annotation = field.type\n                if annotation is None:\n                    continue\n\n                origin = get_origin(annotation)\n                args = get_args(annotation)\n\n                if origin is Union and type(None) in args and len(args) == 2:\n                    annotation = next(arg for arg in args if arg is not type(None))\n                    origin = get_origin(annotation)\n                    args = get_args(annotation)\n\n                _find_and_add_dependency(model_type, annotation)\n\n                if origin in (list, dict, set, tuple):\n                    for arg in args:\n                        arg_origin = get_origin(arg)\n                        arg_args = get_args(arg)\n\n                        if arg_origin is Union and type(None) in arg_args and len(arg_args) == 2:\n                            nested_type = next(t for t in arg_args if t is not type(None))\n                            _find_and_add_dependency(model_type, nested_type)\n                        else:\n                            _find_and_add_dependency(model_type, arg)\n\n        logger.info(\"Dataclass dependency analysis complete.\")\n        # Debug logging moved inside BaseDiscovery\n\n    def get_models_in_registration_order(self) -&gt; list[DataclassType]:\n        \"\"\"\n        Return dataclasses sorted topologically based on dependencies.\n        (Largely similar to Pydantic version, uses DataclassType)\n        \"\"\"\n        if not self.dependencies:\n            logger.warning(\"No dependencies found or analyzed, returning dataclasses in arbitrary order.\")\n            return list(self.filtered_models.values())\n\n        sorted_models = []\n        visited: set[DataclassType] = set()\n        visiting: set[DataclassType] = set()\n        filtered_model_objects = set(self.filtered_models.values())\n\n        def visit(model: DataclassType):\n            if model in visited:\n                return\n            if model in visiting:\n                logger.error(f\"Circular dependency detected involving dataclass {model.__name__}\")\n                # Option: raise TypeError(...)\n                return  # Break cycle\n\n            visiting.add(model)\n\n            if model in self.dependencies:\n                # Use .get for safety, ensure deps are also in filtered set\n                for dep in self.dependencies.get(model, set()):\n                    if dep in filtered_model_objects:\n                        visit(dep)\n\n            visiting.remove(model)\n            visited.add(model)\n            sorted_models.append(model)\n\n        all_target_models = list(self.filtered_models.values())\n        for model in all_target_models:\n            if model not in visited:\n                visit(model)\n\n        logger.info(f\"Dataclasses sorted for registration: {[m.__name__ for m in sorted_models]}\")\n        return sorted_models\n</code></pre> </li> <li> <p>               Bases: <code>BaseModelFactory[DataclassType, Field]</code></p> <p>Dynamically creates Django model classes from dataclasses.</p> Source code in <code>src/pydantic2django/dataclass/factory.py</code> <pre><code>class DataclassModelFactory(BaseModelFactory[DataclassType, dataclasses.Field]):\n    \"\"\"Dynamically creates Django model classes from dataclasses.\"\"\"\n\n    # Cache specific to Dataclass models\n    _converted_models: dict[str, ConversionCarrier[DataclassType]] = {}\n\n    relationship_accessor: RelationshipConversionAccessor  # Changed Optional to required\n    import_handler: ImportHandler  # Added import handler\n\n    def __init__(\n        self,\n        field_factory: DataclassFieldFactory,\n        relationship_accessor: RelationshipConversionAccessor,  # Now required\n        import_handler: Optional[ImportHandler] = None,  # Accept optionally\n    ):\n        \"\"\"Initialize with field factory, relationship accessor, and import handler.\"\"\"\n        self.relationship_accessor = relationship_accessor\n        self.import_handler = import_handler or ImportHandler()\n        # Call super init\n        super().__init__(field_factory)\n\n    def make_django_model(self, carrier: ConversionCarrier[DataclassType]) -&gt; None:\n        \"\"\"\n        Orchestrates the Django model creation process.\n        Subclasses implement _process_source_fields and _build_model_context.\n        Handles caching.\n        Passes import handler down.\n        \"\"\"\n        # --- Pass import handler via carrier --- (or could add to factory state)\n        # Need to set import handler on carrier if passed during init\n        # NOTE: BaseModelFactory.make_django_model does this now.\n        # carrier.import_handler = self.import_handler\n        super().make_django_model(carrier)\n        # Register relationship after successful model creation (moved from original)\n        if carrier.source_model and carrier.django_model:\n            logger.debug(\n                f\"Mapping relationship in accessor: {carrier.source_model.__name__} -&gt; {carrier.django_model.__name__}\"\n            )\n            self.relationship_accessor.map_relationship(\n                source_model=carrier.source_model, django_model=carrier.django_model\n            )\n        # Cache result (moved from original)\n        model_key = carrier.model_key()\n        if carrier.django_model and not carrier.existing_model:\n            self._converted_models[model_key] = carrier\n\n    def _process_source_fields(self, carrier: ConversionCarrier[DataclassType]):\n        \"\"\"Iterate through source dataclass fields, resolve types, create Django fields, and store results.\"\"\"\n        source_model = carrier.source_model\n        if not source_model:\n            logger.error(\n                f\"Cannot process fields: source model missing in carrier for {getattr(carrier, 'target_model_name', '?')}\"\n            )  # Safely access target_model_name\n            carrier.invalid_fields.append((\"_source_model\", \"Source model missing.\"))  # Use invalid_fields\n            return\n\n        # --- Add check: Ensure source_model is a type ---\n        if not isinstance(source_model, type):\n            error_msg = f\"Cannot process fields: expected source_model to be a type, but got {type(source_model)} ({source_model!r}). Problem likely upstream in model discovery/ordering.\"\n            logger.error(error_msg)\n            carrier.invalid_fields.append((\"_source_model\", error_msg))\n            return\n        # --- End Add check ---\n\n        # --- Use dataclasses.fields for introspection ---\n        try:\n            # Resolve type hints first to handle forward references (strings)\n            # Need globals and potentially locals from the source model's module\n            source_module = sys.modules.get(source_model.__module__)\n            globalns = getattr(source_module, \"__dict__\", None)\n            # Revert: Use only globals, assuming types are resolvable in module scope\n\n            # resolved_types = get_type_hints(source_model, globalns=globalns, localns=localns)\n            # logger.debug(f\"Resolved types for {source_model.__name__} using {globalns=}, {localns=}: {resolved_types}\")\n            # Use updated call without potentially incorrect locals:\n            resolved_types = get_type_hints(source_model, globalns=globalns, localns=None)\n            logger.debug(f\"Resolved types for {source_model.__name__} using module globals: {resolved_types}\")\n\n            dataclass_fields = dataclasses.fields(source_model)\n        except (TypeError, NameError) as e:  # Catch errors during type hint resolution or fields() call\n            error_msg = f\"Could not introspect fields or resolve types for {source_model.__name__}: {e}\"\n            logger.error(error_msg, exc_info=True)\n            carrier.invalid_fields.append((\"_introspection\", error_msg))  # Use invalid_fields\n            return\n\n        # Use field definitions directly from carrier\n        # field_definitions: dict[str, str] = {}\n        # context_field_definitions: dict[str, str] = {} # Dataclasses likely won't use this\n\n        for field_info in dataclass_fields:\n            field_name = field_info.name\n\n            # Normalize output Django field identifier\n            try:\n                from ..core.utils.naming import sanitize_field_identifier\n\n                normalized_field_name = sanitize_field_identifier(field_name)\n            except Exception:\n                normalized_field_name = field_name\n\n            # Get the *resolved* type for this field\n            resolved_type = resolved_types.get(field_name)\n            if resolved_type is None:\n                logger.warning(\n                    f\"Could not resolve type hint for field '{field_name}' in {source_model.__name__}. Using original: {field_info.type!r}\"\n                )\n                # Fallback to original, which might be a string\n                resolved_type = field_info.type\n\n            # --- Prepare field type for create_field --- #\n            type_for_create_field = resolved_type  # Start with the result from get_type_hints\n\n            # If the original type annotation was a string (ForwardRef)\n            # and get_type_hints failed to resolve it (resolved_type is None or still the string),\n            # try to find the resolved type from the dict populated earlier.\n            # This specifically handles nested dataclasses defined in local scopes like fixtures.\n            if isinstance(field_info.type, str):\n                explicitly_resolved = resolved_types.get(field_name)\n                if explicitly_resolved and not isinstance(explicitly_resolved, str):\n                    logger.debug(\n                        f\"Using explicitly resolved type {explicitly_resolved!r} for forward ref '{field_info.type}'\"\n                    )\n                    type_for_create_field = explicitly_resolved\n                elif resolved_type is field_info.type:  # Check if resolved_type is still the unresolved string\n                    logger.error(\n                        f\"Type hint for '{field_name}' is string '{field_info.type}' but was not resolved by get_type_hints. Skipping field.\"\n                    )\n                    carrier.invalid_fields.append(\n                        (field_name, f\"Could not resolve forward reference: {field_info.type}\")\n                    )\n                    continue  # Skip this field\n\n            # Temporarily modify a copy of field_info or pass type directly if possible.\n            # Modifying field_info directly is simpler for now.\n            original_type_attr = field_info.type\n            try:\n                field_info.type = type_for_create_field  # Use the determined type\n                logger.debug(f\"Calling create_field for '{field_name}' with type: {field_info.type!r}\")\n\n                field_result = self.field_factory.create_field(\n                    field_info=field_info, model_name=source_model.__name__, carrier=carrier\n                )\n            finally:\n                # Restore original type attribute\n                field_info.type = original_type_attr\n                logger.debug(\"Restored original field_info.type attribute\")\n\n            # Process the result (errors, definitions)\n            if field_result.error_str:\n                carrier.invalid_fields.append((normalized_field_name, field_result.error_str))\n            else:\n                # Store the definition string if available\n                if field_result.field_definition_str:\n                    carrier.django_field_definitions[normalized_field_name] = field_result.field_definition_str\n                else:\n                    logger.warning(\n                        f\"Field '{normalized_field_name}' processing yielded no error and no definition string.\"\n                    )\n\n                # Store the actual field instance in the correct carrier dict\n                if field_result.django_field:\n                    if isinstance(\n                        field_result.django_field, (models.ForeignKey, models.OneToOneField, models.ManyToManyField)\n                    ):\n                        carrier.relationship_fields[normalized_field_name] = field_result.django_field\n                    else:\n                        carrier.django_fields[normalized_field_name] = field_result.django_field\n                elif field_result.context_field:\n                    # Handle context fields if needed (currently seems unused based on logs)\n                    carrier.context_fields[normalized_field_name] = field_result.context_field\n\n                # Merge imports from result into the factory's import handler\n                if field_result.required_imports:\n                    # Use the new add_import method\n                    for module, names in field_result.required_imports.items():\n                        for name in names:\n                            self.import_handler.add_import(module=module, name=name)\n\n        logger.debug(f\"Finished processing fields for {source_model.__name__}. Errors: {len(carrier.invalid_fields)}\")\n\n    # Actual implementation of the abstract method _build_model_context\n    def _build_model_context(self, carrier: ConversionCarrier[DataclassType]):\n        \"\"\"Builds the ModelContext specifically for dataclass source models.\"\"\"\n        if not carrier.source_model or not carrier.django_model:\n            logger.debug(\"Skipping context build: missing source or django model.\")\n            return\n\n        try:\n            # Remove generic type hint if ModelContext is not generic or if causing issues\n            # Assuming ModelContext base class handles the source type appropriately\n            model_context = ModelContext(django_model=carrier.django_model, source_class=carrier.source_model)\n            for field_name, field_info in carrier.context_fields.items():\n                if isinstance(field_info, dataclasses.Field):\n                    # Calculate necessary info for ModelContext.add_field\n                    origin = get_origin(field_info.type)\n                    args = get_args(field_info.type)\n                    is_optional = origin is Union and type(None) in args\n                    field_type_str = repr(field_info.type)  # Use repr for the type string\n\n                    # Call add_field with expected signature\n                    model_context.add_field(\n                        field_name=field_name,\n                        field_type_str=field_type_str,\n                        is_optional=is_optional,\n                        # Pass original annotation if ModelContext uses it\n                        annotation=field_info.type,\n                    )\n                else:\n                    # Log if context field is not the expected type\n                    logger.warning(\n                        f\"Context field '{field_name}' is not a dataclasses.Field ({type(field_info)}), cannot add to ModelContext.\"\n                    )\n            carrier.model_context = model_context\n            logger.debug(f\"Successfully built ModelContext for {carrier.model_key()}\")  # Use method call\n        except Exception as e:\n            logger.error(f\"Failed to build ModelContext for {carrier.model_key()}: {e}\", exc_info=True)\n            carrier.model_context = None\n</code></pre> </li> <li> <p>               Bases: <code>BaseFieldFactory[Field]</code></p> <p>Creates Django model fields from dataclass fields.</p> Source code in <code>src/pydantic2django/dataclass/factory.py</code> <pre><code>class DataclassFieldFactory(BaseFieldFactory[dataclasses.Field]):\n    \"\"\"Creates Django model fields from dataclass fields.\"\"\"\n\n    relationship_accessor: RelationshipConversionAccessor  # Changed Optional to required\n    bidirectional_mapper: BidirectionalTypeMapper  # Added mapper\n\n    def __init__(\n        self, relationship_accessor: RelationshipConversionAccessor, bidirectional_mapper: BidirectionalTypeMapper\n    ):\n        \"\"\"Initializes with dependencies.\"\"\"\n        self.relationship_accessor = relationship_accessor\n        self.bidirectional_mapper = bidirectional_mapper\n        # No super().__init__() needed if BaseFieldFactory.__init__ is empty or handles this\n\n    def create_field(\n        self, field_info: dataclasses.Field, model_name: str, carrier: ConversionCarrier[DataclassType]\n    ) -&gt; FieldConversionResult:\n        \"\"\"\n        Convert a dataclasses.Field to a Django field instance.\n        Uses BidirectionalTypeMapper and local instantiation.\n        Relies on field_info.metadata['django'] for specific overrides.\n        Adds required imports to the result.\n        \"\"\"\n        field_name = field_info.name\n        original_field_type = field_info.type\n        metadata = field_info.metadata or {}  # Ensure metadata is a dict\n        django_meta_options = metadata.get(\"django\", {})\n\n        # --- Resolve Forward Reference String if necessary --- #\n        type_to_map = original_field_type\n        result = FieldConversionResult(field_info=field_info, field_name=field_name)\n\n        if isinstance(original_field_type, str):\n            logger.debug(\n                f\"Field '{field_name}' has string type '{original_field_type}'. Attempting resolution via RelationshipAccessor.\"\n            )\n            # Assume string type is a model name known to the accessor\n            # Use the newly added method:\n            resolved_source_model = self.relationship_accessor.get_source_model_by_name(original_field_type)\n            if resolved_source_model:\n                logger.debug(f\"Resolved string '{original_field_type}' to type {resolved_source_model}\")\n                type_to_map = resolved_source_model\n            else:\n                # Critical Error: If it's a string but not in accessor, mapping will fail.\n                logger.error(\n                    f\"Field '{field_name}' type is string '{original_field_type}' but was not found in RelationshipAccessor. Cannot map.\"\n                )\n                result.error_str = f\"Unresolved forward reference or unknown model name: {original_field_type}\"\n                result.context_field = field_info\n                return result\n        # --- End Forward Reference Resolution ---\n\n        logger.debug(\n            f\"Processing dataclass field {model_name}.{field_name}: Type={original_field_type}, Metadata={metadata}\"\n        )\n\n        try:\n            # --- Use BidirectionalTypeMapper --- #\n            try:\n                # Pass field_info.type, but no Pydantic FieldInfo equivalent for metadata\n                # The mapper primarily relies on the type itself.\n                django_field_class, constructor_kwargs = self.bidirectional_mapper.get_django_mapping(\n                    python_type=type_to_map,\n                    field_info=None,  # Pass None for field_info\n                )\n                # Add import for the Django field class itself using the result's helper\n                result.add_import_for_obj(django_field_class)\n\n            except MappingError as e:\n                logger.error(f\"Mapping error for '{model_name}.{field_name}' (type: {type_to_map}): {e}\")\n                result.error_str = str(e)\n                result.context_field = field_info\n                return result\n            except Exception as e:\n                logger.error(\n                    f\"Unexpected error getting Django mapping for '{model_name}.{field_name}': {e}\", exc_info=True\n                )\n                result.error_str = f\"Unexpected mapping error: {e}\"\n                result.context_field = field_info\n                return result\n\n            # --- Handle GFK signal from mapper (List[Union[...]] of models) --- #\n            gfk_details = constructor_kwargs.pop(\"_gfk_details\", None)\n            if gfk_details and isinstance(gfk_details, dict):\n                if getattr(carrier, \"enable_gfk\", False):\n                    logger.info(\n                        f\"[GFK] (dataclass) Mapper signaled GFK for '{field_name}' on '{model_name}'. Recording as pending GFK child.\"\n                    )\n                    carrier.pending_gfk_children.append(\n                        {\"field_name\": field_name, \"gfk_details\": gfk_details, \"model_name\": model_name}\n                    )\n                    # Do not generate a concrete field\n                    return result\n                else:\n                    logger.warning(\n                        f\"Received _gfk_details for '{field_name}' but enable_gfk is False. Falling back to JSON field.\"\n                    )\n\n            # --- Merge Dataclass Metadata Overrides --- #\n            # Apply explicit options from metadata *after* getting defaults from mapper\n            constructor_kwargs.update(django_meta_options)\n\n            # --- Apply Dataclass Defaults --- #\n            # This logic now handles both `default` and `default_factory`.\n            if \"default\" not in constructor_kwargs:\n                default_value = field_info.default\n                if default_value is dataclasses.MISSING and field_info.default_factory is not dataclasses.MISSING:\n                    default_value = field_info.default_factory\n\n                if default_value is not dataclasses.MISSING:\n                    if callable(default_value):\n                        try:\n                            # Handle regular importable functions\n                            if (\n                                hasattr(default_value, \"__module__\")\n                                and hasattr(default_value, \"__name__\")\n                                and not default_value.__name__ == \"&lt;lambda&gt;\"\n                            ):\n                                module_name = default_value.__module__\n                                func_name = default_value.__name__\n                                # Avoid importing builtins\n                                if module_name != \"builtins\":\n                                    result.add_import(module_name, func_name)\n                                constructor_kwargs[\"default\"] = func_name\n                            # Handle lambdas\n                            else:\n                                source = inspect.getsource(default_value).strip()\n                                # Remove comma if it's trailing in a lambda definition in a list/dict\n                                if source.endswith(\",\"):\n                                    source = source[:-1]\n                                constructor_kwargs[\"default\"] = RawCode(source)\n                        except (TypeError, OSError) as e:\n                            logger.warning(\n                                f\"Could not introspect callable default for '{model_name}.{field_name}': {e}. \"\n                                \"Falling back to `None`.\"\n                            )\n                            constructor_kwargs[\"default\"] = None\n                            constructor_kwargs[\"null\"] = True\n                            constructor_kwargs[\"blank\"] = True\n                    elif not isinstance(default_value, (list, dict, set)):\n                        constructor_kwargs[\"default\"] = default_value\n                    else:\n                        logger.warning(\n                            f\"Field {model_name}.{field_name} has mutable default {default_value}. Skipping Django default.\"\n                        )\n\n            # --- Handle Relationships Specifically (Adjust Kwargs) --- #\n            is_relationship = issubclass(\n                django_field_class, (models.ForeignKey, models.OneToOneField, models.ManyToManyField)\n            )\n\n            if is_relationship:\n                if \"to\" not in constructor_kwargs:\n                    result.error_str = f\"Mapper failed to determine 'to' for relationship field '{field_name}'.\"\n                    logger.error(result.error_str)\n                    result.context_field = field_info\n                    return result\n\n                # Sanitize and ensure unique related_name\n                user_related_name = django_meta_options.get(\"related_name\")  # Check override from metadata\n                target_django_model_str = constructor_kwargs[\"to\"]\n\n                target_model_cls = None\n                target_model_cls_name_only = target_django_model_str\n                try:\n                    app_label, model_cls_name = target_django_model_str.split(\".\")\n                    target_model_cls = apps.get_model(app_label, model_cls_name)\n                    target_model_cls_name_only = model_cls_name\n                    # Add import for the target model using result helper\n                    result.add_import_for_obj(target_model_cls)\n                except Exception:\n                    logger.warning(\n                        f\"Could not get target model class for '{target_django_model_str}' when generating related_name for '{field_name}'. Using model name string.\"\n                    )\n                    target_model_cls_name_only = target_django_model_str.split(\".\")[-1]\n\n                related_name_base = (\n                    user_related_name\n                    if user_related_name\n                    # Use carrier.source_model.__name__ for default related name base\n                    else f\"{carrier.source_model.__name__.lower()}_{field_name}_set\"\n                )\n                final_related_name_base = sanitize_related_name(\n                    str(related_name_base),\n                    target_model_cls.__name__ if target_model_cls else target_model_cls_name_only,\n                    field_name,\n                )\n\n                # Ensure uniqueness using carrier's tracker\n                target_model_key_for_tracker = (\n                    target_model_cls.__name__ if target_model_cls else target_django_model_str\n                )\n                target_related_names = carrier.used_related_names_per_target.setdefault(\n                    target_model_key_for_tracker, set()\n                )\n                unique_related_name = final_related_name_base\n                counter = 1\n                while unique_related_name in target_related_names:\n                    unique_related_name = f\"{final_related_name_base}_{counter}\"\n                    counter += 1\n                target_related_names.add(unique_related_name)\n                constructor_kwargs[\"related_name\"] = unique_related_name\n                logger.debug(f\"[REL] Dataclass Field '{field_name}': Assigning related_name='{unique_related_name}'\")\n\n                # Re-confirm on_delete (mapper sets default based on Optional, but metadata might override)\n                # Need to check optionality of the original type here\n                origin = get_origin(original_field_type)\n                args = get_args(original_field_type)\n                is_optional = origin is Union and type(None) in args\n\n                if (\n                    django_field_class in (models.ForeignKey, models.OneToOneField)\n                    and \"on_delete\" not in constructor_kwargs  # Only set if not specified in metadata\n                ):\n                    constructor_kwargs[\"on_delete\"] = models.SET_NULL if is_optional else models.CASCADE\n                    # Add import using result helper\n                    result.add_import(\"django.db.models\", \"SET_NULL\" if is_optional else \"CASCADE\")\n                elif django_field_class == models.ManyToManyField:\n                    constructor_kwargs.pop(\"on_delete\", None)\n                    constructor_kwargs.pop(\"null\", None)  # M2M cannot be null\n                    if \"blank\" not in constructor_kwargs:  # Default M2M to blank=True if not set\n                        constructor_kwargs[\"blank\"] = True\n\n            # --- Perform Instantiation Locally --- #\n            try:\n                logger.debug(\n                    f\"Instantiating {django_field_class.__name__} for dataclass field '{field_name}' with kwargs: {constructor_kwargs}\"\n                )\n                result.django_field = django_field_class(**constructor_kwargs)\n                result.field_kwargs = constructor_kwargs  # Store final kwargs\n            except Exception as e:\n                error_msg = f\"Failed to instantiate Django field '{field_name}' (type: {django_field_class.__name__}) with kwargs {constructor_kwargs}: {e}\"\n                logger.error(error_msg, exc_info=True)\n                result.error_str = error_msg\n                result.context_field = field_info\n                return result\n\n            # --- Generate Field Definition String --- #\n            result.field_definition_str = self._generate_field_def_string(result, carrier.meta_app_label)\n\n            return result  # Success\n\n        except Exception as e:\n            # Catch-all for unexpected errors during conversion\n            error_msg = f\"Unexpected error converting dataclass field '{model_name}.{field_name}': {e}\"\n            logger.error(error_msg, exc_info=True)\n            result.error_str = error_msg\n            result.context_field = field_info\n            return result\n\n    def _generate_field_def_string(self, result: FieldConversionResult, app_label: str) -&gt; str:\n        \"\"\"Generates the field definition string safely.\"\"\"\n        if not result.django_field:\n            return \"# Field generation failed\"\n        try:\n            # Use stored final kwargs if available\n            if result.field_kwargs:\n                # Pass the result's required_imports to the serialization function\n                return generate_field_definition_string(\n                    type(result.django_field),\n                    result.field_kwargs,\n                    app_label,\n                )\n            else:\n                # Fallback: Basic serialization if final kwargs weren't stored for some reason\n                logger.warning(\n                    f\"Could not generate definition string for '{result.field_name}': final kwargs not found in result. Using basic serialization.\"\n                )\n                return FieldSerializer.serialize_field(result.django_field)\n        except Exception as e:\n            logger.error(\n                f\"Failed to generate field definition string for '{result.field_name}': {e}\",\n                exc_info=True,\n            )\n            return f\"# Error generating definition: {e}\"\n</code></pre> </li> <li> <p>               Bases: <code>BaseStaticGenerator[DataclassType, DataclassFieldInfo]</code></p> <p>Generates Django models.py file content from Python dataclasses.</p> Source code in <code>src/pydantic2django/dataclass/generator.py</code> <pre><code>class DataclassDjangoModelGenerator(\n    BaseStaticGenerator[DataclassType, DataclassFieldInfo]  # Inherit from BaseStaticGenerator\n):\n    \"\"\"Generates Django models.py file content from Python dataclasses.\"\"\"\n\n    def __init__(\n        self,\n        output_path: str,\n        app_label: str,\n        filter_function: Optional[Callable[[DataclassType], bool]],\n        verbose: bool,\n        # Accept specific discovery and factories, or create defaults\n        packages: list[str] | None = None,\n        discovery_instance: Optional[DataclassDiscovery] = None,\n        model_factory_instance: Optional[DataclassModelFactory] = None,\n        field_factory_instance: Optional[DataclassFieldFactory] = None,  # Add field factory param\n        relationship_accessor: Optional[RelationshipConversionAccessor] = None,  # Accept accessor\n        module_mappings: Optional[dict[str, str]] = None,\n        enable_timescale: bool = True,\n        # --- GFK flags ---\n        enable_gfk: bool = True,\n        gfk_policy: str | None = \"threshold_by_children\",\n        gfk_threshold_children: int | None = 8,\n        gfk_value_mode: str | None = \"typed_columns\",\n        gfk_normalize_common_attrs: bool = False,\n    ):\n        # 1. Initialize Dataclass-specific discovery\n        self.dataclass_discovery_instance = discovery_instance or DataclassDiscovery()\n\n        # 2. Initialize Dataclass-specific factories\n        # Dataclass factories might not need RelationshipAccessor, check their definitions\n        # Assuming they don't for now.\n        # --- Correction: They DO need them now ---\n        # Use provided accessor or create a new one\n        self.relationship_accessor = relationship_accessor or RelationshipConversionAccessor()\n        # Create mapper using the (potentially provided) accessor\n        self.bidirectional_mapper = BidirectionalTypeMapper(relationship_accessor=self.relationship_accessor)\n\n        self.dataclass_field_factory = field_factory_instance or DataclassFieldFactory(\n            relationship_accessor=self.relationship_accessor,\n            bidirectional_mapper=self.bidirectional_mapper,\n        )\n        self.dataclass_model_factory = model_factory_instance or DataclassModelFactory(\n            field_factory=self.dataclass_field_factory,\n            relationship_accessor=self.relationship_accessor,  # Pass only accessor\n        )\n\n        # 3. Call the base class __init__\n        super().__init__(\n            output_path=output_path,\n            packages=packages,\n            app_label=app_label,\n            filter_function=filter_function,\n            verbose=verbose,\n            discovery_instance=self.dataclass_discovery_instance,\n            model_factory_instance=self.dataclass_model_factory,\n            module_mappings=module_mappings,\n            base_model_class=self._get_default_base_model_class(),\n            enable_timescale=enable_timescale,\n            enable_gfk=enable_gfk,\n            gfk_policy=gfk_policy,\n            gfk_threshold_children=gfk_threshold_children,\n            gfk_value_mode=gfk_value_mode,\n            gfk_normalize_common_attrs=gfk_normalize_common_attrs,\n        )\n        logger.info(\"DataclassDjangoModelGenerator initialized using BaseStaticGenerator.\")\n        # Timescale classification results cached per run (name -&gt; role)\n        self._timescale_roles: dict[str, TimescaleRole] = {}\n\n    # --- Implement abstract methods from BaseStaticGenerator ---\n\n    def _get_source_model_name(self, carrier: ConversionCarrier[DataclassType]) -&gt; str:\n        \"\"\"Get the name of the original dataclass from the carrier.\"\"\"\n        # Use carrier.source_model (consistent with Base class)\n        if carrier.source_model:\n            return carrier.source_model.__name__\n        # Fallback if source model somehow missing\n        # Check if carrier has pydantic_model attribute as a legacy fallback?\n        legacy_model = getattr(carrier, \"pydantic_model\", None)  # Safely check old attribute\n        if legacy_model:\n            return legacy_model.__name__\n        return \"UnknownDataclass\"\n\n    def _add_source_model_import(self, carrier: ConversionCarrier[DataclassType]):\n        \"\"\"Add the necessary import for the original dataclass.\"\"\"\n        # Use carrier.source_model\n        model_to_import = carrier.source_model\n        if not model_to_import:\n            # Legacy fallback check\n            model_to_import = getattr(carrier, \"pydantic_model\", None)\n\n        if model_to_import:\n            # Use add_pydantic_model_import for consistency? Or add_context_field_type_import?\n            # Let's assume add_context_field_type_import handles dataclasses too.\n            # A dedicated add_dataclass_import or add_general_import would be clearer.\n            self.import_handler.add_context_field_type_import(model_to_import)\n        else:\n            logger.warning(\"Cannot add source model import: source model missing in carrier.\")\n\n    def _prepare_template_context(self, unique_model_definitions, django_model_names, imports) -&gt; dict:\n        \"\"\"Prepare the context specific to dataclasses for the main models_file.py.j2 template.\"\"\"\n        # Base context items are passed in.\n        # Add Dataclass-specific items.\n        base_context = {\n            \"model_definitions\": unique_model_definitions,  # Already joined by base class\n            \"django_model_names\": django_model_names,  # Already list of quoted names\n            # Pass the structured imports dict\n            \"imports\": imports,\n            # --- Dataclass Specific ---\n            \"generation_source_type\": \"dataclass\",  # Flag for template logic\n            # --- Keep compatibility if templates expect these --- (review templates later)\n            # \"django_imports\": sorted(imports.get(\"django\", [])), # Provided by imports dict\n            # \"pydantic_imports\": sorted(imports.get(\"pydantic\", [])), # Likely empty for dataclass\n            # \"general_imports\": sorted(imports.get(\"general\", [])),\n            # \"context_imports\": sorted(imports.get(\"context\", [])),\n            # Add other dataclass specific flags/lists if needed by the template\n            \"context_definitions\": [],  # Dataclasses don't have separate context classes? Assume empty.\n            \"context_class_names\": [],\n            \"model_has_context\": {},  # Assume no context model mapping needed\n        }\n        # Common items added by base class generate_models_file after this call.\n        return base_context\n\n    def _get_models_in_processing_order(self) -&gt; list[DataclassType]:\n        \"\"\"Return dataclasses in dependency order using the discovery instance.\"\"\"\n        # Add assertion for type checker clarity\n        assert isinstance(\n            self.discovery_instance, DataclassDiscovery\n        ), \"Discovery instance must be DataclassDiscovery for this generator\"\n        # Dependencies analyzed by base class discover_models call\n        return self.discovery_instance.get_models_in_registration_order()\n\n    def _get_model_definition_extra_context(self, carrier: ConversionCarrier[DataclassType]) -&gt; dict:\n        \"\"\"Provide extra context specific to dataclasses for model_definition.py.j2.\"\"\"\n        # Removed problematic metadata access from original\n        # Add flags for template conditional logic\n        return {\n            \"is_dataclass_source\": True,\n            \"is_pydantic_source\": False,\n            \"has_context\": False,  # Dataclasses likely don't generate separate context fields/classes\n            # Pass the field definitions dictionary from the carrier\n            \"field_definitions\": carrier.django_field_definitions,\n            # Add other specific details if needed, ensuring they access carrier correctly\n            # Example: \"source_model_module\": carrier.source_model.__module__ if carrier.source_model else \"\"\n        }\n\n    # Choose Timescale base per model (lazy roles computation)\n    def setup_django_model(self, source_model: DataclassType) -&gt; ConversionCarrier | None:  # type: ignore[override]\n        try:\n            from pydantic2django.django.timescale.bases import DataclassTimescaleBase\n            from pydantic2django.django.timescale.heuristics import (\n                classify_dataclass_types,\n                should_use_timescale_base,\n            )\n        except Exception:\n            classify_dataclass_types = None  # type: ignore\n            should_use_timescale_base = None  # type: ignore\n            DataclassTimescaleBase = None  # type: ignore\n\n        # Compute roles lazily if not present\n        if self.enable_timescale and not getattr(self, \"_timescale_roles\", None):\n            roles: dict[str, TimescaleRole] = {}\n            try:\n                models_to_score = []\n                try:\n                    models_to_score = self._get_models_in_processing_order() or []\n                except Exception:\n                    pass\n                if not models_to_score:\n                    models_to_score = [source_model]\n                if classify_dataclass_types:\n                    roles = classify_dataclass_types(models_to_score)\n            except Exception:\n                roles = {}\n            self._timescale_roles = roles\n\n        # Select base class\n        base_cls: type[models.Model] = self.base_model_class\n        if self.enable_timescale:\n            try:\n                name = source_model.__name__\n                if should_use_timescale_base and DataclassTimescaleBase:\n                    if should_use_timescale_base(name, self._timescale_roles):  # type: ignore[arg-type]\n                        base_cls = DataclassTimescaleBase\n            except Exception:\n                pass\n\n        prev_base = self.base_model_class\n        self.base_model_class = base_cls\n        try:\n            carrier = super().setup_django_model(source_model)\n        finally:\n            self.base_model_class = prev_base\n\n        if carrier is not None:\n            carrier.context_data[\"_timescale_roles\"] = getattr(self, \"_timescale_roles\", {})\n        return carrier\n\n    def _get_default_base_model_class(self) -&gt; type[models.Model]:\n        \"\"\"Return the default Django base model for Dataclass conversion.\"\"\"\n        if not django_apps.ready:\n            raise AppRegistryNotReady(\n                \"Django apps are not loaded. Call django.setup() or run within a configured Django context before \"\n                \"instantiating DataclassDjangoModelGenerator.\"\n            )\n        try:\n            from typed2django.django.models import Dataclass2DjangoBaseClass as _Base\n\n            return _Base\n        except Exception as exc:  # pragma: no cover - defensive\n            raise ImportError(\n                \"typed2django.django.models.Dataclass2DjangoBaseClass is required for Dataclass generation.\"\n            ) from exc\n\n    def generate_models_file(self) -&gt; str:\n        \"\"\"Generate models for dataclasses with GFK finalize hook.\"\"\"\n        self.discover_models()\n        models_to_process = self._get_models_in_processing_order()\n\n        # Reset imports and state\n        self.carriers = []\n        self.import_handler.extra_type_imports.clear()\n        self.import_handler.pydantic_imports.clear()\n        self.import_handler.context_class_imports.clear()\n        self.import_handler.imported_names.clear()\n        self.import_handler.processed_field_types.clear()\n        self.import_handler._add_type_import(self.base_model_class)\n\n        # Setup carriers\n        for source_model in models_to_process:\n            self.setup_django_model(source_model)\n\n        # GFK finalize: inject GenericRelation on parents\n        gfk_used = False\n        for carrier in self.carriers:\n            try:\n                if getattr(carrier, \"enable_gfk\", False) and getattr(carrier, \"pending_gfk_children\", None):\n                    self.import_handler.add_import(\"django.contrib.contenttypes.fields\", \"GenericRelation\")\n                    self.import_handler.add_import(\"django.contrib.contenttypes.fields\", \"GenericForeignKey\")\n                    self.import_handler.add_import(\"django.contrib.contenttypes.models\", \"ContentType\")\n                    carrier.django_field_definitions[\n                        \"entries\"\n                    ] = \"GenericRelation('GenericEntry', related_query_name='entries')\"\n                    gfk_used = True\n            except Exception:\n                pass\n\n        # Render model definitions\n        model_definitions: list[str] = []\n        django_model_names: list[str] = []\n        for carrier in self.carriers:\n            if carrier.django_model:\n                try:\n                    model_def = self.generate_model_definition(carrier)\n                    if model_def:\n                        model_definitions.append(model_def)\n                        django_model_names.append(f\"'{self._clean_generic_type(carrier.django_model.__name__)}'\")\n                except Exception:\n                    pass\n\n        if gfk_used:\n            model_definitions.append(self._build_generic_entry_model_definition())\n            django_model_names.append(\"'GenericEntry'\")\n\n        unique_model_definitions = self._deduplicate_definitions(model_definitions)\n        imports = self.import_handler.deduplicate_imports()\n        template_context = self._prepare_template_context(unique_model_definitions, django_model_names, imports)\n        template_context.update(\n            {\n                \"generation_timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n                \"base_model_module\": self.base_model_class.__module__,\n                \"base_model_name\": self.base_model_class.__name__,\n                \"extra_type_imports\": sorted(self.import_handler.extra_type_imports),\n            }\n        )\n        template = self.jinja_env.get_template(\"models_file.py.j2\")\n        return template.render(**template_context)\n\n    def _build_generic_entry_model_definition(self) -&gt; str:\n        \"\"\"Build the GenericEntry model definition for dataclass generation.\"\"\"\n        self.import_handler.add_import(\"django.contrib.contenttypes.fields\", \"GenericForeignKey\")\n        self.import_handler.add_import(\"django.contrib.contenttypes.fields\", \"GenericRelation\")\n        self.import_handler.add_import(\"django.contrib.contenttypes.models\", \"ContentType\")\n\n        fields: list[str] = []\n        fields.append(\"content_type = models.ForeignKey('contenttypes.ContentType', on_delete=models.CASCADE)\")\n        fields.append(\"object_id = models.PositiveIntegerField()\")\n        fields.append(\"content_object = GenericForeignKey('content_type', 'object_id')\")\n        fields.append(\"element_qname = models.CharField(max_length=255)\")\n        fields.append(\"type_qname = models.CharField(max_length=255, null=True, blank=True)\")\n        fields.append(\"attrs_json = models.JSONField(default=dict, blank=True)\")\n        if getattr(self, \"gfk_value_mode\", None) == \"typed_columns\":\n            fields.append(\"text_value = models.TextField(null=True, blank=True)\")\n            fields.append(\"num_value = models.DecimalField(max_digits=20, decimal_places=6, null=True, blank=True)\")\n            fields.append(\"time_value = models.DateTimeField(null=True, blank=True)\")\n        fields.append(\"order_index = models.IntegerField(default=0)\")\n        fields.append(\"path_hint = models.CharField(max_length=255, null=True, blank=True)\")\n\n        indexes_lines = [\"models.Index(fields=['content_type', 'object_id'])\"]\n        if getattr(self, \"gfk_value_mode\", None) == \"typed_columns\":\n            indexes_lines.append(\"models.Index(fields=['element_qname'])\")\n            indexes_lines.append(\"models.Index(fields=['type_qname'])\")\n            indexes_lines.append(\"models.Index(fields=['time_value'])\")\n            indexes_lines.append(\"models.Index(fields=['content_type', 'object_id', '-time_value'])\")\n\n        lines: list[str] = []\n        lines.append(f\"class GenericEntry({self.base_model_class.__name__}):\")\n        for f in fields:\n            lines.append(f\"    {f}\")\n        lines.append(\"\")\n        lines.append(\"    class Meta:\")\n        lines.append(f\"        app_label = '{self.app_label}'\")\n        lines.append(\"        abstract = False\")\n        lines.append(\"        indexes = [\")\n        for idx in indexes_lines:\n            lines.append(f\"            {idx},\")\n        lines.append(\"        ]\")\n        lines.append(\"\")\n        return \"\\n\".join(lines)\n</code></pre> </li> <li> <p>Notes:</p> </li> <li>Reads field metadata and optional per-field <code>django</code> overrides from <code>dataclasses.Field.metadata</code>.</li> <li>Uses <code>typing.get_type_hints</code> to resolve forward references.</li> </ul>"},{"location":"Architecture/OVERVIEW/#pydantic2django.dataclass.discovery.DataclassDiscovery.analyze_dependencies","title":"<code>analyze_dependencies()</code>","text":"<p>Build the dependency graph for the filtered dataclasses.</p> Source code in <code>src/pydantic2django/dataclass/discovery.py</code> <pre><code>def analyze_dependencies(self) -&gt; None:\n    \"\"\"Build the dependency graph for the filtered dataclasses.\"\"\"\n    logger.info(\"Analyzing dependencies between filtered dataclasses...\")\n    self.dependencies: dict[DataclassType, set[DataclassType]] = {}\n\n    filtered_model_qualnames = set(self.filtered_models.keys())\n\n    def _find_and_add_dependency(model_type: DataclassType, potential_dep_type: Any):\n        if not self._is_target_model(potential_dep_type):\n            return\n\n        dep_qualname = f\"{potential_dep_type.__module__}.{potential_dep_type.__name__}\"\n\n        if dep_qualname in filtered_model_qualnames and potential_dep_type is not model_type:\n            dep_model_obj = self.filtered_models.get(dep_qualname)\n            if dep_model_obj:\n                if model_type in self.dependencies:\n                    self.dependencies[model_type].add(dep_model_obj)\n                else:\n                    logger.warning(\n                        f\"Model {model_type.__name__} wasn't pre-initialized in dependencies dict during analysis. Initializing now.\"\n                    )\n                    self.dependencies[model_type] = {dep_model_obj}\n            else:\n                logger.warning(\n                    f\"Inconsistency: Dependency '{dep_qualname}' for dataclass '{model_type.__name__}' found by name but not object in filtered set.\"\n                )\n\n    # Initialize keys based on filtered models\n    for model_type in self.filtered_models.values():\n        self.dependencies[model_type] = set()\n\n    # Analyze fields using dataclasses.fields\n    for model_type in self.filtered_models.values():\n        assert dataclasses.is_dataclass(model_type), f\"Expected {model_type} to be a dataclass\"\n        for field in dataclasses.fields(model_type):\n            annotation = field.type\n            if annotation is None:\n                continue\n\n            origin = get_origin(annotation)\n            args = get_args(annotation)\n\n            if origin is Union and type(None) in args and len(args) == 2:\n                annotation = next(arg for arg in args if arg is not type(None))\n                origin = get_origin(annotation)\n                args = get_args(annotation)\n\n            _find_and_add_dependency(model_type, annotation)\n\n            if origin in (list, dict, set, tuple):\n                for arg in args:\n                    arg_origin = get_origin(arg)\n                    arg_args = get_args(arg)\n\n                    if arg_origin is Union and type(None) in arg_args and len(arg_args) == 2:\n                        nested_type = next(t for t in arg_args if t is not type(None))\n                        _find_and_add_dependency(model_type, nested_type)\n                    else:\n                        _find_and_add_dependency(model_type, arg)\n\n    logger.info(\"Dataclass dependency analysis complete.\")\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.dataclass.discovery.DataclassDiscovery.get_models_in_registration_order","title":"<code>get_models_in_registration_order()</code>","text":"<p>Return dataclasses sorted topologically based on dependencies. (Largely similar to Pydantic version, uses DataclassType)</p> Source code in <code>src/pydantic2django/dataclass/discovery.py</code> <pre><code>def get_models_in_registration_order(self) -&gt; list[DataclassType]:\n    \"\"\"\n    Return dataclasses sorted topologically based on dependencies.\n    (Largely similar to Pydantic version, uses DataclassType)\n    \"\"\"\n    if not self.dependencies:\n        logger.warning(\"No dependencies found or analyzed, returning dataclasses in arbitrary order.\")\n        return list(self.filtered_models.values())\n\n    sorted_models = []\n    visited: set[DataclassType] = set()\n    visiting: set[DataclassType] = set()\n    filtered_model_objects = set(self.filtered_models.values())\n\n    def visit(model: DataclassType):\n        if model in visited:\n            return\n        if model in visiting:\n            logger.error(f\"Circular dependency detected involving dataclass {model.__name__}\")\n            # Option: raise TypeError(...)\n            return  # Break cycle\n\n        visiting.add(model)\n\n        if model in self.dependencies:\n            # Use .get for safety, ensure deps are also in filtered set\n            for dep in self.dependencies.get(model, set()):\n                if dep in filtered_model_objects:\n                    visit(dep)\n\n        visiting.remove(model)\n        visited.add(model)\n        sorted_models.append(model)\n\n    all_target_models = list(self.filtered_models.values())\n    for model in all_target_models:\n        if model not in visited:\n            visit(model)\n\n    logger.info(f\"Dataclasses sorted for registration: {[m.__name__ for m in sorted_models]}\")\n    return sorted_models\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.dataclass.factory.DataclassModelFactory.__init__","title":"<code>__init__(field_factory, relationship_accessor, import_handler=None)</code>","text":"<p>Initialize with field factory, relationship accessor, and import handler.</p> Source code in <code>src/pydantic2django/dataclass/factory.py</code> <pre><code>def __init__(\n    self,\n    field_factory: DataclassFieldFactory,\n    relationship_accessor: RelationshipConversionAccessor,  # Now required\n    import_handler: Optional[ImportHandler] = None,  # Accept optionally\n):\n    \"\"\"Initialize with field factory, relationship accessor, and import handler.\"\"\"\n    self.relationship_accessor = relationship_accessor\n    self.import_handler = import_handler or ImportHandler()\n    # Call super init\n    super().__init__(field_factory)\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.dataclass.factory.DataclassModelFactory.make_django_model","title":"<code>make_django_model(carrier)</code>","text":"<p>Orchestrates the Django model creation process. Subclasses implement _process_source_fields and _build_model_context. Handles caching. Passes import handler down.</p> Source code in <code>src/pydantic2django/dataclass/factory.py</code> <pre><code>def make_django_model(self, carrier: ConversionCarrier[DataclassType]) -&gt; None:\n    \"\"\"\n    Orchestrates the Django model creation process.\n    Subclasses implement _process_source_fields and _build_model_context.\n    Handles caching.\n    Passes import handler down.\n    \"\"\"\n    # --- Pass import handler via carrier --- (or could add to factory state)\n    # Need to set import handler on carrier if passed during init\n    # NOTE: BaseModelFactory.make_django_model does this now.\n    # carrier.import_handler = self.import_handler\n    super().make_django_model(carrier)\n    # Register relationship after successful model creation (moved from original)\n    if carrier.source_model and carrier.django_model:\n        logger.debug(\n            f\"Mapping relationship in accessor: {carrier.source_model.__name__} -&gt; {carrier.django_model.__name__}\"\n        )\n        self.relationship_accessor.map_relationship(\n            source_model=carrier.source_model, django_model=carrier.django_model\n        )\n    # Cache result (moved from original)\n    model_key = carrier.model_key()\n    if carrier.django_model and not carrier.existing_model:\n        self._converted_models[model_key] = carrier\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.dataclass.factory.DataclassFieldFactory.__init__","title":"<code>__init__(relationship_accessor, bidirectional_mapper)</code>","text":"<p>Initializes with dependencies.</p> Source code in <code>src/pydantic2django/dataclass/factory.py</code> <pre><code>def __init__(\n    self, relationship_accessor: RelationshipConversionAccessor, bidirectional_mapper: BidirectionalTypeMapper\n):\n    \"\"\"Initializes with dependencies.\"\"\"\n    self.relationship_accessor = relationship_accessor\n    self.bidirectional_mapper = bidirectional_mapper\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.dataclass.factory.DataclassFieldFactory.create_field","title":"<code>create_field(field_info, model_name, carrier)</code>","text":"<p>Convert a dataclasses.Field to a Django field instance. Uses BidirectionalTypeMapper and local instantiation. Relies on field_info.metadata['django'] for specific overrides. Adds required imports to the result.</p> Source code in <code>src/pydantic2django/dataclass/factory.py</code> <pre><code>def create_field(\n    self, field_info: dataclasses.Field, model_name: str, carrier: ConversionCarrier[DataclassType]\n) -&gt; FieldConversionResult:\n    \"\"\"\n    Convert a dataclasses.Field to a Django field instance.\n    Uses BidirectionalTypeMapper and local instantiation.\n    Relies on field_info.metadata['django'] for specific overrides.\n    Adds required imports to the result.\n    \"\"\"\n    field_name = field_info.name\n    original_field_type = field_info.type\n    metadata = field_info.metadata or {}  # Ensure metadata is a dict\n    django_meta_options = metadata.get(\"django\", {})\n\n    # --- Resolve Forward Reference String if necessary --- #\n    type_to_map = original_field_type\n    result = FieldConversionResult(field_info=field_info, field_name=field_name)\n\n    if isinstance(original_field_type, str):\n        logger.debug(\n            f\"Field '{field_name}' has string type '{original_field_type}'. Attempting resolution via RelationshipAccessor.\"\n        )\n        # Assume string type is a model name known to the accessor\n        # Use the newly added method:\n        resolved_source_model = self.relationship_accessor.get_source_model_by_name(original_field_type)\n        if resolved_source_model:\n            logger.debug(f\"Resolved string '{original_field_type}' to type {resolved_source_model}\")\n            type_to_map = resolved_source_model\n        else:\n            # Critical Error: If it's a string but not in accessor, mapping will fail.\n            logger.error(\n                f\"Field '{field_name}' type is string '{original_field_type}' but was not found in RelationshipAccessor. Cannot map.\"\n            )\n            result.error_str = f\"Unresolved forward reference or unknown model name: {original_field_type}\"\n            result.context_field = field_info\n            return result\n    # --- End Forward Reference Resolution ---\n\n    logger.debug(\n        f\"Processing dataclass field {model_name}.{field_name}: Type={original_field_type}, Metadata={metadata}\"\n    )\n\n    try:\n        # --- Use BidirectionalTypeMapper --- #\n        try:\n            # Pass field_info.type, but no Pydantic FieldInfo equivalent for metadata\n            # The mapper primarily relies on the type itself.\n            django_field_class, constructor_kwargs = self.bidirectional_mapper.get_django_mapping(\n                python_type=type_to_map,\n                field_info=None,  # Pass None for field_info\n            )\n            # Add import for the Django field class itself using the result's helper\n            result.add_import_for_obj(django_field_class)\n\n        except MappingError as e:\n            logger.error(f\"Mapping error for '{model_name}.{field_name}' (type: {type_to_map}): {e}\")\n            result.error_str = str(e)\n            result.context_field = field_info\n            return result\n        except Exception as e:\n            logger.error(\n                f\"Unexpected error getting Django mapping for '{model_name}.{field_name}': {e}\", exc_info=True\n            )\n            result.error_str = f\"Unexpected mapping error: {e}\"\n            result.context_field = field_info\n            return result\n\n        # --- Handle GFK signal from mapper (List[Union[...]] of models) --- #\n        gfk_details = constructor_kwargs.pop(\"_gfk_details\", None)\n        if gfk_details and isinstance(gfk_details, dict):\n            if getattr(carrier, \"enable_gfk\", False):\n                logger.info(\n                    f\"[GFK] (dataclass) Mapper signaled GFK for '{field_name}' on '{model_name}'. Recording as pending GFK child.\"\n                )\n                carrier.pending_gfk_children.append(\n                    {\"field_name\": field_name, \"gfk_details\": gfk_details, \"model_name\": model_name}\n                )\n                # Do not generate a concrete field\n                return result\n            else:\n                logger.warning(\n                    f\"Received _gfk_details for '{field_name}' but enable_gfk is False. Falling back to JSON field.\"\n                )\n\n        # --- Merge Dataclass Metadata Overrides --- #\n        # Apply explicit options from metadata *after* getting defaults from mapper\n        constructor_kwargs.update(django_meta_options)\n\n        # --- Apply Dataclass Defaults --- #\n        # This logic now handles both `default` and `default_factory`.\n        if \"default\" not in constructor_kwargs:\n            default_value = field_info.default\n            if default_value is dataclasses.MISSING and field_info.default_factory is not dataclasses.MISSING:\n                default_value = field_info.default_factory\n\n            if default_value is not dataclasses.MISSING:\n                if callable(default_value):\n                    try:\n                        # Handle regular importable functions\n                        if (\n                            hasattr(default_value, \"__module__\")\n                            and hasattr(default_value, \"__name__\")\n                            and not default_value.__name__ == \"&lt;lambda&gt;\"\n                        ):\n                            module_name = default_value.__module__\n                            func_name = default_value.__name__\n                            # Avoid importing builtins\n                            if module_name != \"builtins\":\n                                result.add_import(module_name, func_name)\n                            constructor_kwargs[\"default\"] = func_name\n                        # Handle lambdas\n                        else:\n                            source = inspect.getsource(default_value).strip()\n                            # Remove comma if it's trailing in a lambda definition in a list/dict\n                            if source.endswith(\",\"):\n                                source = source[:-1]\n                            constructor_kwargs[\"default\"] = RawCode(source)\n                    except (TypeError, OSError) as e:\n                        logger.warning(\n                            f\"Could not introspect callable default for '{model_name}.{field_name}': {e}. \"\n                            \"Falling back to `None`.\"\n                        )\n                        constructor_kwargs[\"default\"] = None\n                        constructor_kwargs[\"null\"] = True\n                        constructor_kwargs[\"blank\"] = True\n                elif not isinstance(default_value, (list, dict, set)):\n                    constructor_kwargs[\"default\"] = default_value\n                else:\n                    logger.warning(\n                        f\"Field {model_name}.{field_name} has mutable default {default_value}. Skipping Django default.\"\n                    )\n\n        # --- Handle Relationships Specifically (Adjust Kwargs) --- #\n        is_relationship = issubclass(\n            django_field_class, (models.ForeignKey, models.OneToOneField, models.ManyToManyField)\n        )\n\n        if is_relationship:\n            if \"to\" not in constructor_kwargs:\n                result.error_str = f\"Mapper failed to determine 'to' for relationship field '{field_name}'.\"\n                logger.error(result.error_str)\n                result.context_field = field_info\n                return result\n\n            # Sanitize and ensure unique related_name\n            user_related_name = django_meta_options.get(\"related_name\")  # Check override from metadata\n            target_django_model_str = constructor_kwargs[\"to\"]\n\n            target_model_cls = None\n            target_model_cls_name_only = target_django_model_str\n            try:\n                app_label, model_cls_name = target_django_model_str.split(\".\")\n                target_model_cls = apps.get_model(app_label, model_cls_name)\n                target_model_cls_name_only = model_cls_name\n                # Add import for the target model using result helper\n                result.add_import_for_obj(target_model_cls)\n            except Exception:\n                logger.warning(\n                    f\"Could not get target model class for '{target_django_model_str}' when generating related_name for '{field_name}'. Using model name string.\"\n                )\n                target_model_cls_name_only = target_django_model_str.split(\".\")[-1]\n\n            related_name_base = (\n                user_related_name\n                if user_related_name\n                # Use carrier.source_model.__name__ for default related name base\n                else f\"{carrier.source_model.__name__.lower()}_{field_name}_set\"\n            )\n            final_related_name_base = sanitize_related_name(\n                str(related_name_base),\n                target_model_cls.__name__ if target_model_cls else target_model_cls_name_only,\n                field_name,\n            )\n\n            # Ensure uniqueness using carrier's tracker\n            target_model_key_for_tracker = (\n                target_model_cls.__name__ if target_model_cls else target_django_model_str\n            )\n            target_related_names = carrier.used_related_names_per_target.setdefault(\n                target_model_key_for_tracker, set()\n            )\n            unique_related_name = final_related_name_base\n            counter = 1\n            while unique_related_name in target_related_names:\n                unique_related_name = f\"{final_related_name_base}_{counter}\"\n                counter += 1\n            target_related_names.add(unique_related_name)\n            constructor_kwargs[\"related_name\"] = unique_related_name\n            logger.debug(f\"[REL] Dataclass Field '{field_name}': Assigning related_name='{unique_related_name}'\")\n\n            # Re-confirm on_delete (mapper sets default based on Optional, but metadata might override)\n            # Need to check optionality of the original type here\n            origin = get_origin(original_field_type)\n            args = get_args(original_field_type)\n            is_optional = origin is Union and type(None) in args\n\n            if (\n                django_field_class in (models.ForeignKey, models.OneToOneField)\n                and \"on_delete\" not in constructor_kwargs  # Only set if not specified in metadata\n            ):\n                constructor_kwargs[\"on_delete\"] = models.SET_NULL if is_optional else models.CASCADE\n                # Add import using result helper\n                result.add_import(\"django.db.models\", \"SET_NULL\" if is_optional else \"CASCADE\")\n            elif django_field_class == models.ManyToManyField:\n                constructor_kwargs.pop(\"on_delete\", None)\n                constructor_kwargs.pop(\"null\", None)  # M2M cannot be null\n                if \"blank\" not in constructor_kwargs:  # Default M2M to blank=True if not set\n                    constructor_kwargs[\"blank\"] = True\n\n        # --- Perform Instantiation Locally --- #\n        try:\n            logger.debug(\n                f\"Instantiating {django_field_class.__name__} for dataclass field '{field_name}' with kwargs: {constructor_kwargs}\"\n            )\n            result.django_field = django_field_class(**constructor_kwargs)\n            result.field_kwargs = constructor_kwargs  # Store final kwargs\n        except Exception as e:\n            error_msg = f\"Failed to instantiate Django field '{field_name}' (type: {django_field_class.__name__}) with kwargs {constructor_kwargs}: {e}\"\n            logger.error(error_msg, exc_info=True)\n            result.error_str = error_msg\n            result.context_field = field_info\n            return result\n\n        # --- Generate Field Definition String --- #\n        result.field_definition_str = self._generate_field_def_string(result, carrier.meta_app_label)\n\n        return result  # Success\n\n    except Exception as e:\n        # Catch-all for unexpected errors during conversion\n        error_msg = f\"Unexpected error converting dataclass field '{model_name}.{field_name}': {e}\"\n        logger.error(error_msg, exc_info=True)\n        result.error_str = error_msg\n        result.context_field = field_info\n        return result\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.dataclass.generator.DataclassDjangoModelGenerator.generate_models_file","title":"<code>generate_models_file()</code>","text":"<p>Generate models for dataclasses with GFK finalize hook.</p> Source code in <code>src/pydantic2django/dataclass/generator.py</code> <pre><code>def generate_models_file(self) -&gt; str:\n    \"\"\"Generate models for dataclasses with GFK finalize hook.\"\"\"\n    self.discover_models()\n    models_to_process = self._get_models_in_processing_order()\n\n    # Reset imports and state\n    self.carriers = []\n    self.import_handler.extra_type_imports.clear()\n    self.import_handler.pydantic_imports.clear()\n    self.import_handler.context_class_imports.clear()\n    self.import_handler.imported_names.clear()\n    self.import_handler.processed_field_types.clear()\n    self.import_handler._add_type_import(self.base_model_class)\n\n    # Setup carriers\n    for source_model in models_to_process:\n        self.setup_django_model(source_model)\n\n    # GFK finalize: inject GenericRelation on parents\n    gfk_used = False\n    for carrier in self.carriers:\n        try:\n            if getattr(carrier, \"enable_gfk\", False) and getattr(carrier, \"pending_gfk_children\", None):\n                self.import_handler.add_import(\"django.contrib.contenttypes.fields\", \"GenericRelation\")\n                self.import_handler.add_import(\"django.contrib.contenttypes.fields\", \"GenericForeignKey\")\n                self.import_handler.add_import(\"django.contrib.contenttypes.models\", \"ContentType\")\n                carrier.django_field_definitions[\n                    \"entries\"\n                ] = \"GenericRelation('GenericEntry', related_query_name='entries')\"\n                gfk_used = True\n        except Exception:\n            pass\n\n    # Render model definitions\n    model_definitions: list[str] = []\n    django_model_names: list[str] = []\n    for carrier in self.carriers:\n        if carrier.django_model:\n            try:\n                model_def = self.generate_model_definition(carrier)\n                if model_def:\n                    model_definitions.append(model_def)\n                    django_model_names.append(f\"'{self._clean_generic_type(carrier.django_model.__name__)}'\")\n            except Exception:\n                pass\n\n    if gfk_used:\n        model_definitions.append(self._build_generic_entry_model_definition())\n        django_model_names.append(\"'GenericEntry'\")\n\n    unique_model_definitions = self._deduplicate_definitions(model_definitions)\n    imports = self.import_handler.deduplicate_imports()\n    template_context = self._prepare_template_context(unique_model_definitions, django_model_names, imports)\n    template_context.update(\n        {\n            \"generation_timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n            \"base_model_module\": self.base_model_class.__module__,\n            \"base_model_name\": self.base_model_class.__name__,\n            \"extra_type_imports\": sorted(self.import_handler.extra_type_imports),\n        }\n    )\n    template = self.jinja_env.get_template(\"models_file.py.j2\")\n    return template.render(**template_context)\n</code></pre>"},{"location":"Architecture/OVERVIEW/#typedclass-experimental","title":"TypedClass (experimental)","text":"<ul> <li>Discovery and factory (subject to change as APIs stabilize):</li> <li> <p>               Bases: <code>BaseDiscovery[TypedClassType]</code></p> <p>Discovers generic Python classes within specified packages.</p> Source code in <code>src/pydantic2django/typedclass/discovery.py</code> <pre><code>class TypedClassDiscovery(BaseDiscovery[TypedClassType]):\n    \"\"\"Discovers generic Python classes within specified packages.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        # TypedClass specific attributes, if any, can be initialized here.\n\n    def _is_pydantic_model(self, obj: Any) -&gt; bool:\n        \"\"\"Checks if an object is a Pydantic model.\"\"\"\n        return inspect.isclass(obj) and issubclass(obj, BaseModel)\n\n    def _is_target_model(self, obj: Any) -&gt; bool:\n        \"\"\"\n        Check if an object is a generic class suitable for conversion.\n        It must be a class, not an ABC, not a Pydantic model, and not a dataclass.\n        \"\"\"\n        if not inspect.isclass(obj):\n            return False\n        if inspect.isabstract(obj):\n            logger.debug(f\"Skipping abstract class {obj.__name__}\")\n            return False\n        if self._is_pydantic_model(obj):  # Check if it's a Pydantic model\n            logger.debug(f\"Skipping Pydantic model {obj.__name__}\")\n            return False\n        if dataclasses.is_dataclass(obj):\n            logger.debug(f\"Skipping dataclass {obj.__name__}\")\n            return False\n\n        # Further checks can be added here, e.g., must be in a specific list\n        # or have certain characteristics. For now, this is a basic filter.\n        logger.debug(f\"Identified potential target typed class: {obj.__name__}\")\n        return True\n\n    def _default_eligibility_filter(self, model: TypedClassType) -&gt; bool:\n        \"\"\"\n        Check default eligibility for generic classes.\n        For example, we might want to ensure it's not an ABC, though\n        _is_target_model should already catch this.\n        \"\"\"\n        # Redundant check if _is_target_model is comprehensive, but good for safety.\n        if inspect.isabstract(model):\n            logger.debug(f\"Filtering out typed class {model.__name__} (is abstract)\")\n            return False\n\n        # Add other default checks if needed.\n        # For instance, are there specific base classes (not ABCs) we want to exclude/include?\n        return True\n\n    def analyze_dependencies(self) -&gt; None:\n        \"\"\"\n        Build the dependency graph for the filtered generic classes.\n        Dependencies are determined by type hints in __init__ arguments\n        and class-level attribute annotations.\n        \"\"\"\n        logger.info(\"Analyzing dependencies between filtered typed classes...\")\n        self.dependencies: dict[TypedClassType, set[TypedClassType]] = {}\n\n        # Ensure all filtered models are keys in the dependencies dict\n        for model_qualname in self.filtered_models:\n            model_obj = self.filtered_models[model_qualname]\n            self.dependencies[model_obj] = set()\n\n        filtered_model_qualnames = set(self.filtered_models.keys())\n\n        def _find_and_add_dependency(source_model: TypedClassType, potential_dep_type: Any):\n            \"\"\"\n            Helper to check if a potential dependency type is a target model\n            and add it to the graph.\n            \"\"\"\n            # Check if potential_dep_type itself is a class and one of our targets\n            if self._is_target_model(potential_dep_type):\n                dep_qualname = f\"{potential_dep_type.__module__}.{potential_dep_type.__name__}\"\n                if dep_qualname in filtered_model_qualnames and potential_dep_type is not source_model:\n                    dep_model_obj = self.filtered_models.get(dep_qualname)\n                    if dep_model_obj:\n                        self.dependencies[source_model].add(dep_model_obj)\n                    else:\n                        logger.warning(\n                            f\"Inconsistency: Dependency '{dep_qualname}' for typed class \"\n                            f\"'{source_model.__name__}' found by name but not as object in filtered set.\"\n                        )\n            # TODO: Handle generics like list[TargetType], dict[str, TargetType], Union[TargetType, None]\n            # This would involve using get_origin and get_args from typing.\n\n        for model_type in self.filtered_models.values():\n            # 1. Analyze __init__ parameters\n            try:\n                init_signature = inspect.signature(model_type.__init__)\n                for param in init_signature.parameters.values():\n                    if param.name == \"self\" or param.annotation is inspect.Parameter.empty:\n                        continue\n                    _find_and_add_dependency(model_type, param.annotation)\n            except (ValueError, TypeError) as e:  # Some built-ins or exotic classes might not have inspectable __init__\n                logger.debug(f\"Could not inspect __init__ for {model_type.__name__}: {e}\")\n\n            # 2. Analyze class-level annotations\n            try:\n                annotations = inspect.get_annotations(model_type, eval_str=True)\n                for _, attr_type in annotations.items():\n                    _find_and_add_dependency(model_type, attr_type)\n            except Exception as e:\n                logger.debug(f\"Could not get annotations for {model_type.__name__}: {e}\")\n\n        logger.info(\"Typed class dependency analysis complete.\")\n        # Debug logging of dependencies will be handled by BaseDiscovery.log_dependencies\n\n    def get_models_in_registration_order(self) -&gt; list[TypedClassType]:\n        \"\"\"\n        Return generic classes sorted topologically based on dependencies.\n        This method can often be inherited from BaseDiscovery if the dependency\n        graph is built correctly.\n        \"\"\"\n        # For now, assume BaseDiscovery's implementation is sufficient.\n        # If specific logic for typed classes is needed, override here.\n        return super().get_models_in_registration_order()\n</code></pre> </li> <li> <p>               Bases: <code>BaseModelFactory[TypedClassType, TypedClassFieldInfo]</code></p> <p>Creates Django model definitions from generic Python classes.</p> Source code in <code>src/pydantic2django/typedclass/factory.py</code> <pre><code>class TypedClassModelFactory(BaseModelFactory[TypedClassType, TypedClassFieldInfo]):\n    \"\"\"Creates Django model definitions from generic Python classes.\"\"\"\n\n    def __init__(\n        self,\n        field_factory: TypedClassFieldFactory,\n        relationship_accessor: RelationshipConversionAccessor,\n        # Add 'reckless_mode: bool = False' if implementing that flag\n    ):\n        super().__init__(field_factory, relationship_accessor)\n        # self.reckless_mode = reckless_mode\n\n    def _get_model_fields_info(\n        self, model_class: TypedClassType, carrier: ConversionCarrier\n    ) -&gt; list[TypedClassFieldInfo]:\n        \"\"\"\n        Extracts attribute information from a generic class.\n        Prioritizes __init__ signature, then class-level annotations.\n        \"\"\"\n        field_infos = []\n        processed_params = set()\n\n        # 1. Inspect __init__ method\n        try:\n            init_signature = inspect.signature(model_class.__init__)\n            for name, param in init_signature.parameters.items():\n                if name == \"self\":\n                    continue\n\n                type_hint = param.annotation if param.annotation is not inspect.Parameter.empty else Any\n                default_val = param.default if param.default is not inspect.Parameter.empty else inspect.Parameter.empty\n\n                field_infos.append(\n                    TypedClassFieldInfo(name=name, type_hint=type_hint, default_value=default_val, is_from_init=True)\n                )\n                processed_params.add(name)\n        except (ValueError, TypeError) as e:\n            logger.debug(\n                f\"Could not inspect __init__ for {model_class.__name__}: {e}. Proceeding with class annotations.\"\n            )\n\n        # 2. Inspect class-level annotations (for attributes not in __init__)\n        try:\n            annotations = inspect.get_annotations(model_class, eval_str=True)\n            for name, type_hint in annotations.items():\n                if name not in processed_params and not name.startswith(\"_\"):  # Avoid private/protected by convention\n                    default_val = getattr(model_class, name, inspect.Parameter.empty)\n                    field_infos.append(\n                        TypedClassFieldInfo(\n                            name=name, type_hint=type_hint, default_value=default_val, is_from_init=False\n                        )\n                    )\n        except Exception as e:  # Broad exception as get_annotations can fail in various ways\n            logger.debug(f\"Could not get class annotations for {model_class.__name__}: {e}\")\n\n        logger.debug(f\"Discovered field infos for {model_class.__name__}: {field_infos}\")\n        return field_infos\n\n    def create_model_definition(\n        self,\n        model_class: TypedClassType,\n        app_label: str,\n        base_model_class: type[models.Model],\n        module_mappings: Optional[dict[str, str]] = None,\n    ) -&gt; ConversionCarrier[TypedClassType]:\n        \"\"\"\n        Generates a ConversionCarrier containing the Django model string and related info.\n        \"\"\"\n        model_name = model_class.__name__\n        django_model_name = f\"{model_name}DjangoModel\"  # Or some other naming convention\n\n        carrier = ConversionCarrier(\n            source_model=model_class,\n            source_model_name=model_name,\n            django_model_name=django_model_name,\n            app_label=app_label,\n            module_mappings=module_mappings or {},\n            relationship_accessor=self.relationship_accessor,\n        )\n\n        # Add import for the base model class\n        carrier.add_django_model_import(base_model_class)\n\n        field_definitions = []\n        model_fields_info = self._get_model_fields_info(model_class, carrier)\n\n        if not model_fields_info:\n            logger.warning(f\"No fields discovered for class {model_name}. Generating an empty Django model.\")\n            # Optionally, add a default placeholder field if empty models are problematic\n            # field_definitions.append(\"    # No convertible fields found\")\n\n        for field_info in model_fields_info:\n            try:\n                field_def_str = self.field_factory.create_field_definition(field_info, carrier)\n                field_definitions.append(f\"    {field_def_str}\")\n            except Exception as e:\n                logger.error(\n                    f\"Error creating field definition for {field_info.name} in {model_name}: {e}\", exc_info=True\n                )\n                # Optionally, add a placeholder or skip this field\n                field_definitions.append(f\"    # Error processing field: {field_info.name} - {e}\")\n\n        carrier.django_field_definitions = field_definitions\n\n        # Meta class\n        carrier.meta_class_string = generate_meta_class_string(\n            app_label=app_label,\n            django_model_name=django_model_name,  # Use the generated Django model name\n            verbose_name=model_name,\n        )\n\n        # __str__ method\n        # Heuristic: use 'name' or 'id' attribute if present in field_infos, else default\n        str_field = \"id\"  # Django models get 'id' by default from models.Model\n        for finfo in model_fields_info:\n            if finfo.name in [\"name\", \"title\", \"identifier\"]:  # common __str__ candidates\n                str_field = finfo.name\n                break\n\n        carrier.str_method_string = f\"    def __str__(self):\\n        return str(self.{str_field})\"\n\n        logger.info(f\"Prepared ConversionCarrier for {model_name} -&gt; {django_model_name}\")\n        return carrier\n</code></pre> </li> <li> <p>Notes:</p> </li> <li>Targets arbitrary typed Python classes (non-Pydantic, non-dataclass). APIs are evolving.</li> </ul>"},{"location":"Architecture/OVERVIEW/#pydantic2django.typedclass.discovery.TypedClassDiscovery.analyze_dependencies","title":"<code>analyze_dependencies()</code>","text":"<p>Build the dependency graph for the filtered generic classes. Dependencies are determined by type hints in init arguments and class-level attribute annotations.</p> Source code in <code>src/pydantic2django/typedclass/discovery.py</code> <pre><code>def analyze_dependencies(self) -&gt; None:\n    \"\"\"\n    Build the dependency graph for the filtered generic classes.\n    Dependencies are determined by type hints in __init__ arguments\n    and class-level attribute annotations.\n    \"\"\"\n    logger.info(\"Analyzing dependencies between filtered typed classes...\")\n    self.dependencies: dict[TypedClassType, set[TypedClassType]] = {}\n\n    # Ensure all filtered models are keys in the dependencies dict\n    for model_qualname in self.filtered_models:\n        model_obj = self.filtered_models[model_qualname]\n        self.dependencies[model_obj] = set()\n\n    filtered_model_qualnames = set(self.filtered_models.keys())\n\n    def _find_and_add_dependency(source_model: TypedClassType, potential_dep_type: Any):\n        \"\"\"\n        Helper to check if a potential dependency type is a target model\n        and add it to the graph.\n        \"\"\"\n        # Check if potential_dep_type itself is a class and one of our targets\n        if self._is_target_model(potential_dep_type):\n            dep_qualname = f\"{potential_dep_type.__module__}.{potential_dep_type.__name__}\"\n            if dep_qualname in filtered_model_qualnames and potential_dep_type is not source_model:\n                dep_model_obj = self.filtered_models.get(dep_qualname)\n                if dep_model_obj:\n                    self.dependencies[source_model].add(dep_model_obj)\n                else:\n                    logger.warning(\n                        f\"Inconsistency: Dependency '{dep_qualname}' for typed class \"\n                        f\"'{source_model.__name__}' found by name but not as object in filtered set.\"\n                    )\n        # TODO: Handle generics like list[TargetType], dict[str, TargetType], Union[TargetType, None]\n        # This would involve using get_origin and get_args from typing.\n\n    for model_type in self.filtered_models.values():\n        # 1. Analyze __init__ parameters\n        try:\n            init_signature = inspect.signature(model_type.__init__)\n            for param in init_signature.parameters.values():\n                if param.name == \"self\" or param.annotation is inspect.Parameter.empty:\n                    continue\n                _find_and_add_dependency(model_type, param.annotation)\n        except (ValueError, TypeError) as e:  # Some built-ins or exotic classes might not have inspectable __init__\n            logger.debug(f\"Could not inspect __init__ for {model_type.__name__}: {e}\")\n\n        # 2. Analyze class-level annotations\n        try:\n            annotations = inspect.get_annotations(model_type, eval_str=True)\n            for _, attr_type in annotations.items():\n                _find_and_add_dependency(model_type, attr_type)\n        except Exception as e:\n            logger.debug(f\"Could not get annotations for {model_type.__name__}: {e}\")\n\n    logger.info(\"Typed class dependency analysis complete.\")\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.typedclass.discovery.TypedClassDiscovery.get_models_in_registration_order","title":"<code>get_models_in_registration_order()</code>","text":"<p>Return generic classes sorted topologically based on dependencies. This method can often be inherited from BaseDiscovery if the dependency graph is built correctly.</p> Source code in <code>src/pydantic2django/typedclass/discovery.py</code> <pre><code>def get_models_in_registration_order(self) -&gt; list[TypedClassType]:\n    \"\"\"\n    Return generic classes sorted topologically based on dependencies.\n    This method can often be inherited from BaseDiscovery if the dependency\n    graph is built correctly.\n    \"\"\"\n    # For now, assume BaseDiscovery's implementation is sufficient.\n    # If specific logic for typed classes is needed, override here.\n    return super().get_models_in_registration_order()\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.typedclass.factory.TypedClassModelFactory.create_model_definition","title":"<code>create_model_definition(model_class, app_label, base_model_class, module_mappings=None)</code>","text":"<p>Generates a ConversionCarrier containing the Django model string and related info.</p> Source code in <code>src/pydantic2django/typedclass/factory.py</code> <pre><code>def create_model_definition(\n    self,\n    model_class: TypedClassType,\n    app_label: str,\n    base_model_class: type[models.Model],\n    module_mappings: Optional[dict[str, str]] = None,\n) -&gt; ConversionCarrier[TypedClassType]:\n    \"\"\"\n    Generates a ConversionCarrier containing the Django model string and related info.\n    \"\"\"\n    model_name = model_class.__name__\n    django_model_name = f\"{model_name}DjangoModel\"  # Or some other naming convention\n\n    carrier = ConversionCarrier(\n        source_model=model_class,\n        source_model_name=model_name,\n        django_model_name=django_model_name,\n        app_label=app_label,\n        module_mappings=module_mappings or {},\n        relationship_accessor=self.relationship_accessor,\n    )\n\n    # Add import for the base model class\n    carrier.add_django_model_import(base_model_class)\n\n    field_definitions = []\n    model_fields_info = self._get_model_fields_info(model_class, carrier)\n\n    if not model_fields_info:\n        logger.warning(f\"No fields discovered for class {model_name}. Generating an empty Django model.\")\n        # Optionally, add a default placeholder field if empty models are problematic\n        # field_definitions.append(\"    # No convertible fields found\")\n\n    for field_info in model_fields_info:\n        try:\n            field_def_str = self.field_factory.create_field_definition(field_info, carrier)\n            field_definitions.append(f\"    {field_def_str}\")\n        except Exception as e:\n            logger.error(\n                f\"Error creating field definition for {field_info.name} in {model_name}: {e}\", exc_info=True\n            )\n            # Optionally, add a placeholder or skip this field\n            field_definitions.append(f\"    # Error processing field: {field_info.name} - {e}\")\n\n    carrier.django_field_definitions = field_definitions\n\n    # Meta class\n    carrier.meta_class_string = generate_meta_class_string(\n        app_label=app_label,\n        django_model_name=django_model_name,  # Use the generated Django model name\n        verbose_name=model_name,\n    )\n\n    # __str__ method\n    # Heuristic: use 'name' or 'id' attribute if present in field_infos, else default\n    str_field = \"id\"  # Django models get 'id' by default from models.Model\n    for finfo in model_fields_info:\n        if finfo.name in [\"name\", \"title\", \"identifier\"]:  # common __str__ candidates\n            str_field = finfo.name\n            break\n\n    carrier.str_method_string = f\"    def __str__(self):\\n        return str(self.{str_field})\"\n\n    logger.info(f\"Prepared ConversionCarrier for {model_name} -&gt; {django_model_name}\")\n    return carrier\n</code></pre>"},{"location":"Architecture/OVERVIEW/#django-base-models-runtime-mapping-and-storage","title":"Django base models (runtime mapping and storage)","text":"<p>These abstract models provide runtime persistence patterns for typed objects.</p> <ul> <li>Store entire objects as JSON or map their fields to columns. See:</li> <li> <p>               Bases: <code>Pydantic2DjangoBase</code>, <code>Generic[PydanticT]</code></p> <p>Base class for mapping Pydantic model fields to Django model fields.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>class Pydantic2DjangoBaseClass(Pydantic2DjangoBase, Generic[PydanticT]):\n    \"\"\"\n    Base class for mapping Pydantic model fields to Django model fields.\n    \"\"\"\n\n    class Meta(Pydantic2DjangoBase.Meta):\n        abstract = True\n        verbose_name = \"Mapped Pydantic Object\"\n        verbose_name_plural = \"Mapped Pydantic Objects\"\n\n    def __new__(cls, *args, **kwargs):\n        \"\"\"\n        Override __new__ to ensure proper type checking.\n        Needed because Django's model metaclass doesn't preserve Generic type parameters well.\n        \"\"\"\n        # The check itself might be complex. This placeholder ensures __new__ is considered.\n        # Proper generic handling might require metaclass adjustments beyond this scope.\n        return super().__new__(cls)\n\n    def __getattr__(self, name: str) -&gt; Any:\n        \"\"\"\n        Forward method calls to the Pydantic model implementation.\n        Enables type checking and execution of methods defined on the Pydantic model.\n        \"\"\"\n        # Get the Pydantic model class\n        try:\n            pydantic_cls = self._get_class()  # Use common method\n            if not issubclass(pydantic_cls, BaseModel):\n                # This path shouldn't be hit if used correctly, but safeguard\n                raise AttributeError(f\"Stored class '{self.class_path}' is not a Pydantic BaseModel.\")\n        except ValueError as e:\n            raise AttributeError(f\"Cannot forward attribute '{name}': {e}\") from e\n\n        # Check if the attribute exists in the Pydantic model\n        if hasattr(pydantic_cls, name):\n            attr = getattr(pydantic_cls, name)\n\n            # If it's a callable method (and not the type itself), wrap it\n            if callable(attr) and not isinstance(attr, type):\n\n                def wrapped_method(*args, **kwargs):\n                    # Convert self (Django model) to Pydantic instance first\n                    try:\n                        pydantic_instance = self.to_pydantic()  # Assuming no context needed here\n                    except ValueError as e:\n                        # Handle potential errors during conversion (e.g., context missing)\n                        raise RuntimeError(\n                            f\"Failed to convert Django model to Pydantic before calling '{name}': {e}\"\n                        ) from e\n\n                    # Call the method on the Pydantic instance\n                    result = getattr(pydantic_instance, name)(*args, **kwargs)\n                    # TODO: Handle potential need to update self from result? Unlikely for most methods.\n                    return result\n\n                return wrapped_method\n            else:\n                # For non-method attributes (like class vars), return directly\n                # This might need refinement depending on desired behavior for class vs instance attrs\n                return attr\n\n        # If attribute doesn't exist on Pydantic model, raise standard AttributeError\n        raise AttributeError(\n            f\"'{self.__class__.__name__}' object has no attribute '{name}', \"\n            f\"and Pydantic model '{pydantic_cls.__name__}' has no attribute '{name}'.\"\n        )\n\n    @classmethod\n    def from_pydantic(cls, pydantic_obj: PydanticT, name: str | None = None) -&gt; \"Pydantic2DjangoBaseClass[PydanticT]\":\n        \"\"\"\n        Create a Django model instance from a Pydantic object, mapping fields.\n\n        Args:\n            pydantic_obj: The Pydantic object to convert.\n            name: Optional name for the Django model instance.\n\n        Returns:\n            A new instance of this Django model subclass.\n\n        Raises:\n            TypeError: If the object is not a Pydantic model or not of the expected type.\n        \"\"\"\n        # Get class info and check type\n        (\n            pydantic_class,\n            class_name,\n            module_name,\n            fully_qualified_name,\n        ) = cls._get_class_info(pydantic_obj)\n        cls._check_expected_type(pydantic_obj, class_name)  # Verifies it's a Pydantic model\n\n        # Derive name\n        derived_name = cls._derive_name(pydantic_obj, name, class_name)\n\n        # Create instance with basic fields\n        instance = cls(\n            name=derived_name,\n            class_path=fully_qualified_name,  # Use renamed field\n        )\n\n        # Update mapped fields\n        instance.update_fields_from_pydantic(pydantic_obj)\n\n        return instance\n\n    def update_fields_from_pydantic(self, pydantic_obj: PydanticT) -&gt; None:\n        \"\"\"\n        Update this Django model's fields from a Pydantic object's fields.\n\n        Args:\n            pydantic_obj: The Pydantic object containing source values.\n        \"\"\"\n        if (\n            not isinstance(pydantic_obj, BaseModel)\n            or pydantic_obj.__class__.__module__ != self._get_class().__module__\n            or pydantic_obj.__class__.__name__ != self._get_class().__name__\n        ):\n            # Check type consistency before proceeding\n            raise TypeError(\n                f\"Provided object type {type(pydantic_obj)} does not match expected type {self.class_path} for update.\"\n            )\n\n        # Get data from the Pydantic object\n        try:\n            pydantic_data = pydantic_obj.model_dump()\n        except AttributeError as err:\n            raise TypeError(\n                \"Failed to dump Pydantic model for update. Ensure you are using Pydantic v2+ with model_dump().\"\n            ) from err\n\n        # Get Django model fields excluding common/meta ones\n        model_field_names = {\n            field.name\n            for field in self._meta.fields\n            if field.name not in (\"id\", \"name\", \"class_path\", \"created_at\", \"updated_at\")\n        }\n\n        # Update each Django field if it matches a field in the Pydantic data\n        for field_name in model_field_names:\n            if field_name in pydantic_data:\n                value = pydantic_data[field_name]\n                # Apply serialization (important for complex types)\n                serialized_value = serialize_value(value)\n                setattr(self, field_name, serialized_value)\n            # Else: Field exists on Django model but not on Pydantic model, leave it unchanged.\n\n    def to_pydantic(self, context: ModelContext | None = None) -&gt; PydanticT:\n        \"\"\"\n        Convert this Django model instance back to a Pydantic object.\n\n        Args:\n            context: Optional ModelContext instance containing values for non-serializable fields.\n\n        Returns:\n            The corresponding Pydantic object.\n\n        Raises:\n            ValueError: If context is required but not provided, or if class load/instantiation fails.\n        \"\"\"\n        pydantic_class = self._get_class()  # Use common method\n        if not issubclass(pydantic_class, BaseModel):\n            raise ValueError(f\"Stored class path '{self.class_path}' does not point to a Pydantic BaseModel.\")\n\n        # Get data from Django fields corresponding to Pydantic fields\n        data = self._get_data_for_pydantic(pydantic_class)\n\n        # Handle context if required and provided\n        required_context_keys = self._get_required_context_fields()  # Check if context is needed\n        if required_context_keys:\n            if not context:\n                raise ValueError(\n                    f\"Conversion to Pydantic model '{pydantic_class.__name__}' requires context \"\n                    f\"for fields: {', '.join(required_context_keys)}. Please provide a ModelContext instance.\"\n                )\n            # Validate and merge context data\n            context_dict = context.to_conversion_dict()\n            context.validate_context(context_dict)  # Validate required keys are present\n            data.update(context_dict)  # Merge context, potentially overwriting DB values if keys overlap\n\n        # Reconstruct the Pydantic object\n        try:\n            # TODO: Add potential deserialization logic here if needed before validation\n            instance = pydantic_class.model_validate(data)\n            # Cast to the generic type variable\n            return cast(PydanticT, instance)\n        except TypeError:\n            raise  # Re-raise TypeError as it's a specific, meaningful exception here\n        except Exception as e:\n            # Catch any other unexpected error during Pydantic object creation\n            logger.error(\n                f\"An unexpected error occurred creating Pydantic model for '{pydantic_class.__name__}': {e}\",\n                exc_info=True,\n            )\n            raise ValueError(f\"Unexpected error creating Pydantic model for '{pydantic_class.__name__}'\") from e\n\n    def _get_data_for_pydantic(self, pydantic_class: type[BaseModel]) -&gt; dict[str, Any]:\n        \"\"\"Get data from Django fields that correspond to the target Pydantic model fields.\"\"\"\n        data = {}\n        try:\n            pydantic_field_names = set(pydantic_class.model_fields.keys())\n        except AttributeError as err:\n            # Should not happen if issubclass(BaseModel) check passed\n            raise ValueError(f\"Could not get fields for non-Pydantic type '{pydantic_class.__name__}'\") from err\n\n        # Add DB fields that are part of the Pydantic model\n        for field in self._meta.fields:\n            if field.name in pydantic_field_names:\n                # TODO: Add potential deserialization based on target Pydantic field type?\n                data[field.name] = getattr(self, field.name)\n\n        # Context values are merged in the calling `to_pydantic` method\n        return data\n\n    def _get_required_context_fields(self) -&gt; set[str]:\n        \"\"\"\n        Get the set of field names that require context when converting to Pydantic.\n        (Placeholder implementation - needs refinement based on how context is defined).\n        \"\"\"\n        # This requires a mechanism to identify which Django fields represent\n        # non-serializable data that must come from context.\n        # For now, assume no context is required by default for the base class.\n        # Subclasses might override this or a more sophisticated mechanism could be added.\n        # Example: Check for a custom field attribute like `is_context_field=True`\n        required_fields = set()\n        # pydantic_class = self._get_class()\n        # pydantic_field_names = set(pydantic_class.model_fields.keys())\n        # for field in self._meta.fields:\n        #     if field.name in pydantic_field_names and getattr(field, 'is_context_field', False):\n        #         required_fields.add(field.name)\n        return required_fields  # Return empty set for now\n\n    def update_from_pydantic(self, pydantic_obj: PydanticT) -&gt; None:\n        \"\"\"\n        Update this Django model with new data from a Pydantic object and save.\n\n        Args:\n            pydantic_obj: The Pydantic object with updated data.\n        \"\"\"\n        # Verify the object type matches first (includes check if it's a BaseModel)\n        fully_qualified_name = self._verify_object_type_match(pydantic_obj)\n\n        # Update the class_path if somehow inconsistent\n        if self.class_path != fully_qualified_name:\n            self.class_path = fully_qualified_name\n\n        self.update_fields_from_pydantic(pydantic_obj)\n        self.save()\n\n    def save_as_pydantic(self) -&gt; PydanticT:\n        \"\"\"\n        Save the Django model and return the corresponding Pydantic object.\n\n        Returns:\n            The corresponding Pydantic object.\n        \"\"\"\n        self.save()\n        return self.to_pydantic(context=None)\n</code></pre> </li> <li> <p>               Bases: <code>Dataclass2DjangoBase</code>, <code>Generic[DataclassT]</code></p> <p>Base class for mapping Python Dataclass fields to Django model fields.</p> <p>Inherits from Dataclass2DjangoBase and provides methods to convert between the Dataclass instance and the Django model instance by matching field names.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>class Dataclass2DjangoBaseClass(Dataclass2DjangoBase, Generic[DataclassT]):\n    \"\"\"\n    Base class for mapping Python Dataclass fields to Django model fields.\n\n    Inherits from Dataclass2DjangoBase and provides methods to convert\n    between the Dataclass instance and the Django model instance by matching field names.\n    \"\"\"\n\n    class Meta(Dataclass2DjangoBase.Meta):\n        abstract = True\n        verbose_name = \"Mapped Dataclass\"\n        verbose_name_plural = \"Mapped Dataclasses\"\n\n    # __getattr__ is less likely needed/useful for standard dataclasses compared to Pydantic models\n    # which might have complex methods. Skip for now.\n\n    @classmethod\n    def from_dataclass(cls, dc_obj: DataclassT, name: str | None = None) -&gt; \"Dataclass2DjangoBaseClass[DataclassT]\":\n        \"\"\"\n        Create a Django model instance from a Dataclass object, mapping fields.\n\n        Args:\n            dc_obj: The Dataclass object to convert.\n            name: Optional name for the Django model instance.\n\n        Returns:\n            A new instance of this Django model subclass.\n\n        Raises:\n            TypeError: If the object is not a dataclass or not of the expected type.\n        \"\"\"\n        # Get class info and check type\n        (\n            dc_class,\n            class_name,\n            module_name,\n            fully_qualified_name,\n        ) = cls._get_class_info(dc_obj)\n        cls._check_expected_type(dc_obj, class_name)  # Verifies it's a dataclass\n\n        # Derive name\n        derived_name = cls._derive_name(dc_obj, name, class_name)\n\n        # Create instance with basic fields\n        instance = cls(\n            name=derived_name,\n            class_path=fully_qualified_name,\n        )\n\n        # Update mapped fields\n        instance.update_fields_from_dataclass(dc_obj)\n\n        return instance\n\n    def update_fields_from_dataclass(self, dc_obj: DataclassT) -&gt; None:\n        \"\"\"\n        Update this Django model's fields from a Dataclass object's fields.\n\n        Args:\n            dc_obj: The Dataclass object containing source values.\n\n        Raises:\n            TypeError: If conversion to dict fails.\n        \"\"\"\n        if (\n            not dataclasses.is_dataclass(dc_obj)\n            or dc_obj.__class__.__module__ != self._get_class().__module__\n            or dc_obj.__class__.__name__ != self._get_class().__name__\n        ):\n            # Check type consistency before proceeding\n            raise TypeError(\n                f\"Provided object type {type(dc_obj)} does not match expected type {self.class_path} for update.\"\n            )\n\n        try:\n            dc_data = dataclasses.asdict(dc_obj)\n        except TypeError as e:\n            raise TypeError(f\"Could not convert dataclass '{dc_obj.__class__.__name__}' to dict for update: {e}\") from e\n\n        # Get Django model fields excluding common/meta ones\n        model_field_names = {\n            field.name\n            for field in self._meta.fields\n            if field.name not in (\"id\", \"name\", \"class_path\", \"created_at\", \"updated_at\")\n        }\n\n        for field_name in model_field_names:\n            if field_name in dc_data:\n                value = dc_data[field_name]\n                # Apply serialization (important for complex types like datetime, UUID, etc.)\n                serialized_value = serialize_value(value)\n                setattr(self, field_name, serialized_value)\n            # Else: Field exists on Django model but not on dataclass, leave it unchanged.\n\n    def to_dataclass(self) -&gt; DataclassT:\n        \"\"\"\n        Convert this Django model instance back to a Dataclass object.\n\n        Returns:\n            The reconstructed Dataclass object.\n\n        Raises:\n            ValueError: If the class cannot be loaded or instantiation fails.\n        \"\"\"\n        dataclass_type = self._get_class()\n        if not dataclasses.is_dataclass(dataclass_type):\n            raise ValueError(f\"Stored class path '{self.class_path}' does not point to a dataclass.\")\n\n        # Get data from Django fields corresponding to dataclass fields\n        data_for_dc = self._get_data_for_dataclass(dataclass_type)\n\n        # Instantiate the dataclass\n        try:\n            # TODO: Add deserialization logic if needed\n            instance = dataclass_type(**data_for_dc)\n            # Cast to the generic type variable for type hinting\n            return cast(DataclassT, instance)\n        except TypeError as e:\n            raise ValueError(\n                f\"Failed to instantiate dataclass '{dataclass_type.__name__}' from Django model fields. \"\n                f\"Ensure required fields exist and types are compatible. Error: {e}\"\n            ) from e\n        except Exception as e:\n            logger.error(f\"An unexpected error occurred during dataclass reconstruction: {e}\", exc_info=True)\n            raise ValueError(f\"An unexpected error occurred during dataclass reconstruction: {e}\") from e\n\n    def _get_data_for_dataclass(self, dataclass_type: type) -&gt; dict[str, Any]:\n        \"\"\"Get data from Django fields that correspond to the target dataclass fields.\"\"\"\n        data = {}\n        try:\n            dc_field_names = {f.name for f in dataclasses.fields(dataclass_type)}\n        except TypeError as err:\n            # Should not happen if is_dataclass check passed, but handle defensively\n            raise ValueError(f\"Could not get fields for non-dataclass type '{dataclass_type.__name__}'\") from err\n\n        # Add DB fields that are part of the dataclass\n        for field in self._meta.fields:\n            if field.name in dc_field_names:\n                # TODO: Add potential deserialization based on target dataclass field type?\n                data[field.name] = getattr(self, field.name)\n\n        # Context handling is usually Pydantic-specific, skip for dataclasses unless needed\n        return data\n\n    def update_from_dataclass(self, dc_obj: DataclassT) -&gt; None:\n        \"\"\"\n        Update this Django model with new data from a Dataclass object and save.\n\n        Args:\n            dc_obj: The Dataclass object with updated data.\n        \"\"\"\n        # Verify the object type matches first (includes check if it's a dataclass)\n        fully_qualified_name = self._verify_object_type_match(dc_obj)\n\n        # Update the class_path if somehow inconsistent\n        if self.class_path != fully_qualified_name:\n            self.class_path = fully_qualified_name\n\n        self.update_fields_from_dataclass(dc_obj)\n        self.save()\n\n    def save_as_dataclass(self) -&gt; DataclassT:\n        \"\"\"\n        Save the Django model and return the corresponding Dataclass object.\n\n        Returns:\n            The corresponding Dataclass object.\n        \"\"\"\n        self.save()\n        return self.to_dataclass()\n</code></pre> </li> <li> <p>               Bases: <code>TypedClass2DjangoBase</code>, <code>Generic[TypedClassT]</code></p> <p>Base class for mapping generic Python class fields to Django model fields.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>class TypedClass2DjangoBaseClass(TypedClass2DjangoBase, Generic[TypedClassT]):\n    \"\"\"\n    Base class for mapping generic Python class fields to Django model fields.\n    \"\"\"\n\n    class Meta(TypedClass2DjangoBase.Meta):\n        abstract = True\n        verbose_name = \"Mapped Typed Class\"\n        verbose_name_plural = \"Mapped Typed Classes\"\n\n    @classmethod\n    def from_typedclass(\n        cls, typed_obj: TypedClassT, name: str | None = None\n    ) -&gt; \"TypedClass2DjangoBaseClass[TypedClassT]\":\n        \"\"\"\n        Create a Django model instance from a generic class object, mapping fields.\n        \"\"\"\n        obj_class, class_name, module_name, fqn = cls._get_class_info(typed_obj)\n        cls._check_expected_type(typed_obj, class_name)\n        derived_name = cls._derive_name(typed_obj, name, class_name)\n\n        instance = cls(\n            name=derived_name,\n            class_path=fqn,\n        )\n        instance.update_fields_from_typedclass(typed_obj)\n        return instance\n\n    def update_fields_from_typedclass(self, typed_obj: TypedClassT) -&gt; None:\n        \"\"\"\n        Update this Django model's fields from a generic class object's fields.\n        \"\"\"\n        if (\n            typed_obj.__class__.__module__ != self._get_class().__module__\n            or typed_obj.__class__.__name__ != self._get_class().__name__\n        ):\n            raise TypeError(\n                f\"Provided object type {type(typed_obj)} does not match expected type {self.class_path} for update.\"\n            )\n\n        # Extract data. For mapped fields, we prefer __init__ params or direct attrs.\n        # Using __dict__ might be too broad here if not all dict items are mapped fields.\n        # Let's assume attributes corresponding to Django fields are directly accessible.\n\n        model_field_names = {\n            field.name\n            for field in self._meta.fields\n            if field.name not in (\"id\", \"name\", \"class_path\", \"created_at\", \"updated_at\")\n        }\n\n        for field_name in model_field_names:\n            if hasattr(typed_obj, field_name):\n                value = getattr(typed_obj, field_name)\n                setattr(self, field_name, serialize_value(value))\n            # Else: Field on Django model, not on typed_obj. Leave as is or handle.\n\n    def to_typedclass(self) -&gt; TypedClassT:\n        \"\"\"\n        Convert this Django model instance back to a generic class object.\n        \"\"\"\n        target_class = self._get_class()\n        data_for_typedclass = self._get_data_for_typedclass(target_class)\n\n        try:\n            # This assumes target_class can be instantiated with these parameters.\n            # Deserialization from serialize_value needs to be considered for complex types.\n            # TODO: Implement robust deserialization\n            deserialized_data = dict(data_for_typedclass.items())  # Placeholder\n\n            init_sig = inspect.signature(target_class.__init__)\n            valid_params = {k: v for k, v in deserialized_data.items() if k in init_sig.parameters}\n\n            instance = target_class(**valid_params)\n\n            # For attributes not in __init__ but set on Django model, try to setattr\n            for key, value in deserialized_data.items():\n                if key not in valid_params and hasattr(instance, key) and not key.startswith(\"_\"):\n                    try:\n                        setattr(instance, key, value)  # value is already deserialized_data[key]\n                    except AttributeError:\n                        logger.debug(\n                            f\"Could not setattr {key} on {target_class.__name__} during to_typedclass reconstruction.\"\n                        )\n\n            return cast(TypedClassT, instance)\n        except TypeError as e:\n            raise ValueError(\n                f\"Failed to instantiate typed class '{target_class.__name__}' from Django fields. Error: {e}\"\n            ) from e\n        except Exception as e:\n            logger.error(f\"An unexpected error occurred during typed class reconstruction: {e}\", exc_info=True)\n            raise ValueError(f\"An unexpected error occurred during typed class reconstruction: {e}\") from e\n\n    def _get_data_for_typedclass(self, target_class: type) -&gt; dict[str, Any]:\n        \"\"\"Get data from Django fields that correspond to the target class's likely attributes.\"\"\"\n        data = {}\n        # Heuristic: get attributes from __init__ signature and class annotations\n        # This is a simplified version. A robust solution might need to align with TypedClassFieldFactory's discovery.\n\n        # Potential attributes: from __init__\n        potential_attrs = set()\n        try:\n            init_sig = inspect.signature(target_class.__init__)\n            potential_attrs.update(p for p in init_sig.parameters if p != \"self\")\n        except (TypeError, ValueError):\n            pass\n\n        # Potential attributes: from class annotations (less reliable for instance data if not in __init__)\n        # try:\n        #     annotations = inspect.get_annotations(target_class)\n        #     potential_attrs.update(annotations.keys())\n        # except Exception:\n        #     pass\n\n        for field in self._meta.fields:\n            if field.name in potential_attrs or hasattr(target_class, field.name):  # Check if it's a likely attribute\n                # TODO: Add deserialization logic based on target_class's type hint for field.name\n                data[field.name] = getattr(self, field.name)\n        return data\n\n    def update_from_typedclass(self, typed_obj: TypedClassT) -&gt; None:\n        \"\"\"\n        Update this Django model with new data from a generic class object and save.\n        \"\"\"\n        fqn = self._verify_object_type_match(typed_obj)\n        if self.class_path != fqn:\n            self.class_path = fqn\n\n        self.update_fields_from_typedclass(typed_obj)\n        self.save()\n\n    def save_as_typedclass(self) -&gt; TypedClassT:\n        \"\"\"\n        Save the Django model and return the corresponding generic class object.\n        \"\"\"\n        self.save()\n        return self.to_typedclass()\n</code></pre> </li> </ul> <p>Additional docs:</p> <ul> <li>Context handling: <code>docs/Architecture/CONTEXT_HANDLING.md</code></li> <li>Pydantic \u2194 Django methods: <code>docs/Architecture/README_PYDANTIC_DJANGO_METHODS.md</code></li> <li>Django storage options: <code>docs/Architecture/README_DJANGO_STORAGE_OPTIONS.md</code></li> </ul>"},{"location":"Architecture/OVERVIEW/#pydantic2django.django.models.Pydantic2DjangoBaseClass.__getattr__","title":"<code>__getattr__(name)</code>","text":"<p>Forward method calls to the Pydantic model implementation. Enables type checking and execution of methods defined on the Pydantic model.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>def __getattr__(self, name: str) -&gt; Any:\n    \"\"\"\n    Forward method calls to the Pydantic model implementation.\n    Enables type checking and execution of methods defined on the Pydantic model.\n    \"\"\"\n    # Get the Pydantic model class\n    try:\n        pydantic_cls = self._get_class()  # Use common method\n        if not issubclass(pydantic_cls, BaseModel):\n            # This path shouldn't be hit if used correctly, but safeguard\n            raise AttributeError(f\"Stored class '{self.class_path}' is not a Pydantic BaseModel.\")\n    except ValueError as e:\n        raise AttributeError(f\"Cannot forward attribute '{name}': {e}\") from e\n\n    # Check if the attribute exists in the Pydantic model\n    if hasattr(pydantic_cls, name):\n        attr = getattr(pydantic_cls, name)\n\n        # If it's a callable method (and not the type itself), wrap it\n        if callable(attr) and not isinstance(attr, type):\n\n            def wrapped_method(*args, **kwargs):\n                # Convert self (Django model) to Pydantic instance first\n                try:\n                    pydantic_instance = self.to_pydantic()  # Assuming no context needed here\n                except ValueError as e:\n                    # Handle potential errors during conversion (e.g., context missing)\n                    raise RuntimeError(\n                        f\"Failed to convert Django model to Pydantic before calling '{name}': {e}\"\n                    ) from e\n\n                # Call the method on the Pydantic instance\n                result = getattr(pydantic_instance, name)(*args, **kwargs)\n                # TODO: Handle potential need to update self from result? Unlikely for most methods.\n                return result\n\n            return wrapped_method\n        else:\n            # For non-method attributes (like class vars), return directly\n            # This might need refinement depending on desired behavior for class vs instance attrs\n            return attr\n\n    # If attribute doesn't exist on Pydantic model, raise standard AttributeError\n    raise AttributeError(\n        f\"'{self.__class__.__name__}' object has no attribute '{name}', \"\n        f\"and Pydantic model '{pydantic_cls.__name__}' has no attribute '{name}'.\"\n    )\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.django.models.Pydantic2DjangoBaseClass.__new__","title":"<code>__new__(*args, **kwargs)</code>","text":"<p>Override new to ensure proper type checking. Needed because Django's model metaclass doesn't preserve Generic type parameters well.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>def __new__(cls, *args, **kwargs):\n    \"\"\"\n    Override __new__ to ensure proper type checking.\n    Needed because Django's model metaclass doesn't preserve Generic type parameters well.\n    \"\"\"\n    # The check itself might be complex. This placeholder ensures __new__ is considered.\n    # Proper generic handling might require metaclass adjustments beyond this scope.\n    return super().__new__(cls)\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.django.models.Pydantic2DjangoBaseClass.from_pydantic","title":"<code>from_pydantic(pydantic_obj, name=None)</code>  <code>classmethod</code>","text":"<p>Create a Django model instance from a Pydantic object, mapping fields.</p> <p>Parameters:</p> Name Type Description Default <code>pydantic_obj</code> <code>PydanticT</code> <p>The Pydantic object to convert.</p> required <code>name</code> <code>str | None</code> <p>Optional name for the Django model instance.</p> <code>None</code> <p>Returns:</p> Type Description <code>Pydantic2DjangoBaseClass[PydanticT]</code> <p>A new instance of this Django model subclass.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the object is not a Pydantic model or not of the expected type.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>@classmethod\ndef from_pydantic(cls, pydantic_obj: PydanticT, name: str | None = None) -&gt; \"Pydantic2DjangoBaseClass[PydanticT]\":\n    \"\"\"\n    Create a Django model instance from a Pydantic object, mapping fields.\n\n    Args:\n        pydantic_obj: The Pydantic object to convert.\n        name: Optional name for the Django model instance.\n\n    Returns:\n        A new instance of this Django model subclass.\n\n    Raises:\n        TypeError: If the object is not a Pydantic model or not of the expected type.\n    \"\"\"\n    # Get class info and check type\n    (\n        pydantic_class,\n        class_name,\n        module_name,\n        fully_qualified_name,\n    ) = cls._get_class_info(pydantic_obj)\n    cls._check_expected_type(pydantic_obj, class_name)  # Verifies it's a Pydantic model\n\n    # Derive name\n    derived_name = cls._derive_name(pydantic_obj, name, class_name)\n\n    # Create instance with basic fields\n    instance = cls(\n        name=derived_name,\n        class_path=fully_qualified_name,  # Use renamed field\n    )\n\n    # Update mapped fields\n    instance.update_fields_from_pydantic(pydantic_obj)\n\n    return instance\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.django.models.Pydantic2DjangoBaseClass.save_as_pydantic","title":"<code>save_as_pydantic()</code>","text":"<p>Save the Django model and return the corresponding Pydantic object.</p> <p>Returns:</p> Type Description <code>PydanticT</code> <p>The corresponding Pydantic object.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>def save_as_pydantic(self) -&gt; PydanticT:\n    \"\"\"\n    Save the Django model and return the corresponding Pydantic object.\n\n    Returns:\n        The corresponding Pydantic object.\n    \"\"\"\n    self.save()\n    return self.to_pydantic(context=None)\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.django.models.Pydantic2DjangoBaseClass.to_pydantic","title":"<code>to_pydantic(context=None)</code>","text":"<p>Convert this Django model instance back to a Pydantic object.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ModelContext | None</code> <p>Optional ModelContext instance containing values for non-serializable fields.</p> <code>None</code> <p>Returns:</p> Type Description <code>PydanticT</code> <p>The corresponding Pydantic object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If context is required but not provided, or if class load/instantiation fails.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>def to_pydantic(self, context: ModelContext | None = None) -&gt; PydanticT:\n    \"\"\"\n    Convert this Django model instance back to a Pydantic object.\n\n    Args:\n        context: Optional ModelContext instance containing values for non-serializable fields.\n\n    Returns:\n        The corresponding Pydantic object.\n\n    Raises:\n        ValueError: If context is required but not provided, or if class load/instantiation fails.\n    \"\"\"\n    pydantic_class = self._get_class()  # Use common method\n    if not issubclass(pydantic_class, BaseModel):\n        raise ValueError(f\"Stored class path '{self.class_path}' does not point to a Pydantic BaseModel.\")\n\n    # Get data from Django fields corresponding to Pydantic fields\n    data = self._get_data_for_pydantic(pydantic_class)\n\n    # Handle context if required and provided\n    required_context_keys = self._get_required_context_fields()  # Check if context is needed\n    if required_context_keys:\n        if not context:\n            raise ValueError(\n                f\"Conversion to Pydantic model '{pydantic_class.__name__}' requires context \"\n                f\"for fields: {', '.join(required_context_keys)}. Please provide a ModelContext instance.\"\n            )\n        # Validate and merge context data\n        context_dict = context.to_conversion_dict()\n        context.validate_context(context_dict)  # Validate required keys are present\n        data.update(context_dict)  # Merge context, potentially overwriting DB values if keys overlap\n\n    # Reconstruct the Pydantic object\n    try:\n        # TODO: Add potential deserialization logic here if needed before validation\n        instance = pydantic_class.model_validate(data)\n        # Cast to the generic type variable\n        return cast(PydanticT, instance)\n    except TypeError:\n        raise  # Re-raise TypeError as it's a specific, meaningful exception here\n    except Exception as e:\n        # Catch any other unexpected error during Pydantic object creation\n        logger.error(\n            f\"An unexpected error occurred creating Pydantic model for '{pydantic_class.__name__}': {e}\",\n            exc_info=True,\n        )\n        raise ValueError(f\"Unexpected error creating Pydantic model for '{pydantic_class.__name__}'\") from e\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.django.models.Pydantic2DjangoBaseClass.update_fields_from_pydantic","title":"<code>update_fields_from_pydantic(pydantic_obj)</code>","text":"<p>Update this Django model's fields from a Pydantic object's fields.</p> <p>Parameters:</p> Name Type Description Default <code>pydantic_obj</code> <code>PydanticT</code> <p>The Pydantic object containing source values.</p> required Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>def update_fields_from_pydantic(self, pydantic_obj: PydanticT) -&gt; None:\n    \"\"\"\n    Update this Django model's fields from a Pydantic object's fields.\n\n    Args:\n        pydantic_obj: The Pydantic object containing source values.\n    \"\"\"\n    if (\n        not isinstance(pydantic_obj, BaseModel)\n        or pydantic_obj.__class__.__module__ != self._get_class().__module__\n        or pydantic_obj.__class__.__name__ != self._get_class().__name__\n    ):\n        # Check type consistency before proceeding\n        raise TypeError(\n            f\"Provided object type {type(pydantic_obj)} does not match expected type {self.class_path} for update.\"\n        )\n\n    # Get data from the Pydantic object\n    try:\n        pydantic_data = pydantic_obj.model_dump()\n    except AttributeError as err:\n        raise TypeError(\n            \"Failed to dump Pydantic model for update. Ensure you are using Pydantic v2+ with model_dump().\"\n        ) from err\n\n    # Get Django model fields excluding common/meta ones\n    model_field_names = {\n        field.name\n        for field in self._meta.fields\n        if field.name not in (\"id\", \"name\", \"class_path\", \"created_at\", \"updated_at\")\n    }\n\n    # Update each Django field if it matches a field in the Pydantic data\n    for field_name in model_field_names:\n        if field_name in pydantic_data:\n            value = pydantic_data[field_name]\n            # Apply serialization (important for complex types)\n            serialized_value = serialize_value(value)\n            setattr(self, field_name, serialized_value)\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.django.models.Pydantic2DjangoBaseClass.update_from_pydantic","title":"<code>update_from_pydantic(pydantic_obj)</code>","text":"<p>Update this Django model with new data from a Pydantic object and save.</p> <p>Parameters:</p> Name Type Description Default <code>pydantic_obj</code> <code>PydanticT</code> <p>The Pydantic object with updated data.</p> required Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>def update_from_pydantic(self, pydantic_obj: PydanticT) -&gt; None:\n    \"\"\"\n    Update this Django model with new data from a Pydantic object and save.\n\n    Args:\n        pydantic_obj: The Pydantic object with updated data.\n    \"\"\"\n    # Verify the object type matches first (includes check if it's a BaseModel)\n    fully_qualified_name = self._verify_object_type_match(pydantic_obj)\n\n    # Update the class_path if somehow inconsistent\n    if self.class_path != fully_qualified_name:\n        self.class_path = fully_qualified_name\n\n    self.update_fields_from_pydantic(pydantic_obj)\n    self.save()\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.django.models.Dataclass2DjangoBaseClass.from_dataclass","title":"<code>from_dataclass(dc_obj, name=None)</code>  <code>classmethod</code>","text":"<p>Create a Django model instance from a Dataclass object, mapping fields.</p> <p>Parameters:</p> Name Type Description Default <code>dc_obj</code> <code>DataclassT</code> <p>The Dataclass object to convert.</p> required <code>name</code> <code>str | None</code> <p>Optional name for the Django model instance.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dataclass2DjangoBaseClass[DataclassT]</code> <p>A new instance of this Django model subclass.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the object is not a dataclass or not of the expected type.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>@classmethod\ndef from_dataclass(cls, dc_obj: DataclassT, name: str | None = None) -&gt; \"Dataclass2DjangoBaseClass[DataclassT]\":\n    \"\"\"\n    Create a Django model instance from a Dataclass object, mapping fields.\n\n    Args:\n        dc_obj: The Dataclass object to convert.\n        name: Optional name for the Django model instance.\n\n    Returns:\n        A new instance of this Django model subclass.\n\n    Raises:\n        TypeError: If the object is not a dataclass or not of the expected type.\n    \"\"\"\n    # Get class info and check type\n    (\n        dc_class,\n        class_name,\n        module_name,\n        fully_qualified_name,\n    ) = cls._get_class_info(dc_obj)\n    cls._check_expected_type(dc_obj, class_name)  # Verifies it's a dataclass\n\n    # Derive name\n    derived_name = cls._derive_name(dc_obj, name, class_name)\n\n    # Create instance with basic fields\n    instance = cls(\n        name=derived_name,\n        class_path=fully_qualified_name,\n    )\n\n    # Update mapped fields\n    instance.update_fields_from_dataclass(dc_obj)\n\n    return instance\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.django.models.Dataclass2DjangoBaseClass.save_as_dataclass","title":"<code>save_as_dataclass()</code>","text":"<p>Save the Django model and return the corresponding Dataclass object.</p> <p>Returns:</p> Type Description <code>DataclassT</code> <p>The corresponding Dataclass object.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>def save_as_dataclass(self) -&gt; DataclassT:\n    \"\"\"\n    Save the Django model and return the corresponding Dataclass object.\n\n    Returns:\n        The corresponding Dataclass object.\n    \"\"\"\n    self.save()\n    return self.to_dataclass()\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.django.models.Dataclass2DjangoBaseClass.to_dataclass","title":"<code>to_dataclass()</code>","text":"<p>Convert this Django model instance back to a Dataclass object.</p> <p>Returns:</p> Type Description <code>DataclassT</code> <p>The reconstructed Dataclass object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the class cannot be loaded or instantiation fails.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>def to_dataclass(self) -&gt; DataclassT:\n    \"\"\"\n    Convert this Django model instance back to a Dataclass object.\n\n    Returns:\n        The reconstructed Dataclass object.\n\n    Raises:\n        ValueError: If the class cannot be loaded or instantiation fails.\n    \"\"\"\n    dataclass_type = self._get_class()\n    if not dataclasses.is_dataclass(dataclass_type):\n        raise ValueError(f\"Stored class path '{self.class_path}' does not point to a dataclass.\")\n\n    # Get data from Django fields corresponding to dataclass fields\n    data_for_dc = self._get_data_for_dataclass(dataclass_type)\n\n    # Instantiate the dataclass\n    try:\n        # TODO: Add deserialization logic if needed\n        instance = dataclass_type(**data_for_dc)\n        # Cast to the generic type variable for type hinting\n        return cast(DataclassT, instance)\n    except TypeError as e:\n        raise ValueError(\n            f\"Failed to instantiate dataclass '{dataclass_type.__name__}' from Django model fields. \"\n            f\"Ensure required fields exist and types are compatible. Error: {e}\"\n        ) from e\n    except Exception as e:\n        logger.error(f\"An unexpected error occurred during dataclass reconstruction: {e}\", exc_info=True)\n        raise ValueError(f\"An unexpected error occurred during dataclass reconstruction: {e}\") from e\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.django.models.Dataclass2DjangoBaseClass.update_fields_from_dataclass","title":"<code>update_fields_from_dataclass(dc_obj)</code>","text":"<p>Update this Django model's fields from a Dataclass object's fields.</p> <p>Parameters:</p> Name Type Description Default <code>dc_obj</code> <code>DataclassT</code> <p>The Dataclass object containing source values.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If conversion to dict fails.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>def update_fields_from_dataclass(self, dc_obj: DataclassT) -&gt; None:\n    \"\"\"\n    Update this Django model's fields from a Dataclass object's fields.\n\n    Args:\n        dc_obj: The Dataclass object containing source values.\n\n    Raises:\n        TypeError: If conversion to dict fails.\n    \"\"\"\n    if (\n        not dataclasses.is_dataclass(dc_obj)\n        or dc_obj.__class__.__module__ != self._get_class().__module__\n        or dc_obj.__class__.__name__ != self._get_class().__name__\n    ):\n        # Check type consistency before proceeding\n        raise TypeError(\n            f\"Provided object type {type(dc_obj)} does not match expected type {self.class_path} for update.\"\n        )\n\n    try:\n        dc_data = dataclasses.asdict(dc_obj)\n    except TypeError as e:\n        raise TypeError(f\"Could not convert dataclass '{dc_obj.__class__.__name__}' to dict for update: {e}\") from e\n\n    # Get Django model fields excluding common/meta ones\n    model_field_names = {\n        field.name\n        for field in self._meta.fields\n        if field.name not in (\"id\", \"name\", \"class_path\", \"created_at\", \"updated_at\")\n    }\n\n    for field_name in model_field_names:\n        if field_name in dc_data:\n            value = dc_data[field_name]\n            # Apply serialization (important for complex types like datetime, UUID, etc.)\n            serialized_value = serialize_value(value)\n            setattr(self, field_name, serialized_value)\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.django.models.Dataclass2DjangoBaseClass.update_from_dataclass","title":"<code>update_from_dataclass(dc_obj)</code>","text":"<p>Update this Django model with new data from a Dataclass object and save.</p> <p>Parameters:</p> Name Type Description Default <code>dc_obj</code> <code>DataclassT</code> <p>The Dataclass object with updated data.</p> required Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>def update_from_dataclass(self, dc_obj: DataclassT) -&gt; None:\n    \"\"\"\n    Update this Django model with new data from a Dataclass object and save.\n\n    Args:\n        dc_obj: The Dataclass object with updated data.\n    \"\"\"\n    # Verify the object type matches first (includes check if it's a dataclass)\n    fully_qualified_name = self._verify_object_type_match(dc_obj)\n\n    # Update the class_path if somehow inconsistent\n    if self.class_path != fully_qualified_name:\n        self.class_path = fully_qualified_name\n\n    self.update_fields_from_dataclass(dc_obj)\n    self.save()\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.django.models.TypedClass2DjangoBaseClass.from_typedclass","title":"<code>from_typedclass(typed_obj, name=None)</code>  <code>classmethod</code>","text":"<p>Create a Django model instance from a generic class object, mapping fields.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>@classmethod\ndef from_typedclass(\n    cls, typed_obj: TypedClassT, name: str | None = None\n) -&gt; \"TypedClass2DjangoBaseClass[TypedClassT]\":\n    \"\"\"\n    Create a Django model instance from a generic class object, mapping fields.\n    \"\"\"\n    obj_class, class_name, module_name, fqn = cls._get_class_info(typed_obj)\n    cls._check_expected_type(typed_obj, class_name)\n    derived_name = cls._derive_name(typed_obj, name, class_name)\n\n    instance = cls(\n        name=derived_name,\n        class_path=fqn,\n    )\n    instance.update_fields_from_typedclass(typed_obj)\n    return instance\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.django.models.TypedClass2DjangoBaseClass.save_as_typedclass","title":"<code>save_as_typedclass()</code>","text":"<p>Save the Django model and return the corresponding generic class object.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>def save_as_typedclass(self) -&gt; TypedClassT:\n    \"\"\"\n    Save the Django model and return the corresponding generic class object.\n    \"\"\"\n    self.save()\n    return self.to_typedclass()\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.django.models.TypedClass2DjangoBaseClass.to_typedclass","title":"<code>to_typedclass()</code>","text":"<p>Convert this Django model instance back to a generic class object.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>def to_typedclass(self) -&gt; TypedClassT:\n    \"\"\"\n    Convert this Django model instance back to a generic class object.\n    \"\"\"\n    target_class = self._get_class()\n    data_for_typedclass = self._get_data_for_typedclass(target_class)\n\n    try:\n        # This assumes target_class can be instantiated with these parameters.\n        # Deserialization from serialize_value needs to be considered for complex types.\n        # TODO: Implement robust deserialization\n        deserialized_data = dict(data_for_typedclass.items())  # Placeholder\n\n        init_sig = inspect.signature(target_class.__init__)\n        valid_params = {k: v for k, v in deserialized_data.items() if k in init_sig.parameters}\n\n        instance = target_class(**valid_params)\n\n        # For attributes not in __init__ but set on Django model, try to setattr\n        for key, value in deserialized_data.items():\n            if key not in valid_params and hasattr(instance, key) and not key.startswith(\"_\"):\n                try:\n                    setattr(instance, key, value)  # value is already deserialized_data[key]\n                except AttributeError:\n                    logger.debug(\n                        f\"Could not setattr {key} on {target_class.__name__} during to_typedclass reconstruction.\"\n                    )\n\n        return cast(TypedClassT, instance)\n    except TypeError as e:\n        raise ValueError(\n            f\"Failed to instantiate typed class '{target_class.__name__}' from Django fields. Error: {e}\"\n        ) from e\n    except Exception as e:\n        logger.error(f\"An unexpected error occurred during typed class reconstruction: {e}\", exc_info=True)\n        raise ValueError(f\"An unexpected error occurred during typed class reconstruction: {e}\") from e\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.django.models.TypedClass2DjangoBaseClass.update_fields_from_typedclass","title":"<code>update_fields_from_typedclass(typed_obj)</code>","text":"<p>Update this Django model's fields from a generic class object's fields.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>def update_fields_from_typedclass(self, typed_obj: TypedClassT) -&gt; None:\n    \"\"\"\n    Update this Django model's fields from a generic class object's fields.\n    \"\"\"\n    if (\n        typed_obj.__class__.__module__ != self._get_class().__module__\n        or typed_obj.__class__.__name__ != self._get_class().__name__\n    ):\n        raise TypeError(\n            f\"Provided object type {type(typed_obj)} does not match expected type {self.class_path} for update.\"\n        )\n\n    # Extract data. For mapped fields, we prefer __init__ params or direct attrs.\n    # Using __dict__ might be too broad here if not all dict items are mapped fields.\n    # Let's assume attributes corresponding to Django fields are directly accessible.\n\n    model_field_names = {\n        field.name\n        for field in self._meta.fields\n        if field.name not in (\"id\", \"name\", \"class_path\", \"created_at\", \"updated_at\")\n    }\n\n    for field_name in model_field_names:\n        if hasattr(typed_obj, field_name):\n            value = getattr(typed_obj, field_name)\n            setattr(self, field_name, serialize_value(value))\n</code></pre>"},{"location":"Architecture/OVERVIEW/#pydantic2django.django.models.TypedClass2DjangoBaseClass.update_from_typedclass","title":"<code>update_from_typedclass(typed_obj)</code>","text":"<p>Update this Django model with new data from a generic class object and save.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>def update_from_typedclass(self, typed_obj: TypedClassT) -&gt; None:\n    \"\"\"\n    Update this Django model with new data from a generic class object and save.\n    \"\"\"\n    fqn = self._verify_object_type_match(typed_obj)\n    if self.class_path != fqn:\n        self.class_path = fqn\n\n    self.update_fields_from_typedclass(typed_obj)\n    self.save()\n</code></pre>"},{"location":"Architecture/OVERVIEW/#extending-the-system","title":"Extending the system","text":"<ul> <li>To support a new source type, implement:</li> <li>A <code>Discovery</code> subclass of <code>BaseDiscovery</code>.</li> <li>A <code>FieldFactory</code> and <code>ModelFactory</code> pair using <code>BidirectionalTypeMapper</code>.</li> <li>A <code>Generator</code> subclass of <code>BaseStaticGenerator</code> that wires everything together and prepares template context.</li> <li>To add or refine type mappings, provide new <code>TypeMappingUnit</code> subclasses and ensure <code>BidirectionalTypeMapper</code> registry order or match scores select them appropriately.</li> </ul>"},{"location":"Architecture/OVERVIEW/#mkdocstrings-usage","title":"mkdocstrings usage","text":"<p>Each <code>:::</code> block above renders live API documentation using mkdocstrings with the Python handler, configured in <code>mkdocs.yml</code> with <code>handlers.python.paths: [src]</code>. See the official guide: mkdocstrings usage.</p>"},{"location":"Architecture/comparisons/README_DJANGO_STORAGE_OPTIONS/","title":"Pydantic to Django Storage Approaches","text":"<p>This document compares two different approaches for storing Pydantic models in Django: Dynamic Field Conversion vs Direct Serialization.</p>"},{"location":"Architecture/comparisons/README_DJANGO_STORAGE_OPTIONS/#overview-of-approaches","title":"Overview of Approaches","text":"<p>The library provides two distinct mechanisms for storing Pydantic models in Django:</p> <ol> <li>Dynamic Field Conversion (<code>Pydantic2DjangoBaseClass</code>)</li> <li>Direct Serialization (<code>Pydantic2DjangoStorePydanticObject</code>)</li> </ol>"},{"location":"Architecture/comparisons/README_DJANGO_STORAGE_OPTIONS/#core-approach-differences","title":"Core Approach Differences","text":""},{"location":"Architecture/comparisons/README_DJANGO_STORAGE_OPTIONS/#dynamic-field-conversion","title":"Dynamic Field Conversion","text":"<ul> <li>Field Mapping: Creates individual Django model fields for each Pydantic field</li> <li>Database Schema: Maintains a structured database schema matching the Pydantic model</li> <li>Relationship Handling: Proper foreign key relationships in the database</li> <li>Query Support: Full Django ORM query capabilities on individual fields</li> </ul>"},{"location":"Architecture/comparisons/README_DJANGO_STORAGE_OPTIONS/#direct-serialization","title":"Direct Serialization","text":"<ul> <li>JSON Storage: Stores entire Pydantic object as JSON in a single field</li> <li>Schema Flexibility: No need to modify database schema for model changes</li> <li>Simplicity: Simpler codebase with less conversion logic</li> <li>Document Style: Better suited for document-style data storage</li> </ul>"},{"location":"Architecture/comparisons/README_DJANGO_STORAGE_OPTIONS/#technical-implementation-differences","title":"Technical Implementation Differences","text":""},{"location":"Architecture/comparisons/README_DJANGO_STORAGE_OPTIONS/#storage-structure","title":"Storage Structure","text":"<ul> <li>Dynamic Field Conversion:</li> <li>Each Pydantic field becomes a database column</li> <li>Proper database types for each field</li> <li>Support for indexes and constraints</li> <li> <p>Native database relationships</p> </li> <li> <p>Direct Serialization:</p> </li> <li>Single JSON field contains all data</li> <li>No individual field columns</li> <li>No direct database relationships</li> <li>Flexible schema evolution</li> </ul>"},{"location":"Architecture/comparisons/README_DJANGO_STORAGE_OPTIONS/#query-capabilities","title":"Query Capabilities","text":"<ul> <li> <p>Dynamic Field Conversion:   <pre><code># Can query on specific fields\nMyModel.objects.filter(specific_field=\"value\")\nMyModel.objects.exclude(number_field__gt=100)\nMyModel.objects.values('specific_field')\n</code></pre></p> </li> <li> <p>Direct Serialization:   <pre><code># Must query on JSON field\nMyModel.objects.filter(data__contains={\"field\": \"value\"})\n# Must load entire object to access specific fields\ninstance = MyModel.objects.get(id=1)\npydantic_obj = instance.to_pydantic()\n</code></pre></p> </li> </ul>"},{"location":"Architecture/comparisons/README_DJANGO_STORAGE_OPTIONS/#advantages-and-disadvantages","title":"Advantages and Disadvantages","text":""},{"location":"Architecture/comparisons/README_DJANGO_STORAGE_OPTIONS/#dynamic-field-conversion_1","title":"Dynamic Field Conversion","text":""},{"location":"Architecture/comparisons/README_DJANGO_STORAGE_OPTIONS/#advantages","title":"Advantages","text":"<ul> <li>Better query performance on specific fields</li> <li>Proper database-level relationships</li> <li>Full Django ORM capabilities</li> <li>Database-level constraints and validation</li> <li>Better support for complex queries and joins</li> <li>Field-level indexing</li> <li>Efficient partial data retrieval</li> </ul>"},{"location":"Architecture/comparisons/README_DJANGO_STORAGE_OPTIONS/#disadvantages","title":"Disadvantages","text":"<ul> <li>More complex implementation</li> <li>Requires database migrations for model changes</li> <li>More code to maintain</li> <li>Higher initial development overhead</li> <li>More complex relationship handling</li> </ul>"},{"location":"Architecture/comparisons/README_DJANGO_STORAGE_OPTIONS/#direct-serialization_1","title":"Direct Serialization","text":""},{"location":"Architecture/comparisons/README_DJANGO_STORAGE_OPTIONS/#advantages_1","title":"Advantages","text":"<ul> <li>Simpler implementation</li> <li>No database migrations needed for model changes</li> <li>Easier to maintain</li> <li>Better for rapidly evolving schemas</li> <li>Simpler relationship handling</li> <li>More flexible data structure</li> <li>Faster development iteration</li> </ul>"},{"location":"Architecture/comparisons/README_DJANGO_STORAGE_OPTIONS/#disadvantages_1","title":"Disadvantages","text":"<ul> <li>Limited query capabilities</li> <li>Must load entire object to access fields</li> <li>No database-level relationships</li> <li>Less efficient for partial data access</li> <li>Limited database-level validation</li> <li>No field-level indexing</li> </ul>"},{"location":"Architecture/comparisons/README_DJANGO_STORAGE_OPTIONS/#use-case-recommendations","title":"Use Case Recommendations","text":""},{"location":"Architecture/comparisons/README_DJANGO_STORAGE_OPTIONS/#use-dynamic-field-conversion-when","title":"Use Dynamic Field Conversion When:","text":"<ul> <li>You need efficient querying on specific fields</li> <li>You have complex relationships that benefit from proper foreign keys</li> <li>You need database-level constraints</li> <li>Performance of field-specific queries is important</li> <li>You're integrating heavily with other Django models</li> <li>You need aggregation and complex filtering</li> <li>Your schema is relatively stable</li> </ul>"},{"location":"Architecture/comparisons/README_DJANGO_STORAGE_OPTIONS/#use-direct-serialization-when","title":"Use Direct Serialization When:","text":"<ul> <li>Your models change frequently</li> <li>You don't need to query specific fields often</li> <li>You're dealing with document-style data</li> <li>You want simpler code maintenance</li> <li>You don't need database-level relationships</li> <li>You prioritize development speed over query performance</li> <li>You need maximum schema flexibility</li> </ul>"},{"location":"Architecture/comparisons/README_DJANGO_STORAGE_OPTIONS/#code-examples","title":"Code Examples","text":""},{"location":"Architecture/comparisons/README_DJANGO_STORAGE_OPTIONS/#dynamic-field-conversion_2","title":"Dynamic Field Conversion","text":"<pre><code>from pydantic import BaseModel\nfrom pydantic2django import Pydantic2DjangoBaseClass\n\nclass MyPydanticModel(BaseModel):\n    name: str\n    age: int\n    email: str\n\nclass MyDjangoModel(Pydantic2DjangoBaseClass[MyPydanticModel]):\n    class Meta:\n        app_label = 'myapp'\n\n# Efficient field-specific queries\nyoung_users = MyDjangoModel.objects.filter(age__lt=25)\nemail_list = MyDjangoModel.objects.values_list('email', flat=True)\n</code></pre>"},{"location":"Architecture/comparisons/README_DJANGO_STORAGE_OPTIONS/#direct-serialization_2","title":"Direct Serialization","text":"<pre><code>from pydantic2django import Pydantic2DjangoStorePydanticObject\n\nclass MyDjangoModel(Pydantic2DjangoStorePydanticObject):\n    class Meta:\n        app_label = 'myapp'\n\n# Store entire object\ninstance = MyDjangoModel.from_pydantic(pydantic_obj)\ninstance.save()\n\n# Must load entire object to access fields\nloaded = MyDjangoModel.objects.get(id=1)\npydantic_obj = loaded.to_pydantic()\n</code></pre>"},{"location":"Architecture/comparisons/README_DJANGO_STORAGE_OPTIONS/#performance-implications","title":"Performance Implications","text":""},{"location":"Architecture/comparisons/README_DJANGO_STORAGE_OPTIONS/#query-performance","title":"Query Performance","text":"<ul> <li>Dynamic Field Conversion: Better performance for field-specific queries</li> <li>Direct Serialization: Better performance for whole-object operations</li> </ul>"},{"location":"Architecture/comparisons/README_DJANGO_STORAGE_OPTIONS/#storage-efficiency","title":"Storage Efficiency","text":"<ul> <li>Dynamic Field Conversion: More efficient for partial data access</li> <li>Direct Serialization: More efficient for whole-object storage</li> </ul>"},{"location":"Architecture/comparisons/README_DJANGO_STORAGE_OPTIONS/#memory-usage","title":"Memory Usage","text":"<ul> <li>Dynamic Field Conversion: More efficient when accessing specific fields</li> <li>Direct Serialization: Must load entire object into memory</li> </ul>"},{"location":"Architecture/comparisons/README_DJANGO_STORAGE_OPTIONS/#conclusion","title":"Conclusion","text":"<p>The choice between these approaches depends on your specific requirements:</p> <ul> <li>Choose Dynamic Field Conversion if you need:</li> <li>Efficient field-specific queries</li> <li>Database-level relationships</li> <li>Complex filtering and aggregation</li> <li> <p>Integration with Django's ORM</p> </li> <li> <p>Choose Direct Serialization if you need:</p> </li> <li>Schema flexibility</li> <li>Simpler maintenance</li> <li>Faster development iteration</li> <li>Document-style storage</li> </ul> <p>In practice, you might use both approaches in different parts of your application based on specific needs. The library's design allows for this flexibility by providing both implementations.</p>"},{"location":"Architecture/comparisons/README_PYDANTIC_DJANGO_METHODS/","title":"Pydantic to Django Method Integration Approaches","text":"<p>This document compares two different approaches for integrating Pydantic model methods with Django models in the pydantic2django library.</p>"},{"location":"Architecture/comparisons/README_PYDANTIC_DJANGO_METHODS/#overview-of-approaches","title":"Overview of Approaches","text":"<p>The library provides two distinct mechanisms for making Pydantic model methods available on Django models:</p> <ol> <li>Static Method Copying (<code>methods.py</code>)</li> <li>Dynamic Method Resolution (<code>__getattr__</code> in <code>Pydantic2DjangoBaseClass</code>)</li> </ol>"},{"location":"Architecture/comparisons/README_PYDANTIC_DJANGO_METHODS/#core-approach-differences","title":"Core Approach Differences","text":""},{"location":"Architecture/comparisons/README_PYDANTIC_DJANGO_METHODS/#methodspy-approach","title":"<code>methods.py</code> Approach","text":"<ul> <li>Static Method Copying: Copies methods from Pydantic models to Django models at class definition time</li> <li>Direct Integration: Methods become actual attributes of the Django model class</li> <li>One-time Process: Methods are copied once when the Django model is created</li> <li>Transformation-based: Transforms and wraps methods to handle return type conversions</li> </ul>"},{"location":"Architecture/comparisons/README_PYDANTIC_DJANGO_METHODS/#__getattr__-approach-in-pydantic2djangobaseclass","title":"<code>__getattr__</code> Approach in <code>Pydantic2DjangoBaseClass</code>","text":"<ul> <li>Dynamic Method Resolution: Intercepts attribute access at runtime</li> <li>Delegation Pattern: Forwards method calls to the Pydantic model</li> <li>On-demand Conversion: Converts Django model to Pydantic instance for each method call</li> <li>Proxy-based: Acts as a proxy to the Pydantic model's methods</li> </ul>"},{"location":"Architecture/comparisons/README_PYDANTIC_DJANGO_METHODS/#technical-implementation-differences","title":"Technical Implementation Differences","text":""},{"location":"Architecture/comparisons/README_PYDANTIC_DJANGO_METHODS/#method-access-timing","title":"Method Access Timing","text":"<ul> <li><code>methods.py</code>: Methods are available at class definition time</li> <li><code>__getattr__</code>: Methods are resolved dynamically at runtime</li> </ul>"},{"location":"Architecture/comparisons/README_PYDANTIC_DJANGO_METHODS/#performance-characteristics","title":"Performance Characteristics","text":"<ul> <li><code>methods.py</code>: Better performance as methods are directly available</li> <li><code>__getattr__</code>: Incurs overhead for each method call (conversion + delegation)</li> </ul>"},{"location":"Architecture/comparisons/README_PYDANTIC_DJANGO_METHODS/#memory-usage","title":"Memory Usage","text":"<ul> <li><code>methods.py</code>: Higher memory usage as methods are duplicated</li> <li><code>__getattr__</code>: Lower memory usage as methods are not duplicated</li> </ul>"},{"location":"Architecture/comparisons/README_PYDANTIC_DJANGO_METHODS/#method-return-handling","title":"Method Return Handling","text":"<ul> <li><code>methods.py</code>: Sophisticated handling of return types with conversion based on type annotations</li> <li><code>__getattr__</code>: Simpler approach that returns results directly from Pydantic method calls</li> </ul>"},{"location":"Architecture/comparisons/README_PYDANTIC_DJANGO_METHODS/#method-type-preservation","title":"Method Type Preservation","text":"<ul> <li><code>methods.py</code>: Preserves method types (regular, class, static) with proper wrapping</li> <li><code>__getattr__</code>: Only handles instance methods, not class or static methods</li> </ul>"},{"location":"Architecture/comparisons/README_PYDANTIC_DJANGO_METHODS/#advantages-and-disadvantages","title":"Advantages and Disadvantages","text":""},{"location":"Architecture/comparisons/README_PYDANTIC_DJANGO_METHODS/#methodspy-approach_1","title":"<code>methods.py</code> Approach","text":""},{"location":"Architecture/comparisons/README_PYDANTIC_DJANGO_METHODS/#advantages","title":"Advantages","text":"<ul> <li>Better IDE support (methods appear in autocompletion)</li> <li>Better performance (no runtime conversion for each call)</li> <li>Handles class methods and static methods</li> <li>Sophisticated return type conversion based on annotations</li> <li>Works with inheritance (methods are part of the class)</li> </ul>"},{"location":"Architecture/comparisons/README_PYDANTIC_DJANGO_METHODS/#disadvantages","title":"Disadvantages","text":"<ul> <li>More complex implementation</li> <li>Duplicates code (methods exist in both Pydantic and Django models)</li> <li>Requires explicit copying of methods</li> <li>May get out of sync if Pydantic model changes</li> </ul>"},{"location":"Architecture/comparisons/README_PYDANTIC_DJANGO_METHODS/#__getattr__-approach","title":"<code>__getattr__</code> Approach","text":""},{"location":"Architecture/comparisons/README_PYDANTIC_DJANGO_METHODS/#advantages_1","title":"Advantages","text":"<ul> <li>Simpler implementation</li> <li>No code duplication (methods only exist in Pydantic model)</li> <li>Automatically handles new methods added to Pydantic model</li> <li>Consistent conversion between Django and Pydantic models</li> </ul>"},{"location":"Architecture/comparisons/README_PYDANTIC_DJANGO_METHODS/#disadvantages_1","title":"Disadvantages","text":"<ul> <li>Poorer IDE support (methods don't appear in autocompletion)</li> <li>Performance overhead for each method call</li> <li>Doesn't handle class methods or static methods</li> <li>Less control over return type conversion</li> <li>May not work well with inheritance</li> </ul>"},{"location":"Architecture/comparisons/README_PYDANTIC_DJANGO_METHODS/#how-factorypy-and-discoverypy-fit-in","title":"How <code>factory.py</code> and <code>discovery.py</code> Fit In","text":"<p>Both <code>factory.py</code> and <code>discovery.py</code> are foundational components that work with either method integration approach:</p>"},{"location":"Architecture/comparisons/README_PYDANTIC_DJANGO_METHODS/#factorypy","title":"<code>factory.py</code>","text":"<ul> <li>Creates the Django model classes from Pydantic models</li> <li>Sets up the <code>object_type</code> reference that enables the <code>__getattr__</code> approach</li> <li>Provides the structure that both method integration approaches build upon</li> <li>Does not directly implement either approach, but creates the necessary infrastructure</li> </ul>"},{"location":"Architecture/comparisons/README_PYDANTIC_DJANGO_METHODS/#discoverypy","title":"<code>discovery.py</code>","text":"<ul> <li>Provides high-level functionality for discovering and registering models</li> <li>Uses <code>factory.py</code> to create Django models</li> <li>Manages model dependencies and registration order</li> <li>Works with both approaches and doesn't explicitly choose one over the other</li> </ul> <p>In practice, the library's architecture allows for flexibility:</p> <ol> <li><code>factory.py</code> creates Django model classes with proper type hints</li> <li>The <code>__getattr__</code> approach in <code>Pydantic2DjangoBaseClass</code> provides automatic method delegation</li> <li><code>methods.py</code> can be used optionally to copy specific methods for better performance and IDE support</li> <li><code>discovery.py</code> provides high-level discovery and registration regardless of which method approach is used</li> </ol>"},{"location":"Architecture/comparisons/README_PYDANTIC_DJANGO_METHODS/#use-case-differences","title":"Use Case Differences","text":""},{"location":"Architecture/comparisons/README_PYDANTIC_DJANGO_METHODS/#development-experience","title":"Development Experience","text":"<ul> <li><code>methods.py</code> provides better IDE support and type checking</li> <li><code>__getattr__</code> is more transparent but less discoverable</li> </ul>"},{"location":"Architecture/comparisons/README_PYDANTIC_DJANGO_METHODS/#maintenance","title":"Maintenance","text":"<ul> <li><code>methods.py</code> requires updating Django models when Pydantic models change</li> <li><code>__getattr__</code> automatically adapts to changes in Pydantic models</li> </ul>"},{"location":"Architecture/comparisons/README_PYDANTIC_DJANGO_METHODS/#performance-requirements","title":"Performance Requirements","text":"<ul> <li><code>methods.py</code> is better for performance-critical code</li> <li><code>__getattr__</code> is simpler but has runtime overhead</li> </ul>"},{"location":"Architecture/comparisons/README_PYDANTIC_DJANGO_METHODS/#how-they-complement-each-other","title":"How They Complement Each Other","text":"<p>These approaches can be complementary:</p> <ol> <li><code>factory.py</code> creates Django model classes</li> <li><code>methods.py</code> can copy frequently used methods directly to the Django model</li> <li><code>__getattr__</code> can serve as a fallback for less common methods</li> </ol> <p>This combination would provide: - Direct access to common methods with good performance - Fallback access to all other methods - Best of both worlds for IDE support and flexibility</p>"},{"location":"Architecture/comparisons/README_PYDANTIC_DJANGO_METHODS/#practical-implications","title":"Practical Implications","text":""},{"location":"Architecture/comparisons/README_PYDANTIC_DJANGO_METHODS/#for-library-users","title":"For Library Users","text":"<ul> <li><code>methods.py</code> approach requires explicit copying of methods</li> <li><code>__getattr__</code> approach works automatically for all methods</li> </ul>"},{"location":"Architecture/comparisons/README_PYDANTIC_DJANGO_METHODS/#for-library-maintainers","title":"For Library Maintainers","text":"<ul> <li><code>methods.py</code> provides more control over method behavior</li> <li><code>__getattr__</code> is easier to maintain but has less control</li> </ul>"},{"location":"Architecture/comparisons/README_PYDANTIC_DJANGO_METHODS/#for-performance","title":"For Performance","text":"<ul> <li><code>methods.py</code> is better for frequently called methods</li> <li><code>__getattr__</code> is acceptable for occasionally used methods</li> </ul>"},{"location":"Architecture/comparisons/README_PYDANTIC_DJANGO_METHODS/#conclusion","title":"Conclusion","text":"<p>The two approaches represent different design philosophies:</p> <ul> <li><code>methods.py</code> follows a static, explicit approach that prioritizes performance and IDE support</li> <li><code>__getattr__</code> follows a dynamic, implicit approach that prioritizes simplicity and maintenance</li> </ul> <p>The choice between them depends on specific requirements around performance, maintainability, and developer experience. In practice, a hybrid approach might offer the best balance, using <code>methods.py</code> for critical methods and <code>__getattr__</code> as a fallback.</p>"},{"location":"bugs/2025-10-01-xml-wrapper-child-fk-direction/","title":"XML Schema wrapper children incorrectly attached as parent-side FKs","text":""},{"location":"bugs/2025-10-01-xml-wrapper-child-fk-direction/#summary","title":"Summary","text":"<p>In XML Schema generation, wrapper children (e.g., <code>Samples</code>, <code>Events</code>) should hold child-side ForeignKeys pointing back to their parent container (e.g., <code>ComponentStreamType</code>) when <code>list_relationship_style=\"child_fk\"</code>. Currently, the parent gets FKs to the wrappers instead.</p>"},{"location":"bugs/2025-10-01-xml-wrapper-child-fk-direction/#impact","title":"Impact","text":"<ul> <li>Generated models violate intended relationship direction for wrapper container patterns (e.g., MTConnect-like schemas).</li> <li>Ingestors relying on child-side FKs for ordering and deletion semantics break or require workarounds.</li> </ul>"},{"location":"bugs/2025-10-01-xml-wrapper-child-fk-direction/#repro","title":"Repro","text":"<p>Schema (abridged): <code>DeviceStreamType</code> \u2192 <code>ComponentStreamType</code> contains <code>SamplesType</code> and <code>EventsType</code>.</p> <p>Expected: - <code>SamplesType.componentstreamtype = models.ForeignKey(ComponentStreamType, ...)</code> - <code>EventsType.componentstreamtype = models.ForeignKey(ComponentStreamType, ...)</code></p> <p>Actual: - <code>ComponentStreamType.samples = models.ForeignKey(SamplesType, ...)</code> - <code>ComponentStreamType.events = models.ForeignKey(EventsType, ...)</code></p>"},{"location":"bugs/2025-10-01-xml-wrapper-child-fk-direction/#root-cause","title":"Root cause","text":"<p><code>xmlschema/generator.py</code> builds <code>carriers_by_name</code> like this:</p> <pre><code>carriers_by_name = {\n    getattr(c.source_model, \"__name__\", \"\"): c for c in self.carriers if c.django_model is not None\n}\n</code></pre> <p><code>XmlSchemaComplexType</code> exposes <code>.name</code>, not <code>__name__</code>. Keys become empty strings, so the factory\u2019s <code>finalize_relationships(...)</code> cannot find the involved models, and the FK flip never happens.</p>"},{"location":"bugs/2025-10-01-xml-wrapper-child-fk-direction/#fix","title":"Fix","text":"<p>Key by XML name when present, fallback to <code>__name__</code>:</p> <pre><code>carriers_by_name = {\n    (getattr(c.source_model, \"name\", None) or getattr(c.source_model, \"__name__\", \"\")): c\n    for c in self.carriers\n    if c.django_model is not None\n}\n</code></pre>"},{"location":"bugs/2025-10-01-xml-wrapper-child-fk-direction/#validation","title":"Validation","text":"<ul> <li>Add tests for wrapper-container pattern asserting child-side FKs and absence of parent-side FKs.</li> <li>Run existing nested and list tests.</li> </ul>"},{"location":"bugs/2025-10-01-xml-wrapper-child-fk-direction/#notes","title":"Notes","text":"<ul> <li>Defaults already prefer child-side FKs; this is a keying bug preventing the relationship finalizer from executing correctly.</li> </ul>"},{"location":"how_it_works/core_modules_and_concepts/","title":"Core modules and concepts","text":""},{"location":"how_it_works/core_modules_and_concepts/#core-modules-and-concepts","title":"Core modules and concepts","text":"<p>This page summarizes the core building blocks in <code>src/pydantic2django/core/</code>, what they do, and the most important classes and methods to be aware of.</p> <p>For full, up-to-date API details, see the API Reference generated by mkdocstrings: API Reference.</p>"},{"location":"how_it_works/core_modules_and_concepts/#base_generatorpy","title":"<code>base_generator.py</code>","text":"<ul> <li>purpose: Shared orchestration for generating static Django <code>models.py</code> from source models.</li> <li>key class: <code>BaseStaticGenerator[SourceModelType, FieldInfoType]</code></li> <li>generate(): Top-level entrypoint; writes the rendered models file.</li> <li>generate_models_file(): Orchestrates discovery, field/model setup, import dedupe, and template rendering.</li> <li>discover_models(): Uses a <code>BaseDiscovery</code> to find candidate models and analyze dependencies.</li> <li>setup_django_model(source_model): Runs the configured <code>BaseModelFactory</code> to populate a <code>ConversionCarrier</code>.</li> <li>generate_model_definition(carrier): Renders a single model using <code>model_definition.py.j2</code>.</li> <li>Abstract hooks to implement in concrete generators:<ul> <li><code>_get_source_model_name(carrier)</code>, <code>_add_source_model_import(carrier)</code>, <code>_prepare_template_context(defs, names, imports)</code>, <code>_get_models_in_processing_order()</code>, <code>_get_model_definition_extra_context(carrier)</code>.</li> </ul> </li> <li>templates: Uses Jinja2 with templates under <code>django/templates/</code>.</li> </ul>"},{"location":"how_it_works/core_modules_and_concepts/#discoverypy","title":"<code>discovery.py</code>","text":"<ul> <li>purpose: Pluggable model discovery for different source types.</li> <li>key class: <code>BaseDiscovery[TModel]</code></li> <li>discover_models(packages, app_label, user_filters): Scans packages/modules, applies built-in and user filters.</li> <li>analyze_dependencies(): Build dependency graph (abstract).</li> <li>get_models_in_registration_order(): Topological order (abstract).</li> <li>Protected hooks: <code>_is_target_model(obj)</code>, <code>_default_eligibility_filter(model)</code>, <code>_post_discovery_hook()</code>, <code>_resolve_forward_refs()</code>.</li> </ul>"},{"location":"how_it_works/core_modules_and_concepts/#factoriespy","title":"<code>factories.py</code>","text":"<ul> <li>purpose: Convert a source model and its fields into an in-memory Django model representation.</li> <li>data objects:</li> <li><code>ConversionCarrier[SourceModelType]</code>: Conversion state (django fields, relationships, context, Meta, etc.).</li> <li><code>FieldConversionResult</code>: Result of converting a single field, including kwargs and imports.</li> <li>base classes:</li> <li><code>BaseFieldFactory[SourceFieldType]</code>: <code>create_field(field_info, model_name, carrier)</code> (abstract).</li> <li><code>BaseModelFactory[SourceModelType, SourceFieldType]</code>: Implements orchestration via <code>make_django_model(carrier)</code> and common helpers:<ul> <li><code>_process_source_fields(carrier)</code> (abstract)</li> <li><code>_handle_field_collisions(carrier)</code></li> <li><code>_create_django_meta(carrier)</code></li> <li><code>_assemble_django_model_class(carrier)</code></li> <li><code>_build_model_context(carrier)</code> (abstract)</li> </ul> </li> </ul>"},{"location":"how_it_works/core_modules_and_concepts/#bidirectional_mapperpy","title":"<code>bidirectional_mapper.py</code>","text":"<ul> <li>purpose: Central, bidirectional mapping between Python/Pydantic types and Django <code>Field</code>s.</li> <li>key class: <code>BidirectionalTypeMapper</code></li> <li>get_django_mapping(python_type, field_info, parent_pydantic_model=None) \u2192 <code>(FieldType, kwargs)</code></li> <li>get_pydantic_mapping(django_field) \u2192 <code>(python_type, field_info_kwargs)</code></li> <li>Internals: <code>_find_unit_for_pydantic_type</code>, <code>_find_unit_for_django_field</code>, registry of <code>TypeMappingUnit</code> implementations.</li> <li>exceptions: <code>MappingError</code> for invalid/unsupported cases.</li> </ul>"},{"location":"how_it_works/core_modules_and_concepts/#mapping_unitspy","title":"<code>mapping_units.py</code>","text":"<ul> <li>purpose: Concrete <code>TypeMappingUnit</code> classes that implement mapping rules.</li> <li>base class: <code>TypeMappingUnit</code></li> <li><code>matches(py_type, field_info)</code> scoring; <code>pydantic_to_django_kwargs(...)</code>; <code>django_to_pydantic_field_info_kwargs(dj_field)</code>.</li> <li>notable units: primitives (<code>IntFieldMapping</code>, <code>StrFieldMapping</code>, <code>TextFieldMapping</code>, <code>BoolFieldMapping</code>, <code>FloatFieldMapping</code>), numerics (<code>DecimalFieldMapping</code>), dates (<code>DateFieldMapping</code>, <code>DateTimeFieldMapping</code>, <code>TimeFieldMapping</code>, <code>DurationFieldMapping</code>), identifiers (<code>UUIDFieldMapping</code>), network/paths (<code>EmailFieldMapping</code>, <code>URLFieldMapping</code>, <code>IPAddressFieldMapping</code>, <code>FilePathFieldMapping</code>), files (<code>FileFieldMapping</code>, <code>ImageFieldMapping</code>), JSON (<code>JsonFieldMapping</code>), enums/choices (<code>EnumFieldMapping</code>), relationships (<code>ForeignKeyMapping</code>, <code>OneToOneFieldMapping</code>, <code>ManyToManyFieldMapping</code>), and the conceptual <code>GenericForeignKeyMappingUnit</code>.</li> </ul>"},{"location":"how_it_works/core_modules_and_concepts/#typingpy","title":"<code>typing.py</code>","text":"<ul> <li>purpose: Type processing utilities for introspection and code generation.</li> <li>key class: <code>TypeHandler</code></li> <li>process_field_type(field_type): Normalize Optional/List/Union/Annotated; produce simplified type object and metadata.</li> <li>get_required_imports(type_obj): Compute import map for a type (including nested generics).</li> <li>format_type_string(type_obj): Render a readable type string for generated code.</li> <li>get_class_name(type_obj): Extract a canonical type name.</li> <li>Logging helper: <code>configure_core_typing_logging(...)</code>.</li> </ul>"},{"location":"how_it_works/core_modules_and_concepts/#importspy","title":"<code>imports.py</code>","text":"<ul> <li>purpose: Collect, categorize, and de-duplicate imports used in generated code.</li> <li>key class: <code>ImportHandler</code></li> <li>add_import(module, name), add_pydantic_model_import(model_class), add_context_field_type_import(field_type)</li> <li>deduplicate_imports(): Merge and flatten import statements for output.</li> <li>Helpers for nested/generic types and cleaning generic names.</li> </ul>"},{"location":"how_it_works/core_modules_and_concepts/#relationshipspy","title":"<code>relationships.py</code>","text":"<ul> <li>purpose: Track and resolve mappings between source models and generated Django models.</li> <li>key classes:</li> <li><code>RelationshipMapper</code>: Holds a single mapping between a source model and a Django model.</li> <li><code>RelationshipConversionAccessor</code>: Registry and lookup utilities:<ul> <li><code>map_relationship(source_model, django_model)</code>, <code>is_source_model_known(model)</code></li> <li><code>get_django_model_for_pydantic(pydantic_model)</code>, <code>get_pydantic_model_for_django(django_model)</code></li> <li><code>get_django_model_for_dataclass(dataclass_model)</code>, <code>available_source_models</code>, <code>available_django_models</code></li> </ul> </li> </ul>"},{"location":"how_it_works/core_modules_and_concepts/#contextpy","title":"<code>context.py</code>","text":"<ul> <li>purpose: Context storage and codegen for data that cannot be represented as first-class Django fields.</li> <li>key classes:</li> <li><code>FieldContext</code>: Per-field context metadata and value.</li> <li><code>ModelContext[SourceModelType]</code>: Collection of context fields for a model; import synthesis; serialization helpers.<ul> <li>Methods: <code>add_field(...)</code>, <code>validate_context(...)</code>, <code>get_required_imports()</code>, <code>to_conversion_dict()</code>.</li> </ul> </li> <li><code>ContextClassGenerator</code>: Renders a context dataclass using the <code>context_class.py.j2</code> template.</li> </ul>"},{"location":"how_it_works/core_modules_and_concepts/#serializationpy","title":"<code>serialization.py</code>","text":"<ul> <li>purpose: Safe value serialization utilities.</li> <li>API: <code>SerializationMethod</code> enum, <code>get_serialization_method(obj)</code>, <code>serialize_value(value)</code>, <code>is_serializable(obj)</code>.</li> </ul>"},{"location":"how_it_works/core_modules_and_concepts/#defspy","title":"<code>defs.py</code>","text":"<ul> <li>purpose: Shared type helpers and mapping definition structure.</li> <li>API: <code>is_serializable_type(field_type)</code>, <code>is_pydantic_model(obj)</code>, <code>TypeMappingDefinition</code>.</li> </ul>"},{"location":"how_it_works/core_modules_and_concepts/#filter_helperspy","title":"<code>filter_helpers.py</code>","text":"<ul> <li>purpose: Small helpers to build discovery filters.</li> <li>API: <code>exclude_models([...])</code>, <code>include_models([...])</code>, <code>has_field(name)</code>, <code>always_include(...)</code>.</li> </ul>"},{"location":"how_it_works/core_modules_and_concepts/#utils","title":"<code>utils/</code>","text":"<ul> <li>strings.py: <code>sanitize_string(value)</code> for safe codegen strings; <code>balanced(s)</code> for quick bracket balance checks.</li> <li>relationships.py: <code>get_relationship_metadata(field_type)</code> and type aliases for relationship detection.</li> </ul>"},{"location":"how_it_works/core_modules_and_concepts/#timescaledb-integration-overview","title":"TimescaleDB integration (overview)","text":"<ul> <li>namespace: <code>pydantic2django.django.timescale</code></li> <li>Bases: <code>XmlTimescaleBase</code>, <code>PydanticTimescaleBase</code>, <code>DataclassTimescaleBase</code> (combine the source-specific base with <code>TimescaleModel</code> when available).</li> <li>Heuristics: <code>classify_xml_complex_types(...)</code> scores XML complex types as hypertable vs dimension.</li> <li>Helpers: <code>should_use_timescale_base(model_name, roles)</code> and <code>should_soft_reference(src, tgt, roles)</code> for safe schema generation.</li> <li>rules enforced:</li> <li>Hypertable \u2192 Regular table (FK): allowed.</li> <li>Regular \u2192 Regular (FK): allowed.</li> <li>Hypertable \u2192 Hypertable (FK): not allowed. Generator emits a soft reference field (e.g., <code>UUIDField(db_index=True)</code>) instead of a <code>ForeignKey</code>.</li> <li>where used:</li> <li>XML generator classifies types and selects base per model; factories consult roles to convert illegal FKs to soft refs.</li> <li>Other generators can adopt the same helpers to enable Timescale consistently.</li> </ul> <p>See also the detailed API pages for these modules under the site\u2019s API section: API Reference.</p>"},{"location":"how_it_works/core_modules_and_concepts/#heuristics-and-scoring-details","title":"Heuristics and scoring (details)","text":"<p>For XML complex types, the generator classifies hypertables vs dimensions using a lightweight scoring model (default threshold: 3):</p> <ul> <li>+2 if a time-like element/attribute exists (e.g., <code>time</code>, <code>timestamp</code>, <code>sequence</code>, <code>effectiveTime</code>, <code>sampleRate</code>)</li> <li>+2 if the type name matches observation/event patterns (e.g., <code>Samples</code>, <code>Events</code>, <code>Condition</code>, <code>*Changed</code>, <code>*Removed</code>, <code>*Added</code>, <code>Streams</code>)</li> <li>+1 if the schema suggests unbounded/high-cardinality growth (e.g., any element with <code>is_list=True</code>)</li> <li>\u22122 if the name matches definition/metadata categories (e.g., <code>*Definition*</code>, <code>*Definitions*</code>, <code>Constraints</code>, <code>Properties</code>, <code>Parameters</code>, <code>Header</code>, <code>Counts</code>, <code>Configuration</code>, <code>Description</code>, <code>Location</code>, <code>Limits</code>, <code>Reference</code>, <code>Relationships</code>)</li> </ul> <p>Types with score \u2265 threshold are treated as hypertables; otherwise they are dimensions. Illegal hypertable \u2192 hypertable relationships are emitted as indexed soft references (<code>UUIDField(db_index=True, null=True, blank=True)</code>) instead of <code>ForeignKey</code>.</p> <p>You can override ambiguous types and adjust the threshold using <code>TimescaleConfig</code> and <code>overrides</code> in <code>classify_xml_complex_types(...)</code>.</p>"},{"location":"how_it_works/generic_entries_contenttypes/","title":"Generic entries contenttypes","text":""},{"location":"how_it_works/generic_entries_contenttypes/#generic-entries-mode-django-contenttypes","title":"Generic Entries mode (Django ContentTypes)","text":"<p>This optional mode stores polymorphic/repeating nested structures as rows in a single <code>GenericEntry</code> model, attached to owning parents via <code>GenericRelation</code>. It reduces generated model count and migration size while remaining queryable.</p> <ul> <li>Defaults (new projects):</li> <li><code>enable_gfk=True</code></li> <li><code>gfk_policy=\"all_nested\"</code> (favor minimal, GFK\u2011heavy output)</li> <li><code>gfk_threshold_children=4</code> (only used when <code>gfk_policy='threshold_by_children'</code>)</li> <li><code>gfk_value_mode=\"typed_columns\"</code></li> <li> <p><code>gfk_normalize_common_attrs=False</code></p> </li> <li> <p>Flags (tunable via generators):</p> </li> <li><code>enable_gfk: bool</code></li> <li><code>gfk_policy: \"substitution_only\" | \"repeating_only\" | \"all_nested\" | \"threshold_by_children\"</code></li> <li><code>gfk_threshold_children: int</code> (only for <code>threshold_by_children</code>)</li> <li><code>gfk_value_mode: \"json_only\" | \"typed_columns\"</code></li> <li><code>gfk_normalize_common_attrs: bool</code> (reserved)</li> </ul>"},{"location":"how_it_works/generic_entries_contenttypes/#flag-reference","title":"Flag reference","text":"<ul> <li><code>enable_gfk</code></li> <li> <p>Enables Generic Entries mode globally for the generator. If <code>False</code>, legacy behavior is preserved and no <code>GenericEntry</code> is emitted.</p> </li> <li> <p><code>gfk_policy</code></p> </li> <li>Controls which nested elements are routed to <code>GenericEntry</code>.</li> <li> <p>Values:</p> <ul> <li><code>substitution_only</code>: Treat elements under wrapper-like owners that participate in substitution groups as GFK entries. Use this when many polymorphic variants share a base type (e.g., MTConnect observations).</li> <li><code>repeating_only</code>: Route repeating complex leaves (<code>maxOccurs&gt;1</code> or <code>unbounded</code>) through GFK.</li> <li><code>all_nested</code>: Route all eligible nested complex elements through GFK (including single-nested). Useful to aggressively minimize model count.</li> <li><code>threshold_by_children</code>: For wrapper-like containers, route to GFK when the number of distinct child complex types in the wrapper \u2265 <code>gfk_threshold_children</code>.</li> </ul> </li> <li> <p><code>gfk_threshold_children</code></p> </li> <li>Only used with <code>threshold_by_children</code>. An integer bound determining when a wrapper is considered polymorphic enough to justify GFK.</li> <li> <p>Example: <code>gfk_threshold_children=8</code> means wrappers with \u22658 distinct child complex types are routed via GFK.</p> </li> <li> <p><code>gfk_value_mode</code></p> </li> <li> <p>Controls how element text is stored on <code>GenericEntry</code>:</p> <ul> <li><code>json_only</code>: Store the element text under <code>attrs_json[\"value\"]</code>.</li> <li><code>typed_columns</code>: Attempt to extract text into typed columns: <code>text_value</code>, <code>num_value</code>, and <code>time_value</code> when unambiguous; remaining attributes go to <code>attrs_json</code>.</li> </ul> </li> <li> <p><code>gfk_normalize_common_attrs</code></p> </li> <li> <p>Reserved for promoting commonly-used attributes (e.g., timestamp, subtype) to normalized columns when <code>gfk_value_mode=\"typed_columns\"</code>. Default <code>False</code>.</p> </li> <li> <p><code>gfk_overrides: dict[str, bool]</code></p> </li> <li> <p>Optional per-element overrides by local element name. <code>True</code> forces GFK for that element; <code>False</code> disables it even if policy would apply.</p> </li> <li> <p>Behavior:</p> </li> <li>When policy matches, concrete child models are not generated; parents get <code>entries = GenericRelation('GenericEntry')</code>.</li> <li>In ingestion, each matching XML element becomes a <code>GenericEntry</code> with:<ul> <li><code>element_qname</code>, <code>type_qname</code>, <code>attrs_json</code>, <code>order_index</code>, <code>path_hint</code></li> <li>Optional typed value columns: <code>text_value</code>, <code>num_value</code>, <code>time_value</code> (when <code>gfk_value_mode='typed_columns'</code>)</li> </ul> </li> <li> <p>Indexes are added on <code>(content_type, object_id)</code> and, in typed mode, on <code>element_qname</code>, <code>type_qname</code>, <code>time_value</code>, and <code>(content_type, object_id, -time_value)</code>.</p> </li> <li> <p>Policies:</p> </li> <li><code>repeating_only</code>: route repeating complex leaves through <code>GenericEntry</code>.</li> <li><code>all_nested</code>: route all eligible nested complex elements (including single nested) through <code>GenericEntry</code>.</li> <li><code>threshold_by_children</code>: for wrapper-like containers, use <code>GenericEntry</code> when the number of distinct child complex types \u2265 <code>gfk_threshold_children</code>.</li> <li> <p><code>substitution_only</code>: for wrapper-like containers, collapse substitution-group members to <code>GenericEntry</code> rows regardless of the number of distinct child types.</p> </li> <li> <p>Example query: <pre><code>samples.entries.filter(element_qname=\"Angle\", time_value__gte=..., attrs_json__subType=\"ACTUAL\")\n</code></pre></p> </li> </ul>"},{"location":"how_it_works/generic_entries_contenttypes/#notes-and-interactions","title":"Notes and interactions","text":"<ul> <li>Wrapper detection (generic): Prefer structural checks over names. Owners are selected when:</li> <li>Policy is <code>all_nested</code> (all nested routed), or</li> <li>Policy is <code>threshold_by_children</code> and the wrapper\u2019s set of distinct child complex types \u2265 <code>gfk_threshold_children</code>, or</li> <li>Policy is <code>substitution_only</code> and the wrapper contains substitution-group members.</li> <li>Name hints (TitleCase element, <code>*WrapperType</code>) can assist but are not required.</li> <li>Discovery gating: When a path is routed via GFK, the corresponding concrete child complex types may be excluded from the generated model set to keep the surface area small.</li> <li>Ingestion: When parents expose <code>entries</code>, matching XML child elements are persisted as <code>GenericEntry</code> rows, preserving order and attributes. Concrete child instances are not created.</li> </ul>"},{"location":"how_it_works/generic_entries_contenttypes/#eitheror-guarantee","title":"Either/Or guarantee","text":"<ul> <li>When a model is selected as a GFK owner, its child element fields are suppressed entirely (no JSON placeholders or concrete child relations). If configuration flags would cause both entries and JSON/relations on the same owner, generation fails fast with a clear error.</li> </ul> <p>See also: <code>docs/plans/gfk_generic_entries_plan.md</code> and <code>docs/how_it_works/xmlschema_element_and_type_handling.md</code>.</p>"},{"location":"how_it_works/ingestor_strict_and_contract/","title":"Ingestor strict and contract","text":""},{"location":"how_it_works/ingestor_strict_and_contract/#ingestor-strict-mode-and-modelschema-contract","title":"Ingestor Strict Mode and Model/Schema Contract","text":"<p>This page explains the ingestion contract checks and strict mode behavior added to the XML ingestor.</p>"},{"location":"how_it_works/ingestor_strict_and_contract/#what-strict-mode-enforces","title":"What strict mode enforces","text":"<p>When <code>strict=True</code> on <code>XmlInstanceIngestor</code>:</p> <ul> <li>Unexpected child elements in the XML that are not declared in the complex type cause a hard failure.</li> <li>Unmapped XML attributes that don\u2019t correspond to declared attributes cause a hard failure.</li> <li>Contract validation (<code>validate_models(strict=True)</code>) raises on drift between discovered XML types and installed Django models.</li> <li>Timescale-only requirement: models that inherit a Timescale base must have a non-null <code>time</code> field at ingestion time; otherwise <code>TimeseriesTimestampMissingError</code> is raised. Non-Timescale models are not forced to provide <code>time</code>.</li> </ul> <p>All strict failures raise <code>SchemaSyncError</code> with a clear remediation message:</p> <p>\u201cSchema and static model are out of sync; verify schema and/or regenerate static Django models and re-migrate.\u201d</p>"},{"location":"how_it_works/ingestor_strict_and_contract/#contract-validation","title":"Contract validation","text":"<p>Use <code>validate_models()</code> to verify alignment between schemas and installed models:</p> <ul> <li>The ingestor computes expected field names for each complex type:</li> <li>all declared attributes (converted to snake_case)</li> <li>all simple elements</li> <li>for single nested complex elements, the parent\u2019s field for the child (snake_case of the element name)</li> <li>It then compares these with the installed model\u2019s <code>model._meta.fields</code>.</li> <li>For Timescale-based models, it also ensures a <code>time</code> field is declared.</li> </ul> <p>Behavior:</p> <ul> <li><code>validate_models(strict=False)</code> returns a list of issues (if any) without raising.</li> <li><code>validate_models(strict=True)</code> raises <code>SchemaSyncError</code> if issues are found.</li> </ul>"},{"location":"how_it_works/ingestor_strict_and_contract/#why-enforce-timescale-only","title":"Why enforce Timescale only","text":"<p>The <code>time</code> field is required for hypertables. Enforcing it only when the model inherits a Timescale base avoids over-constraining standard models that may have optional timestamps. The ingestor still remaps common aliases (e.g., <code>creationTime</code>, <code>timeStamp</code>, etc.) into <code>time</code> for convenience, but only Timescale models must provide it.</p>"},{"location":"how_it_works/ingestor_strict_and_contract/#interaction-with-generator-demotion","title":"Interaction with generator demotion","text":"<ul> <li>Non-strict default behavior: During generation, types that lack a direct time/date field are demoted to <code>dimension</code> even if their name/list scoring would otherwise classify them as hypertables. This prevents runtime <code>TimeseriesTimestampMissingError</code> for metadata/container types (e.g., MTConnect <code>Header</code>, <code>Agent</code>).</li> <li>Strict mode: When <code>timescale_strict=True</code> on the generator, any type classified as a hypertable that lacks a direct time/date field causes generation to fail with a clear message, prompting schema/override correction rather than deferring to ingestion-time errors.</li> </ul>"},{"location":"how_it_works/ingestor_strict_and_contract/#failure-messages-and-remediation","title":"Failure messages and remediation","text":"<p>On failures, you\u2019ll see a compact summary including the affected model and missing fields. Recommended steps:</p> <ul> <li>Verify your XSD changes and regenerate static Django models if needed.</li> <li>Re-run migrations to apply schema changes.</li> <li>For Timescale models, ensure a non-null <code>time</code> value is present in the XML (or adjust your mapping).</li> </ul>"},{"location":"how_it_works/provider_backend_pattern/","title":"Provider backend pattern","text":""},{"location":"how_it_works/provider_backend_pattern/#providerbackends-pattern","title":"Provider/backends pattern","text":"<p>This document explains the common pattern shared by the provider backends in <code>src/pydantic2django/</code>:</p> <ul> <li><code>dataclass/</code></li> <li><code>pydantic/</code></li> <li><code>typedclass/</code></li> <li><code>xmlschema/</code></li> </ul> <p>These backends implement the same high-level pipeline and reuse the core abstractions in <code>src/pydantic2django/core/</code>.</p>"},{"location":"how_it_works/provider_backend_pattern/#the-shared-pipeline","title":"The shared pipeline","text":"<p>1) Discovery (find source models) - Each backend provides a <code>Discovery</code> that subclasses <code>core.discovery.BaseDiscovery[TModel]</code>. - It scans the provided packages or inputs, applies default and user filters, and builds a dependency graph.</p> <p>2) Factory (turn fields into Django <code>Field</code>s, then a model) - Each backend provides a <code>FieldFactory</code> and a <code>ModelFactory</code> that subclass <code>core.factories.BaseFieldFactory</code> and <code>BaseModelFactory</code>. - Field factories translate a single source field to a Django field and kwargs. - Model factories iterate fields, handle collisions, build a <code>Meta</code> class, assemble the Django <code>Model</code> type, and construct optional context.</p> <p>3) Generator (orchestrate and render) - Each backend provides a <code>...ModelGenerator</code> that subclasses <code>core.base_generator.BaseStaticGenerator</code>. - It calls discovery, invokes the model factory for each source model to produce carriers, renders Jinja templates (<code>django/templates/*.j2</code>), and writes the output <code>models.py</code>.</p>"},{"location":"how_it_works/provider_backend_pattern/#core-building-blocks-reused-by-all-backends","title":"Core building blocks reused by all backends","text":"<ul> <li><code>BaseDiscovery</code>: common scanning/filtering hooks and dependency analysis entrypoints.</li> <li><code>BaseModelFactory</code> / <code>BaseFieldFactory</code>: single-field conversion and whole-model assembly with a <code>ConversionCarrier</code> for state.</li> <li><code>BidirectionalTypeMapper</code> + <code>mapping_units</code>: type mapping for Pydantic/Dataclass fields to Django fields, including enums, collections, and relationships.</li> <li><code>RelationshipConversionAccessor</code>: resolves and tracks mappings between source models and generated Django models, used by mappers/factories.</li> <li><code>ImportHandler</code>: collects and de-duplicates imports across definitions and context classes.</li> <li><code>ModelContext</code> / <code>ContextClassGenerator</code>: holds non-serializable context and renders companion context classes (primarily for Pydantic).</li> <li><code>TypeHandler</code>: normalizes types (Optional/List/Union/Annotated), produces readable type strings, and calculates required imports.</li> </ul>"},{"location":"how_it_works/provider_backend_pattern/#backend-specifics","title":"Backend specifics","text":""},{"location":"how_it_works/provider_backend_pattern/#pydantic-pydantic","title":"Pydantic (<code>pydantic/</code>)","text":"<ul> <li>Discovery: <code>PydanticDiscovery</code> finds <code>BaseModel</code> classes and builds a dependency graph from field annotations.</li> <li>Factories: <code>PydanticFieldFactory</code> + <code>PydanticModelFactory</code> use <code>BidirectionalTypeMapper</code> to convert <code>FieldInfo</code> to Django fields (includes union/multi-FK handling, relationships, enums/choices, defaults).</li> <li>Generator: <code>StaticPydanticModelGenerator</code> orchestrates generation and also renders per-model context classes where needed.</li> </ul>"},{"location":"how_it_works/provider_backend_pattern/#dataclass-dataclass","title":"Dataclass (<code>dataclass/</code>)","text":"<ul> <li>Discovery: <code>DataclassDiscovery</code> identifies Python dataclasses and derives dependencies from dataclass field annotations.</li> <li>Factories: <code>DataclassFieldFactory</code> + <code>DataclassModelFactory</code> use the same bidirectional mapper; default/metadata resolution is tailored to dataclasses.</li> <li>Generator: <code>DataclassDjangoModelGenerator</code> follows the shared flow; context classes are typically not emitted.</li> </ul>"},{"location":"how_it_works/provider_backend_pattern/#typedclass-typedclass","title":"TypedClass (<code>typedclass/</code>)","text":"<ul> <li>Discovery: <code>TypedClassDiscovery</code> targets plain Python classes (not Pydantic/dataclasses), leveraging <code>__init__</code> parameters and class annotations for fields/deps.</li> <li>Factories: <code>TypedClassFieldFactory</code> + <code>TypedClassModelFactory</code> translate type hints using a type translator suited to generic classes.</li> <li>Generator: <code>TypedClassDjangoModelGenerator</code> reuses the shared pipeline; focuses on simple type-hint based field generation.</li> </ul>"},{"location":"how_it_works/provider_backend_pattern/#xml-schema-xmlschema","title":"XML Schema (<code>xmlschema/</code>)","text":"<ul> <li>Discovery: <code>XmlSchemaDiscovery</code> parses XSDs via <code>XmlSchemaParser</code>, gathers complex types, applies filters, and computes inter-type dependencies.</li> <li>Factories: <code>XmlSchemaFieldFactory</code> + <code>XmlSchemaModelFactory</code> map XSD simple/complex types to Django fields. Handles enumerations (TextChoices), nillability/occurrence constraints, and post-pass injection of relationships (e.g., child FKs) via <code>finalize_relationships</code>.</li> <li>Generator: <code>XmlSchemaDjangoModelGenerator</code> extends the shared generator logic, calling <code>finalize_relationships</code> before rendering and passing enum/context info to templates.</li> </ul>"},{"location":"how_it_works/provider_backend_pattern/#relationship-handling","title":"Relationship handling","text":"<ul> <li>Pydantic/Dataclass: <code>BidirectionalTypeMapper</code> inspects type hints and uses <code>RelationshipConversionAccessor</code> to resolve related model references (FK, O2O, M2M). It detects lists of known models for M2M and unions of models for multi-FK patterns.</li> <li>XML Schema: relationships are derived from <code>key</code>/<code>keyref</code>, element nesting, and configuration (fk/json/m2m). Some relations are injected after all models exist.</li> </ul>"},{"location":"how_it_works/provider_backend_pattern/#imports-and-templates","title":"Imports and templates","text":"<ul> <li><code>ImportHandler</code> accumulates typing/pydantic/context and model imports during field conversion and context rendering, then de-duplicates for the templates.</li> <li>All generators render with the shared Jinja templates in <code>src/pydantic2django/django/templates/</code> (<code>model_definition.py.j2</code>, <code>models_file.py.j2</code>, <code>context_class.py.j2</code>, <code>imports_block.py.j2</code>).</li> </ul>"},{"location":"how_it_works/provider_backend_pattern/#adding-a-new-backend","title":"Adding a new backend","text":"<ul> <li>Implement a <code>Discovery</code> subclass to find your source models and compute dependencies.</li> <li>Implement a <code>FieldFactory</code> + <code>ModelFactory</code> pairing to convert fields and assemble models.</li> <li>Implement a <code>...ModelGenerator</code> subclass of <code>BaseStaticGenerator</code> and wire your discovery/factory and any special context/relationship steps.</li> <li>Reuse <code>BidirectionalTypeMapper</code> where possible; add new <code>TypeMappingUnit</code>s if your source introduces new type shapes.</li> <li>Leverage <code>ImportHandler</code>, <code>RelationshipConversionAccessor</code>, and <code>TypeHandler</code> to keep logic consistent and DRY.</li> </ul>"},{"location":"how_it_works/provider_backend_pattern/#api-reference","title":"API reference","text":"<p>For full, up-to-date API details of these modules and classes, see the generated reference: API Reference.</p>"},{"location":"how_it_works/readme_bidirectional_mapper/","title":"Pydantic2Django Bidirectional Mapper Details","text":"<p>This document provides details on how the <code>BidirectionalTypeMapper</code> handles specific field type conversions between Django models and Pydantic models.</p>"},{"location":"how_it_works/readme_bidirectional_mapper/#how-selection-works-at-a-glance","title":"How selection works (at a glance)","text":"<p>The mapper discovers and orders mapping units (e.g., <code>StrFieldMapping</code>, <code>IntFieldMapping</code>, relationships) and then selects the best match per field based on scoring and context.</p> <p>```94:141:src/pydantic2django/core/bidirectional_mapper.py def _build_registry(self) -&gt; list[type[TypeMappingUnit]]:     ordered_units = [         BigAutoFieldMapping, SmallAutoFieldMapping, AutoFieldMapping,         PositiveBigIntFieldMapping, PositiveSmallIntFieldMapping, PositiveIntFieldMapping,         EmailFieldMapping, URLFieldMapping, SlugFieldMapping, IPAddressFieldMapping, FilePathFieldMapping,         ImageFieldMapping, FileFieldMapping,         UUIDFieldMapping, JsonFieldMapping,         ManyToManyFieldMapping, OneToOneFieldMapping, ForeignKeyMapping,         DecimalFieldMapping, DateTimeFieldMapping, DateFieldMapping, TimeFieldMapping,         DurationFieldMapping, BinaryFieldMapping, FloatFieldMapping,         TextFieldMapping, StrFieldMapping,         BigIntFieldMapping, SmallIntFieldMapping, IntFieldMapping,         EnumFieldMapping,     ] <pre><code>## Django `choices` Field Mapping\n\nWhen mapping a Django field (like `CharField` or `IntegerField`) that has the `choices` attribute set, the `BidirectionalTypeMapper` employs a hybrid approach for the resulting Pydantic field:\n\n1.  **Pydantic Type:** The Python type hint for the Pydantic field is set to `typing.Literal[...]`, where the literal values are the *raw database values* defined in the Django `choices` (e.g., `Literal['S', 'M', 'L']` or `Literal[1, 2, 3]`). If the Django field has `null=True`, the type becomes `Optional[Literal[...]]`.\n    *   **Benefit:** This provides strong typing and allows Pydantic to perform validation, ensuring that only the allowed raw values are assigned to the field.\n\n2.  **Metadata:** The original Django `choices` tuple, containing the `(raw_value, human_readable_label)` pairs (e.g., `[('S', 'Small'), ('M', 'Medium'), ('L', 'Large')]`), is preserved within the Pydantic `FieldInfo` associated with the field. Specifically, it's stored under the `json_schema_extra` key:\n    ```python\n    FieldInfo(..., json_schema_extra={'choices': [('S', 'Small'), ('M', 'Medium')]})\n    ```\n    *   **Benefit:** This keeps the human-readable labels associated with the field, making them available for other purposes like generating API documentation (e.g., OpenAPI schemas), building UI components (like dropdowns), or custom logic, without sacrificing the validation provided by the `Literal` type.\n\n**Trade-off:** This approach prioritizes data validation using Pydantic's `Literal` type based on the raw stored values. The human-readable labels are available as metadata but are not part of the core Pydantic type validation itself. The Django `get_FOO_display()` method is not directly used during the conversion process, as the focus is on mapping the underlying data values and types.\n\nRelevant code references:\n\n```193:199:src/pydantic2django/core/bidirectional_mapper.py\noriginal_origin = get_origin(original_type_for_cache)\nif original_origin is Literal:\n    best_unit = EnumFieldMapping\n    self._pydantic_cache[cache_key] = best_unit\n    return best_unit\n</code></pre></p> <p>```739:748:src/pydantic2django/core/bidirectional_mapper.py if is_choices:     final_pydantic_type = base_instance_unit.python_type     if dj_field.choices:         choice_values = tuple(choice[0] for choice in dj_field.choices)         if choice_values:             final_pydantic_type = Literal[choice_values]  # type: ignore <pre><code>```842:850:src/pydantic2django/core/bidirectional_mapper.py\nif (\n    is_choices\n    and \"json_schema_extra\" in field_info_kwargs\n    and \"choices\" in field_info_kwargs[\"json_schema_extra\"]\n):\n    logger.debug(f\"Kept choices in json_schema_extra for Literal field '{dj_field.name}'\")\n</code></pre></p>"},{"location":"how_it_works/readme_bidirectional_mapper/#other-field-mappings","title":"Other Field Mappings","text":"<p>(This section can be expanded later with details about other interesting or complex mappings, such as relationships, JSON fields, etc.)</p>"},{"location":"how_it_works/readme_bidirectional_mapper/#pydantic-django-examples","title":"Pydantic \u279c Django examples","text":"<ul> <li>int: <code>int</code> \u279c <code>models.IntegerField</code></li> <li>With non-negative constraint (e.g., <code>ge=0</code>) \u279c <code>models.PositiveIntegerField</code>.</li> <li> <p>Auto PKs are handled separately (see below).</p> </li> <li> <p>float/Decimal:</p> </li> <li><code>float</code> \u279c <code>models.FloatField</code></li> <li> <p><code>Decimal</code> \u279c <code>models.DecimalField</code> (uses <code>max_digits</code>/<code>decimal_places</code> if provided)</p> </li> <li> <p>bool: <code>bool</code> \u279c <code>models.BooleanField</code> (defaults aligned)</p> </li> <li> <p>str-family:</p> </li> <li><code>str</code> with <code>max_length</code> \u279c <code>models.CharField(max_length=...)</code></li> <li><code>str</code> without <code>max_length</code> \u279c <code>models.TextField</code></li> <li><code>EmailStr</code> \u279c <code>models.EmailField</code></li> <li><code>HttpUrl</code> \u279c <code>models.URLField</code></li> <li><code>IPvAnyAddress</code> \u279c <code>models.GenericIPAddressField</code></li> <li><code>str</code> with slug pattern \u279c <code>models.SlugField</code></li> <li><code>pathlib.Path</code> \u279c <code>models.FilePathField</code></li> <li> <p>File/Image hints \u279c <code>models.FileField</code>/<code>models.ImageField</code> (stored as path/URL in Pydantic)</p> </li> <li> <p>UUID/datetime:</p> </li> <li><code>UUID</code> \u279c <code>models.UUIDField</code></li> <li> <p><code>datetime</code>, <code>date</code>, <code>time</code>, <code>timedelta</code> \u279c respective <code>DateTimeField</code>, <code>DateField</code>, <code>TimeField</code>, <code>DurationField</code></p> </li> <li> <p>bytes: <code>bytes</code> \u279c <code>models.BinaryField</code></p> </li> <li> <p>JSON-like/Any:</p> </li> <li> <p><code>dict</code>, <code>list</code>, <code>tuple</code>, <code>set</code>, or <code>Any</code> \u279c <code>models.JSONField</code></p> </li> <li> <p>Enums/Literals:</p> </li> <li> <p><code>Literal['A','B']</code> or <code>Enum</code> \u279c underlying <code>CharField</code>/<code>IntegerField</code> with <code>choices</code> set.</p> </li> <li> <p>Relationships:</p> </li> <li><code>RelatedModel</code> \u279c <code>models.ForeignKey(\"app.relatedmodel\", on_delete=...)</code></li> <li><code>list[RelatedModel]</code> \u279c <code>models.ManyToManyField(\"app.relatedmodel\")</code></li> <li><code>Union[ModelA, ModelB]</code> \u279c stored as <code>JSONField</code> with <code>_union_details</code> metadata for downstream handling.</li> </ul> <p>Key relationship handling:</p> <p>```631:705:src/pydantic2django/core/bidirectional_mapper.py if unit_cls in (ForeignKeyMapping, OneToOneFieldMapping, ManyToManyFieldMapping):     ...     if is_self_ref:         model_ref = \"self\"     else:         target_django_model = ...         model_ref = getattr(target_django_model._meta, \"label_lower\", target_django_model.name)     kwargs[\"to\"] = model_ref     django_field_type = unit_cls.django_field_type     if unit_cls in (ForeignKeyMapping, OneToOneFieldMapping):         kwargs[\"on_delete\"] = (models.SET_NULL if is_optional else models.CASCADE) <pre><code>M2M/list and unions detection:\n\n```435:488:src/pydantic2django/core/bidirectional_mapper.py\nif is_list:\n    ...\n    if inner_origin in (Union, UnionType) and inner_args:\n        ...\n        unit_cls = JsonFieldMapping  # GFK-like signal\n    elif ... inner_type is known model ...:\n        unit_cls = ManyToManyFieldMapping\n    else:\n        unit_cls = JsonFieldMapping  # list of non-models\n</code></pre></p> <p>Union of models (multi-FK signal):</p> <p>```502:563:src/pydantic2django/core/bidirectional_mapper.py if simplified_origin in (Union, UnionType) and simplified_args:     ...     if union_models and not other_types_in_union:         union_details = {\"type\": \"multi_fk\", \"models\": union_models, \"is_optional\": is_optional}         unit_cls = JsonFieldMapping <pre><code>Null/blank defaults:\n\n```703:709:src/pydantic2django/core/bidirectional_mapper.py\nif django_field_type != models.ManyToManyField and not union_details:\n    kwargs[\"null\"] = is_optional\n    kwargs[\"blank\"] = is_optional\n</code></pre></p> <p>Auto-increment PKs (wrapped as Optional in Pydantic when mapping Django \u279c Pydantic):</p> <p>```804:812:src/pydantic2django/core/bidirectional_mapper.py is_auto_pk = dj_field.primary_key and isinstance(     dj_field, (models.AutoField, models.BigAutoField, models.SmallAutoField) ) if is_auto_pk:     final_pydantic_type = Optional[int]     is_optional = True <pre><code>### Minimal Pydantic \u279c Django examples\n\n```python\nfrom pydantic import BaseModel, Field\nfrom pydantic import EmailStr\n\nclass User(BaseModel):\n    id: int | None = Field(default=None, title=\"ID\")                 # Auto PK \u279c AutoField (null/blank True)\n    email: EmailStr = Field(title=\"Email\")                           # \u279c EmailField(max_length=254)\n    name: str = Field(title=\"Name\", max_length=120)                  # \u279c CharField(max_length=120)\n    bio: str = Field(title=\"Bio\")                                    # \u279c TextField\n    settings: dict = Field(title=\"Settings\")                          # \u279c JSONField\n</code></pre></p>"},{"location":"how_it_works/readme_bidirectional_mapper/#django-pydantic-examples","title":"Django \u279c Pydantic examples","text":"<ul> <li>Choices \u279c Literal[...] with metadata: as detailed above.</li> <li>Relationships:</li> <li><code>ForeignKey(User, null=True)</code> \u279c <code>Optional[UserModel]</code></li> <li><code>ManyToManyField(Tag)</code> \u279c <code>list[TagModel]</code></li> <li> <p>Self-references are represented conservatively; see code reference.</p> </li> <li> <p>Auto PKs: AutoField/BigAutoField/SmallAutoField \u279c <code>Optional[int]</code> with <code>frozen=True</code> in <code>FieldInfo</code>.</p> </li> </ul>"},{"location":"how_it_works/readme_bidirectional_mapper/#minimal-django-pydantic-examples","title":"Minimal Django \u279c Pydantic examples","text":"<pre><code>from django.db import models\n\nclass User(models.Model):\n    id = models.AutoField(primary_key=True)\n    email = models.EmailField()\n    name = models.CharField(max_length=120)\n    bio = models.TextField(blank=True)\n    settings = models.JSONField(default=dict)\n\n# \u279c Pydantic (conceptual):\n# class UserModel(BaseModel):\n#     id: int | None = Field(default=None, frozen=True)\n#     email: EmailStr\n#     name: str = Field(max_length=120)\n#     bio: str | None = None\n#     settings: dict = Field(default_factory=dict)\n</code></pre>"},{"location":"how_it_works/readme_bidirectional_mapper/#notes","title":"Notes","text":"<ul> <li>Constraint hints (e.g., <code>max_length</code>, <code>ge</code>, <code>decimal_places</code>) are read from <code>FieldInfo.metadata</code> when available and mapped accordingly by the specific mapping units.</li> <li>Lists of non-models map to <code>JSONField</code>; lists of known models map to <code>ManyToManyField</code>.</li> <li>Unions of known models are represented as <code>JSONField</code> with <code>_union_details</code> to signal multi-FK semantics to generators.</li> </ul>"},{"location":"how_it_works/readme_context/","title":"Pydantic2Django Context Storage and Generated Context Classes","text":"<p>This document explains why the context mechanism exists, how <code>ModelContext</code> and <code>FieldContext</code> work, and how the <code>context_class.py.j2</code> template generates per-model context classes used during Django \u2194\ufe0e Pydantic/Dataclass conversion.</p>"},{"location":"how_it_works/readme_context/#why-a-context-mechanism-is-required","title":"Why a context mechanism is required","text":"<p>Some information necessary to round-trip between Django and source models (Pydantic or Dataclass) cannot be represented as first-class Django fields or inferred at runtime solely from the database. Examples include:</p> <ul> <li>Non-serializable or runtime-only values needed to reconstruct source objects</li> <li>Disambiguation for unions/polymorphic fields and special relationship shapes</li> <li>Additional metadata required to map back from Django \u2192 Pydantic with fidelity</li> </ul> <p>To support lossless conversion, these values are captured as named context fields. They travel alongside model instances (not stored in the database) and are injected back into the source models during reconstruction.</p>"},{"location":"how_it_works/readme_context/#core-data-structures","title":"Core data structures","text":"<p><code>FieldContext</code> describes a single context field: its name, type, optionality, list-ness, metadata, and value.</p> <p>```27:39:src/pydantic2django/core/context.py @dataclass class FieldContext:     \"\"\"     Represents context information for a single field.     \"\"\"</p> <pre><code>field_name: str\nfield_type_str: str  # Renamed from field_type for clarity\nis_optional: bool = False\nis_list: bool = False\nadditional_metadata: dict[str, Any] = field(default_factory=dict)\nvalue: Optional[Any] = None\n</code></pre> <p><code>`ModelContext[SourceModelType]` is the in-memory container for all context fields associated with a Django model and its corresponding source model.</code>50:58:src/pydantic2django/core/context.py     django_model: type[models.Model]     source_class: type[SourceModelType]  # Changed from pydantic_class     context_fields: dict[str, FieldContext] = field(default_factory=dict)     context_data: dict[str, Any] = field(default_factory=dict)      @property     def required_context_keys(self) -&gt; set[str]:         required_fields = {fc.field_name for fc in self.context_fields.values() if not fc.is_optional}         return required_fields ```</p> <p>Add fields programmatically using <code>add_field(...)</code>:</p> <p>```60:87:src/pydantic2django/core/context.py     def add_field(         self,         field_name: str,         field_type_str: str,         is_optional: bool = False,         is_list: bool = False,         **kwargs: Any,     ) -&gt; None:         \"\"\"         Add a field to the context storage.         \"\"\"         field_context = FieldContext(             field_name=field_name,             field_type_str=field_type_str,             is_optional=is_optional,             is_list=is_list,             additional_metadata=kwargs,         )         self.context_fields[field_name] = field_context <pre><code>Validation and value access helpers:\n\n```88:159:src/pydantic2django/core/context.py\n    def validate_context(self, context: dict[str, Any]) -&gt; None:\n        ...\n\n    def get_field_type_str(self, field_name: str) -&gt; Optional[str]:\n        ...\n\n    def get_field_by_name(self, field_name: str) -&gt; Optional[FieldContext]:\n        ...\n\n    def to_conversion_dict(self) -&gt; dict[str, Any]:\n        ...\n\n    def set_value(self, field_name: str, value: Any) -&gt; None:\n        ...\n\n    def get_value(self, field_name: str) -&gt; Optional[Any]:\n        ...\n</code></pre></p> <p>Automatic import synthesis for generated classes uses the central <code>TypeHandler</code>:</p> <p>```160:201:src/pydantic2django/core/context.py     def get_required_imports(self) -&gt; dict[str, set[str]]:  # Return sets for auto-dedup         \"\"\"         Get all required imports for the context class fields using TypeHandler.         \"\"\"         imports: dict[str, set[str]] = {\"typing\": set(), \"custom\": set()}         ...         return imports <pre><code>## Generated context classes\n\nAt generation time, a lightweight, per-model dataclass is rendered from `context_class.py.j2`. It mirrors the `ModelContext` instance but provides a concrete, importable Python class specific to the Django model.\n\n```1:46:src/pydantic2django/django/templates/context_class.py.j2\n@dataclass\nclass {{ model_name }}Context(ModelContext):\n    \"\"\"\n    Context class for {{ model_name }}.\n    Contains non-serializable fields that need to be provided when converting from Django to Pydantic.\n    \"\"\"\n    model_name: str = \"{{ model_name }}\"\n    pydantic_class: type = {{ pydantic_class }}\n    django_model: type[models.Model]\n    context_fields: dict[str, FieldContext] = field(default_factory=dict)\n\n    def __post_init__(self):\n        \"\"\"Initialize context fields after instance creation.\"\"\"\n        {% for field in field_definitions %}\n        self.add_field(\n            field_name=\"{{ field.name }}\",\n            field_type={{ field.literal_type }},\n            is_optional={{ field.is_optional }},\n            is_list={{ field.is_list }},\n            additional_metadata={{ field.metadata }}\n        )\n        {% endfor %}\n{% if field_definitions %}\n    @classmethod\n    def create(cls,\n             django_model: Type[models.Model],\n{% for field in field_definitions %}\n             {{ field.name }}: {{ field.raw_type }},\n{% endfor %}\n             ):\n        \"\"\"\n        Create a context instance with the required field values.\n        \"\"\"\n        context = cls(django_model=django_model)\n        {% for field in field_definitions %}\n        context.set_value(\"{{ field.name }}\", {{ field.name }})\n        {% endfor %}\n        return context\n{% endif %}\n</code></pre></p> <p>The generator responsible for rendering these classes is <code>ContextClassGenerator</code>:</p> <p>```202:217:src/pydantic2django/core/context.py     @classmethod     def generate_context_class_code(cls, model_context: \"ModelContext\", jinja_env: Any | None = None) -&gt; str:         \"\"\"         Generate a string representation of the context class.         \"\"\"         generator = ContextClassGenerator(jinja_env=jinja_env)         return generator.generate_context_class(model_context) <pre><code>```285:329:src/pydantic2django/core/context.py\n    def generate_context_class(self, model_context: ModelContext) -&gt; str:\n        \"\"\"\n        Generates the Python code string for a context dataclass.\n        \"\"\"\n        template = self._load_template(\"context_class.py.j2\")\n        self.imports = model_context.get_required_imports()  # Get imports first\n        ...\n        return template.render(\n            model_name=model_name,\n            source_class_name=source_class_name,\n            source_module=model_context.source_class.__module__,\n            field_definitions=\"\\n\".join(field_definitions),\n            typing_imports=typing_imports_str,\n            custom_imports=custom_imports_list,\n        )\n</code></pre></p>"},{"location":"how_it_works/readme_context/#what-the-generated-class-gives-you","title":"What the generated class gives you","text":"<ul> <li>A stable, importable dataclass per Django model that knows which context fields exist</li> <li>A <code>create(...)</code> helper that constructs the context and sets required values</li> <li>Type-annotated fields, with <code>Optional</code>/<code>List</code> added when applicable</li> <li>Imports deduplicated and synthesized automatically via <code>TypeHandler</code></li> </ul>"},{"location":"how_it_works/readme_context/#typical-workflow","title":"Typical workflow","text":"<ol> <li>During generation, the factory builds a <code>ModelContext</code> for each model and computes required context fields.</li> <li><code>ContextClassGenerator</code> renders a concrete <code>&lt;ModelName&gt;Context</code> class from the template.</li> <li>When converting Django \u2192 source model, populate an instance of <code>&lt;ModelName&gt;Context</code> with required values.</li> <li>Use <code>to_conversion_dict()</code> to pass the captured values into the reconstruction step.</li> </ol> <p>Minimal example sketch:</p> <pre><code>from pydantic2django.core.context import ModelContext\n\n# model_context is prepared by the generation pipeline\nmodel_context: ModelContext = ...\n\n# At runtime, set values that cannot be read from Django fields alone\nmodel_context.set_value(\"token\", external_token)\nmodel_context.set_value(\"polymorphic_type\", \"SubTypeA\")\n\n# Later, feed context back into reconstruction\ncontext_values = model_context.to_conversion_dict()\n# conversion_layer.reconstruct_from_django(instance, context=context_values)\n</code></pre>"},{"location":"how_it_works/readme_context/#design-notes","title":"Design notes","text":"<ul> <li>Context data is not persisted in the database; it is runtime-only.</li> <li>Fields are explicit and validated: <code>required_context_keys</code> ensures consumers supply all required values.</li> <li>Import handling is centralized, so generated classes remain clean and minimal.</li> <li>The mechanism is agnostic to source type (Pydantic or Dataclass) via the generic <code>ModelContext[SourceModelType]</code>.</li> </ul> <p>See also: - <code>docs/how_it_works/core_modules_and_concepts.md</code> for an overview of <code>context.py</code> in the core module set. - <code>docs/how_it_works/readme_bidirectional_mapper.md</code> and <code>docs/how_it_works/readme_relationships.md</code> for related mapping and relationship infrastructure that may supply or require context values.</p>"},{"location":"how_it_works/readme_ingestion/","title":"Ingestion Pattern Across Modules","text":"<p>This document describes the ingestion mechanism used to create Django model instances from external representations (XML instances, Pydantic models, and Python dataclasses).</p>"},{"location":"how_it_works/readme_ingestion/#goals","title":"Goals","text":"<ul> <li>Provide a clear, consistent adapter pattern for \"ingest\" (external \u2192 Django) and \"emit\" (Django \u2192 external).</li> <li>Keep module-specific logic encapsulated while preserving a common mental model across modules.</li> </ul>"},{"location":"how_it_works/readme_ingestion/#common-pattern","title":"Common Pattern","text":"<ul> <li>Base classes expose high-level helpers:</li> <li>Pydantic: <code>from_pydantic(...)</code>, <code>to_pydantic(...)</code></li> <li>Dataclasses: <code>from_dataclass(...)</code>, <code>to_dataclass(...)</code></li> <li> <p>XML: <code>from_xml_dict(...)</code>, <code>from_xml_string(...)</code>, <code>to_xml_dict(...)</code>, <code>to_xml_string(...)</code></p> </li> <li> <p>For complex sources that need schema-aware walking (XML), a dedicated ingestor class is used:</p> </li> <li>XML: <code>XmlInstanceIngestor</code> parses an XML document with <code>lxml</code>, consults the parsed <code>XmlSchemaDefinition</code>/<code>XmlSchemaComplexType</code> graph, and materializes Django instances following the same relationship strategy used during model generation.</li> </ul>"},{"location":"how_it_works/readme_ingestion/#xml-ingestion","title":"XML Ingestion","text":"<p><code>XmlInstanceIngestor</code> lives in <code>pydantic2django.xmlschema.ingestor</code> and:</p> <ul> <li>Accepts:</li> <li><code>schema_files</code>: list of XSD files used during generation</li> <li><code>app_label</code>: Django app label where generated models live</li> <li>Resolves the root complex type via global <code>xs:element</code> or matching complex type name</li> <li>Creates the root Django instance and recursively processes nested elements:</li> <li>Simple elements/attributes \u2192 mapped to Django fields (camelCase \u2192 snake_case)</li> <li>Single nested complex types \u2192 stored as FK on parent</li> <li>Repeated nested complex types (<code>maxOccurs=\"unbounded\"</code>) \u2192 child instances created with FK to parent (<code>child_fk</code> strategy)</li> </ul>"},{"location":"how_it_works/readme_ingestion/#singleton-style-reuse-and-warmup","title":"Singleton-style reuse and warmup","text":"<p>Long-lived processes can avoid repeated schema discovery/model registration using process-wide helpers in <code>pydantic2django.xmlschema.ingestor</code>:</p> <ul> <li><code>warmup_xmlschema_models(schema_files, app_label=...)</code></li> <li>Pre-generates and registers XML-derived Django model classes in an in-memory registry</li> <li> <p>No-op on subsequent calls for the same <code>(app_label, schema file set + mtimes)</code></p> </li> <li> <p><code>get_shared_ingestor(schema_files=..., app_label=...)</code></p> </li> <li>Returns a shared <code>XmlInstanceIngestor</code> keyed by <code>(app_label, normalized schema paths, schema mtimes)</code></li> <li>Reuses the same instance across calls; entries are cached with an LRU + TTL policy</li> <li>Optional: <code>dynamic_model_fallback</code> (default: false). When set to <code>true</code>, the ingestor will fall back   to dynamically generated stand-in classes when an installed Django model cannot be found. When left   as default (<code>false</code>), the ingestor raises a detailed error if a discovered complex type has no installed   model.</li> <li>Default cache policy: LRU maxsize=4, TTL=600s; configurable via <code>set_ingestor_cache()</code></li> <li>Public controls:<ul> <li><code>set_ingestor_cache(maxsize: int | None = None, ttl_seconds: float | None = None)</code></li> <li><code>clear_ingestor_cache()</code> to clear (useful in tests)</li> <li><code>ingestor_cache_stats()</code> for diagnostics</li> </ul> </li> </ul> <p>Example (task runner):</p> <pre><code>from pydantic2django.xmlschema.ingestor import warmup_xmlschema_models, get_shared_ingestor\n\nSCHEMAS = [\n    \"/abs/path/MTConnectStreams_1.7.xsd\",\n    \"/abs/path/MTConnectDevices_1.7.xsd\",\n    \"/abs/path/MTConnectAssets_1.7.xsd\",\n    \"/abs/path/MTConnectError_1.7.xsd\",\n]\nAPP_LABEL = \"tests\"\n\n# At process start\nwarmup_xmlschema_models(SCHEMAS, app_label=APP_LABEL)\n\n# In each task (default: require installed models)\ningestor = get_shared_ingestor(schema_files=SCHEMAS, app_label=APP_LABEL)\nroot = ingestor.ingest_from_file(\"/abs/path/example.xml\", save=False)\n</code></pre> <p>Notes: - <code>save=True</code> requires concrete, migrated models in an installed app - Warmup/registry enables object instantiation without DB tables for dry-runs and analysis paths - You can tune cache behavior globally at process start:</p> <pre><code>from pydantic2django.xmlschema.ingestor import set_ingestor_cache\n\n# Keep up to 8 different ingestors for 10 minutes each\nset_ingestor_cache(maxsize=8, ttl_seconds=600)\n</code></pre>"},{"location":"how_it_works/readme_ingestion/#dynamic-model-fallback-explicit-opt-in","title":"Dynamic model fallback (explicit opt-in)","text":"<p>By default (<code>dynamic_model_fallback=False</code>), only installed models are used. If a discovered type has no installed model, ingestion raises a <code>ModelResolutionError</code> with the <code>app_label</code> and <code>model_name</code>.</p> <p>To support ephemeral, non-persisting workflows (e.g., dry-run validation, schema exploration), set <code>dynamic_model_fallback=True</code> to enable fallback to dynamically generated stand-in classes.</p> <p>Future extensions: - Namespace-scoped matching beyond simple local-name stripping - Post-pass to resolve <code>xs:key</code> / <code>xs:keyref</code> (e.g., ID/IDREF) lookups - Configurable relationship strategies at ingest time (e.g., JSON/M2M)</p>"},{"location":"how_it_works/readme_ingestion/#note-on-timescale-soft-references","title":"Note on Timescale soft references","text":"<p>When generation replaces illegal hypertable\u2192hypertable FKs with soft references (e.g., <code>UUIDField(db_index=True)</code>), the ingestor will persist the identifier value. If you need strong integrity across hypertables, add an application-level validator or a periodic job that checks the referenced IDs exist (or maintain a regular \u201clatest snapshot\u201d table which hypertables can FK to).</p>"},{"location":"how_it_works/readme_ingestion/#pydantic-and-dataclasses","title":"Pydantic and Dataclasses","text":"<ul> <li>Ingestion is simpler:</li> <li>Pydantic: <code>from_pydantic(model)</code> maps fields and stores types using <code>serialize_value</code>, <code>to_pydantic()</code> reconstructs via <code>model_validate</code></li> <li>Dataclasses: <code>from_dataclass(instance)</code> uses <code>dataclasses.asdict()</code>, <code>to_dataclass()</code> reconstructs using a direct constructor</li> </ul> <p>Both benefit from <code>ModelContext</code> when non-serializable values need to be provided round-trip.</p>"},{"location":"how_it_works/readme_ingestion/#consistency-checklist","title":"Consistency Checklist","text":"<ul> <li>Each module offers <code>from_*</code> and <code>to_*</code> helpers on its base class.</li> <li>Complex adapters (XML) provide an external ingestor class with a narrow API.</li> <li>Relationship handling during ingestion mirrors the generation factory\u2019s strategy.</li> <li>Name conversion rules are mirrored in both directions (e.g., XML name \u2194 Django field name).</li> </ul>"},{"location":"how_it_works/readme_ingestion/#identifier-normalization-and-enum-choices","title":"Identifier normalization and enum choices","text":"<p>When generating Django models from XML Schema, identifiers are normalized to valid Python/Django field names, and enumerations are emitted as <code>TextChoices</code>.</p> <ul> <li>Field name normalization:</li> <li>Namespace separators and punctuation (<code>:</code>, <code>.</code>, <code>-</code>, spaces) become <code>_</code>.</li> <li>CamelCase \u2192 snake_case (e.g., <code>camelCase</code> \u2192 <code>camel_case</code>).</li> <li>Invalid characters are replaced with <code>_</code>; multiple <code>_</code> are collapsed.</li> <li>Leading character is ensured to be a letter or <code>_</code> (prefix <code>_</code> if needed).</li> <li> <p>Lowercased output (e.g., <code>xlink:type</code> \u2192 <code>xlink_type</code>).</p> </li> <li> <p>Special cases:</p> </li> <li>Element named <code>id</code> that is not an XML <code>xs:ID</code> is renamed to <code>xml_id</code> to avoid Django primary key conflicts.</li> <li> <p>Names like <code>type</code> remain <code>type</code> (no suffixing), but their related enums use a proper class symbol (see below).</p> </li> <li> <p>Enum emission:</p> </li> <li>For <code>xs:simpleType</code> with <code>xs:restriction</code>/<code>xs:enumeration</code>, a <code>models.TextChoices</code> class is generated.</li> <li>The enum class name is derived from the (cleaned) field name in PascalCase (e.g., <code>type</code> \u2192 <code>Type</code>).</li> <li>Fields reference choices via the enum symbol: <code>choices=Type.choices</code> and default via <code>Type.MEMBER</code>.</li> </ul> <p>Example:</p> <pre><code>class CoordinateSystemType(Xml2DjangoBaseClass):\n    # Generated alongside the model\n    class Type(models.TextChoices):\n        CARTESIAN = \"cartesian\", \"Cartesian\"\n        POLAR = \"polar\", \"Polar\"\n\n    # Field named 'type' correctly references the enum class\n    type = models.CharField(choices=Type.choices, max_length=9)\n\nclass FileLocationType(Xml2DjangoBaseClass):\n    # Namespaced attribute xlink:type \u2192 xlink_type\n    xlink_type = models.CharField(max_length=255, null=True, blank=True)\n</code></pre> <p>Notes: - The normalization is applied uniformly to elements and attributes before code generation. - If you encounter a source name that still produces an invalid identifier, please report it with the original XML/XSD name so we can extend the rules.</p>"},{"location":"how_it_works/readme_relationships/","title":"Pydantic2Django Relationships Accessor Details","text":"<p>This document explains how relationships between source models (Pydantic/Dataclass) and Django models are tracked and resolved via <code>RelationshipMapper</code> and <code>RelationshipConversionAccessor</code>.</p>"},{"location":"how_it_works/readme_relationships/#core-data-structures","title":"Core Data Structures","text":"<p>```15:31:src/pydantic2django/core/relationships.py @dataclass class RelationshipMapper:     \"\"\"     Bidirectional mapper between source models (Pydantic/Dataclass) and Django models.     \"\"\"</p> <pre><code># Allow storing either source type\npydantic_model: Optional[type[BaseModel]] = None\ndataclass_model: Optional[type] = None\ndjango_model: Optional[type[models.Model]] = None\ncontext: Optional[ModelContext] = None  # Keep context if needed later\n\n@property\ndef source_model(self) -&gt; Optional[type]:\n    \"\"\"Return the source model (either Pydantic or Dataclass).\"\"\"\n    return self.pydantic_model or self.dataclass_model\n</code></pre> <p><code></code>37:41:src/pydantic2django/core/relationships.py @dataclass class RelationshipConversionAccessor:     available_relationships: list[RelationshipMapper] = field(default_factory=list) ```</p>"},{"location":"how_it_works/readme_relationships/#importing-and-exporting-mappings","title":"Importing and Exporting Mappings","text":"<p>```42:73:src/pydantic2django/core/relationships.py @classmethod def from_dict(cls, relationship_mapping_dict: dict) -&gt; \"RelationshipConversionAccessor\":     \"\"\"     Convert a dictionary of strings representing model qualified names to a RelationshipConversionAccessor     \"\"\"     available_relationships = []     for pydantic_mqn, django_mqn in relationship_mapping_dict.items():         ...         available_relationships.append(RelationshipMapper(pydantic_model, django_model, context=None))     return cls(available_relationships) <pre><code>```74:92:src/pydantic2django/core/relationships.py\ndef to_dict(self) -&gt; dict:\n    \"\"\"\n    Convert the relationships to a dictionary of strings representing\n    model qualified names for bidirectional conversion.\n    \"\"\"\n    relationship_mapping_dict = {}\n    for relationship in self.available_relationships:\n        ...\n        relationship_mapping_dict[pydantic_mqn] = django_mqn\n    return relationship_mapping_dict\n</code></pre></p> <p>Name formats for stable serialization:</p> <p>```93:104:src/pydantic2django/core/relationships.py def _get_pydantic_model_qualified_name(self, model: type[BaseModel] | None) -&gt; str:     if model is None:         return \"\"     return f\"{model.module}.{model.name}\"</p> <p>def _get_django_model_qualified_name(self, model: type[models.Model] | None) -&gt; str:     if model is None:         return \"\"     return f\"{model._meta.app_label}.{model.name}\" <pre><code>## Discovering and Adding Models\n\nList and query known models:\n\n```105:120:src/pydantic2django/core/relationships.py\n@property\ndef available_source_models(self) -&gt; list[type]:\n    ...\n\n@property\ndef available_django_models(self) -&gt; list[type[models.Model]]:\n    return [r.django_model for r in self.available_relationships if r.django_model is not None]\n</code></pre></p> <p>Add models incrementally:</p> <p>```121:129:src/pydantic2django/core/relationships.py def add_pydantic_model(self, model: type[BaseModel]) -&gt; None:     ...     self.available_relationships.append(RelationshipMapper(model, None, context=None)) <pre><code>```130:140:src/pydantic2django/core/relationships.py\ndef add_dataclass_model(self, model: type) -&gt; None:\n    ...\n    self.available_relationships.append(RelationshipMapper(dataclass_model=model))\n</code></pre></p> <p>```141:149:src/pydantic2django/core/relationships.py def add_django_model(self, model: type[models.Model]) -&gt; None:     ...     self.available_relationships.append(RelationshipMapper(None, None, model, context=None)) <pre><code>## Mapping and Lookup APIs\n\nLookups in either direction:\n\n```150:170:src/pydantic2django/core/relationships.py\ndef get_django_model_for_pydantic(self, pydantic_model: type[BaseModel]) -&gt; Optional[type[models.Model]]:\n    for relationship in self.available_relationships:\n        if relationship.pydantic_model == pydantic_model and relationship.django_model is not None:\n            return relationship.django_model\n    return None\n\ndef get_pydantic_model_for_django(self, django_model: type[models.Model]) -&gt; Optional[type[BaseModel]]:\n    for relationship in self.available_relationships:\n        if relationship.django_model == django_model and relationship.pydantic_model is not None:\n            return relationship.pydantic_model\n    return None\n</code></pre></p> <p>Dataclass to Django lookup:</p> <p>```172:181:src/pydantic2django/core/relationships.py def get_django_model_for_dataclass(self, dataclass_model: type) -&gt; Optional[type[models.Model]]:     for relationship in self.available_relationships:         if relationship.dataclass_model == dataclass_model and relationship.django_model is not None:             return relationship.django_model     return None <pre><code>Create or update mappings in a single call:\n\n```183:241:src/pydantic2django/core/relationships.py\ndef map_relationship(self, source_model: type, django_model: type[models.Model]) -&gt; None:\n    source_type = (\n        \"pydantic\" if isinstance(source_model, type) and issubclass(source_model, BaseModel) else\n        \"dataclass\" if dataclasses.is_dataclass(source_model) else\n        \"unknown\"\n    )\n    if source_type == \"unknown\":\n        logger.warning(...)\n        return\n    # Update existing or append a new RelationshipMapper\n    ...\n</code></pre></p> <p>Known source-model checks and name-based lookup:</p> <p>```242:253:src/pydantic2django/core/relationships.py def is_source_model_known(self, model: type) -&gt; bool:     is_pydantic = isinstance(model, type) and issubclass(model, BaseModel)     is_dataclass = dataclasses.is_dataclass(model)     ... <pre><code>```254:263:src/pydantic2django/core/relationships.py\ndef get_source_model_by_name(self, model_name: str) -&gt; Optional[type]:\n    for r in self.available_relationships:\n        if r.pydantic_model and r.pydantic_model.__name__ == model_name:\n            return r.pydantic_model\n        if r.dataclass_model and r.dataclass_model.__name__ == model_name:\n            return r.dataclass_model\n    return None\n</code></pre></p>"},{"location":"how_it_works/readme_relationships/#typical-usage-patterns","title":"Typical Usage Patterns","text":"<ul> <li>Initialize empty, add and map on the fly:</li> </ul> <pre><code>from pydantic import BaseModel\nfrom django.db import models\nfrom pydantic2django.core.relationships import RelationshipConversionAccessor\n\nclass PostModel(BaseModel):\n    ...\n\nclass BlogPost(models.Model):\n    ...\n\nrel = RelationshipConversionAccessor()\nrel.add_pydantic_model(PostModel)\nrel.add_django_model(BlogPost)\nrel.map_relationship(PostModel, BlogPost)\n\n# Lookups\nassert rel.get_django_model_for_pydantic(PostModel) is BlogPost\n</code></pre> <ul> <li>Serialize/restore mapping (e.g., persisted in JSONField):</li> </ul> <pre><code>mapping_dict = rel.to_dict()\nrel2 = RelationshipConversionAccessor.from_dict(mapping_dict)\n</code></pre>"},{"location":"how_it_works/readme_relationships/#integration-with-type-mapping","title":"Integration with Type Mapping","text":"<p>The <code>RelationshipConversionAccessor</code> is used by the type mapping system to: - Check if a source model is known before selecting relationship units (FK/M2M/O2O) - Resolve <code>to</code> targets (including self-references and app labels)</p> <p>See the relationship checks and resolution within the mapper where it calls into the accessor to determine known models and resolve target Django models.</p>"},{"location":"how_it_works/readme_relationships/#timescaledb-constraints-soft-references-and-fk-inversion","title":"TimescaleDB constraints, soft references, and FK inversion","text":"<p>TimescaleDB does not support ForeignKeys that point to hypertables. To respect this:</p> <ul> <li>The XML generator classifies models as either hypertable (time-series facts) or dimension (regular tables).</li> <li>Relationship rules during generation:</li> <li>Hypertable \u2192 Regular: generate a normal <code>ForeignKey</code>.</li> <li>Regular \u2192 Regular: generate a normal <code>ForeignKey</code>.</li> <li>Hypertable \u2192 Hypertable: generate a soft reference (e.g., <code>UUIDField(db_index=True)</code>) and leave referential validation to application code or background jobs.</li> <li>Regular \u2192 Hypertable (inverted): invert the relationship and emit the <code>ForeignKey</code> on the hypertable back to the regular dimension with <code>on_delete=SET_NULL, null=True, blank=True</code>.<ul> <li>Also auto-generate indexes on the hypertable to preserve performance:</li> <li><code>Index(fields=['&lt;dimension_field&gt;'])</code></li> <li><code>Index(fields=['&lt;dimension_field&gt;', '-time'])</code> when a <code>time</code> field exists on the hypertable.</li> </ul> </li> </ul> <p>Heuristics and helpers live under <code>pydantic2django.django.timescale</code>:</p> <ul> <li><code>classify_xml_complex_types(...)</code> produces a <code>{name: role}</code> map.</li> <li><code>should_soft_reference(source_name, target_name, roles)</code> returns <code>True</code> for hypertable\u2192hypertable edges.</li> </ul> <p>This keeps schemas Timescale-safe while preserving joinability to dimensions and the ability to validate soft references at the application layer. The inversion avoids invalid FKs to hypertables after hypertable creation drops the primary key on the base table.</p>"},{"location":"how_it_works/timescale/","title":"Timescale","text":""},{"location":"how_it_works/timescale/#timescale-integration","title":"Timescale integration","text":"<p>This document consolidates how Pydantic2Django integrates with TimescaleDB across model bases, relationship policy, classification heuristics, and migrations/testing.</p>"},{"location":"how_it_works/timescale/#goals","title":"Goals","text":"<ul> <li>Treat true time\u2011series entities as Timescale hypertables.</li> <li>Keep container/metadata structures as regular (dimension) tables.</li> <li>Prevent fragile/illegal FKs to hypertables while preserving joinability.</li> </ul>"},{"location":"how_it_works/timescale/#model-bases-and-constraints","title":"Model bases and constraints","text":"<ul> <li>Hypertables inherit from <code>XmlTimescaleBase</code>, which combines the project\u2019s base with <code>TimescaleModel</code> (from <code>django-timescaledb</code>).</li> <li>Because <code>django-timescaledb</code> drops the primary key during hypertable conversion, P2D guarantees a separate <code>UniqueConstraint</code> on <code>id</code> for all Timescale bases. This is attached both via <code>__init_subclass__</code> and a <code>class_prepared</code> signal to avoid edge cases. This allows downstream <code>ForeignKey</code> references to remain valid post\u2011conversion.</li> </ul>"},{"location":"how_it_works/timescale/#relationship-policy","title":"Relationship policy","text":"<ul> <li>Hypertable \u2192 hypertable FKs are disallowed. The generator emits a soft reference (e.g., <code>UUIDField(db_index=True, null=True, blank=True)</code>) instead.</li> <li>If a dimension would FK to a hypertable, the FK is inverted so the hypertable points to the dimension with <code>on_delete=SET_NULL</code> and appropriate indexes are added:</li> <li><code>Index(fields=['&lt;dimension&gt;'])</code></li> <li>If the hypertable also defines <code>time</code>, a composite: <code>Index(fields=['&lt;dimension&gt;', '-time'])</code></li> </ul>"},{"location":"how_it_works/timescale/#ingestion-timestamp-mapping","title":"Ingestion timestamp mapping","text":"<ul> <li>When a model inherits from <code>XmlTimescaleBase</code>, it requires a non-null <code>time</code> column.</li> <li>The XML ingestor maps common timestamp attributes to this canonical <code>time</code> field when a direct <code>time</code> value is not provided:</li> <li>Examples include <code>creationTime</code> (\u2192 <code>creation_time</code>), <code>timestamp</code>, <code>effectiveTime</code>, <code>dateTime</code>, <code>datetime</code>.</li> <li>For MTConnect Streams (v1.7), <code>Header/creationTime</code> is remapped into the model\u2019s <code>time</code> field to avoid NOT NULL violations and to preserve the canonical hypertable timestamp.</li> <li>This remapping occurs transparently during instance construction for both unsaved and DB-backed flows.</li> <li>Name normalization: attribute names are normalized to safe Django identifiers. CamelCase is converted to snake_case, and non-alphanumeric separators (e.g., <code>-</code>) are converted to <code>_</code>. This allows variants like <code>timeStamp</code> and <code>time-stamp</code> to map to <code>time</code>.</li> <li>If a Timescale-enabled model still lacks a <code>time</code> value at save time, ingestion raises a clear error indicating the expected aliases rather than failing later with a database NOT NULL violation.</li> </ul>"},{"location":"how_it_works/timescale/#classification-heuristics-xml","title":"Classification heuristics (XML)","text":"<p>We classify XML complex types into roles using a point system; a score \u2265 threshold (default 3) \u21d2 hypertable: - +2 if any time\u2011like element/attribute appears: names containing <code>time</code>, <code>timestamp</code>, <code>sequence</code>, <code>effectiveTime</code>, <code>sampleRate</code>, <code>date</code>, <code>datetime</code>. - +2 if the type name looks like an observation/event container: <code>Samples</code>, <code>Events</code>, <code>Condition</code>, <code>*Changed</code>, <code>*Removed</code>, <code>*Added</code>, <code>Streams</code>. - +1 if any element indicates unbounded/high\u2011cardinality growth (<code>is_list=True</code> or <code>maxOccurs=\"unbounded\"</code>). - \u22122 for definition/metadata\u2011like names: <code>*Definition*</code>, <code>*Definitions*</code>, <code>Constraints</code>, <code>Properties</code>, <code>Parameters</code>, <code>Header</code>, <code>Counts</code>, <code>Configuration</code>, <code>Description</code>, <code>Location</code>, <code>Limits</code>, <code>Reference</code>, <code>Relationships</code>.</p> <p>Container demotion, leaf promotion, and no\u2011time demotion:</p> <ul> <li>After the initial score pass, any type that lacks a direct time/date field but has descendant complex types that do contain such fields is treated as a container and demoted to a dimension.</li> <li>Those descendant types with direct time/date fields are promoted to hypertables.</li> <li>If the container itself has a direct time/date field, it remains a hypertable.</li> <li>New: any type that lacks a direct time/date field is demoted to a dimension even if its name/list scoring would have met the threshold (safety demotion to avoid hypertables without timestamps). Explicit overrides can opt the type back into hypertable.</li> </ul> <p>These rules tend to classify top\u2011level \u201cStreams\u201d/\u201cDevices\u201d containers as dimensions while promoting their leaf observation/time\u2011bearing children to hypertables.</p>"},{"location":"how_it_works/timescale/#generator-flow","title":"Generator flow","text":"<ol> <li>Discovery loads complex types and determines processing order.</li> <li>We classify each type via the heuristics above (including the safety demotion for no\u2011time types).</li> <li>During model assembly, if generated fields collide with base fields, we still assemble a bare model class so the finalize phase can inject inverted FKs and add indexes.</li> <li>Relationship finalization ensures:</li> <li>no hypertable\u2192hypertable FKs (soft refs instead),</li> <li>FK inversion from dimension\u2192hypertable to hypertable\u2192dimension,</li> <li>helpful indexes on hypertables.</li> </ol>"},{"location":"how_it_works/timescale/#disabling-timescale-in-generators","title":"Disabling Timescale in generators","text":"<ul> <li>Flag: <code>enable_timescale</code> (default: true) on all generators (<code>XmlSchemaDjangoModelGenerator</code>, <code>StaticPydanticModelGenerator</code>, <code>DataclassDjangoModelGenerator</code>).</li> <li>When <code>enable_timescale=False</code>:</li> <li>No Timescale classification runs.</li> <li>Generators never select a Timescale base; models use the configured non-Timescale base.</li> <li>Ingestion timestamp remapping/error only applies to models that actually declare a <code>time</code> field; if disabled at generation time, such fields won\u2019t exist unless added manually.</li> </ul> <p>Example:</p> <pre><code>gen = StaticPydanticModelGenerator(\n    output_path=\"/tmp/out.py\",\n    packages=[\"my_pkg\"],\n    app_label=\"my_app\",\n    enable_timescale=False,\n)\n</code></pre>"},{"location":"how_it_works/timescale/#generator-configuration-and-strict-mode","title":"Generator configuration and strict mode","text":"<ul> <li>Flags/params on <code>XmlSchemaDjangoModelGenerator</code>:</li> <li><code>timescale_overrides: dict[str, TimescaleRole] | None</code> \u2014 force specific types to <code>hypertable</code> or <code>dimension</code>.</li> <li><code>timescale_config: TimescaleConfig | None</code> \u2014 adjust threshold/knobs.</li> <li><code>timescale_strict: bool</code> \u2014 when true, generation fails if any type classified as <code>hypertable</code> lacks a direct time/date field. When false (default), such types are safely demoted to <code>dimension</code> by the heuristics.</li> </ul> <p>Notes: - Explicit <code>timescale_overrides</code> are respected and will not be auto\u2011demoted; combine with <code>timescale_strict=True</code> to surface mistakes early.</p>"},{"location":"how_it_works/timescale/#migrations-and-timescale-specifics","title":"Migrations and Timescale specifics","text":"<ul> <li>Timescale hypertable conversion drops the PK; our unique constraint on <code>id</code> preserves FK validity.</li> <li>django-timescaledb quirks:</li> <li>Primary key is dropped during hypertable conversion; ensure any downstream references use the preserved unique constraint on <code>id</code>.</li> <li>Hypertables require a non-null time column; the ingestor provides a best-effort mapping from XML attributes where appropriate.</li> <li>For integration testing, we run migrations against a local TimescaleDB container:</li> <li><code>scripts/start_timescaledb.sh</code> starts/ensures the container, creates the database, and enables the <code>timescaledb</code> extension.</li> <li><code>make test-timescale-integration</code> runs the FK/Timescale integration tests with appropriate environment variables (<code>DB_HOST</code>, <code>DB_PORT</code>, <code>DB_NAME</code>, <code>DB_USER</code>, <code>DB_PASSWORD</code>, <code>P2D_INTEGRATION_DB=1</code>).</li> </ul>"},{"location":"how_it_works/timescale/#tests","title":"Tests","text":"<ul> <li>Heuristics unit tests (<code>tests/timescale/test_heuristics_scoring.py</code>) cover:</li> <li>time/date/datetime field/attribute scoring,</li> <li>container demotion and leaf promotion,</li> <li>containers retaining hypertable role when they have direct time fields,</li> <li>list/unbounded growth scoring,</li> <li>explicit overrides.</li> <li>Relationship and generator behavior (<code>tests/timescale/test_generator_timescale.py</code>) verifies FK inversion and index creation.</li> <li>End\u2011to\u2011end generation and migration:</li> <li>Minimal FK to hypertable behavior (<code>tests/integration/test_timescale_fk.py</code>).</li> <li>Real XSD (<code>MTConnectAssets_2.4.xsd</code>) generation \u2192 makemigrations \u2192 migrate against TimescaleDB (<code>tests/integration/test_assets_generate_migrate.py</code>).</li> </ul>"},{"location":"how_it_works/timescale/#edge-cases-and-notes","title":"Edge cases and notes","text":"<ul> <li>Some complex/simple content patterns emit warnings (limited support). They do not affect FK uniqueness or hypertable conversion and can be iterated on independently.</li> <li>If your schema contains special containers that truly should remain hypertables, you can force roles via overrides at classification time.</li> </ul>"},{"location":"how_it_works/xmlschema_element_and_type_handling/","title":"XML Schema Handling: Elements, Attributes, Types, Relationships","text":"<p>This document explains how Pydantic2Django (P2D) interprets XML Schema (XSD) and generates Django models with validation and relationships. As context, the Open311 conventions (e.g., Parker/Spark/BadgerFish) focus on converting XML instances to JSON instances. P2D goes beyond serialization: it reads schema structure and produces a typed, relational model (Django ORM) with constraints, relationships, and optional Timescale integration. See Open311 background: <code>https://wiki.open311.org/JSON_and_XML_Conversion/#json-representation</code>.</p>"},{"location":"how_it_works/xmlschema_element_and_type_handling/#highlevel-flow","title":"High\u2011level flow","text":"<ol> <li>Parse XSD into internal models (<code>XmlSchemaDefinition</code>, <code>XmlSchemaComplexType</code>, <code>XmlSchemaSimpleType</code>, <code>XmlSchemaElement</code>, <code>XmlSchemaAttribute</code>, <code>XmlSchemaKey</code>, <code>XmlSchemaKeyRef</code>).</li> <li>Map XSD constructs to Django fields and relationships via the XMLSchema factory, applying validation and choices.</li> <li>Finalize cross\u2011model relationships (child FKs, optional M2M, soft references) and emit indexes.</li> <li>Render concrete Django model classes with imports and metadata.</li> </ol> <p>Key modules: - <code>src/pydantic2django/xmlschema/models.py</code> \u2014 in\u2011memory XSD model types. - <code>src/pydantic2django/xmlschema/factory.py</code> \u2014 element/attribute \u2192 Django field/relationship mapping. - <code>src/pydantic2django/xmlschema/generator.py</code> \u2014 orchestration, Timescale roles, rendering.</p>"},{"location":"how_it_works/xmlschema_element_and_type_handling/#elements-xselement","title":"Elements (<code>xs:element</code>)","text":"<ul> <li>Base simple types map to Django fields (see FIELD_TYPE_MAP):</li> <li>Strings \u2192 <code>CharField</code> (or <code>TextField</code> when unconstrained), numerics \u2192 integer/decimal fields, booleans \u2192 <code>BooleanField</code>, dates/times \u2192 <code>DateField</code>/<code>DateTimeField</code>/<code>TimeField</code>, binary \u2192 <code>BinaryField</code>.</li> <li>Occurrence/optionality:</li> <li><code>minOccurs=0</code> or <code>nillable=true</code> \u21d2 <code>null=True</code>, <code>blank=True</code>.</li> <li>Repeating elements (<code>maxOccurs&gt;1</code> or <code>unbounded</code>) are represented as relationships (see \u201cRepeating content and containers\u201d).</li> <li>Defaults/fixed values: populate Django <code>default</code> when present and safe.</li> <li>Inline restrictions map to validators and precision (see \u201cRestrictions and validators\u201d).</li> <li><code>xs:ID</code> elements become <code>CharField(primary_key=True)</code> to preserve identifiers.</li> <li><code>xs:IDREF</code> elements map to a <code>ForeignKey</code> when target resolution is possible (via <code>xs:key/xs:keyref</code>), otherwise a placeholder relation is emitted with a warning or a soft reference may be used depending on Timescale rules.</li> </ul>"},{"location":"how_it_works/xmlschema_element_and_type_handling/#complex-element-references-typetnssometype","title":"Complex element references (<code>type=\"tns:SomeType\"</code>)","text":"<p>When an element references a complex type: - Single nested complex element: by default becomes a <code>ForeignKey</code> to the child model unless Timescale rules require a soft reference or FK inversion. - Repeating complex element: represented as a child relation (FK from child \u2192 parent) or <code>ManyToManyField</code> when configured; parent may expose only a reverse accessor. JSON fallback is available. - If the type cannot be resolved to a generated model (e.g., filtered out), the field falls back to a <code>JSONField</code> for safety.</p>"},{"location":"how_it_works/xmlschema_element_and_type_handling/#attributes-xsattribute","title":"Attributes (<code>xs:attribute</code>)","text":"<ul> <li>Map similarly to simple elements: base type \u2192 Django field.</li> <li>Optional attributes \u21d2 <code>null=True</code>, <code>blank=True</code>.</li> <li>Defaults map to Django <code>default</code>.</li> <li><code>xs:ID</code> attributes can be <code>primary_key=True</code> when appropriate.</li> <li>Restrictions on attribute simple types feed into validators/length just like elements.</li> <li>Attribute groups (<code>xs:attributeGroup</code>) are resolved: named groups declared at the schema root are flattened into complex types wherever referenced, including within <code>complexContent/extension</code> and <code>simpleContent/extension</code>.</li> </ul>"},{"location":"how_it_works/xmlschema_element_and_type_handling/#simple-types-xssimpletype-and-restrictions","title":"Simple types (<code>xs:simpleType</code>) and restrictions","text":"<p>P2D preserves key XSD facets as Django validators/field options: - <code>enumeration</code> \u2192 <code>models.CharField(choices=TextChoices)</code>; choices classes are shared and emitted once per type; defaults may emit as enum members. - <code>pattern</code> \u2192 <code>RegexValidator</code>. - <code>minInclusive</code>/<code>maxInclusive</code> \u2192 <code>MinValueValidator</code>/<code>MaxValueValidator</code>. - <code>minExclusive</code>/<code>maxExclusive</code> \u2192 adjusted min/max validators. - <code>totalDigits</code>/<code>fractionDigits</code> \u2192 <code>max_digits</code>/<code>decimal_places</code> for decimals. - <code>maxLength</code> \u2192 <code>max_length</code> for string fields.</p> <p>Notes: - Unconstrained strings default to <code>TextField</code> (no <code>max_length</code>), while constrained strings retain <code>CharField</code>. - Validator imports are tracked and automatically added to the rendered file only when used.</p>"},{"location":"how_it_works/xmlschema_element_and_type_handling/#complex-types-xscomplextype","title":"Complex types (<code>xs:complexType</code>)","text":"<ul> <li>Content models: <code>sequence</code>, <code>choice</code>, <code>all</code> are parsed; child <code>xs:element</code>s are collected on the type and mapped per the rules above.</li> <li>Attributes declared on complex types are included.</li> <li><code>simpleContent</code> (minimal): attribute extensions are captured so attribute\u2011only types are not dropped.</li> <li><code>complexContent</code>:</li> <li><code>extension</code>: inherit base members; parse local particles and attributes.</li> <li><code>restriction</code>: currently treated extension\u2011like (inherits members); specific restriction semantics are not fully enforced yet.</li> </ul> <p>See <code>docs/how_it_works/xmlschema_notes.md</code> for current fidelity and planned enhancements.</p>"},{"location":"how_it_works/xmlschema_element_and_type_handling/#repeating-content-and-containers-wrappers","title":"Repeating content and containers (wrappers)","text":"<p>Tip: For large polymorphic/repeating regions you can enable Generic Entries mode to store children as <code>GenericEntry</code> rows via Django ContentTypes. See <code>docs/how_it_works/generic_entries_contenttypes.md</code>.</p> <p>For elements containing repeated complex children: - Default style is <code>child_fk</code>: no concrete list field on the parent; instead, the child receives a <code>ForeignKey</code> to the parent during the finalize pass, and the parent accesses children via Django reverse relation. Related names are disambiguated when the same child appears under multiple parents. - <code>m2m</code> style: when configured and a leaf child type is identifiable, emit <code>ManyToManyField</code> from the parent to the leaf. - JSON fallback: per strategy or when a referenced type will not be generated, the field becomes <code>JSONField</code>.</p> <p>Wrapper heuristics: - TitleCase element names or type names ending in <code>WrapperType</code> are treated as containers; we prefer <code>child_fk</code> and inject the child FK(s) on finalize.</p>"},{"location":"how_it_works/xmlschema_element_and_type_handling/#discovery-tradeoff-only-generate-toplevel-selected-types","title":"Discovery tradeoff: only generate top\u2011level, selected types","text":"<p>P2D\u2019s discovery deliberately scopes what becomes a Django model. Only top\u2011level <code>xs:complexType</code> (and types not filtered out) are generated; nested/inline/anonymous leaf types are not promoted automatically. This keeps the model surface manageable and reduces noise, but it has tradeoffs for deeply nested wrappers.</p> <p>Consequences: - Leaf/wrapper children that are not generated cannot be targets of FKs in the finalize pass. - We preserve content via JSON fallback or soft references, but lose some relational fidelity unless explicitly opted into generating those leaves.</p> <p>Example structure impacted by this rule:</p> <pre><code>graph TD\n    DeviceStreamType --&gt; ComponentStreamType\n    ComponentStreamType --&gt; SamplesType\n    SamplesType --&gt;|repeats| SampleType\n\n    classDef gen fill:#dfe,stroke:#090\n    classDef notGen fill:#fed,stroke:#c60\n\n    class DeviceStreamType,ComponentStreamType,SamplesType gen\n    class SampleType notGen\n</code></pre> <p>In this scenario, <code>SampleType</code> is a repeated leaf and may not be generated unless configured. The finalize pass must not attempt to emit FKs to <code>SampleType</code> when it is not included.</p>"},{"location":"how_it_works/xmlschema_element_and_type_handling/#refined-finalize-behavior-at-discovery-boundaries","title":"Refined finalize behavior at discovery boundaries","text":"<p>We refine <code>finalize_relationships</code> to respect discovery scope and degrade cleanly: - Gate: before injecting a FK, verify both child and parent exist in <code>carriers_by_name</code> and are included in <code>included_model_names</code>. - Fallbacks:   - If child or parent is missing, skip FK injection for that relation and prefer the already\u2011emitted JSON placeholder on the parent, or emit a soft UUID reference when Timescale policy requires. - Logging: emit a concise INFO/WARNING with enough context: child, parent, element name, and decision (skipped/soft/JSON) to aid debugging. - Config switch:   - Add a generator option (e.g., <code>auto_generate_missing_leaves: bool = False</code>), and a per\u2011factory flag, to allow promoting missing leaf types into generated models in future enhancements. Default remains False to avoid surprising schema growth.</p> <p>This approach preserves data (JSON fallback) and avoids broken relations, while giving teams an opt\u2011in path to richer relational models when desired.</p>"},{"location":"how_it_works/xmlschema_element_and_type_handling/#wrapper-behavior-current-implementation-and-flags","title":"Wrapper behavior: current implementation and flags","text":"<p>This section documents how wrappers and repeated leaves are mapped today, and which flags influence behavior.</p>"},{"location":"how_it_works/xmlschema_element_and_type_handling/#defaults-what-you-get-out-of-the-box","title":"Defaults (what you get out of the box)","text":"<ul> <li>Repeating complex elements (maxOccurs&gt;1/unbounded): one\u2011to\u2011many modeled as child\u2011side FK.</li> <li>The repeated leaf type receives a <code>ForeignKey</code> to its immediate wrapper/container.</li> <li>The wrapper/container exposes a reverse relation (via <code>related_name</code>).</li> <li>Single nested wrapper elements (TitleCase element or <code>*WrapperType</code>): treated as containers.</li> <li>The child wrapper model holds a <code>ForeignKey</code> to the immediate parent wrapper (not the other way around).</li> <li>For multi\u2011level wrappers, additional child FKs connect intermediate wrappers up to the outer container (resulting in a simple chain of FKs).</li> <li>Identity constraints (<code>xs:key</code>/<code>xs:keyref</code>): add additional FKs as specified, but do not invert the one\u2011to\u2011many direction implied by repetition.</li> <li>Discovery boundary respected: if a referenced child/leaf is not generated, FK injection is skipped; the parent retains JSON fallback or a soft reference (Timescale policies).</li> </ul>"},{"location":"how_it_works/xmlschema_element_and_type_handling/#flags-and-their-effects","title":"Flags and their effects","text":"<ul> <li><code>nested_relationship_strategy</code>: <code>auto</code> (default), <code>fk</code>, <code>json</code>.</li> <li><code>auto</code>: depth\u2011sensitive; shallow wrappers prefer <code>fk</code>, deeply nested content may fall back to <code>json</code>.</li> <li><code>fk</code>: force relational representation where supported.</li> <li><code>json</code>: force JSON representation for nested content.</li> <li><code>list_relationship_style</code>: <code>child_fk</code> (default), <code>m2m</code>, <code>json</code>.</li> <li><code>child_fk</code>: repeated leaf receives <code>ForeignKey</code> back to the container (default relational shape).</li> <li><code>m2m</code>: parent emits <code>ManyToManyField</code> to a resolvable leaf type (when identifiable), instead of child\u2011side FKs.</li> <li><code>json</code>: repeated content represented as <code>JSONField</code> on the parent.</li> <li>Timescale roles (<code>enable_timescale</code>, overrides, strict):</li> <li>Hypertable\u2192Hypertable edges become soft references (UUID with index) instead of FKs.</li> <li>Dimension\u2192Hypertable edges may invert: FK injected on the hypertable back to the dimension, with SET_NULL and helpful indexes.</li> <li><code>timescale_strict=True</code> can fail generation if a hypertable lacks a direct time field.</li> <li>Missing leaf policy (discovery boundary):</li> <li>FK injection is gated by generated types; when missing, we log a skip and keep JSON/soft fallback.</li> <li>Future: <code>auto_generate_missing_leaves</code> (planned) to opt\u2011in to promote leaves into generated models.</li> </ul>"},{"location":"how_it_works/xmlschema_element_and_type_handling/#related-names-and-field-names","title":"Related names and field names","text":"<ul> <li><code>related_name</code> derivation is based on the element path (e.g., <code>items</code> \u2192 <code>items</code>).</li> <li>When the same child appears under multiple parents, we suffix the parent name for uniqueness (e.g., <code>items_parenttype</code>).</li> <li>When multiple FKs to the same parent from one child are required (e.g., multiple wrapper elements), we also suffix by element (e.g., field name <code>parenttype_items</code>).</li> </ul>"},{"location":"how_it_works/xmlschema_element_and_type_handling/#does-the-parent-ever-hold-the-fk-to-the-leaf","title":"Does the parent ever hold the FK to the leaf?","text":"<ul> <li>Under <code>child_fk</code> (default): no. The child (repeated) side holds the FK; the parent sees only the reverse relation.</li> <li>Under <code>m2m</code>: the parent holds a <code>ManyToManyField</code> to the leaf.</li> <li>Under JSON strategy or when the leaf is not generated: the parent holds a <code>JSONField</code> instead of a relation.</li> </ul>"},{"location":"how_it_works/xmlschema_element_and_type_handling/#relationships-from-identity-constraints","title":"Relationships from identity constraints","text":"<ul> <li><code>xs:key</code>/<code>xs:keyref</code> are used to construct <code>ForeignKey</code> targets:</li> <li><code>keyref.selector</code> and <code>refer</code> help resolve the target model name (<code>&lt;X&gt;Type</code>, global element type, or direct complex type name).</li> <li><code>related_name</code> is derived from the selector path for readability.</li> <li>Optional references (<code>minOccurs=0</code>/<code>nillable</code>) use <code>on_delete=SET_NULL</code>, <code>null=True</code>, <code>blank=True</code>.</li> <li>If a keyref cannot be fully resolved, a safe placeholder is emitted and a log warning is produced; Timescale rules may cause a soft reference (UUID) instead of FK.</li> </ul>"},{"location":"how_it_works/xmlschema_element_and_type_handling/#timescaleaware-behavior-optional","title":"Timescale\u2011aware behavior (optional)","text":"<p>When enabled, the generator classifies complex types as <code>hypertable</code> or <code>dimension</code> and adjusts relationships accordingly: - Hypertable \u2192 Hypertable edges become soft references (UUID with index) instead of FKs. - Dimension \u2192 Hypertable edges are inverted: FK is created on the hypertable back to the dimension with <code>on_delete=SET_NULL</code>, and helpful indexes are emitted. - Strict mode can fail generation if a hypertable lacks a direct time field.</p> <p>See <code>docs/how_it_works/timescale.md</code> for details and ingestion timestamp mapping behavior.</p>"},{"location":"how_it_works/xmlschema_element_and_type_handling/#naming-namespaces-and-imports","title":"Naming, namespaces, and imports","text":"<ul> <li>Field names are sanitized to valid Django identifiers (normalize case, replace unsafe characters, avoid plain <code>id</code> unless it is a primary key).</li> <li>Type/element/attribute names are resolved using local names when prefixed (e.g., <code>tns:Foo</code> \u2192 <code>Foo</code>).</li> <li>Validator usage and extra imports are tracked during mapping to avoid unused imports.</li> </ul>"},{"location":"how_it_works/xmlschema_element_and_type_handling/#unsupported-or-partially-supported-xsd-constructs","title":"Unsupported or partially supported XSD constructs","text":"<p>The following are either mapped conservatively (often via JSON) or not yet fully supported: - <code>xs:group</code>. - <code>xs:any</code>, <code>xs:anyAttribute</code> (consider JSON or key\u2011value side tables; future configurable handling). - Strict restriction enforcement for <code>complexContent/restriction</code> (occurrence tightening, prohibitions). - Mixed content and advanced substitution-group semantics beyond basic head\u2192member expansion.</p> <p>Where fidelity is limited, P2D prefers safe fallbacks (e.g., <code>JSONField</code>) and clear logging. See <code>xmlschema_notes.md</code> for roadmap items.</p>"},{"location":"how_it_works/xmlschema_element_and_type_handling/#configuration-knobs","title":"Configuration knobs","text":"<ul> <li><code>nested_relationship_strategy</code>: <code>auto</code> (default), <code>fk</code>, or <code>json</code>.</li> <li><code>list_relationship_style</code>: <code>child_fk</code> (default), <code>m2m</code>, or <code>json</code>.</li> <li><code>nesting_depth_threshold</code>: depth limit for <code>auto</code> strategy.</li> <li><code>enable_timescale</code>: enable classification and Timescale\u2011safe relationship policies.</li> <li><code>timescale_overrides</code>, <code>timescale_config</code>, <code>timescale_strict</code>: fine\u2011tune roles and safety checks.</li> </ul>"},{"location":"how_it_works/xmlschema_element_and_type_handling/#practical-outcomes-vs-open311-conventions","title":"Practical outcomes vs. Open311 conventions","text":"<ul> <li>Open311 conventions define how an XML instance becomes a JSON instance (attributes kept/dropped, arrays inferred, mixed content encoding). They do not generate a database schema or enforce constraints.</li> <li>P2D generates a relational schema with validators, choices, keys/refs as relationships, wrapper/container handling, and Timescale\u2011aware constraints. JSON is used as a deliberate fallback for complex/nested areas that are better denormalized or unsupported.</li> </ul>"},{"location":"how_it_works/xmlschema_element_and_type_handling/#references","title":"References","text":"<ul> <li>Parser/model types: <code>src/pydantic2django/xmlschema/models.py</code></li> <li>Field and relationship mapping: <code>src/pydantic2django/xmlschema/factory.py</code></li> <li>Generator and finalization: <code>src/pydantic2django/xmlschema/generator.py</code></li> <li>Timescale integration: <code>docs/how_it_works/timescale.md</code></li> <li>Notes and roadmap: <code>docs/how_it_works/xmlschema_notes.md</code></li> </ul>"},{"location":"how_it_works/xmlschema_element_and_type_handling/#mtconnect-streams-quick-guide","title":"MTConnect Streams: quick guide","text":"<p>This short guide shows how MTConnect Streams (2.4) maps to generated Django models and how to traverse wrappers and observations.</p> <p>1) Generate models (downloads XSDs, writes models, and creates migrations):</p> <pre><code>uv run python examples/mtconnect_example/mtconnect_xml_schema_example.py --version 2.4\n</code></pre> <p>2) Import generated models in a Django shell and navigate:</p> <pre><code>from examples.mtconnect_example.generated_models.v2_4.mtconnect_streams import models as s\n\n# Pick a component stream\ncs = s.ComponentStreamType.objects.first()\n\n# Wrapper relations: reverse managers from wrappers (e.g., Samples, Events)\nsamples_wrappers = cs.samples.all()\n\n# Concrete observation models (substitution groups expanded)\n# Example: PositionDataSetType (availability depends on the schema/content)\npos = s.PositionDataSetType.objects.filter(componentstreamtype=cs).first()\nif pos:\n    # Common attributes resolved from attribute groups\n    print(pos.timestamp, getattr(pos, \"sub_type\", None))\n\n# Tip: Explore available reverse accessors and fields\nprint([f.name for f in cs._meta.get_fields()])\n</code></pre> <p>Notes - Substitution groups are expanded: observation heads under wrappers (Samples/Events/Condition) produce concrete classes like <code>PositionDataSetType</code>, <code>TemperatureDataSetType</code>, etc. - Attribute groups are resolved: observation classes include fields like <code>timestamp</code>, <code>subType</code>/<code>sub_type</code>, <code>dataItemId</code>, etc., as defined by the schema. - Wrapper elements (e.g., <code>SamplesType</code>, <code>EventsType</code>) are treated as containers; you typically traverse from <code>ComponentStreamType</code> via reverse relations to wrappers and then to observations, or directly query concrete observation classes filtered by the parent.</p>"},{"location":"how_it_works/xmlschema_notes/","title":"XML Schema Notes: complexType Handling","text":"<p>This note documents the current level of <code>xs:complexType</code> support in the XML Schema parser and outlines next steps for improved fidelity.</p>"},{"location":"how_it_works/xmlschema_notes/#whats-supported-today","title":"What\u2019s supported today","text":"<ul> <li>Content models inside <code>xs:complexType</code>:</li> <li><code>xs:sequence</code>, <code>xs:choice</code>, <code>xs:all</code> are parsed and their child <code>xs:element</code> entries are collected onto the type.</li> <li><code>xs:attribute</code> entries on the type are parsed and attached.</li> <li><code>xs:simpleContent</code> (minimal):</li> <li>We capture attributes declared on the <code>xs:extension</code> node so attribute-only types aren\u2019t dropped.</li> <li>Other <code>simpleContent</code> patterns remain limited and may emit warnings.</li> <li><code>xs:complexContent</code> (minimal but usable):</li> <li><code>xs:extension</code>: the derived type inherits elements and attributes from the base complex type, then parses any locally declared particles (<code>sequence</code>/<code>choice</code>/<code>all</code>) and attributes. This enables basic inheritance in common schemas.</li> <li><code>xs:restriction</code>: currently treated like an extension (inherits base members); local particles/attributes are parsed. Restriction facets are not enforced yet. A warning is logged to indicate limited handling.</li> </ul> <p>Notes and limits: - Base type lookup assumes the same schema target namespace. Cross-schema/import resolution is limited. - <code>xs:group</code>, <code>xs:attributeGroup</code>, <code>xs:any</code>, and <code>xs:anyAttribute</code> are not yet handled. - Restriction semantics (e.g., occurrence tightening, element/attribute prohibition) are not enforced. - Mixed content and substitution groups are not modeled. - Identity constraints (<code>xs:key</code>/<code>xs:keyref</code>) are parsed at the schema level but not enforced during ingest yet.</p>"},{"location":"how_it_works/xmlschema_notes/#impact-on-ingestion","title":"Impact on ingestion","text":"<ul> <li>Derived complex types appear with inherited elements/attributes, so ingestion can proceed for most extension-based hierarchies.</li> <li>Because restriction rules aren\u2019t enforced, ingest may accept instances that a strict validator would reject. If strict validation is required, keep producer-side validation enabled or post-validate with a dedicated XML library.</li> <li>Timescale-aware timestamp mapping: when a generated model inherits from <code>XmlTimescaleBase</code> (requiring a non-null <code>time</code> column), the ingestor remaps common XML timestamp attributes to the canonical <code>time</code> field when not explicitly set. For example, <code>creationTime</code> is mapped to <code>time</code> for MTConnect Streams headers.</li> </ul>"},{"location":"how_it_works/xmlschema_notes/#discovery-promotion-demotion-and-collapsing","title":"Discovery, promotion, demotion, and collapsing","text":"<p>This section explains how P2D decides which XSD types become concrete Django models (promotion), when we intentionally avoid creating a model and store data in a denormalized form (demotion), and how we resolve relationship direction across wrapper chains (collapsing).</p>"},{"location":"how_it_works/xmlschema_notes/#discovery-and-promotion","title":"Discovery and promotion","text":"<ul> <li>By default, only top\u2011level, selected <code>xs:complexType</code> entries are promoted to Django models. Nested/inline/anonymous leaf types are not automatically promoted.</li> <li>Promotion scope is controlled by discovery filters and per\u2011run options. This keeps generated models concise and avoids exploding the schema surface.</li> </ul> <p>Mermaid: promoted vs. not promoted</p> <pre><code>graph TD\n    A[DeviceStreamType] --&gt; B[ComponentStreamType]\n    B --&gt; C[SamplesType]\n    C --&gt;|repeats| D[SampleType]\n\n    classDef gen fill:#dfe,stroke:#090\n    classDef notGen fill:#fed,stroke:#c60\n\n    class A,B,C gen\n    class D notGen\n</code></pre> <p>In this example, <code>SampleType</code> is a repeated leaf and may not be generated unless explicitly selected. The system must therefore avoid emitting FKs that target <code>SampleType</code> unless it is in scope.</p>"},{"location":"how_it_works/xmlschema_notes/#demotion-safe-fallback","title":"Demotion (safe fallback)","text":"<p>When a referenced type is outside discovery scope or cannot be resolved to a generated model, we demote that structure to a safe representation:</p> <ul> <li>JSON fallback on the parent field for nested structures (used during the element mapping stage).</li> <li>Soft references (UUID with index) when Timescale rules indicate FK inversion or hypertable\u2192hypertable safety.</li> </ul> <p>Demotion preserves data while sacrificing some relational fidelity, and is logged to aid debugging. This is preferable to emitting broken FKs.</p>"},{"location":"how_it_works/xmlschema_notes/#collapsing-wrapper-chains-relationship-direction","title":"Collapsing wrapper chains (relationship direction)","text":"<p>P2D uses wrapper heuristics and a finalize pass to place FKs on children while keeping parents minimal:</p> <ul> <li>Repeating complex children: default <code>child_fk</code> places FK on the child back to the parent.</li> <li>Single wrapper elements (TitleCase or <code>*WrapperType</code>): treat as containers and inject child\u2011side FK(s) to the immediate wrapper (not the grandparent), unless an M2M configuration or Timescale policy applies.</li> <li>Deep wrapper chains: we may \u201ccollapse\u201d chains to an appropriate ancestor, but only when it makes sense and does not cross discovery boundaries. Immediate wrapper preference is maintained for direct elements.</li> </ul> <p>Mermaid: collapsing behavior (immediate wrapper vs. top ancestor)</p> <pre><code>graph TD\n    A[DeviceStreamType] --&gt; B[ComponentStreamType]\n    B --&gt; C[SamplesType]\n    C --&gt;|repeats| D[SampleType]\n\n    D -.child_fk.-&gt; C\n    %% In some configurations, a leaf could collapse further, but P2D prefers immediate wrapper here\n\n    classDef gen fill:#dfe,stroke:#090\n    classDef notGen fill:#fed,stroke:#c60\n    class A,B,C gen\n    class D notGen\n</code></pre> <p>Finalization respects discovery boundaries and will skip FK injection whenever either side is not generated, preferring the previously emitted JSON placeholder or soft reference.</p>"},{"location":"how_it_works/xmlschema_notes/#configuration-knobs-related","title":"Configuration knobs (related)","text":"<ul> <li><code>list_relationship_style</code>: <code>child_fk</code> (default), <code>m2m</code>, or <code>json</code>.</li> <li><code>nested_relationship_strategy</code>: <code>fk</code>, <code>json</code>, or <code>auto</code>.</li> <li>Timescale flags to invert or soften relationships for hypertables.</li> <li>(Planned) <code>auto_generate_missing_leaves</code>: opt\u2011in to promote leaf types on demand; default False to avoid uncontrolled schema growth.</li> </ul>"},{"location":"how_it_works/xmlschema_notes/#recommended-next-steps","title":"Recommended next steps","text":"<ol> <li>Complex content completeness</li> <li>Implement <code>xs:group</code> and <code>xs:attributeGroup</code> expansion.</li> <li>Add support for <code>xs:any</code> / <code>xs:anyAttribute</code> (map to flexible fields, e.g., JSON or key-value side tables; make this configurable).</li> <li>Resolve base types across imported schemas and different namespaces.</li> <li>Proper restriction handling</li> <li>Enforce occurrence constraints and member prohibitions for <code>xs:restriction</code>.</li> <li>Apply facet-driven narrowing where applicable.</li> <li>Mixed content and substitution groups</li> <li>Add mixed content support (e.g., serialize as text + children, or a structured representation).</li> <li>Implement substitution group resolution for element polymorphism.</li> <li>Identity constraints</li> <li>Add a post-pass to resolve <code>xs:key</code> / <code>xs:keyref</code> (e.g., ID/IDREF) during ingestion.</li> <li>Diagnostics &amp; configuration</li> <li>Reduce warnings once features are implemented; add a strict mode to fail on unsupported constructs.</li> <li>Expose per-feature toggles (e.g., treat restriction as error vs. extension-like fallback).</li> </ol>"},{"location":"how_it_works/xmlschema_notes/#practical-guidance","title":"Practical guidance","text":"<ul> <li>If you encounter warnings about unsupported <code>complexContent</code> but your schema mostly uses <code>xs:extension</code>, the current implementation will usually be sufficient for ingesting instances.</li> <li>For schemas relying heavily on <code>xs:restriction</code>, consider flattening types or relaxing validation on the ingest path until restriction semantics are implemented.</li> <li>You can silence specific parser warnings by adjusting the logger for <code>pydantic2django.xmlschema.parser</code> if the limits don\u2019t affect your ingestion semantics.</li> </ul>"},{"location":"how_it_works/xmlschema_notes/#references","title":"References","text":"<ul> <li>Implementation locations:</li> <li>Parser: <code>src/pydantic2django/xmlschema/parser.py</code></li> <li>Ingestor: <code>src/pydantic2django/xmlschema/ingestor.py</code></li> </ul>"},{"location":"how_to_use/bidirectional_generation/","title":"Bidirectional generation","text":""},{"location":"how_to_use/bidirectional_generation/#how-to-generate-static-django-models-and-convert-bidirectionally","title":"How to generate static Django models and convert bidirectionally","text":"<p>This guide shows how to turn several source types into static Django models and convert data back and forth: - Pydantic \u21c4 Django - Dataclass \u21c4 Django - TypedClass \u21c4 Django - XML Schema \u21c4 Django/XML - Django \u21c4 Pydantic (dynamic at runtime)</p> <p>All generators write a single <code>generated_models.py</code> that contains Django model classes and convenience helpers. See templates in <code>src/pydantic2django/django/templates/</code> for the rendered structure.</p>"},{"location":"how_to_use/bidirectional_generation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Configure Django before using any generator (minimal example shown below).</li> <li>Python 3.11+, Pydantic v2, UV/pytest per project guidelines.</li> </ul> <pre><code>import django\nfrom django.conf import settings\n\nif not settings.configured:\n    settings.configure(\n        INSTALLED_APPS=[\n            \"django.contrib.contenttypes\",\n            \"django.contrib.auth\",\n            # Add your app label if needed\n        ],\n        DATABASES={\n            \"default\": {\"ENGINE\": \"django.db.backends.sqlite3\", \"NAME\": \":memory:\"},\n        },\n        DEFAULT_AUTO_FIELD=\"django.db.models.BigAutoField\",\n    )\n    django.setup()\n</code></pre>"},{"location":"how_to_use/bidirectional_generation/#pydantic-django","title":"Pydantic \u21c4 Django","text":"<p>Use <code>StaticPydanticModelGenerator</code> to scan packages for <code>BaseModel</code> classes and generate static Django models. Generated classes inherit <code>Pydantic2DjangoBaseClass</code> and include helpers like <code>.from_pydantic(...)</code> and <code>.to_pydantic(...)</code>.</p> <pre><code>from pydantic import BaseModel, Field\nfrom pydantic2django.pydantic.generator import StaticPydanticModelGenerator\n\n# 1) Generate static Django models from your Pydantic package\ngen = StaticPydanticModelGenerator(\n    output_path=\"generated_models.py\",\n    packages=[\"your_pydantic_pkg\"],\n    app_label=\"your_app\",\n    filter_function=None,  # or a callable to include/exclude models\n    module_mappings={\"__main__\": \"your_app.models\"},  # optional import fixups\n    verbose=True,\n)\ngen.generate()\n\n# 2) Use the generated models\nfrom your_pydantic_pkg import User as PydUser\nfrom generated_models import DjangoUser  # class name prefixed; see output\n\np = PydUser(id=1, email=\"john@example.com\", name=\"John\")\ndj = DjangoUser.from_pydantic(p)\ndj.save()\n\n# Convert back\nround_trip = dj.to_pydantic()\n</code></pre> <p>Notes: - Choices map to <code>Literal[...]</code> in Pydantic with original choices preserved in <code>json_schema_extra</code>. - Relationships, JSON, auto PKs, and file/image fields are handled by the bidirectional mapper.</p>"},{"location":"how_to_use/bidirectional_generation/#dataclass-django","title":"Dataclass \u21c4 Django","text":"<p>Use <code>DataclassDjangoModelGenerator</code> to generate static Django models from Python dataclasses. Generated classes inherit <code>Dataclass2DjangoBaseClass</code> and include <code>.from_dataclass(...)</code> and <code>.to_dataclass(...)</code>.</p> <pre><code>import dataclasses\nfrom pydantic2django.dataclass.generator import DataclassDjangoModelGenerator\n\n@dataclasses.dataclass\nclass Person:\n    id: int\n    name: str\n\ngen = DataclassDjangoModelGenerator(\n    output_path=\"generated_models.py\",\n    app_label=\"dc_app\",\n    packages=[\"your_dc_pkg\"],\n    filter_function=lambda cls: dataclasses.is_dataclass(cls),\n    verbose=True,\n)\ngen.generate()\n\nfrom your_dc_pkg import Person as DCPerson\nfrom generated_models import DjangoPerson\n\ndc = DCPerson(id=1, name=\"Ada\")\ndj = DjangoPerson.from_dataclass(dc)\ndj.save()\n\nback = dj.to_dataclass()\n</code></pre>"},{"location":"how_to_use/bidirectional_generation/#typedclass-django","title":"TypedClass \u21c4 Django","text":"<p>Use <code>TypedClassDjangoModelGenerator</code> for plain Python classes with type hints you want to map to Django fields. Generated classes inherit <code>TypedClass2DjangoBaseClass</code> and include <code>.from_typedclass(...)</code> and <code>.to_typedclass(...)</code>.</p> <pre><code>from pydantic2django.typedclass.generator import TypedClassDjangoModelGenerator\n\nclass Settings:\n    def __init__(self, retries: int, region: str):\n        self.retries = retries\n        self.region = region\n\ngen = TypedClassDjangoModelGenerator(\n    output_path=\"generated_models.py\",\n    app_label=\"typed_app\",\n    packages=[\"your_typed_pkg\"],\n    filter_function=None,\n    verbose=True,\n)\ngen.generate()\n\nfrom generated_models import DjangoSettings\n\ncfg = Settings(retries=3, region=\"us-east-1\")\ndj = DjangoSettings.from_typedclass(cfg)\ndj.save()\n\nrestored = dj.to_typedclass()\n</code></pre>"},{"location":"how_to_use/bidirectional_generation/#xml-schema-djangoxml","title":"XML Schema \u21c4 Django/XML","text":"<p>Use <code>XmlSchemaDjangoModelGenerator</code> to parse <code>.xsd</code> files and generate static Django models that inherit <code>Xml2DjangoBaseClass</code>. These provide XML-focused helpers like <code>.from_xml_dict(...)</code>, <code>.to_xml_dict(...)</code> and <code>.to_xml_string(...)</code>.</p> <pre><code>from pydantic2django.xmlschema.generator import XmlSchemaDjangoModelGenerator\n\ngen = XmlSchemaDjangoModelGenerator(\n    schema_files=[\"tests/xmlschema/fixtures/simple_schema.xsd\"],\n    output_path=\"generated_models.py\",\n    app_label=\"xsd_app\",\n    nested_relationship_strategy=\"auto\",  # \"fk\" | \"json\" | \"auto\"\n    list_relationship_style=\"child_fk\",   # \"child_fk\" | \"m2m\" | \"json\"\n    verbose=True,\n)\ngen.generate()\n\nfrom generated_models import Address  # example class from the schema\n\naddr = Address.from_xml_dict({\"street\": \"123 Main\", \"zipCode\": \"94107\"})\nxml = addr.to_xml_string()\n</code></pre>"},{"location":"how_to_use/bidirectional_generation/#overriding-the-base-model-class","title":"Overriding the base model class","text":"<p>All generators keep their intended base class on <code>gen.base_model_class</code>. You can override it before calling <code>generate()</code> to inject your own abstract base or enable integrations.</p> <p>Example: enable TimescaleDB hypertables for XML-generated models using the provided <code>XmlTimescaleBase</code> (which combines <code>Xml2DjangoBaseClass</code> and <code>TimescaleModel</code>):</p> <pre><code>from pydantic2django.django.models import XmlTimescaleBase\nfrom pydantic2django.xmlschema.generator import XmlSchemaDjangoModelGenerator\n\ngen = XmlSchemaDjangoModelGenerator(\n    schema_files=[\"...\"],\n    output_path=\"generated_models.py\",\n    app_label=\"xsd_app\",\n)\ngen.base_model_class = XmlTimescaleBase\ngen.generate()\n</code></pre> <p>If a generator exposes a <code>base_model_class=</code> constructor argument, you can pass it directly. Otherwise, assigning to <code>gen.base_model_class</code> prior to <code>generate()</code> is supported.</p> <p>Note: TimescaleDB expects a <code>time</code> column; see the <code>django-timescaledb</code> README for details (<code>https://github.com/jamessewell/django-timescaledb?tab=readme-ov-file</code>).</p>"},{"location":"how_to_use/bidirectional_generation/#django-pydantic-dynamic","title":"Django \u21c4 Pydantic (dynamic)","text":"<p>To project a Django model to a Pydantic model (and back) at runtime, use <code>DjangoPydanticConverter</code>. This does not write static files; it generates a Pydantic model class on the fly using the same bidirectional mapper.</p> <pre><code>from pydantic2django.django.conversion import DjangoPydanticConverter\nfrom your_django_app.models import User\n\nconv = DjangoPydanticConverter(User)\nPydUser = conv.generated_pydantic_model\n\nu = User.objects.get(pk=1)\np = conv.to_pydantic(u)\n\n# Modify and persist back\nu2 = conv.to_django(p)  # updates or creates\n</code></pre>"},{"location":"how_to_use/bidirectional_generation/#common-options-and-tips","title":"Common options and tips","text":"<ul> <li>Filtering: <code>filter_function</code> lets you keep only specific source models.</li> <li>Import fixes: <code>module_mappings</code> rewrites imports (e.g., replace <code>__main__</code> with <code>your_app.models</code>).</li> <li>Relationships: Pydantic/Dataclass/TypedClass generators resolve dependencies and order automatically.</li> <li>Context fields: Pydantic-only context values are surfaced via generated context classes; pass them when calling <code>.to_pydantic(...)</code> if required by your model.</li> <li>Templates: Jinja2 templates under <code>django/templates/</code> control the generated file layout.</li> </ul>"},{"location":"how_to_use/ingestor_validation/","title":"Ingestor validation","text":""},{"location":"how_to_use/ingestor_validation/#using-the-xml-ingestor-strict-mode-contract-checks-and-timescale","title":"Using the XML Ingestor: Strict Mode, Contract Checks, and Timescale","text":"<p>This guide shows how to enable strict ingestion, run contract validation, and handle Timescale-specific requirements.</p>"},{"location":"how_to_use/ingestor_validation/#enable-strict-ingestion","title":"Enable strict ingestion","text":"<pre><code>from pydantic2django.xmlschema import get_shared_ingestor\n\ningestor = get_shared_ingestor(\n    schema_files=[\"/path/to/schema.xsd\"],\n    app_label=\"myapp\",\n    dynamic_model_fallback=False,  # prefer installed models in production\n    strict=True,                   # fail fast on drift\n)\n</code></pre> <p>Strict mode will:</p> <ul> <li>Error on unexpected XML child elements or unmapped attributes</li> <li>Enforce contract checks (if you call <code>validate_models(strict=True)</code>)</li> <li>Require <code>time</code> only for Timescale models</li> </ul>"},{"location":"how_to_use/ingestor_validation/#validate-models-before-ingesting","title":"Validate models before ingesting","text":"<pre><code>issues = ingestor.validate_models(strict=False)\nif issues:\n    # Inspect and fix issues, or enforce:\n    ingestor.validate_models(strict=True)  # raises SchemaSyncError on drift\n</code></pre> <p>Typical remediation: \u201cSchema and static model are out of sync; verify schema and/or regenerate static Django models and re-migrate.\u201d</p>"},{"location":"how_to_use/ingestor_validation/#timescale-only-time-requirement","title":"Timescale-only <code>time</code> requirement","text":"<p>The ingestor always tries to remap common timestamp aliases to <code>time</code> when the model declares a <code>time</code> field. However, missing <code>time</code> becomes an error only if the model inherits a Timescale base:</p> <ul> <li>Timescale model, missing timestamp \u2192 raises <code>TimeseriesTimestampMissingError</code></li> <li>Non-Timescale model, missing timestamp \u2192 no error (still attempts alias remapping)</li> </ul>"},{"location":"how_to_use/ingestor_validation/#recommended-settings-for-production","title":"Recommended settings for production","text":"<ul> <li>Use <code>dynamic_model_fallback=False</code> to ensure only concrete installed models are used</li> <li>Use <code>strict=True</code> to catch drift early</li> <li>Run <code>validate_models(strict=True)</code> during startup or CI</li> <li>Set a modest ingestor cache TTL or clear cache on deployments if schemas change</li> </ul>"},{"location":"how_to_use/integrate_into_django_app/","title":"Integrate into django app","text":""},{"location":"how_to_use/integrate_into_django_app/#how-to-integrate-generated-models-into-your-django-app","title":"How to integrate generated models into your Django app","text":"<p>This guide shows how to place the generated static models file into your Django app, wire it up, and ship it with migrations.</p>"},{"location":"how_to_use/integrate_into_django_app/#1-choose-where-to-write-the-generated-file","title":"1) Choose where to write the generated file","text":"<p>Recommended: generate into your app package and set the <code>app_label</code> to your app name. Example for app <code>myapp</code>:</p> <pre><code>from pydantic2django.pydantic.generator import StaticPydanticModelGenerator\n\ngen = StaticPydanticModelGenerator(\n    output_path=\"myapp/models_generated.py\",   # inside your app\n    app_label=\"myapp\",                         # matches your app label\n    packages=[\"your_models_pkg\"],\n    module_mappings={\"__main__\": \"myapp.models_generated\"},  # optional import fixups\n    verbose=True,\n)\ngen.generate()\n</code></pre> <p>Notes: - <code>app_label</code> is embedded in each generated model\u2019s <code>Meta</code>. This ensures migrations belong to your app. - If your source classes are defined in scripts (module <code>__main__</code>), use <code>module_mappings</code> to rewrite imports to a stable module path.</p>"},{"location":"how_to_use/integrate_into_django_app/#2-expose-generated-models-from-myappmodelspy","title":"2) Expose generated models from <code>myapp/models.py</code>","text":"<p>Keep your hand-written <code>models.py</code>, and re-export generated models:</p> <pre><code># myapp/models.py\nfrom .models_generated import *  # re-export generated models\n\n# (Optional) Your custom models can live here alongside the generated ones.\n</code></pre> <p>This lets admin, serializers, and other app code import everything from <code>myapp.models</code> as usual.</p>"},{"location":"how_to_use/integrate_into_django_app/#3-add-your-app-to-installed_apps","title":"3) Add your app to INSTALLED_APPS","text":"<pre><code># settings.py\nINSTALLED_APPS = [\n    # ...\n    \"myapp\",\n]\nDEFAULT_AUTO_FIELD = \"django.db.models.BigAutoField\"\n</code></pre>"},{"location":"how_to_use/integrate_into_django_app/#4-create-and-apply-migrations","title":"4) Create and apply migrations","text":"<p>Generated models are standard Django models; create migrations like normal:</p> <pre><code>python manage.py makemigrations myapp\npython manage.py migrate\n</code></pre> <p>Regenerate models after source changes, then run <code>makemigrations</code> again if the schema changed.</p>"},{"location":"how_to_use/integrate_into_django_app/#5-register-in-the-django-admin-optional","title":"5) Register in the Django admin (optional)","text":"<p>If you want to auto-register all generated models:</p> <pre><code># myapp/admin.py\nfrom django.contrib import admin\nfrom . import models_generated as gm\n\nfor name in getattr(gm, \"__all__\", []):\n    admin.site.register(getattr(gm, name))\n</code></pre> <p>Or import specific classes and register explicitly.</p>"},{"location":"how_to_use/integrate_into_django_app/#6-use-conversions-in-your-app","title":"6) Use conversions in your app","text":"<ul> <li>Pydantic \u2192 Django (with generated model helpers):</li> </ul> <pre><code>from generated_models import DjangoUser\nfrom your_models_pkg import User as PydUser\n\nu = PydUser(id=1, email=\"j@example.com\", name=\"Jane\")\ndj = DjangoUser.from_pydantic(u)\ndj.save()\n\nu_back = dj.to_pydantic()\n</code></pre> <ul> <li>Django \u2192 Pydantic (dynamic, no files written):</li> </ul> <pre><code>from pydantic2django.django.conversion import DjangoPydanticConverter\nfrom myapp.models import DjangoUser\n\nconv = DjangoPydanticConverter(DjangoUser)\nPydUser = conv.generated_pydantic_model\n\ndj = DjangoUser.objects.first()\nu = conv.to_pydantic(dj)\n</code></pre>"},{"location":"how_to_use/integrate_into_django_app/#7-projectci-tips","title":"7) Project/CI tips","text":"<ul> <li>Check in the generated file and migrations, or generate as part of your build before <code>makemigrations</code> (be consistent across environments).</li> <li>If you regenerate frequently, consider a small script or management command to run the generator.</li> <li>Keep <code>pydantic2django</code> installed in runtime environments if your generated file imports base classes (e.g., <code>Pydantic2DjangoBaseClass</code>).</li> </ul>"},{"location":"how_to_use/integrate_into_django_app/#override-the-generated-base-class","title":"Override the generated base class","text":"<p>By default, generated models inherit a source-specific base (e.g., <code>Pydantic2DjangoBaseClass</code>, <code>Dataclass2DjangoBaseClass</code>, <code>Xml2DjangoBaseClass</code>). You can override the base to add mixins, managers, or TimescaleDB support.</p> <ul> <li>Define your custom abstract base:</li> </ul> <pre><code>from django.db import models\nfrom pydantic2django.django.models import Xml2DjangoBaseClass\n\nclass MyXmlBase(Xml2DjangoBaseClass):\n    class Meta:\n        abstract = True\n\n    # your common fields/methods here\n</code></pre> <ul> <li>Use it with a generator before calling <code>generate()</code>:</li> </ul> <pre><code>from pydantic2django.xmlschema.generator import XmlSchemaDjangoModelGenerator\n\ngen = XmlSchemaDjangoModelGenerator(\n    schema_files=[\"path/to/schema.xsd\"],\n    output_path=\"myapp/models_generated.py\",\n    app_label=\"myapp\",\n)\ngen.base_model_class = MyXmlBase  # override the base\ngen.generate()\n</code></pre> <ul> <li>TimescaleDB example (hypertable-ready base):</li> </ul> <pre><code># Combines Xml2DjangoBaseClass with TimescaleModel from django-timescaledb\nfrom pydantic2django.django.models import XmlTimescaleBase\nfrom pydantic2django.xmlschema.generator import XmlSchemaDjangoModelGenerator\n\ngen = XmlSchemaDjangoModelGenerator(\n    schema_files=[\"path/to/streams.xsd\"],\n    output_path=\"myapp/models_generated.py\",\n    app_label=\"myapp\",\n)\ngen.base_model_class = XmlTimescaleBase\ngen.generate()\n</code></pre> <p>Notes: - The Timescale base expects a <code>time</code> column; see <code>django-timescaledb</code> docs for details (<code>https://github.com/jamessewell/django-timescaledb?tab=readme-ov-file</code>). - If the generator supports a constructor parameter <code>base_model_class</code>, you can pass it directly instead of setting the attribute after construction. If not, setting <code>gen.base_model_class</code> before <code>generate()</code> is sufficient.</p>"},{"location":"how_to_use/integrate_into_django_app/#timescaledb-integration-automatic-classification","title":"TimescaleDB integration (automatic classification)","text":"<p>The XML generator can automatically classify types into hypertables (time-series facts) and dimensions (regular tables) and will:</p> <ul> <li>Use <code>XmlTimescaleBase</code> for hypertables and the regular <code>Xml2DjangoBaseClass</code> for dimensions.</li> <li>Convert illegal hypertable\u2192hypertable relationships into soft references (<code>UUIDField(db_index=True)</code>), keeping schemas Timescale-safe.</li> </ul> <p>What you need:</p> <ul> <li>Optionally install <code>django-timescaledb</code> to enable hypertable behavior; otherwise the Timescale mixin is a no-op.</li> <li>Run migrations as usual. Hypertables will be created where supported by <code>django-timescaledb</code>.</li> </ul> <p>Tip: If you want to override the classification for specific types, you can pre/post-process the roles map returned by <code>classify_xml_complex_types(...)</code> in a custom generator wrapper before invoking <code>generate()</code>.</p>"},{"location":"how_to_use/parametrized_assert_tests/","title":"Parametrized Single-Assertion Tests for End-to-End Load/Process/Persist","text":"<p>This guide describes a robust testing pattern using parameterized, single-assert test cases (via <code>pytest.mark.parametrize</code>) for any end-to-end workflow that loads a file/resource, processes it, and (optionally) persists results.</p> <p>It applies equally to XML, JSON, CSV, logs, binary artifacts, or remote resources\u2014swap in the appropriate loader/processor.</p>"},{"location":"how_to_use/parametrized_assert_tests/#why-this-pattern","title":"Why this pattern?","text":"<ul> <li>Clarity: Each assertion runs as an individual test case with a clear ID, so failures are obvious.</li> <li>Faster diagnosis: End-to-end tests often fail at specific steps (load, transform, persist); parameterization pinpoints where.</li> <li>Reuse: One setup fixture (arrange) powers many small checks (assert), keeping tests DRY.</li> <li>Stability: Centralized setup/teardown avoids brittle per-test scaffolding.</li> </ul>"},{"location":"how_to_use/parametrized_assert_tests/#core-principles","title":"Core principles","text":"<ul> <li>One assertion per parameterized case.</li> <li>Encapsulate preparation and the end-to-end action in a fixture that returns a small context object.</li> <li>Keep the context minimal (the result object, a couple of models/helpers) to reduce coupling.</li> <li>Name each check by intent (e.g., <code>loaded_ok</code>, <code>has_required_fields</code>, <code>persisted_rows</code>, <code>links_resolved</code>).</li> </ul>"},{"location":"how_to_use/parametrized_assert_tests/#generic-template-no-framework-assumptions","title":"Generic template (no framework assumptions)","text":"<p>Use this when your workflow only needs to read/process data and verify results in memory.</p> <pre><code>from pathlib import Path\nimport pytest\n\n# Define how to load/process your input (replace with your own)\ndef load_any(path: str) -&gt; dict:\n    # Example loader stub; replace with JSON/CSV/XML/etc.\n    # Return a rich result object or dataclass as appropriate.\n    return {\"path\": path, \"content\": Path(path).read_text(encoding=\"utf-8\")}\n\n\n@pytest.fixture(scope=\"function\")\ndef e2e_ctx(tmp_path: Path):\n    source_path = \"&lt;ABSOLUTE_OR_RELATIVE_SOURCE_PATH&gt;\"  # e.g., tests/data/sample.json\n\n    # Arrange: run the real load/process function\n    result = load_any(source_path)\n\n    # Optionally write derived files/artifacts under tmp_path if your flow produces outputs\n    # (e.g., parsed CSVs, intermediate JSONs)\n\n    # Return only what checks need\n    return {\"result\": result, \"source_path\": source_path}\n\n\n@pytest.mark.parametrize(\n    \"check\",\n    [\n        \"loaded_ok\",\n        \"has_content\",\n        \"has_expected_marker\",\n    ],\n)\ndef test_e2e_checks(e2e_ctx, check: str):\n    result = e2e_ctx[\"result\"]\n\n    if check == \"loaded_ok\":\n        assert result is not None\n        return\n\n    if check == \"has_content\":\n        assert isinstance(result.get(\"content\"), str) and len(result[\"content\"]) &gt; 0\n        return\n\n    if check == \"has_expected_marker\":\n        assert \"SOME_MARKER\" in result[\"content\"]  # tailor to your data\n        return\n\n    raise AssertionError(f\"Unknown check: {check}\")\n</code></pre>"},{"location":"how_to_use/parametrized_assert_tests/#django-backed-variant-optional-persistence","title":"Django-backed variant (optional persistence)","text":"<p>If your workflow persists to a database (e.g., Django models), add a setup block that ensures prerequisite tables exist and then perform the save step. This keeps the end-to-end test deterministic in in-memory databases.</p> <p>Key ideas: - Ensure system/app prerequisite tables exist when using an in-memory DB (example: <code>django_content_type</code>, <code>auth_permission</code>). - Create tables for your installed models via <code>django_apps.get_model</code> (avoid creating tables for transient/abstract classes). - Perform the load/process, then save to the DB; return persisted instances/models in the context.</p> <pre><code>import importlib.util\nimport sys\nfrom pathlib import Path\n\nimport pytest\nfrom django.apps import apps as django_apps\nfrom django.db import connection\n\n# Domain-specific: your generator/ingestor/persistor\n# from yourpackage import run_generation, load_and_persist\n\n\ndef _ensure_tables(models_to_ensure: list[type]) -&gt; None:\n    with connection.schema_editor() as se:\n        existing = set(connection.introspection.table_names())\n        for model in models_to_ensure:\n            if model._meta.db_table not in existing:\n                se.create_model(model)\n\n\n@pytest.fixture(scope=\"function\")\n@pytest.mark.django_db(transaction=True)\ndef e2e_ctx(tmp_path: Path):\n    app_label = \"tests\"  # the installed app label that holds your models\n    source_path = \"&lt;ABSOLUTE_OR_RELATIVE_SOURCE_PATH&gt;\"\n\n    # 1) (Optional) Generate/prepare domain models and dynamically import a file that defines them\n    # generated_file = run_generation(config=..., output_path=tmp_path / \"models.py\")\n    # spec = importlib.util.spec_from_file_location(\"tests_generated_models\", str(generated_file))\n    # module = importlib.util.module_from_spec(spec)\n    # sys.modules[\"tests_generated_models\"] = module\n    # assert spec and spec.loader\n    # spec.loader.exec_module(module)  # type: ignore\n\n    # 2) Ensure framework/system tables (example for Django contenttypes/auth)\n    try:\n        from django.contrib.contenttypes.models import ContentType\n        from django.contrib.auth.models import Permission\n        _ensure_tables([ContentType, Permission])\n    except Exception:\n        # Not required for all projects; safe to ignore if not installed\n        pass\n\n    # 3) Ensure your installed model tables exist (example)\n    # MyModel = django_apps.get_model(app_label, \"MyModel\")\n    # RelatedModel = django_apps.get_model(app_label, \"RelatedModel\")\n    # _ensure_tables([m for m in (MyModel, RelatedModel) if m is not None])\n\n    # 4) Execute end-to-end load and persist\n    # root = load_and_persist(source_path, save=True)\n    root = object()  # replace with your persisted root/domain instance\n\n    return {\n        \"root\": root,\n        # include key models if your checks need them\n        # \"MyModel\": MyModel,\n    }\n\n\n@pytest.mark.parametrize(\n    \"check\",\n    [\n        \"root_saved\",\n        # add more checks like: has_children, has_links, counts_match, etc.\n    ],\n)\n@pytest.mark.django_db(transaction=True)\ndef test_e2e_checks(e2e_ctx, check: str):\n    root = e2e_ctx[\"root\"]\n\n    if check == \"root_saved\":\n        # tailor to your model/ORM of choice; for Django, check pk is not None\n        assert getattr(root, \"pk\", None) is not None\n        return\n\n    raise AssertionError(f\"Unknown check: {check}\")\n</code></pre>"},{"location":"how_to_use/parametrized_assert_tests/#tips-for-adapting-the-pattern","title":"Tips for adapting the pattern","text":"<ul> <li>Replace <code>load_any</code> with your real loader (e.g., JSON, CSV, parquet, XML, image, or HTTP fetcher).</li> <li>If your flow has hooks (pre/post), keep them inside the fixture\u2014tests remain simple.</li> <li>Parameter names should read like behavior, not implementation details.</li> <li>When a check fails frequently, promote logging in the fixture to surface context quickly.</li> <li>If you have multiple end-to-end flows (e.g., different file types), make the fixture accept a <code>source_path</code>/<code>load_fn</code> param and <code>pytest.mark.parametrize</code> those too.</li> </ul>"},{"location":"how_to_use/parametrized_assert_tests/#example-mappings","title":"Example mappings","text":"<ul> <li>JSON file \u2192 <code>json.load(open(path))</code> \u2192 validate required keys/shape \u2192 persist to <code>MyModel</code>.</li> <li>CSV file \u2192 <code>pandas.read_csv(path)</code> \u2192 normalize columns \u2192 bulk create rows.</li> <li>Binary artifact \u2192 custom parser \u2192 checksum/metadata \u2192 persist artifact + derived rows.</li> <li>Remote resource \u2192 <code>httpx.get(url)</code> \u2192 parse \u2192 upsert into tables.</li> </ul>"},{"location":"how_to_use/parametrized_assert_tests/#when-to-use-the-framework-variant","title":"When to use the framework variant","text":"<ul> <li>Use the framework (e.g., Django) variant when persistence is part of the observable behavior you must verify.</li> <li>Otherwise, stick to the generic template for speed and simplicity.</li> </ul>"},{"location":"plans/gfk_generic_entries_plan/","title":"Provider\u2011agnostic GenericForeignKey (GFK) plan","text":""},{"location":"plans/gfk_generic_entries_plan/#what","title":"What","text":"<p>Introduce a generator\u2011controlled, provider\u2011agnostic mode to model polymorphic/repeating nested XML (and multi\u2011type pydantic/dataclass fields) using Django\u2019s contenttypes (GenericForeignKey) instead of emitting thousands of concrete leaf models or large numbers of JSONFields. This keeps the model surface small, migrations fast, and data queryable across all ingests.</p>"},{"location":"plans/gfk_generic_entries_plan/#why","title":"Why","text":"<ul> <li>Current xmlschema output (e.g., MTConnect Streams) can generate 90k+ line models and lengthy migrations due to substitution groups and deeply nested, repeating leaves.</li> <li>Many \u201cleaf\u201d combinations require either per\u2011leaf classes or JSON fallback; both scale poorly and are hard to query.</li> <li>GenericForeignKey lets us store polymorphic entries in a single normalized table with a reverse link to the owning parent, drastically reducing generated code while remaining queryable.</li> <li>Approach must be provider\u2011agnostic to support xmlschema, pydantic, and dataclass paths without domain\u2011specific types.</li> </ul> <p>Reference: Django contenttypes The contenttypes framework.</p>"},{"location":"plans/gfk_generic_entries_plan/#current-status-2025-10-01","title":"Current status (2025-10-01)","text":"<ul> <li>XmlSchema path: initial GFK plumbing implemented.</li> <li>Flags supported on <code>XmlSchemaDjangoModelGenerator</code>: <code>enable_gfk</code>, <code>gfk_policy</code>, <code>gfk_threshold_children</code>, <code>gfk_value_mode</code>, <code>gfk_normalize_common_attrs</code>.</li> <li>Detection: repeating nested complex elements (including wrapper-like containers) can be routed to GFK under <code>gfk_policy</code> \u2208 {<code>repeating_only</code>, <code>all_nested</code>, <code>threshold_by_children</code>}.</li> <li>Generation finalize: when any parent has GFK-eligible children, a single <code>GenericEntry</code> model is emitted for the app and the parent gets <code>entries = GenericRelation('GenericEntry', related_query_name='entries')</code>.</li> <li>Ingestion: when a parent exposes <code>entries</code> and a repeating nested child is encountered, concrete child instances are skipped and <code>GenericEntry</code> rows are persisted with <code>element_qname</code>, <code>type_qname</code>, <code>attrs_json</code> (including attributes and text value), and <code>order_index</code>.</li> <li>Tests: added a basic xmlschema generation test to assert <code>GenericEntry</code> emission and parent <code>GenericRelation</code>. Full suite passing with GFK enabled in targeted tests.</li> </ul>"},{"location":"plans/gfk_generic_entries_plan/#scope-and-behavior","title":"Scope and behavior","text":"<p>Add core flags (consumed by all generators): - <code>enable_gfk: bool</code> (default: false) - <code>gfk_policy: Literal[\"substitution_only\", \"repeating_only\", \"all_nested\", \"threshold_by_children\"]</code> - <code>gfk_threshold_children: int</code> (used when <code>threshold_by_children</code>) - <code>gfk_value_mode: Literal[\"json_only\", \"typed_columns\"]</code> - <code>gfk_normalize_common_attrs: bool</code> (normalize timestamp/subType\u2011like attrs; default: false)</p> <p>When enabled and a field/element meets policy: - Do not generate a concrete leaf model or schedule child FK injection - Instead, store instances as rows in a single <code>GenericEntry</code> model and expose a <code>GenericRelation</code> on the parent for reverse access</p>"},{"location":"plans/gfk_generic_entries_plan/#design-details","title":"Design details","text":""},{"location":"plans/gfk_generic_entries_plan/#core-additions","title":"Core additions","text":"<ul> <li>Generic entry model (emitted per app in generated output):</li> <li><code>content_type</code>, <code>object_id</code>, <code>content_object</code> (GenericForeignKey)</li> <li><code>element_qname: CharField</code></li> <li><code>type_qname: CharField | null</code></li> <li><code>attrs_json: JSONField</code></li> <li>Optional typed value columns (if <code>typed_columns</code>): <code>text_value</code>, <code>num_value</code>, <code>time_value</code></li> <li><code>order_index: IntegerField</code></li> <li> <p>Optional <code>path_hint: CharField</code> for debugging/tracing</p> </li> <li> <p>Parent reverse access:</p> </li> <li> <p>Add <code>GenericRelation('GenericEntry', related_query_name='entries')</code> to parents (wrappers/components) that own qualifying children</p> </li> <li> <p>Core policy hook points:</p> </li> <li>In field factories: if policy says GFK, don\u2019t create a concrete field; record a marker on the <code>ConversionCarrier</code> (e.g., <code>carrier.context_data[\"_pending_gfk_children\"]</code>)</li> <li>In model factory finalize: ensure the <code>GenericEntry</code> model exists, inject <code>GenericRelation</code> on parents, add indexes (e.g., <code>(content_type, object_id)</code>)</li> </ul>"},{"location":"plans/gfk_generic_entries_plan/#generatorspecific-detection","title":"Generator\u2011specific detection","text":"<ul> <li>xmlschema (<code>src/pydantic2django/xmlschema/</code>):</li> <li>Identify substitution groups, repeating complex leaves, deeply nested wrappers; apply chosen policy</li> <li> <p>Ingestor should create <code>GenericEntry</code> rows instead of concrete leaf instances</p> </li> <li> <p>pydantic (<code>src/pydantic2django/pydantic/</code>) and dataclass (<code>src/pydantic2django/dataclass/</code>):</p> </li> <li>Detect <code>Union</code>/variant fields (e.g., <code>List[Union[A, B, ...]]</code>); when enabled, route through GFK instead of many concrete field alternatives</li> </ul>"},{"location":"plans/gfk_generic_entries_plan/#tests-and-docs","title":"Tests and docs","text":"<ul> <li>Tests:</li> <li>xmlschema path: verify that enabling <code>enable_gfk</code> yields a single <code>GenericEntry</code> model, a <code>GenericRelation</code> on the parent, and entries persisted by the ingestor</li> <li>pydantic/dataclass path: verify <code>List[Union[...]]</code> maps to GFK when enabled and persists entries</li> <li>Negative controls: with GFK disabled, ensure legacy behavior unchanged</li> <li> <p>Performance: smoke test that migrations run significantly faster with GFK enabled for large schemas (coarse check)</p> </li> <li> <p>Docs:</p> </li> <li>New \u201cGeneric Entries (ContentTypes) mode\u201d page describing flags, trade\u2011offs, and examples</li> <li>Update xmlschema element handling doc to mention GFK option</li> </ul>"},{"location":"plans/gfk_generic_entries_plan/#files-to-touch-entry-points-references","title":"Files to touch (entry points &amp; references)","text":"<ul> <li>Core detection/mapping hooks:</li> <li><code>src/pydantic2django/core/bidirectional_mapper.py</code></li> <li><code>src/pydantic2django/core/mapping_units.py</code> (has <code>GenericForeignKeyMappingUnit</code>)</li> <li><code>src/pydantic2django/core/relationships.py</code></li> <li><code>src/pydantic2django/core/factories.py</code> (if present) or module\u2011specific factories</li> <li> <p><code>src/pydantic2django/django/conversion.py</code> (ensure conversion respects GFK; current code path includes <code>GenericForeignKey</code> handling around line ~484)</p> </li> <li> <p>Generator (xmlschema):</p> </li> <li><code>src/pydantic2django/xmlschema/generator.py</code></li> <li><code>src/pydantic2django/xmlschema/factory.py</code></li> <li> <p><code>src/pydantic2django/xmlschema/ingestor.py</code></p> </li> <li> <p>Generator (pydantic):</p> </li> <li><code>src/pydantic2django/pydantic/generator.py</code></li> <li><code>src/pydantic2django/pydantic/factory.py</code></li> <li> <p><code>src/pydantic2django/pydantic/discovery.py</code></p> </li> <li> <p>Generator (dataclass):</p> </li> <li><code>src/pydantic2django/dataclass/generator.py</code></li> <li><code>src/pydantic2django/dataclass/factory.py</code></li> <li> <p><code>src/pydantic2django/dataclass/discovery.py</code></p> </li> <li> <p>Docs:</p> </li> <li><code>docs/how_it_works/xmlschema_element_and_type_handling.md</code> (add GFK mode reference)</li> <li><code>docs/how_it_works/</code> (new: <code>generic_entries_contenttypes.md</code>)</li> </ul>"},{"location":"plans/gfk_generic_entries_plan/#migration-and-runtime-considerations","title":"Migration and runtime considerations","text":"<ul> <li>Ensure <code>django.contrib.contenttypes</code> in generated app settings/migrations as needed</li> <li>Add DB indexes on <code>GenericEntry(content_type, object_id)</code>; optionally on <code>element_qname</code>, <code>type_qname</code>, <code>timestamp</code>/<code>time_value</code></li> <li>Keep <code>attrs_json</code> as spillover for long\u2011tail attributes</li> </ul>"},{"location":"plans/gfk_generic_entries_plan/#phased-todos","title":"Phased TODOs","text":"<p>1) Core plumbing and flags [core]    - Add <code>enable_gfk</code>, <code>gfk_policy</code>, <code>gfk_value_mode</code>, <code>gfk_normalize_common_attrs</code> to generator configs    - Wire <code>GenericForeignKeyMappingUnit</code> as the conceptual trigger; factories record <code>_pending_gfk_children</code> instead of emitting a field    - Implement finalize hook to emit <code>GenericEntry</code> model and inject <code>GenericRelation</code></p> <p>2) xmlschema integration [xmlschema]    - Policy detection for substitution groups and repeating complex leaves    - Ingestor: write <code>GenericEntry</code> rows (preserve order and attrs) when GFK is enabled    - Tests: verify model emission and round\u2011trip ingestion</p> <p>3) pydantic/dataclass integration [pydantic/dataclass]    - Detect <code>List[Union[...]]</code> (and similar) and route via GFK under policy    - Tests: ensure persistence and reverse queries via <code>GenericRelation</code></p> <p>4) Documentation and examples [docs]    - Author <code>generic_entries_contenttypes.md</code> with examples, flags, and trade\u2011offs    - Update existing docs to mention the mode as an optional strategy</p> <p>5) Performance sanity [ops]    - Add a smoke test or doc snippet comparing migration time/size with/without GFK on a large schema</p>"},{"location":"plans/gfk_generic_entries_plan/#acceptance-criteria","title":"Acceptance criteria","text":"<ul> <li>Enabling GFK mode reduces generated model size and migration durations on large schemas</li> <li>Reverse access via <code>GenericRelation</code> works and is filterable (e.g., by <code>element_qname</code>, <code>attrs_json</code> keys)</li> <li>Legacy behavior unchanged when GFK disabled</li> <li>Tests cover xmlschema + pydantic/dataclass paths</li> </ul>"},{"location":"plans/gfk_generic_entries_plan/#xml-schema-handling-detailed-design","title":"XML schema handling \u2013 detailed design","text":"<p>This section specifies precisely how XML Schema (XSD) constructs are mapped when <code>enable_gfk=True</code>.</p>"},{"location":"plans/gfk_generic_entries_plan/#detection-and-policy-evaluation-discoveryfactory","title":"Detection and policy evaluation (discovery/factory)","text":"<ul> <li>Input models come from xmlschema discovery: <code>XmlSchemaComplexType</code>, their <code>elements</code>, <code>attributes</code>, and metadata.</li> <li>For each complex type\u2019s content model (sequence/choice/all), inspect each child element E:</li> <li>Repeating leaf: <code>maxOccurs&gt;1</code> or <code>unbounded</code> and <code>E</code> resolves to a simple type or a complex type that functions as a leaf \u2192 candidate for GFK when <code>gfk_policy</code> \u2208 {<code>repeating_only</code>, <code>all_nested</code>, <code>threshold_by_children</code>}.</li> <li>Substitution groups: if E is a head, expand members (already implemented); if E is a member under a wrapper, treat these polymorphic variants as candidates when <code>gfk_policy</code> \u2208 {<code>substitution_only</code>, <code>all_nested</code>, <code>threshold_by_children</code>}.</li> <li>Threshold policy: for a wrapper/container (TitleCase element name or <code>*WrapperType</code>), if the number of distinct child types \u2265 <code>gfk_threshold_children</code>, use GFK for those children.</li> <li>Attribute groups: already resolved into attributes; when GFK is used, attributes are copied into <code>attrs_json</code> (or normalized to typed columns when configured).</li> </ul>"},{"location":"plans/gfk_generic_entries_plan/#parent-owner-selection","title":"Parent (owner) selection","text":"<ul> <li>Owner is typically the nearest non\u2011leaf container on the path:</li> <li>If a wrapper node (e.g., <code>Samples</code>, <code>Events</code>, <code>Condition</code>) exists, prefer the wrapper as owner.</li> <li>Otherwise, use the next higher parent such as <code>ComponentStreamType</code> (or analogous container) to avoid over\u2011fragmenting owners.</li> <li>Implement as: choose the current complex type as parent unless the child is inside a recognized wrapper pattern; in that case, use the wrapper type (if generated) as the owner.</li> </ul>"},{"location":"plans/gfk_generic_entries_plan/#mapping-from-xsd-to-genericentry","title":"Mapping from XSD to GenericEntry","text":"<ul> <li>Entry fields:</li> <li><code>element_qname</code>: the element name for the leaf, local name preferred (include prefix/namespace if required by config).</li> <li><code>type_qname</code>: the resolved target type (simple or complex), using local name (store namespace if available).</li> <li><code>attrs_json</code>: all attributes on the leaf element (post attributeGroup expansion). If <code>gfk_normalize_common_attrs=True</code>, pull out known keys (e.g., timestamp\u2011like, subtype\u2011like, dataItemId\u2011like) into typed columns.</li> <li>Value mapping (<code>gfk_value_mode</code>):<ul> <li><code>json_only</code>: put element text (and possibly child simple content) into <code>attrs_json[\"value\"]</code>.</li> <li><code>typed_columns</code>: detect and map</li> <li>Element text to <code>text_value</code></li> <li>Numeric text to <code>num_value</code> where parsing is unambiguous</li> <li>Date/time text to <code>time_value</code> where parsing is unambiguous</li> <li>Any remaining additional content goes to <code>attrs_json</code>.</li> </ul> </li> <li><code>order_index</code>: increment as entries are encountered under the owner to preserve original order.</li> <li><code>path_hint</code> (optional): dotted path or lightweight xpath within the owner to aid debugging.</li> </ul>"},{"location":"plans/gfk_generic_entries_plan/#identity-constraints-xskey-xskeyref","title":"Identity constraints (xs:key / xs:keyref)","text":"<ul> <li>When GFK is chosen, strong FK injection to leaf models isn\u2019t applicable. We handle keys/refs as follows:</li> <li>Represent keyref targets via soft references: copy key values into <code>attrs_json</code> and/or into normalized columns (if configured).</li> <li>Optionally add indexes on these soft reference columns for faster lookup.</li> <li>Preserve original key/keyref metadata on the parent context for audit/validation (future validation hook could enforce referential checks on ingest).</li> </ul>"},{"location":"plans/gfk_generic_entries_plan/#mixed-content-and-nested-simple-content","title":"Mixed content and nested simple content","text":"<ul> <li>Mixed content (<code>mixed=true</code>) or interleaved text: store raw text in <code>text_value</code> (typed mode) or <code>attrs_json[\"text\"]</code> (json mode). Child non\u2011leaf content under a GFK leaf is not further decomposed unless configured; it remains in <code>attrs_json</code>.</li> </ul>"},{"location":"plans/gfk_generic_entries_plan/#timescale-integration","title":"Timescale integration","text":"<ul> <li>If Timescale classification marks parent as hypertable, keep GFK on entries while parent remains Timescale\u2011backed. Suggested indexes:</li> <li><code>(owner, -time_value)</code> composite (owner via <code>(content_type, object_id)</code>), if <code>time_value</code> is present.</li> <li>Basic <code>(content_type, object_id)</code> always.</li> </ul>"},{"location":"plans/gfk_generic_entries_plan/#indexing","title":"Indexing","text":"<ul> <li>Always add <code>Index(fields=[\"content_type\", \"object_id\"])</code> on <code>GenericEntry</code>.</li> <li>Optional indexes when <code>typed_columns</code> enabled:</li> <li><code>Index(fields=[\"element_qname\"])</code></li> <li><code>Index(fields=[\"type_qname\"])</code></li> <li><code>Index(fields=[\"time_value\"])</code> and composite <code>(content_type, object_id, -time_value)</code> when enabled.</li> </ul>"},{"location":"plans/gfk_generic_entries_plan/#generation-and-migrations","title":"Generation and migrations","text":"<ul> <li>The generator emits:</li> <li>A single <code>GenericEntry</code> model per app when GFK is used anywhere in that app.</li> <li>A <code>GenericRelation</code> field on each parent owner model.</li> <li>Ensure <code>django.contrib.contenttypes</code> is present in app settings for migration contexts.</li> </ul>"},{"location":"plans/gfk_generic_entries_plan/#ingestor-algorithm-xml-instance-models","title":"Ingestor algorithm (xml instance \u2192 models)","text":"<ol> <li>Traverse the XML instance using schema guidance (existing ingestor).</li> <li>For each candidate child matching GFK policy:</li> <li>Resolve owner instance (parent/wrapper chain context).</li> <li>Create <code>GenericEntry</code> with <code>content_object=owner</code>, fill <code>element_qname</code>, <code>type_qname</code>.</li> <li>Copy attributes; map value to <code>text_value</code>/<code>num_value</code>/<code>time_value</code> or <code>attrs_json</code> per <code>gfk_value_mode</code>.</li> <li>Assign <code>order_index</code> monotonically for that owner.</li> <li>For non\u2011GFK children, keep legacy behavior (FKs/JSON fallback).</li> </ol>"},{"location":"plans/gfk_generic_entries_plan/#overrides-and-configuration","title":"Overrides and configuration","text":"<ul> <li>Per\u2011namespace or per\u2011element overrides: provide a mapping to force or disable GFK for certain element names or paths.</li> <li>Per\u2011run flags: enable/disable GFK globally or by policy (as defined above).</li> <li>Backward compatibility: with <code>enable_gfk=False</code>, all legacy generation and ingestion paths remain unchanged.</li> </ul>"},{"location":"plans/gfk_generic_entries_plan/#example-mtconnectstyle-pattern","title":"Example (MTConnect\u2011style pattern)","text":"<ul> <li><code>ComponentStreamType</code> \u2192 wrapper <code>SamplesType</code> \u2192 many observation leaves (<code>Angle</code>, <code>Position</code>, \u2026)</li> <li>With <code>gfk_policy=substitution_only</code> or <code>repeating_only</code>, observation leaves become <code>GenericEntry</code> rows with owner=<code>SamplesType</code> (preferred) or <code>ComponentStreamType</code> (if wrapper is not generated).</li> <li><code>attrs_json</code> contains <code>dataItemId</code>, <code>subType</code>; <code>timestamp</code> goes to <code>time_value</code> (typed mode) or <code>attrs_json[\"timestamp\"]</code>.</li> <li>Query pattern:<ul> <li><code>samples.entries.filter(element_qname=\"Angle\", time_value__gte=..., attrs_json__subType=\"ACTUAL\")</code></li> </ul> </li> </ul>"},{"location":"plans/gfk_generic_entries_plan/#current-status","title":"CURRENT STATUS:","text":"<p>Core GFK flags added to BaseStaticGenerator with defaults: enable_gfk=True, gfk_policy=\"threshold_by_children\", gfk_threshold_children=8, gfk_value_mode=\"typed_columns\". Conversion pipeline now records pending_gfk_children and injects GenericRelation('entries') on parents. GenericEntry model emitted with appropriate indexes; ingestor maps text\u2192typed columns. xmlschema GFK precedence moved before JSON fallback; threshold_by_children implemented; nested_relationship_strategy defaulted to 'fk'; per-element overrides supported. Wrapper owners marked so their nested children route to GFK (structural, not name-based). Tests verify GenericEntry emission, relation injection, typed columns/indexes, and suppression of JSON placeholders for repeating nested. pydantic/dataclass List[Union[...]] routed through GFK; tests added. Docs Updated to state GFK is default with recommended defaults. Reality check (Streams) Despite passing unit tests, MTConnect Streams still generates very large models with many observation \u201centry-type\u201d JSONFields alongside GFK. Why: current structural detection doesn\u2019t yet capture the full MTConnect pattern: Substitution-group expansions under wrappers (Samples/Events/Condition) and derived Event/DataSet types still travel code paths that emit JSON placeholders and generate many concrete observation models. included_model_names/discovery still admits many observation complex types; GFK detection doesn\u2019t demote those to \u201cstored in GenericEntry\u201d comprehensively. Our tests are too simple (single repeating nested/wrapper cases) and don\u2019t replicate substitution groups and the breadth of observation variants in Streams. Next steps (to make it truly compact) 1) Strengthen structural detection for MTConnect-like polymorphism Treat substitution-group members under recognized wrapper owners as GFK candidates (policy \"substitution_only\" as an option, and include in threshold_by_children computation). When a parent is marked a GFK owner, route all eligible nested complex leaves (including substitution-group variants) to GFK and do not emit placeholders or generate their concrete models. Gate discovery/inclusion: if a leaf path is GFK-routed, don\u2019t include that leaf\u2019s concrete type in included_model_names. 2) Add representative tests derived from Streams constructs Build a minimal fixture capturing the pattern: A wrapper type (e.g., SamplesType) with a head element resolved via substitution group to many member types, including a repeated \u201cdataset-like\u201d element. Assertions: GenericEntry emitted; wrapper parent has GenericRelation('entries'). No JSONField placeholders on wrapper/observation models even when element names don\u2019t include \u201centry\u201d. Concrete observation/member types are not generated when GFK is applied for that path. Add a smoke test that counts total models and fields for the fixture and asserts a bounded maximum (proxy for \u201ccompactness\u201d). 3) Instrumentation/diagnostics Log, per parent model, how many children were routed to GFK vs emitted as fields; optionally expose a summary (to make gaps obvious in CI). 4) Optional policy control Introduce gfk_policy=\"substitution_only\" and test it on the new fixture; confirm it collapses member variants to GenericEntry rows. How to evolve the tests to be representative Include substitution groups: head element plus multiple member declarations resolved via discovery. Include a wrapper/container owner that aggregates many heterogeneous members (mirroring Samples/Events/Condition). Assert: No JSON placeholders on the wrapper or on derived observation classes (even with non-\u201centry\u201d names). Member complex types are not present in all when routed via GFK. GenericEntry count/index definitions exist; reverse queries via entries are available. These changes will align the generator with the Streams reality by structurally routing polymorphic observation leaves into GenericEntry and eliminating dual emission.</p>"},{"location":"reference/pydantic2django/","title":"pydantic2django","text":"<p>Pydantic2Django - Generate Django models from Pydantic models.</p> <p>This package provides utilities for generating Django models from Pydantic models and converting between them.</p>"},{"location":"reference/pydantic2django/#pydantic2django.ImportHandler","title":"<code>ImportHandler</code>","text":"<p>Handles import statements for generated Django models and their context classes. Tracks and deduplicates imports from multiple sources while ensuring all necessary dependencies are included.</p> Source code in <code>src/pydantic2django/core/imports.py</code> <pre><code>class ImportHandler:\n    \"\"\"\n    Handles import statements for generated Django models and their context classes.\n    Tracks and deduplicates imports from multiple sources while ensuring all necessary\n    dependencies are included.\n    \"\"\"\n\n    def __init__(self, module_mappings: Optional[dict[str, str]] = None):\n        \"\"\"\n        Initialize empty collections for different types of imports.\n\n        Args:\n            module_mappings: Optional mapping of modules to remap (e.g. {\"__main__\": \"my_app.models\"})\n        \"\"\"\n        # Track imports by category\n        self.extra_type_imports: set[str] = set()  # For typing and other utility imports\n        self.pydantic_imports: set[str] = set()  # For Pydantic model imports\n        self.context_class_imports: set[str] = set()  # For context class and field type imports\n\n        # For tracking imported names to avoid duplicates\n        self.imported_names: dict[str, str] = {}  # Maps type name to its module\n\n        # For tracking field type dependencies we've already processed\n        self.processed_field_types: set[str] = set()\n\n        # Module mappings to remap imports (e.g. \"__main__\" -&gt; \"my_app.models\")\n        self.module_mappings = module_mappings or {}\n\n        logger.info(\"ImportHandler initialized\")\n        if self.module_mappings:\n            logger.info(f\"Using module mappings: {self.module_mappings}\")\n\n    def add_import(self, module: str, name: str):\n        \"\"\"Adds a single import based on module and name strings.\"\"\"\n        if not module or module == \"builtins\":\n            return\n\n        # Apply module mappings\n        if module in self.module_mappings:\n            module = self.module_mappings[module]\n\n        # Clean name (e.g., remove generics for import statement)\n        clean_name = self._clean_generic_type(name)\n\n        # Check if already imported\n        if name in self.imported_names:\n            # Could verify module matches, but usually name is unique enough\n            logger.debug(f\"Skipping already imported name: {name} (from module {module})\")\n            return\n        if clean_name != name and clean_name in self.imported_names:\n            logger.debug(f\"Skipping already imported clean name: {clean_name} (from module {module})\")\n            return\n\n        # Determine category\n        # Simplistic: If module is known Pydantic, Django, or common stdlib -&gt; context\n        # Otherwise, if it's 'typing' -&gt; extra_type\n        # TODO: Refine categorization if needed (e.g., dedicated django_imports set)\n        import_statement = f\"from {module} import {clean_name}\"\n        if module == \"typing\":\n            self.extra_type_imports.add(clean_name)  # Add only name to typing imports set\n            logger.debug(f\"Adding typing import: {clean_name}\")\n        # elif module.startswith(\"django.\"):\n        # Add to a dedicated django set if we create one\n        #    self.context_class_imports.add(import_statement)\n        #    logger.info(f\"Adding Django import: {import_statement}\")\n        else:\n            # Default to context imports for non-typing\n            self.context_class_imports.add(import_statement)\n            logger.info(f\"Adding context class import: {import_statement}\")\n\n        # Mark as imported\n        self.imported_names[name] = module\n        if clean_name != name:\n            self.imported_names[clean_name] = module\n\n    def add_pydantic_model_import(self, model_class: type) -&gt; None:\n        \"\"\"\n        Add an import statement for a Pydantic model.\n\n        Args:\n            model_class: The Pydantic model class to import\n        \"\"\"\n        if not hasattr(model_class, \"__module__\") or not hasattr(model_class, \"__name__\"):\n            logger.warning(f\"Cannot add import for {model_class}: missing __module__ or __name__\")\n            return\n\n        module_path = model_class.__module__\n        model_name = self._clean_generic_type(model_class.__name__)\n\n        # Apply module mappings if needed\n        if module_path in self.module_mappings:\n            actual_module = self.module_mappings[module_path]\n            logger.debug(f\"Remapping module import: {module_path} -&gt; {actual_module}\")\n            module_path = actual_module\n\n        logger.debug(f\"Processing Pydantic model import: {model_name} from {module_path}\")\n\n        # Skip if already imported\n        if model_name in self.imported_names:\n            logger.debug(f\"Skipping already imported model: {model_name}\")\n            return\n\n        import_statement = f\"from {module_path} import {model_name}\"\n        logger.info(f\"Adding Pydantic import: {import_statement}\")\n        self.pydantic_imports.add(import_statement)\n        self.imported_names[model_name] = module_path\n\n    def add_context_field_type_import(self, field_type: Any) -&gt; None:\n        \"\"\"\n        Add an import statement for a context field type with recursive dependency detection.\n\n        Args:\n            field_type: The field type to import\n        \"\"\"\n        # Skip if we've already processed this field type\n        field_type_str = str(field_type)\n        if field_type_str in self.processed_field_types:\n            logger.debug(f\"Skipping already processed field type: {field_type_str}\")\n            return\n\n        logger.info(f\"Processing context field type: {field_type_str}\")\n        self.processed_field_types.add(field_type_str)\n\n        # Try to add direct import for the field type if it's a class\n        self._add_type_import(field_type)\n\n        # Handle nested types in generics, unions, etc.\n        self._process_nested_types(field_type)\n\n        # Add typing imports based on the field type string\n        self._add_typing_imports(field_type_str)\n\n    def _add_type_import(self, field_type: Any) -&gt; None:\n        \"\"\"\n        Add an import for a single type object if it has module and name attributes.\n\n        Args:\n            field_type: The type to import\n        \"\"\"\n        try:\n            if hasattr(field_type, \"__module__\") and hasattr(field_type, \"__name__\"):\n                type_module = field_type.__module__\n                type_name = field_type.__name__\n\n                # Apply module mappings if needed\n                if type_module in self.module_mappings:\n                    actual_module = self.module_mappings[type_module]\n                    logger.debug(f\"Remapping module import: {type_module} -&gt; {actual_module}\")\n                    type_module = actual_module\n\n                logger.debug(f\"Examining type: {type_name} from module {type_module}\")\n\n                # Skip built-in types and typing module types\n                if (\n                    type_module.startswith(\"typing\")\n                    or type_module == \"builtins\"\n                    or type_name in [\"str\", \"int\", \"float\", \"bool\", \"dict\", \"list\"]\n                ):\n                    logger.debug(f\"Skipping built-in or typing type: {type_name}\")\n                    return\n\n                # Skip TypeVar definitions to avoid conflicts\n                if type_name == \"T\" or type_name == \"TypeVar\":\n                    logger.debug(f\"Skipping TypeVar definition: {type_name} - will be defined locally\")\n                    return\n\n                # Clean up any parametrized generic types for the import statement\n                clean_type_name = self._clean_generic_type(type_name)\n\n                # Use the original type_name (potentially with generics) for the imported_names check\n                if type_name in self.imported_names:\n                    logger.debug(f\"Skipping already imported type: {type_name}\")\n                    return\n\n                # Add to context class imports *before* marking as imported\n                # Use the clean name for the import statement itself\n                import_statement = f\"from {type_module} import {clean_type_name}\"\n                logger.info(f\"Adding context class import: {import_statement}\")\n                self.context_class_imports.add(import_statement)\n\n                # Add the original type name to imported_names to prevent re-processing\n                self.imported_names[type_name] = type_module\n                # Also add the cleaned name in case it's encountered separately\n                if clean_type_name != type_name:\n                    self.imported_names[clean_type_name] = type_module\n\n        except (AttributeError, TypeError) as e:\n            logger.warning(f\"Error processing type import for {field_type}: {e}\")\n\n    def _process_nested_types(self, field_type: Any) -&gt; None:\n        \"\"\"\n        Recursively process nested types in generics, unions, etc.\n\n        Args:\n            field_type: The type that might contain nested types\n        \"\"\"\n        # Handle __args__ for generic types, unions, etc.\n        if hasattr(field_type, \"__args__\"):\n            logger.debug(f\"Processing nested types for {field_type}\")\n            for arg_type in field_type.__args__:\n                logger.debug(f\"Found nested type argument: {arg_type}\")\n                # Recursively process each argument type\n                self.add_context_field_type_import(arg_type)\n\n        # Handle __origin__ for generic types (like List, Dict, etc.)\n        if hasattr(field_type, \"__origin__\"):\n            logger.debug(f\"Processing origin type for {field_type}: {field_type.__origin__}\")\n            self.add_context_field_type_import(field_type.__origin__)\n\n    def _add_typing_imports(self, field_type_str: str) -&gt; None:\n        \"\"\"\n        Add required typing imports based on the string representation of the field type.\n\n        Args:\n            field_type_str: String representation of the field type\n        \"\"\"\n        # Check for common typing constructs\n        if \"List[\" in field_type_str or \"list[\" in field_type_str:\n            logger.debug(f\"Adding List import from {field_type_str}\")\n            self.extra_type_imports.add(\"List\")\n\n        if \"Dict[\" in field_type_str or \"dict[\" in field_type_str:\n            logger.debug(f\"Adding Dict import from {field_type_str}\")\n            self.extra_type_imports.add(\"Dict\")\n\n        if \"Tuple[\" in field_type_str or \"tuple[\" in field_type_str:\n            logger.debug(f\"Adding Tuple import from {field_type_str}\")\n            self.extra_type_imports.add(\"Tuple\")\n\n        if \"Optional[\" in field_type_str or \"Union[\" in field_type_str or \"None\" in field_type_str:\n            logger.debug(f\"Adding Optional import from {field_type_str}\")\n            self.extra_type_imports.add(\"Optional\")\n\n        if \"Union[\" in field_type_str:\n            logger.debug(f\"Adding Union import from {field_type_str}\")\n            self.extra_type_imports.add(\"Union\")\n\n        if \"Callable[\" in field_type_str:\n            logger.debug(f\"Adding Callable import from {field_type_str}\")\n            self.extra_type_imports.add(\"Callable\")\n\n        if \"Any\" in field_type_str:\n            logger.debug(f\"Adding Any import from {field_type_str}\")\n            self.extra_type_imports.add(\"Any\")\n\n        # Extract custom types from the field type string\n        self._extract_custom_types_from_string(field_type_str)\n\n    def _extract_custom_types_from_string(self, field_type_str: str) -&gt; None:\n        \"\"\"\n        Extract custom type names from a string representation of a field type.\n\n        Args:\n            field_type_str: String representation of the field type\n        \"\"\"\n        # Extract potential type names from the string\n        # This regex looks for capitalized words that might be type names\n        type_names = re.findall(r\"[A-Z][a-zA-Z0-9]*\", field_type_str)\n\n        logger.debug(f\"Extracted potential type names from string {field_type_str}: {type_names}\")\n\n        for type_name in type_names:\n            # Skip common type names that are already handled\n            if type_name in [\"List\", \"Dict\", \"Optional\", \"Union\", \"Tuple\", \"Callable\", \"Any\"]:\n                logger.debug(f\"Skipping common typing name: {type_name}\")\n                continue\n\n            # Skip if already in imported names\n            if type_name in self.imported_names:\n                logger.debug(f\"Skipping already imported name: {type_name}\")\n                continue\n\n            # Log potential custom type\n            logger.info(f\"Adding potential custom type to extra_type_imports: {type_name}\")\n\n            # Add to extra type imports - these are types that we couldn't resolve to a module\n            # They'll need to be imported elsewhere or we might generate an error\n            self.extra_type_imports.add(type_name)\n\n    def get_required_imports(self, field_type_str: str) -&gt; dict[str, list[str]]:\n        \"\"\"\n        Get typing and custom type imports required for a field type.\n\n        Args:\n            field_type_str: String representation of a field type\n\n        Returns:\n            Dictionary with \"typing\" and \"custom\" import lists\n        \"\"\"\n        logger.debug(f\"Getting required imports for: {field_type_str}\")\n        self._add_typing_imports(field_type_str)\n\n        # Get custom types (non-typing types)\n        custom_types = [\n            name\n            for name in self.extra_type_imports\n            if name not in [\"List\", \"Dict\", \"Tuple\", \"Set\", \"Optional\", \"Union\", \"Any\", \"Callable\"]\n        ]\n\n        logger.debug(f\"Found custom types: {custom_types}\")\n\n        # Return the latest state of imports\n        return {\n            \"typing\": list(self.extra_type_imports),\n            \"custom\": custom_types,\n        }\n\n    def deduplicate_imports(self) -&gt; dict[str, set[str]]:\n        \"\"\"\n        De-duplicate imports between Pydantic models and context field types.\n\n        Returns:\n            Dict with de-duplicated import sets\n        \"\"\"\n        logger.info(\"Deduplicating imports\")\n        logger.debug(f\"Current pydantic imports: {self.pydantic_imports}\")\n        logger.debug(f\"Current context imports: {self.context_class_imports}\")\n\n        # Extract class names and modules from import statements\n        pydantic_classes = {}\n        context_classes = {}\n\n        # Handle special case for TypeVar imports\n        typevars = set()\n\n        for import_stmt in self.pydantic_imports:\n            if import_stmt.startswith(\"from \") and \" import \" in import_stmt:\n                module, classes = import_stmt.split(\" import \")\n                module = module.replace(\"from \", \"\")\n\n                # Skip __main__ and rewrite to real module paths if possible\n                if module == \"__main__\":\n                    logger.warning(f\"Skipping __main__ import: {import_stmt} - these won't work when imported\")\n                    continue\n\n                for cls in classes.split(\", \"):\n                    # Check if it's a TypeVar to handle duplicate definitions\n                    if cls == \"T\" or cls == \"TypeVar\":\n                        typevars.add(cls)\n                        continue\n\n                    # Clean up any parameterized generic types in class names\n                    cls = self._clean_generic_type(cls)\n                    pydantic_classes[cls] = module\n\n        for import_stmt in self.context_class_imports:\n            if import_stmt.startswith(\"from \") and \" import \" in import_stmt:\n                module, classes = import_stmt.split(\" import \")\n                module = module.replace(\"from \", \"\")\n\n                # Skip __main__ imports or rewrite to real module paths if possible\n                if module == \"__main__\":\n                    logger.warning(f\"Skipping __main__ import: {import_stmt} - these won't work when imported\")\n                    continue\n\n                for cls in classes.split(\", \"):\n                    # Check if it's a TypeVar to handle duplicate definitions\n                    if cls == \"T\" or cls == \"TypeVar\":\n                        typevars.add(cls)\n                        continue\n\n                    # Clean up any parameterized generic types in class names\n                    cls = self._clean_generic_type(cls)\n                    # If this class is already imported in pydantic imports, skip it\n                    if cls in pydantic_classes:\n                        logger.debug(f\"Skipping duplicate context import for {cls}, already in pydantic imports\")\n                        continue\n                    context_classes[cls] = module\n\n        # Rebuild import statements\n        module_to_classes = {}\n        for cls, module in pydantic_classes.items():\n            if module not in module_to_classes:\n                module_to_classes[module] = []\n            module_to_classes[module].append(cls)\n\n        deduplicated_pydantic_imports = set()\n        for module, classes in module_to_classes.items():\n            deduplicated_pydantic_imports.add(f\"from {module} import {', '.join(sorted(classes))}\")\n\n        # Same for context imports\n        module_to_classes = {}\n        for cls, module in context_classes.items():\n            if module not in module_to_classes:\n                module_to_classes[module] = []\n            module_to_classes[module].append(cls)\n\n        deduplicated_context_imports = set()\n        for module, classes in module_to_classes.items():\n            deduplicated_context_imports.add(f\"from {module} import {', '.join(sorted(classes))}\")\n\n        logger.info(f\"Final pydantic imports: {deduplicated_pydantic_imports}\")\n        logger.info(f\"Final context imports: {deduplicated_context_imports}\")\n\n        # Log any TypeVar names we're skipping\n        if typevars:\n            logger.info(f\"Skipping TypeVar imports: {typevars} - these will be defined locally\")\n\n        return {\"pydantic\": deduplicated_pydantic_imports, \"context\": deduplicated_context_imports}\n\n    def _clean_generic_type(self, name: str) -&gt; str:\n        \"\"\"\n        Clean generic parameters from a type name.\n\n        Args:\n            name: The type name to clean\n\n        Returns:\n            The cleaned type name without generic parameters\n        \"\"\"\n        if \"[\" in name or \"&lt;\" in name:\n            cleaned = re.sub(r\"\\[.*\\]\", \"\", name)\n            logger.debug(f\"Cleaned generic type {name} to {cleaned}\")\n            return cleaned\n        return name\n</code></pre>"},{"location":"reference/pydantic2django/#pydantic2django.ImportHandler.__init__","title":"<code>__init__(module_mappings=None)</code>","text":"<p>Initialize empty collections for different types of imports.</p> <p>Parameters:</p> Name Type Description Default <code>module_mappings</code> <code>Optional[dict[str, str]]</code> <p>Optional mapping of modules to remap (e.g. {\"main\": \"my_app.models\"})</p> <code>None</code> Source code in <code>src/pydantic2django/core/imports.py</code> <pre><code>def __init__(self, module_mappings: Optional[dict[str, str]] = None):\n    \"\"\"\n    Initialize empty collections for different types of imports.\n\n    Args:\n        module_mappings: Optional mapping of modules to remap (e.g. {\"__main__\": \"my_app.models\"})\n    \"\"\"\n    # Track imports by category\n    self.extra_type_imports: set[str] = set()  # For typing and other utility imports\n    self.pydantic_imports: set[str] = set()  # For Pydantic model imports\n    self.context_class_imports: set[str] = set()  # For context class and field type imports\n\n    # For tracking imported names to avoid duplicates\n    self.imported_names: dict[str, str] = {}  # Maps type name to its module\n\n    # For tracking field type dependencies we've already processed\n    self.processed_field_types: set[str] = set()\n\n    # Module mappings to remap imports (e.g. \"__main__\" -&gt; \"my_app.models\")\n    self.module_mappings = module_mappings or {}\n\n    logger.info(\"ImportHandler initialized\")\n    if self.module_mappings:\n        logger.info(f\"Using module mappings: {self.module_mappings}\")\n</code></pre>"},{"location":"reference/pydantic2django/#pydantic2django.ImportHandler.add_context_field_type_import","title":"<code>add_context_field_type_import(field_type)</code>","text":"<p>Add an import statement for a context field type with recursive dependency detection.</p> <p>Parameters:</p> Name Type Description Default <code>field_type</code> <code>Any</code> <p>The field type to import</p> required Source code in <code>src/pydantic2django/core/imports.py</code> <pre><code>def add_context_field_type_import(self, field_type: Any) -&gt; None:\n    \"\"\"\n    Add an import statement for a context field type with recursive dependency detection.\n\n    Args:\n        field_type: The field type to import\n    \"\"\"\n    # Skip if we've already processed this field type\n    field_type_str = str(field_type)\n    if field_type_str in self.processed_field_types:\n        logger.debug(f\"Skipping already processed field type: {field_type_str}\")\n        return\n\n    logger.info(f\"Processing context field type: {field_type_str}\")\n    self.processed_field_types.add(field_type_str)\n\n    # Try to add direct import for the field type if it's a class\n    self._add_type_import(field_type)\n\n    # Handle nested types in generics, unions, etc.\n    self._process_nested_types(field_type)\n\n    # Add typing imports based on the field type string\n    self._add_typing_imports(field_type_str)\n</code></pre>"},{"location":"reference/pydantic2django/#pydantic2django.ImportHandler.add_import","title":"<code>add_import(module, name)</code>","text":"<p>Adds a single import based on module and name strings.</p> Source code in <code>src/pydantic2django/core/imports.py</code> <pre><code>def add_import(self, module: str, name: str):\n    \"\"\"Adds a single import based on module and name strings.\"\"\"\n    if not module or module == \"builtins\":\n        return\n\n    # Apply module mappings\n    if module in self.module_mappings:\n        module = self.module_mappings[module]\n\n    # Clean name (e.g., remove generics for import statement)\n    clean_name = self._clean_generic_type(name)\n\n    # Check if already imported\n    if name in self.imported_names:\n        # Could verify module matches, but usually name is unique enough\n        logger.debug(f\"Skipping already imported name: {name} (from module {module})\")\n        return\n    if clean_name != name and clean_name in self.imported_names:\n        logger.debug(f\"Skipping already imported clean name: {clean_name} (from module {module})\")\n        return\n\n    # Determine category\n    # Simplistic: If module is known Pydantic, Django, or common stdlib -&gt; context\n    # Otherwise, if it's 'typing' -&gt; extra_type\n    # TODO: Refine categorization if needed (e.g., dedicated django_imports set)\n    import_statement = f\"from {module} import {clean_name}\"\n    if module == \"typing\":\n        self.extra_type_imports.add(clean_name)  # Add only name to typing imports set\n        logger.debug(f\"Adding typing import: {clean_name}\")\n    # elif module.startswith(\"django.\"):\n    # Add to a dedicated django set if we create one\n    #    self.context_class_imports.add(import_statement)\n    #    logger.info(f\"Adding Django import: {import_statement}\")\n    else:\n        # Default to context imports for non-typing\n        self.context_class_imports.add(import_statement)\n        logger.info(f\"Adding context class import: {import_statement}\")\n\n    # Mark as imported\n    self.imported_names[name] = module\n    if clean_name != name:\n        self.imported_names[clean_name] = module\n</code></pre>"},{"location":"reference/pydantic2django/#pydantic2django.ImportHandler.add_pydantic_model_import","title":"<code>add_pydantic_model_import(model_class)</code>","text":"<p>Add an import statement for a Pydantic model.</p> <p>Parameters:</p> Name Type Description Default <code>model_class</code> <code>type</code> <p>The Pydantic model class to import</p> required Source code in <code>src/pydantic2django/core/imports.py</code> <pre><code>def add_pydantic_model_import(self, model_class: type) -&gt; None:\n    \"\"\"\n    Add an import statement for a Pydantic model.\n\n    Args:\n        model_class: The Pydantic model class to import\n    \"\"\"\n    if not hasattr(model_class, \"__module__\") or not hasattr(model_class, \"__name__\"):\n        logger.warning(f\"Cannot add import for {model_class}: missing __module__ or __name__\")\n        return\n\n    module_path = model_class.__module__\n    model_name = self._clean_generic_type(model_class.__name__)\n\n    # Apply module mappings if needed\n    if module_path in self.module_mappings:\n        actual_module = self.module_mappings[module_path]\n        logger.debug(f\"Remapping module import: {module_path} -&gt; {actual_module}\")\n        module_path = actual_module\n\n    logger.debug(f\"Processing Pydantic model import: {model_name} from {module_path}\")\n\n    # Skip if already imported\n    if model_name in self.imported_names:\n        logger.debug(f\"Skipping already imported model: {model_name}\")\n        return\n\n    import_statement = f\"from {module_path} import {model_name}\"\n    logger.info(f\"Adding Pydantic import: {import_statement}\")\n    self.pydantic_imports.add(import_statement)\n    self.imported_names[model_name] = module_path\n</code></pre>"},{"location":"reference/pydantic2django/#pydantic2django.ImportHandler.deduplicate_imports","title":"<code>deduplicate_imports()</code>","text":"<p>De-duplicate imports between Pydantic models and context field types.</p> <p>Returns:</p> Type Description <code>dict[str, set[str]]</code> <p>Dict with de-duplicated import sets</p> Source code in <code>src/pydantic2django/core/imports.py</code> <pre><code>def deduplicate_imports(self) -&gt; dict[str, set[str]]:\n    \"\"\"\n    De-duplicate imports between Pydantic models and context field types.\n\n    Returns:\n        Dict with de-duplicated import sets\n    \"\"\"\n    logger.info(\"Deduplicating imports\")\n    logger.debug(f\"Current pydantic imports: {self.pydantic_imports}\")\n    logger.debug(f\"Current context imports: {self.context_class_imports}\")\n\n    # Extract class names and modules from import statements\n    pydantic_classes = {}\n    context_classes = {}\n\n    # Handle special case for TypeVar imports\n    typevars = set()\n\n    for import_stmt in self.pydantic_imports:\n        if import_stmt.startswith(\"from \") and \" import \" in import_stmt:\n            module, classes = import_stmt.split(\" import \")\n            module = module.replace(\"from \", \"\")\n\n            # Skip __main__ and rewrite to real module paths if possible\n            if module == \"__main__\":\n                logger.warning(f\"Skipping __main__ import: {import_stmt} - these won't work when imported\")\n                continue\n\n            for cls in classes.split(\", \"):\n                # Check if it's a TypeVar to handle duplicate definitions\n                if cls == \"T\" or cls == \"TypeVar\":\n                    typevars.add(cls)\n                    continue\n\n                # Clean up any parameterized generic types in class names\n                cls = self._clean_generic_type(cls)\n                pydantic_classes[cls] = module\n\n    for import_stmt in self.context_class_imports:\n        if import_stmt.startswith(\"from \") and \" import \" in import_stmt:\n            module, classes = import_stmt.split(\" import \")\n            module = module.replace(\"from \", \"\")\n\n            # Skip __main__ imports or rewrite to real module paths if possible\n            if module == \"__main__\":\n                logger.warning(f\"Skipping __main__ import: {import_stmt} - these won't work when imported\")\n                continue\n\n            for cls in classes.split(\", \"):\n                # Check if it's a TypeVar to handle duplicate definitions\n                if cls == \"T\" or cls == \"TypeVar\":\n                    typevars.add(cls)\n                    continue\n\n                # Clean up any parameterized generic types in class names\n                cls = self._clean_generic_type(cls)\n                # If this class is already imported in pydantic imports, skip it\n                if cls in pydantic_classes:\n                    logger.debug(f\"Skipping duplicate context import for {cls}, already in pydantic imports\")\n                    continue\n                context_classes[cls] = module\n\n    # Rebuild import statements\n    module_to_classes = {}\n    for cls, module in pydantic_classes.items():\n        if module not in module_to_classes:\n            module_to_classes[module] = []\n        module_to_classes[module].append(cls)\n\n    deduplicated_pydantic_imports = set()\n    for module, classes in module_to_classes.items():\n        deduplicated_pydantic_imports.add(f\"from {module} import {', '.join(sorted(classes))}\")\n\n    # Same for context imports\n    module_to_classes = {}\n    for cls, module in context_classes.items():\n        if module not in module_to_classes:\n            module_to_classes[module] = []\n        module_to_classes[module].append(cls)\n\n    deduplicated_context_imports = set()\n    for module, classes in module_to_classes.items():\n        deduplicated_context_imports.add(f\"from {module} import {', '.join(sorted(classes))}\")\n\n    logger.info(f\"Final pydantic imports: {deduplicated_pydantic_imports}\")\n    logger.info(f\"Final context imports: {deduplicated_context_imports}\")\n\n    # Log any TypeVar names we're skipping\n    if typevars:\n        logger.info(f\"Skipping TypeVar imports: {typevars} - these will be defined locally\")\n\n    return {\"pydantic\": deduplicated_pydantic_imports, \"context\": deduplicated_context_imports}\n</code></pre>"},{"location":"reference/pydantic2django/#pydantic2django.ImportHandler.get_required_imports","title":"<code>get_required_imports(field_type_str)</code>","text":"<p>Get typing and custom type imports required for a field type.</p> <p>Parameters:</p> Name Type Description Default <code>field_type_str</code> <code>str</code> <p>String representation of a field type</p> required <p>Returns:</p> Type Description <code>dict[str, list[str]]</code> <p>Dictionary with \"typing\" and \"custom\" import lists</p> Source code in <code>src/pydantic2django/core/imports.py</code> <pre><code>def get_required_imports(self, field_type_str: str) -&gt; dict[str, list[str]]:\n    \"\"\"\n    Get typing and custom type imports required for a field type.\n\n    Args:\n        field_type_str: String representation of a field type\n\n    Returns:\n        Dictionary with \"typing\" and \"custom\" import lists\n    \"\"\"\n    logger.debug(f\"Getting required imports for: {field_type_str}\")\n    self._add_typing_imports(field_type_str)\n\n    # Get custom types (non-typing types)\n    custom_types = [\n        name\n        for name in self.extra_type_imports\n        if name not in [\"List\", \"Dict\", \"Tuple\", \"Set\", \"Optional\", \"Union\", \"Any\", \"Callable\"]\n    ]\n\n    logger.debug(f\"Found custom types: {custom_types}\")\n\n    # Return the latest state of imports\n    return {\n        \"typing\": list(self.extra_type_imports),\n        \"custom\": custom_types,\n    }\n</code></pre>"},{"location":"reference/pydantic2django/#pydantic2django.configure_core_typing_logging","title":"<code>configure_core_typing_logging(level=logging.WARNING, format_str='%Y-%m-%d %H:%M:%S - %(name)s - %(levelname)s - %(message)s')</code>","text":"<p>Configure the logging for core typing module.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>The logging level (e.g., logging.DEBUG, logging.INFO)</p> <code>WARNING</code> <code>format_str</code> <code>str</code> <p>The format string for log messages</p> <code>'%Y-%m-%d %H:%M:%S - %(name)s - %(levelname)s - %(message)s'</code> Source code in <code>src/pydantic2django/core/typing.py</code> <pre><code>def configure_core_typing_logging(\n    level: int = logging.WARNING,\n    format_str: str = \"%Y-%m-%d %H:%M:%S - %(name)s - %(levelname)s - %(message)s\",\n) -&gt; None:\n    \"\"\"\n    Configure the logging for core typing module.\n\n    Args:\n        level: The logging level (e.g., logging.DEBUG, logging.INFO)\n        format_str: The format string for log messages\n    \"\"\"\n    handler = logging.StreamHandler()\n    formatter = logging.Formatter(format_str)\n    handler.setFormatter(formatter)\n\n    logger.setLevel(level)\n    logger.addHandler(handler)\n    logger.propagate = False  # Prevent duplicate logging\n\n    logger.debug(\"Core typing logging configured\")\n</code></pre>"},{"location":"reference/pydantic2django/core/","title":"pydantic2django.core","text":""},{"location":"reference/pydantic2django/core/base_generator/","title":"pydantic2django.core.base_generator","text":""},{"location":"reference/pydantic2django/core/base_generator/#pydantic2django.core.base_generator.BaseStaticGenerator","title":"<code>BaseStaticGenerator</code>","text":"<p>               Bases: <code>ABC</code>, <code>Generic[SourceModelType, FieldInfoType]</code></p> <p>Abstract base class for generating static Django models from source models (like Pydantic or Dataclasses).</p> Source code in <code>src/pydantic2django/core/base_generator.py</code> <pre><code>class BaseStaticGenerator(ABC, Generic[SourceModelType, FieldInfoType]):\n    \"\"\"\n    Abstract base class for generating static Django models from source models (like Pydantic or Dataclasses).\n    \"\"\"\n\n    def __init__(\n        self,\n        output_path: str,\n        app_label: str,\n        filter_function: Callable[[type[SourceModelType]], bool] | None,\n        verbose: bool,\n        discovery_instance: BaseDiscovery[SourceModelType],\n        model_factory_instance: BaseModelFactory[SourceModelType, FieldInfoType],\n        module_mappings: dict[str, str] | None,\n        base_model_class: type[models.Model],\n        packages: list[str] | None = None,\n        class_name_prefix: str = \"Django\",\n        enable_timescale: bool = True,\n        # --- GFK flags ---\n        enable_gfk: bool = True,\n        gfk_policy: str | None = \"all_nested\",\n        gfk_threshold_children: int | None = 4,\n        gfk_value_mode: str | None = \"typed_columns\",\n        gfk_normalize_common_attrs: bool = False,\n    ):\n        \"\"\"\n        Initialize the base generator.\n\n        Args:\n            output_path: Path to output the generated models.py file.\n            packages: List of packages to scan for source models.\n            app_label: Django app label to use for the models.\n            filter_function: Optional function to filter which source models to include.\n            verbose: Print verbose output.\n            discovery_instance: An instance of a BaseDiscovery subclass.\n            model_factory_instance: An instance of a BaseModelFactory subclass.\n            module_mappings: Optional mapping of modules to remap imports.\n            base_model_class: The base Django model class to inherit from.\n            class_name_prefix: Prefix for generated Django model class names.\n            enable_timescale: Whether to enable TimescaleDB support for hypertables.\n        \"\"\"\n        self.output_path = output_path\n        self.packages = packages\n        self.app_label = app_label\n        self.filter_function = filter_function\n        self.verbose = verbose\n        self.discovery_instance = discovery_instance\n        self.model_factory_instance = model_factory_instance\n        # Base model class must be provided explicitly by subclass at call site.\n        self.base_model_class = base_model_class\n        self.class_name_prefix = class_name_prefix\n        self.carriers: list[ConversionCarrier[SourceModelType]] = []  # Stores results from model factory\n        # Feature flags\n        self.enable_timescale: bool = bool(enable_timescale)\n        # GFK feature flags\n        self.enable_gfk: bool = bool(enable_gfk)\n        self.gfk_policy: str | None = gfk_policy\n        self.gfk_threshold_children: int | None = gfk_threshold_children\n        self.gfk_value_mode: str | None = gfk_value_mode\n        self.gfk_normalize_common_attrs: bool = bool(gfk_normalize_common_attrs)\n\n        self.import_handler = ImportHandler(module_mappings=module_mappings)\n\n        # Initialize Jinja2 environment\n        # Look for templates in the django/templates subdirectory\n        # package_templates_dir = os.path.join(os.path.dirname(__file__), \"..\", \"templates\") # Old path\n        package_templates_dir = os.path.join(os.path.dirname(__file__), \"..\", \"django\", \"templates\")  # Corrected path\n\n        # If templates don't exist in the package, use the ones relative to the execution?\n        # This might need adjustment based on packaging/distribution.\n        # For now, assume templates are relative to the package structure.\n        if not os.path.exists(package_templates_dir):\n            # Fallback or raise error might be needed\n            package_templates_dir = os.path.join(pathlib.Path(__file__).parent.parent.absolute(), \"templates\")\n            if not os.path.exists(package_templates_dir):\n                logger.warning(\n                    f\"Templates directory not found at expected location: {package_templates_dir}. Jinja might fail.\"\n                )\n\n        self.jinja_env = jinja2.Environment(\n            loader=jinja2.FileSystemLoader(package_templates_dir),\n            trim_blocks=True,\n            lstrip_blocks=True,\n        )\n\n        # Register common custom filters\n        self.jinja_env.filters[\"format_type_string\"] = TypeHandler.format_type_string\n        # Provide an escaping filter for embedding strings safely in generated Python code\n        from ..core.utils.strings import sanitize_string as _escape_py_str  # Local import to avoid cycles\n\n        self.jinja_env.filters[\"escape_py_str\"] = _escape_py_str\n        # Add more common filters if needed\n\n        # Add base model import\n        self.import_handler._add_type_import(self.base_model_class)\n\n    # --- Abstract Methods to be Implemented by Subclasses ---\n\n    @abstractmethod\n    def _get_source_model_name(self, carrier: ConversionCarrier[SourceModelType]) -&gt; str:\n        \"\"\"Get the name of the original source model from the carrier.\"\"\"\n        pass\n\n    @abstractmethod\n    def _add_source_model_import(self, carrier: ConversionCarrier[SourceModelType]):\n        \"\"\"Add the necessary import for the original source model.\"\"\"\n        pass\n\n    @abstractmethod\n    def _prepare_template_context(\n        self, unique_model_definitions: list[str], django_model_names: list[str], imports: dict\n    ) -&gt; dict:\n        \"\"\"Prepare the subclass-specific context for the main models_file.py.j2 template.\"\"\"\n        pass\n\n    @abstractmethod\n    def _get_models_in_processing_order(self) -&gt; list[SourceModelType]:\n        \"\"\"Return source models in the correct processing (dependency) order.\"\"\"\n        pass\n\n    @abstractmethod\n    def _get_model_definition_extra_context(self, carrier: ConversionCarrier[SourceModelType]) -&gt; dict:\n        \"\"\"Provide extra context specific to the source type for model_definition.py.j2.\"\"\"\n        pass\n\n    @abstractmethod\n    def _get_default_base_model_class(self) -&gt; type[models.Model]:\n        \"\"\"Return the required base Django model class for this generator.\n\n        Subclasses implement this so they can easily and explicitly resolve\n        the correct base and pass it into the base initializer.\n        \"\"\"\n        pass\n\n    # --- Common Methods ---\n\n    def generate(self) -&gt; str:\n        \"\"\"\n        Main entry point: Generate and write the models file.\n\n        Returns:\n            The path to the generated models file.\n        \"\"\"\n        try:\n            content = self.generate_models_file()\n            self._write_models_file(content)\n            logger.info(f\"Successfully generated models file at {self.output_path}\")\n            return self.output_path\n        except Exception as e:\n            logger.exception(f\"Error generating models file: {e}\", exc_info=True)  # Use exc_info for traceback\n            raise\n\n    def _write_models_file(self, content: str) -&gt; None:\n        \"\"\"Write the generated content to the output file.\"\"\"\n        if self.verbose:\n            logger.info(f\"Writing models to {self.output_path}\")\n\n        output_dir = os.path.dirname(self.output_path)\n        if output_dir and not os.path.exists(output_dir):\n            try:\n                os.makedirs(output_dir)\n                if self.verbose:\n                    logger.info(f\"Created output directory: {output_dir}\")\n            except OSError as e:\n                logger.error(f\"Failed to create output directory {output_dir}: {e}\")\n                raise  # Re-raise after logging\n\n        try:\n            with open(self.output_path, \"w\", encoding=\"utf-8\") as f:  # Specify encoding\n                f.write(content)\n            if self.verbose:\n                logger.info(f\"Successfully wrote models to {self.output_path}\")\n        except OSError as e:\n            logger.error(f\"Failed to write to output file {self.output_path}: {e}\")\n            raise  # Re-raise after logging\n\n    def discover_models(self) -&gt; None:\n        \"\"\"Discover source models using the configured discovery instance.\"\"\"\n        if self.verbose:\n            logger.info(f\"Discovering models from packages: {self.packages}\")\n\n        # Corrected call matching BaseDiscovery signature\n        self.discovery_instance.discover_models(\n            self.packages or [],  # Pass empty list if None\n            app_label=self.app_label,\n            user_filters=self.filter_function,  # Keep as is for now\n        )\n\n        # Analyze dependencies after discovery\n        self.discovery_instance.analyze_dependencies()\n\n        if self.verbose:\n            logger.info(f\"Discovered {len(self.discovery_instance.filtered_models)} models after filtering.\")\n            if self.discovery_instance.filtered_models:\n                for name in self.discovery_instance.filtered_models.keys():\n                    logger.info(f\"  - {name}\")\n            else:\n                logger.info(\"  (No models found or passed filter)\")\n            logger.info(\"Dependency analysis complete.\")\n\n    def setup_django_model(self, source_model: SourceModelType) -&gt; ConversionCarrier[SourceModelType] | None:\n        \"\"\"\n        Uses the model factory to create a Django model representation from a source model.\n\n        Args:\n            source_model: The source model instance (e.g., Pydantic class, Dataclass).\n\n        Returns:\n            A ConversionCarrier containing the results, or None if creation failed.\n        \"\"\"\n        source_model_name = getattr(source_model, \"__name__\", str(source_model))\n        if self.verbose:\n            logger.info(f\"Setting up Django model for {source_model_name}\")\n\n        # Instantiate the carrier here\n        carrier = ConversionCarrier(\n            source_model=cast(type[SourceModelType], source_model),\n            meta_app_label=self.app_label,\n            base_django_model=self.base_model_class,\n            class_name_prefix=self.class_name_prefix,\n            # Add other defaults/configs if needed, e.g., strict mode\n            strict=False,  # Example default\n            # GFK flags\n            enable_gfk=self.enable_gfk,\n            gfk_policy=self.gfk_policy,\n            gfk_threshold_children=self.gfk_threshold_children,\n            gfk_value_mode=self.gfk_value_mode,\n            gfk_normalize_common_attrs=self.gfk_normalize_common_attrs,\n        )\n\n        try:\n            # Use the factory to process the source model and populate the carrier\n            self.model_factory_instance.make_django_model(carrier)  # Pass carrier to factory\n\n            if carrier.django_model:\n                self.carriers.append(carrier)\n                if self.verbose:\n                    logger.info(f\"Successfully processed {source_model_name} -&gt; {carrier.django_model.__name__}\")\n                return carrier\n            else:\n                # Log if model creation resulted in None (e.g., only context fields)\n                # Check carrier.context_fields or carrier.invalid_fields for details\n                if carrier.context_fields and not carrier.django_fields and not carrier.relationship_fields:\n                    logger.info(f\"Skipped Django model class for {source_model_name} - only context fields found.\")\n                elif carrier.invalid_fields:\n                    logger.warning(\n                        f\"Skipped Django model class for {source_model_name} due to invalid fields: {carrier.invalid_fields}\"\n                    )\n                else:\n                    logger.warning(f\"Django model was not generated for {source_model_name} for unknown reasons.\")\n                return None  # Return None if no Django model was created\n\n        except Exception as e:\n            logger.error(f\"Error processing {source_model_name} with factory: {e}\", exc_info=True)\n            return None\n\n    def generate_model_definition(self, carrier: ConversionCarrier[SourceModelType]) -&gt; str:\n        \"\"\"\n        Generates a string definition for a single Django model using a template.\n\n        Args:\n            carrier: The ConversionCarrier containing the generated Django model and context.\n\n        Returns:\n            The string representation of the Django model definition.\n        \"\"\"\n        if not carrier.django_model:\n            # It's possible a carrier exists only for context, handle gracefully.\n            source_name = self._get_source_model_name(carrier)\n            if carrier.model_context and carrier.model_context.context_fields:\n                logger.info(f\"Skipping Django model definition for {source_name} (likely context-only).\")\n                return \"\"\n            else:\n                logger.warning(\n                    f\"Cannot generate model definition for {source_name}: django_model is missing in carrier.\"\n                )\n                return \"\"\n\n        django_model_name = self._clean_generic_type(carrier.django_model.__name__)\n        source_model_name = self._get_source_model_name(carrier)  # Get original name via abstract method\n\n        # --- Prepare Fields ---\n        fields_info = []\n        # Combine regular and relationship fields from the carrier\n        all_django_fields = {**carrier.django_fields, **carrier.relationship_fields}\n\n        for field_name, field_object in all_django_fields.items():\n            # The field_object is already an instantiated Django field\n            # Add (name, object) tuple directly for the template\n            fields_info.append((field_name, field_object))\n\n        # --- Prepare Meta ---\n        meta_options = {}\n        if hasattr(carrier.django_model, \"_meta\"):\n            model_meta = carrier.django_model._meta\n            meta_options = {\n                \"db_table\": getattr(model_meta, \"db_table\", f\"{self.app_label}_{django_model_name.lower()}\"),\n                \"app_label\": self.app_label,\n                \"verbose_name\": getattr(model_meta, \"verbose_name\", django_model_name),\n                \"verbose_name_plural\": getattr(model_meta, \"verbose_name_plural\", f\"{django_model_name}s\"),\n                # Add other meta options if needed\n            }\n\n        # --- Prepare Base Class Info ---\n        base_model_name = self.base_model_class.__name__\n        if carrier.django_model.__bases__ and carrier.django_model.__bases__[0] != models.Model:\n            # Use the immediate parent if it's not the absolute base 'models.Model'\n            # Assumes single inheritance for the generated model besides the ultimate base\n            parent_class = carrier.django_model.__bases__[0]\n            # Check if the parent is our intended base_model_class or something else\n            # This logic might need refinement depending on how complex the inheritance gets\n            if issubclass(parent_class, models.Model) and parent_class != models.Model:\n                base_model_name = parent_class.__name__\n                # Add import for the parent if it's not the configured base_model_class\n                if parent_class != self.base_model_class:\n                    self.import_handler._add_type_import(parent_class)\n\n        # --- Get Subclass Specific Context ---\n        extra_context = self._get_model_definition_extra_context(carrier)\n\n        # --- Process Pending Multi-FK Unions and add to definitions dict ---\n        multi_fk_field_names = []  # Keep track for validation hint\n        validation_needed = False\n        if carrier.pending_multi_fk_unions:\n            validation_needed = True\n            for original_field_name, union_details in carrier.pending_multi_fk_unions:\n                pydantic_models = union_details.get(\"models\", [])\n                for pydantic_model in pydantic_models:\n                    # Construct field name (e.g., original_name_relatedmodel)\n                    fk_field_name = f\"{original_field_name}_{pydantic_model.__name__.lower()}\"\n                    multi_fk_field_names.append(fk_field_name)\n                    # Get corresponding Django model\n                    pydantic_factory = cast(PydanticModelFactory, self.model_factory_instance)\n                    django_model_rel = pydantic_factory.relationship_accessor.get_django_model_for_pydantic(\n                        pydantic_model\n                    )\n                    if not django_model_rel:\n                        logger.error(\n                            f\"Could not find Django model for Pydantic model {pydantic_model.__name__} referenced in multi-FK union for {original_field_name}. Skipping FK field.\"\n                        )\n                        continue\n                    # Use string for model ref in kwargs\n                    target_model_str = f\"'{django_model_rel._meta.app_label}.{django_model_rel.__name__}'\"\n                    # Add import for the related Django model\n                    self.import_handler._add_type_import(django_model_rel)\n\n                    # Define FK kwargs (always null=True, blank=True)\n                    # Use strings for values that need to be represented in code\n                    fk_kwargs = {\n                        \"to\": target_model_str,\n                        \"on_delete\": \"models.SET_NULL\",  # Use string for template\n                        \"null\": True,\n                        \"blank\": True,\n                        # Generate related_name to avoid clashes\n                        \"related_name\": f\"'{carrier.django_model.__name__.lower()}_{fk_field_name}_set'\",  # Ensure related_name is quoted string\n                    }\n                    # Generate the definition string\n                    fk_def_string = generate_field_definition_string(models.ForeignKey, fk_kwargs, self.app_label)\n                    # Add to the main definitions dictionary\n                    carrier.django_field_definitions[fk_field_name] = fk_def_string\n\n        # --- Prepare Final Context --- #\n        # Ensure the context uses the potentially updated definitions dict from the carrier\n        # Subclass _get_model_definition_extra_context should already provide this\n        # via `field_definitions=carrier.django_field_definitions`\n        template_context = {\n            \"model_name\": django_model_name,\n            \"pydantic_model_name\": source_model_name,\n            \"base_model_name\": base_model_name,\n            \"is_timescale_model\": bool(str(base_model_name).endswith(\"TimescaleBase\")),\n            \"meta\": meta_options,\n            \"app_label\": self.app_label,\n            \"multi_fk_field_names\": multi_fk_field_names,  # Pass names for validation hint\n            \"validation_needed\": validation_needed,  # Signal if validation needed\n            # Include extra context from subclass (should include field_definitions)\n            **extra_context,\n        }\n\n        # --- Render Template --- #\n        template = self.jinja_env.get_template(\"model_definition.py.j2\")\n        definition_str = template.render(**template_context)\n\n        # Add import for the original source model\n        self._add_source_model_import(carrier)\n\n        return definition_str\n\n    def _deduplicate_definitions(self, definitions: list[str]) -&gt; list[str]:\n        \"\"\"Remove duplicate model definitions based on class name.\"\"\"\n        unique_definitions = []\n        seen_class_names = set()\n        for definition in definitions:\n            # Basic regex to find 'class ClassName(' - might need adjustment for complex cases\n            match = re.search(r\"^\\s*class\\s+(\\w+)\\(\", definition, re.MULTILINE)\n            if match:\n                class_name = match.group(1)\n                if class_name not in seen_class_names:\n                    unique_definitions.append(definition)\n                    seen_class_names.add(class_name)\n                # else: logger.debug(f\"Skipping duplicate definition for class: {class_name}\")\n            else:\n                # If no class definition found (e.g., comments, imports), keep it? Or discard?\n                # For now, keep non-class definitions assuming they might be needed context/comments.\n                unique_definitions.append(definition)\n                logger.warning(\"Could not extract class name from definition block for deduplication.\")\n\n        return unique_definitions\n\n    def _clean_generic_type(self, name: str) -&gt; str:\n        \"\"\"Remove generic parameters like [T] or &lt;T&gt; from a type name.\"\"\"\n        # Handles Class[Param] or Class&lt;Param&gt;\n        cleaned_name = re.sub(r\"[\\[&lt;].*?[\\]&gt;]\", \"\", name)\n        # Also handle cases like 'ModelName.T' if typevars are used this way\n        cleaned_name = cleaned_name.split(\".\")[-1]\n        return cleaned_name\n\n    def generate_models_file(self) -&gt; str:\n        \"\"\"\n        Generates the complete content for the models.py file.\n        This method orchestrates discovery, model setup, definition generation,\n        import collection, and template rendering.\n        Subclasses might override this to add specific steps (like context class generation).\n        \"\"\"\n        self.discover_models()  # Populates discovery instance\n        models_to_process = self._get_models_in_processing_order()  # Abstract method\n\n        # Reset state for this run\n        self.carriers = []\n        self.import_handler.extra_type_imports.clear()\n        self.import_handler.pydantic_imports.clear()\n        self.import_handler.context_class_imports.clear()\n        self.import_handler.imported_names.clear()\n        self.import_handler.processed_field_types.clear()\n\n        # Re-add base model import after clearing\n        self.import_handler._add_type_import(self.base_model_class)\n\n        model_definitions = []\n        django_model_names = []  # For __all__\n\n        # Setup Django models first (populates self.carriers)\n        for source_model in models_to_process:\n            self.setup_django_model(source_model)  # Calls factory, populates carrier\n\n        # Generate definitions from carriers\n        for carrier in self.carriers:\n            # Generate Django model definition if model exists\n            if carrier.django_model:\n                try:\n                    model_def = self.generate_model_definition(carrier)  # Uses template\n                    if model_def:  # Only add if definition was generated\n                        model_definitions.append(model_def)\n                        django_model_name = self._clean_generic_type(carrier.django_model.__name__)\n                        django_model_names.append(f\"'{django_model_name}'\")\n                except Exception as e:\n                    source_name = self._get_source_model_name(carrier)\n                    logger.error(f\"Error generating definition for source model {source_name}: {e}\", exc_info=True)\n\n            # Subclasses might add context class generation here by overriding this method\n            # or by generate_model_definition adding context-related imports.\n\n        # Deduplicate definitions\n        unique_model_definitions = self._deduplicate_definitions(model_definitions)\n\n        # Deduplicate imports gathered during the process\n        imports = self.import_handler.deduplicate_imports()\n\n        # Prepare context using subclass method (_prepare_template_context)\n        template_context = self._prepare_template_context(unique_model_definitions, django_model_names, imports)\n\n        # Add common context items\n        template_context.update(\n            {\n                \"generation_timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n                \"base_model_module\": self.base_model_class.__module__,\n                \"base_model_name\": self.base_model_class.__name__,\n                \"extra_type_imports\": sorted(self.import_handler.extra_type_imports),\n                # Add other common items as needed\n            }\n        )\n\n        # Render the main template\n        template = self.jinja_env.get_template(\"models_file.py.j2\")\n        return template.render(**template_context)\n</code></pre>"},{"location":"reference/pydantic2django/core/base_generator/#pydantic2django.core.base_generator.BaseStaticGenerator.__init__","title":"<code>__init__(output_path, app_label, filter_function, verbose, discovery_instance, model_factory_instance, module_mappings, base_model_class, packages=None, class_name_prefix='Django', enable_timescale=True, enable_gfk=True, gfk_policy='all_nested', gfk_threshold_children=4, gfk_value_mode='typed_columns', gfk_normalize_common_attrs=False)</code>","text":"<p>Initialize the base generator.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>str</code> <p>Path to output the generated models.py file.</p> required <code>packages</code> <code>list[str] | None</code> <p>List of packages to scan for source models.</p> <code>None</code> <code>app_label</code> <code>str</code> <p>Django app label to use for the models.</p> required <code>filter_function</code> <code>Callable[[type[SourceModelType]], bool] | None</code> <p>Optional function to filter which source models to include.</p> required <code>verbose</code> <code>bool</code> <p>Print verbose output.</p> required <code>discovery_instance</code> <code>BaseDiscovery[SourceModelType]</code> <p>An instance of a BaseDiscovery subclass.</p> required <code>model_factory_instance</code> <code>BaseModelFactory[SourceModelType, FieldInfoType]</code> <p>An instance of a BaseModelFactory subclass.</p> required <code>module_mappings</code> <code>dict[str, str] | None</code> <p>Optional mapping of modules to remap imports.</p> required <code>base_model_class</code> <code>type[Model]</code> <p>The base Django model class to inherit from.</p> required <code>class_name_prefix</code> <code>str</code> <p>Prefix for generated Django model class names.</p> <code>'Django'</code> <code>enable_timescale</code> <code>bool</code> <p>Whether to enable TimescaleDB support for hypertables.</p> <code>True</code> Source code in <code>src/pydantic2django/core/base_generator.py</code> <pre><code>def __init__(\n    self,\n    output_path: str,\n    app_label: str,\n    filter_function: Callable[[type[SourceModelType]], bool] | None,\n    verbose: bool,\n    discovery_instance: BaseDiscovery[SourceModelType],\n    model_factory_instance: BaseModelFactory[SourceModelType, FieldInfoType],\n    module_mappings: dict[str, str] | None,\n    base_model_class: type[models.Model],\n    packages: list[str] | None = None,\n    class_name_prefix: str = \"Django\",\n    enable_timescale: bool = True,\n    # --- GFK flags ---\n    enable_gfk: bool = True,\n    gfk_policy: str | None = \"all_nested\",\n    gfk_threshold_children: int | None = 4,\n    gfk_value_mode: str | None = \"typed_columns\",\n    gfk_normalize_common_attrs: bool = False,\n):\n    \"\"\"\n    Initialize the base generator.\n\n    Args:\n        output_path: Path to output the generated models.py file.\n        packages: List of packages to scan for source models.\n        app_label: Django app label to use for the models.\n        filter_function: Optional function to filter which source models to include.\n        verbose: Print verbose output.\n        discovery_instance: An instance of a BaseDiscovery subclass.\n        model_factory_instance: An instance of a BaseModelFactory subclass.\n        module_mappings: Optional mapping of modules to remap imports.\n        base_model_class: The base Django model class to inherit from.\n        class_name_prefix: Prefix for generated Django model class names.\n        enable_timescale: Whether to enable TimescaleDB support for hypertables.\n    \"\"\"\n    self.output_path = output_path\n    self.packages = packages\n    self.app_label = app_label\n    self.filter_function = filter_function\n    self.verbose = verbose\n    self.discovery_instance = discovery_instance\n    self.model_factory_instance = model_factory_instance\n    # Base model class must be provided explicitly by subclass at call site.\n    self.base_model_class = base_model_class\n    self.class_name_prefix = class_name_prefix\n    self.carriers: list[ConversionCarrier[SourceModelType]] = []  # Stores results from model factory\n    # Feature flags\n    self.enable_timescale: bool = bool(enable_timescale)\n    # GFK feature flags\n    self.enable_gfk: bool = bool(enable_gfk)\n    self.gfk_policy: str | None = gfk_policy\n    self.gfk_threshold_children: int | None = gfk_threshold_children\n    self.gfk_value_mode: str | None = gfk_value_mode\n    self.gfk_normalize_common_attrs: bool = bool(gfk_normalize_common_attrs)\n\n    self.import_handler = ImportHandler(module_mappings=module_mappings)\n\n    # Initialize Jinja2 environment\n    # Look for templates in the django/templates subdirectory\n    # package_templates_dir = os.path.join(os.path.dirname(__file__), \"..\", \"templates\") # Old path\n    package_templates_dir = os.path.join(os.path.dirname(__file__), \"..\", \"django\", \"templates\")  # Corrected path\n\n    # If templates don't exist in the package, use the ones relative to the execution?\n    # This might need adjustment based on packaging/distribution.\n    # For now, assume templates are relative to the package structure.\n    if not os.path.exists(package_templates_dir):\n        # Fallback or raise error might be needed\n        package_templates_dir = os.path.join(pathlib.Path(__file__).parent.parent.absolute(), \"templates\")\n        if not os.path.exists(package_templates_dir):\n            logger.warning(\n                f\"Templates directory not found at expected location: {package_templates_dir}. Jinja might fail.\"\n            )\n\n    self.jinja_env = jinja2.Environment(\n        loader=jinja2.FileSystemLoader(package_templates_dir),\n        trim_blocks=True,\n        lstrip_blocks=True,\n    )\n\n    # Register common custom filters\n    self.jinja_env.filters[\"format_type_string\"] = TypeHandler.format_type_string\n    # Provide an escaping filter for embedding strings safely in generated Python code\n    from ..core.utils.strings import sanitize_string as _escape_py_str  # Local import to avoid cycles\n\n    self.jinja_env.filters[\"escape_py_str\"] = _escape_py_str\n    # Add more common filters if needed\n\n    # Add base model import\n    self.import_handler._add_type_import(self.base_model_class)\n</code></pre>"},{"location":"reference/pydantic2django/core/base_generator/#pydantic2django.core.base_generator.BaseStaticGenerator.discover_models","title":"<code>discover_models()</code>","text":"<p>Discover source models using the configured discovery instance.</p> Source code in <code>src/pydantic2django/core/base_generator.py</code> <pre><code>def discover_models(self) -&gt; None:\n    \"\"\"Discover source models using the configured discovery instance.\"\"\"\n    if self.verbose:\n        logger.info(f\"Discovering models from packages: {self.packages}\")\n\n    # Corrected call matching BaseDiscovery signature\n    self.discovery_instance.discover_models(\n        self.packages or [],  # Pass empty list if None\n        app_label=self.app_label,\n        user_filters=self.filter_function,  # Keep as is for now\n    )\n\n    # Analyze dependencies after discovery\n    self.discovery_instance.analyze_dependencies()\n\n    if self.verbose:\n        logger.info(f\"Discovered {len(self.discovery_instance.filtered_models)} models after filtering.\")\n        if self.discovery_instance.filtered_models:\n            for name in self.discovery_instance.filtered_models.keys():\n                logger.info(f\"  - {name}\")\n        else:\n            logger.info(\"  (No models found or passed filter)\")\n        logger.info(\"Dependency analysis complete.\")\n</code></pre>"},{"location":"reference/pydantic2django/core/base_generator/#pydantic2django.core.base_generator.BaseStaticGenerator.generate","title":"<code>generate()</code>","text":"<p>Main entry point: Generate and write the models file.</p> <p>Returns:</p> Type Description <code>str</code> <p>The path to the generated models file.</p> Source code in <code>src/pydantic2django/core/base_generator.py</code> <pre><code>def generate(self) -&gt; str:\n    \"\"\"\n    Main entry point: Generate and write the models file.\n\n    Returns:\n        The path to the generated models file.\n    \"\"\"\n    try:\n        content = self.generate_models_file()\n        self._write_models_file(content)\n        logger.info(f\"Successfully generated models file at {self.output_path}\")\n        return self.output_path\n    except Exception as e:\n        logger.exception(f\"Error generating models file: {e}\", exc_info=True)  # Use exc_info for traceback\n        raise\n</code></pre>"},{"location":"reference/pydantic2django/core/base_generator/#pydantic2django.core.base_generator.BaseStaticGenerator.generate_model_definition","title":"<code>generate_model_definition(carrier)</code>","text":"<p>Generates a string definition for a single Django model using a template.</p> <p>Parameters:</p> Name Type Description Default <code>carrier</code> <code>ConversionCarrier[SourceModelType]</code> <p>The ConversionCarrier containing the generated Django model and context.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The string representation of the Django model definition.</p> Source code in <code>src/pydantic2django/core/base_generator.py</code> <pre><code>def generate_model_definition(self, carrier: ConversionCarrier[SourceModelType]) -&gt; str:\n    \"\"\"\n    Generates a string definition for a single Django model using a template.\n\n    Args:\n        carrier: The ConversionCarrier containing the generated Django model and context.\n\n    Returns:\n        The string representation of the Django model definition.\n    \"\"\"\n    if not carrier.django_model:\n        # It's possible a carrier exists only for context, handle gracefully.\n        source_name = self._get_source_model_name(carrier)\n        if carrier.model_context and carrier.model_context.context_fields:\n            logger.info(f\"Skipping Django model definition for {source_name} (likely context-only).\")\n            return \"\"\n        else:\n            logger.warning(\n                f\"Cannot generate model definition for {source_name}: django_model is missing in carrier.\"\n            )\n            return \"\"\n\n    django_model_name = self._clean_generic_type(carrier.django_model.__name__)\n    source_model_name = self._get_source_model_name(carrier)  # Get original name via abstract method\n\n    # --- Prepare Fields ---\n    fields_info = []\n    # Combine regular and relationship fields from the carrier\n    all_django_fields = {**carrier.django_fields, **carrier.relationship_fields}\n\n    for field_name, field_object in all_django_fields.items():\n        # The field_object is already an instantiated Django field\n        # Add (name, object) tuple directly for the template\n        fields_info.append((field_name, field_object))\n\n    # --- Prepare Meta ---\n    meta_options = {}\n    if hasattr(carrier.django_model, \"_meta\"):\n        model_meta = carrier.django_model._meta\n        meta_options = {\n            \"db_table\": getattr(model_meta, \"db_table\", f\"{self.app_label}_{django_model_name.lower()}\"),\n            \"app_label\": self.app_label,\n            \"verbose_name\": getattr(model_meta, \"verbose_name\", django_model_name),\n            \"verbose_name_plural\": getattr(model_meta, \"verbose_name_plural\", f\"{django_model_name}s\"),\n            # Add other meta options if needed\n        }\n\n    # --- Prepare Base Class Info ---\n    base_model_name = self.base_model_class.__name__\n    if carrier.django_model.__bases__ and carrier.django_model.__bases__[0] != models.Model:\n        # Use the immediate parent if it's not the absolute base 'models.Model'\n        # Assumes single inheritance for the generated model besides the ultimate base\n        parent_class = carrier.django_model.__bases__[0]\n        # Check if the parent is our intended base_model_class or something else\n        # This logic might need refinement depending on how complex the inheritance gets\n        if issubclass(parent_class, models.Model) and parent_class != models.Model:\n            base_model_name = parent_class.__name__\n            # Add import for the parent if it's not the configured base_model_class\n            if parent_class != self.base_model_class:\n                self.import_handler._add_type_import(parent_class)\n\n    # --- Get Subclass Specific Context ---\n    extra_context = self._get_model_definition_extra_context(carrier)\n\n    # --- Process Pending Multi-FK Unions and add to definitions dict ---\n    multi_fk_field_names = []  # Keep track for validation hint\n    validation_needed = False\n    if carrier.pending_multi_fk_unions:\n        validation_needed = True\n        for original_field_name, union_details in carrier.pending_multi_fk_unions:\n            pydantic_models = union_details.get(\"models\", [])\n            for pydantic_model in pydantic_models:\n                # Construct field name (e.g., original_name_relatedmodel)\n                fk_field_name = f\"{original_field_name}_{pydantic_model.__name__.lower()}\"\n                multi_fk_field_names.append(fk_field_name)\n                # Get corresponding Django model\n                pydantic_factory = cast(PydanticModelFactory, self.model_factory_instance)\n                django_model_rel = pydantic_factory.relationship_accessor.get_django_model_for_pydantic(\n                    pydantic_model\n                )\n                if not django_model_rel:\n                    logger.error(\n                        f\"Could not find Django model for Pydantic model {pydantic_model.__name__} referenced in multi-FK union for {original_field_name}. Skipping FK field.\"\n                    )\n                    continue\n                # Use string for model ref in kwargs\n                target_model_str = f\"'{django_model_rel._meta.app_label}.{django_model_rel.__name__}'\"\n                # Add import for the related Django model\n                self.import_handler._add_type_import(django_model_rel)\n\n                # Define FK kwargs (always null=True, blank=True)\n                # Use strings for values that need to be represented in code\n                fk_kwargs = {\n                    \"to\": target_model_str,\n                    \"on_delete\": \"models.SET_NULL\",  # Use string for template\n                    \"null\": True,\n                    \"blank\": True,\n                    # Generate related_name to avoid clashes\n                    \"related_name\": f\"'{carrier.django_model.__name__.lower()}_{fk_field_name}_set'\",  # Ensure related_name is quoted string\n                }\n                # Generate the definition string\n                fk_def_string = generate_field_definition_string(models.ForeignKey, fk_kwargs, self.app_label)\n                # Add to the main definitions dictionary\n                carrier.django_field_definitions[fk_field_name] = fk_def_string\n\n    # --- Prepare Final Context --- #\n    # Ensure the context uses the potentially updated definitions dict from the carrier\n    # Subclass _get_model_definition_extra_context should already provide this\n    # via `field_definitions=carrier.django_field_definitions`\n    template_context = {\n        \"model_name\": django_model_name,\n        \"pydantic_model_name\": source_model_name,\n        \"base_model_name\": base_model_name,\n        \"is_timescale_model\": bool(str(base_model_name).endswith(\"TimescaleBase\")),\n        \"meta\": meta_options,\n        \"app_label\": self.app_label,\n        \"multi_fk_field_names\": multi_fk_field_names,  # Pass names for validation hint\n        \"validation_needed\": validation_needed,  # Signal if validation needed\n        # Include extra context from subclass (should include field_definitions)\n        **extra_context,\n    }\n\n    # --- Render Template --- #\n    template = self.jinja_env.get_template(\"model_definition.py.j2\")\n    definition_str = template.render(**template_context)\n\n    # Add import for the original source model\n    self._add_source_model_import(carrier)\n\n    return definition_str\n</code></pre>"},{"location":"reference/pydantic2django/core/base_generator/#pydantic2django.core.base_generator.BaseStaticGenerator.generate_models_file","title":"<code>generate_models_file()</code>","text":"<p>Generates the complete content for the models.py file. This method orchestrates discovery, model setup, definition generation, import collection, and template rendering. Subclasses might override this to add specific steps (like context class generation).</p> Source code in <code>src/pydantic2django/core/base_generator.py</code> <pre><code>def generate_models_file(self) -&gt; str:\n    \"\"\"\n    Generates the complete content for the models.py file.\n    This method orchestrates discovery, model setup, definition generation,\n    import collection, and template rendering.\n    Subclasses might override this to add specific steps (like context class generation).\n    \"\"\"\n    self.discover_models()  # Populates discovery instance\n    models_to_process = self._get_models_in_processing_order()  # Abstract method\n\n    # Reset state for this run\n    self.carriers = []\n    self.import_handler.extra_type_imports.clear()\n    self.import_handler.pydantic_imports.clear()\n    self.import_handler.context_class_imports.clear()\n    self.import_handler.imported_names.clear()\n    self.import_handler.processed_field_types.clear()\n\n    # Re-add base model import after clearing\n    self.import_handler._add_type_import(self.base_model_class)\n\n    model_definitions = []\n    django_model_names = []  # For __all__\n\n    # Setup Django models first (populates self.carriers)\n    for source_model in models_to_process:\n        self.setup_django_model(source_model)  # Calls factory, populates carrier\n\n    # Generate definitions from carriers\n    for carrier in self.carriers:\n        # Generate Django model definition if model exists\n        if carrier.django_model:\n            try:\n                model_def = self.generate_model_definition(carrier)  # Uses template\n                if model_def:  # Only add if definition was generated\n                    model_definitions.append(model_def)\n                    django_model_name = self._clean_generic_type(carrier.django_model.__name__)\n                    django_model_names.append(f\"'{django_model_name}'\")\n            except Exception as e:\n                source_name = self._get_source_model_name(carrier)\n                logger.error(f\"Error generating definition for source model {source_name}: {e}\", exc_info=True)\n\n        # Subclasses might add context class generation here by overriding this method\n        # or by generate_model_definition adding context-related imports.\n\n    # Deduplicate definitions\n    unique_model_definitions = self._deduplicate_definitions(model_definitions)\n\n    # Deduplicate imports gathered during the process\n    imports = self.import_handler.deduplicate_imports()\n\n    # Prepare context using subclass method (_prepare_template_context)\n    template_context = self._prepare_template_context(unique_model_definitions, django_model_names, imports)\n\n    # Add common context items\n    template_context.update(\n        {\n            \"generation_timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n            \"base_model_module\": self.base_model_class.__module__,\n            \"base_model_name\": self.base_model_class.__name__,\n            \"extra_type_imports\": sorted(self.import_handler.extra_type_imports),\n            # Add other common items as needed\n        }\n    )\n\n    # Render the main template\n    template = self.jinja_env.get_template(\"models_file.py.j2\")\n    return template.render(**template_context)\n</code></pre>"},{"location":"reference/pydantic2django/core/base_generator/#pydantic2django.core.base_generator.BaseStaticGenerator.setup_django_model","title":"<code>setup_django_model(source_model)</code>","text":"<p>Uses the model factory to create a Django model representation from a source model.</p> <p>Parameters:</p> Name Type Description Default <code>source_model</code> <code>SourceModelType</code> <p>The source model instance (e.g., Pydantic class, Dataclass).</p> required <p>Returns:</p> Type Description <code>ConversionCarrier[SourceModelType] | None</code> <p>A ConversionCarrier containing the results, or None if creation failed.</p> Source code in <code>src/pydantic2django/core/base_generator.py</code> <pre><code>def setup_django_model(self, source_model: SourceModelType) -&gt; ConversionCarrier[SourceModelType] | None:\n    \"\"\"\n    Uses the model factory to create a Django model representation from a source model.\n\n    Args:\n        source_model: The source model instance (e.g., Pydantic class, Dataclass).\n\n    Returns:\n        A ConversionCarrier containing the results, or None if creation failed.\n    \"\"\"\n    source_model_name = getattr(source_model, \"__name__\", str(source_model))\n    if self.verbose:\n        logger.info(f\"Setting up Django model for {source_model_name}\")\n\n    # Instantiate the carrier here\n    carrier = ConversionCarrier(\n        source_model=cast(type[SourceModelType], source_model),\n        meta_app_label=self.app_label,\n        base_django_model=self.base_model_class,\n        class_name_prefix=self.class_name_prefix,\n        # Add other defaults/configs if needed, e.g., strict mode\n        strict=False,  # Example default\n        # GFK flags\n        enable_gfk=self.enable_gfk,\n        gfk_policy=self.gfk_policy,\n        gfk_threshold_children=self.gfk_threshold_children,\n        gfk_value_mode=self.gfk_value_mode,\n        gfk_normalize_common_attrs=self.gfk_normalize_common_attrs,\n    )\n\n    try:\n        # Use the factory to process the source model and populate the carrier\n        self.model_factory_instance.make_django_model(carrier)  # Pass carrier to factory\n\n        if carrier.django_model:\n            self.carriers.append(carrier)\n            if self.verbose:\n                logger.info(f\"Successfully processed {source_model_name} -&gt; {carrier.django_model.__name__}\")\n            return carrier\n        else:\n            # Log if model creation resulted in None (e.g., only context fields)\n            # Check carrier.context_fields or carrier.invalid_fields for details\n            if carrier.context_fields and not carrier.django_fields and not carrier.relationship_fields:\n                logger.info(f\"Skipped Django model class for {source_model_name} - only context fields found.\")\n            elif carrier.invalid_fields:\n                logger.warning(\n                    f\"Skipped Django model class for {source_model_name} due to invalid fields: {carrier.invalid_fields}\"\n                )\n            else:\n                logger.warning(f\"Django model was not generated for {source_model_name} for unknown reasons.\")\n            return None  # Return None if no Django model was created\n\n    except Exception as e:\n        logger.error(f\"Error processing {source_model_name} with factory: {e}\", exc_info=True)\n        return None\n</code></pre>"},{"location":"reference/pydantic2django/core/bidirectional_mapper/","title":"pydantic2django.core.bidirectional_mapper","text":"<p>Provides a bidirectional mapping system between Django fields and Pydantic types/FieldInfo.</p> <p>This module defines: - <code>TypeMappingUnit</code>: Base class for defining a single bidirectional mapping rule. - Specific subclasses of <code>TypeMappingUnit</code> for various field types. - <code>BidirectionalTypeMapper</code>: A central registry and entry point for performing conversions.</p> <p>It utilizes: - <code>core.typing.TypeHandler</code>: For introspecting Pydantic type hints. - <code>core.relationships.RelationshipConversionAccessor</code>: For resolving model-to-model relationships.</p>"},{"location":"reference/pydantic2django/core/bidirectional_mapper/#pydantic2django.core.bidirectional_mapper.BidirectionalTypeMapper","title":"<code>BidirectionalTypeMapper</code>","text":"<p>Registry and entry point for bidirectional type mapping.</p> Source code in <code>src/pydantic2django/core/bidirectional_mapper.py</code> <pre><code>class BidirectionalTypeMapper:\n    \"\"\"Registry and entry point for bidirectional type mapping.\"\"\"\n\n    def __init__(self, relationship_accessor: Optional[RelationshipConversionAccessor] = None):\n        self.relationship_accessor = relationship_accessor or RelationshipConversionAccessor()\n        self._registry: list[type[TypeMappingUnit]] = self._build_registry()\n        # Caches\n        self._pydantic_cache: dict[Any, Optional[type[TypeMappingUnit]]] = {}\n        self._django_cache: dict[type[models.Field], Optional[type[TypeMappingUnit]]] = {}\n\n    def _build_registry(self) -&gt; list[type[TypeMappingUnit]]:\n        \"\"\"Discover and order TypeMappingUnit subclasses.\"\"\"\n        # Order matters less for selection now, but still useful for tie-breaking?\n        # References mapping units imported from .mapping_units\n        ordered_units = [\n            # Specific PKs first (subclass of IntField)\n            BigAutoFieldMapping,\n            SmallAutoFieldMapping,\n            AutoFieldMapping,\n            # Specific Numerics (subclass of IntField/FloatField/DecimalField)\n            PositiveBigIntFieldMapping,\n            PositiveSmallIntFieldMapping,\n            PositiveIntFieldMapping,\n            # Specific Strings (subclass of CharField/TextField)\n            EmailFieldMapping,\n            URLFieldMapping,\n            SlugFieldMapping,\n            IPAddressFieldMapping,\n            FilePathFieldMapping,  # Needs Path, but Django field is specific\n            # File Fields (map Path/str, Django fields are specific)\n            ImageFieldMapping,  # Subclass of FileField\n            FileFieldMapping,\n            # Other specific types before bases\n            UUIDFieldMapping,\n            JsonFieldMapping,  # Before generic collections/Any might map elsewhere\n            # Base Relationship types (before fields they might inherit from like FK &lt; Field)\n            ManyToManyFieldMapping,\n            OneToOneFieldMapping,\n            ForeignKeyMapping,\n            # General Base Types LAST\n            DecimalFieldMapping,\n            DateTimeFieldMapping,\n            DateFieldMapping,\n            TimeFieldMapping,\n            DurationFieldMapping,\n            BinaryFieldMapping,\n            FloatFieldMapping,\n            BoolFieldMapping,\n            # Str/Text: Order now primarily determined by `matches` score overrides\n            TextFieldMapping,\n            StrFieldMapping,\n            # Specific Int types first\n            BigIntFieldMapping,  # Map int to BigInt before Int\n            SmallIntFieldMapping,\n            IntFieldMapping,\n            # Enum handled dynamically by find method\n            EnumFieldMapping,  # Include EnumFieldMapping here for the loop\n        ]\n        # Remove duplicates just in case\n        seen = set()\n        unique_units = []\n        for unit in ordered_units:\n            if unit not in seen:\n                unique_units.append(unit)\n                seen.add(unit)\n        return unique_units\n\n    def _find_unit_for_pydantic_type(\n        self, py_type: Any, field_info: Optional[FieldInfo] = None\n    ) -&gt; Optional[type[TypeMappingUnit]]:\n        \"\"\"\n        Find the best mapping unit for a given Pydantic type and FieldInfo.\n        Uses a scoring system based on the `matches` classmethod of each unit.\n        Handles Optional unwrapping and caching.\n        \"\"\"\n        original_type_for_cache = py_type  # Use the original type as the cache key\n\n        # --- Unwrap Optional ---\n        origin = get_origin(py_type)\n        if origin is Optional:\n            args = get_args(py_type)\n            # Get the first non-None type argument\n            type_to_match = next((arg for arg in args if arg is not type(None)), Any)\n            logger.debug(f\"Unwrapped Optional[{type_to_match.__name__}] to {type_to_match.__name__}\")\n        # Handle X | None syntax (UnionType)\n        elif origin is UnionType:\n            args = get_args(py_type)\n            non_none_args = [arg for arg in args if arg is not type(None)]\n            if len(non_none_args) == 1:  # If it's just `T | None`\n                type_to_match = non_none_args[0]\n                logger.debug(f\"Unwrapped Union[{py_type}] with None to {type_to_match}\")\n            else:  # Keep the original UnionType if it's Union[A, B, ...]\n                type_to_match = py_type\n                logger.debug(f\"Keeping UnionType {py_type} as is for matching.\")\n        else:\n            type_to_match = py_type  # Use the original type if not Optional or simple T | None\n\n        logger.debug(\n            f\"Final type_to_match for scoring: {type_to_match} (origin: {get_origin(type_to_match)}, args: {get_args(type_to_match)})\"\n        )\n\n        # --- Cache Check ---\n        # Re-enable caching\n        cache_key = (original_type_for_cache, field_info)\n        if cache_key in self._pydantic_cache:\n            # logger.debug(f\"Cache hit for {cache_key}\")\n            return self._pydantic_cache[cache_key]\n        # logger.debug(f\"Cache miss for {cache_key}\")\n\n        # --- Literal Type Check (using original type) --- #\n        original_origin = get_origin(original_type_for_cache)\n        if original_origin is Literal:\n            logger.debug(f\"Type {original_type_for_cache} is Literal. Selecting EnumFieldMapping directly.\")\n            best_unit = EnumFieldMapping\n            self._pydantic_cache[cache_key] = best_unit\n            return best_unit\n\n        # --- Prioritize Collection Types -&gt; JSON --- #\n        # Use the unwrapped origin for this check\n        # unwrapped_origin = get_origin(type_to_match)\n        # if unwrapped_origin in (list, dict, set, tuple):\n        #     logger.debug(f\"Type {type_to_match} is a collection. Selecting JsonFieldMapping directly.\")\n        #     best_unit = JsonFieldMapping\n        #     self._pydantic_cache[cache_key] = best_unit\n        #     return best_unit\n\n        # --- Initialization --- #\n        best_unit: Optional[type[TypeMappingUnit]] = None\n        highest_score = 0.0\n        scores: dict[str, float | str] = {}  # Store scores for debugging\n\n        # --- Relationship Check (Specific Model Types and Lists of Models) BEFORE Scoring --- #\n        # Check if the type_to_match itself is a known model\n        try:\n            is_direct_known_model = (\n                inspect.isclass(type_to_match)\n                and (issubclass(type_to_match, BaseModel) or dataclasses.is_dataclass(type_to_match))\n                and self.relationship_accessor.is_source_model_known(type_to_match)\n            )\n        except TypeError:\n            is_direct_known_model = False\n\n        if is_direct_known_model:\n            logger.debug(\n                f\"Type {type_to_match.__name__} is a known related model. Selecting ForeignKeyMapping directly.\"\n            )\n            best_unit = ForeignKeyMapping\n            self._pydantic_cache[cache_key] = best_unit\n            return best_unit\n\n        # Check if it's a list/set of known models (potential M2M)\n        unwrapped_origin = get_origin(type_to_match)\n        unwrapped_args = get_args(type_to_match)\n        if unwrapped_origin in (list, set) and unwrapped_args:  # Check for list or set\n            inner_type = unwrapped_args[0]\n            try:\n                is_list_of_known_models = (\n                    inspect.isclass(inner_type)\n                    and (issubclass(inner_type, BaseModel) or dataclasses.is_dataclass(inner_type))\n                    and self.relationship_accessor.is_source_model_known(inner_type)\n                )\n            except TypeError:\n                is_list_of_known_models = False\n                logger.error(f\"TypeError checking if {inner_type} is a known model list item.\", exc_info=True)\n\n            logger.debug(\n                f\"Checking list/set: unwrapped_origin={unwrapped_origin}, inner_type={inner_type}, is_list_of_known_models={is_list_of_known_models}\"\n            )\n            if is_list_of_known_models:\n                logger.debug(\n                    f\"Type {type_to_match} is a list/set of known models ({inner_type.__name__}). Selecting ManyToManyFieldMapping directly.\"\n                )\n                best_unit = ManyToManyFieldMapping\n                self._pydantic_cache[cache_key] = best_unit\n                return best_unit\n            else:\n                logger.debug(\n                    f\"Type {type_to_match} is a list/set, but inner type {inner_type} is not a known model. Proceeding.\"\n                )\n\n        # --- Specific Union Handling BEFORE Scoring --- #\n        unwrapped_args = get_args(type_to_match)\n        # Check for non-model Unions (Model unions handled in get_django_mapping signal)\n        if unwrapped_origin in (Union, UnionType) and unwrapped_args:\n            logger.debug(f\"Evaluating specific Union type {type_to_match} args: {unwrapped_args} before scoring.\")\n            has_str = any(arg is str for arg in unwrapped_args)\n            has_collection_or_any = any(\n                get_origin(arg) in (dict, list, set, tuple) or arg is Any\n                for arg in unwrapped_args\n                if arg is not type(None)\n            )\n            # Don't handle Union[ModelA, ModelB] here, that needs the signal mechanism\n            is_model_union = any(\n                inspect.isclass(arg) and (issubclass(arg, BaseModel) or dataclasses.is_dataclass(arg))\n                for arg in unwrapped_args\n                if arg is not type(None)\n            )\n\n            if not is_model_union:\n                if has_str and not has_collection_or_any:\n                    logger.debug(f\"Union {type_to_match} contains str, selecting TextFieldMapping directly.\")\n                    best_unit = TextFieldMapping\n                    self._pydantic_cache[cache_key] = best_unit\n                    return best_unit\n                elif has_collection_or_any:\n                    logger.debug(f\"Union {type_to_match} contains complex types, selecting JsonFieldMapping directly.\")\n                    best_unit = JsonFieldMapping\n                    self._pydantic_cache[cache_key] = best_unit\n                    return best_unit\n                # Else: Union of simple types (e.g., int | float) - let scoring handle it.\n                else:\n                    logger.debug(f\"Union {type_to_match} is non-model, non-str/complex. Proceeding to scoring.\")\n            else:\n                logger.debug(\n                    f\"Union {type_to_match} contains models. Proceeding to scoring (expecting JsonField fallback).\"\n                )\n\n        # --- Scoring Loop (Only if not a known related model or specific Union handled above) --- #\n        # Use type_to_match (unwrapped) for matching\n        # --- EDIT: Removed redundant check `if best_unit is None:` --- #\n        # This loop now runs only if no direct selection happened above.\n        for unit_cls in self._registry:\n            try:  # Add try-except around matches call for robustness\n                # Pass the unwrapped type to matches\n                score = unit_cls.matches(type_to_match, field_info)\n                if score &gt; 0:  # Log all positive scores\n                    scores[unit_cls.__name__] = score  # Store score regardless of whether it's the highest\n                    logger.debug(\n                        f\"Scoring {unit_cls.__name__}.matches({type_to_match}, {field_info=}) -&gt; {score}\"\n                    )  # Added logging\n                if score &gt; highest_score:\n                    highest_score = score\n                    best_unit = unit_cls\n                    # Store the winning score as well - Moved above\n                    # scores[unit_cls.__name__] = score  # Overwrite if it was a lower score before\n                # elif score &gt; 0:  # Log non-winning positive scores too - Moved above\n                # Only add if not already present (first positive score encountered)\n                # scores.setdefault(unit_cls.__name__, score)\n            except Exception as e:\n                logger.error(f\"Error calling {unit_cls.__name__}.matches for {type_to_match}: {e}\", exc_info=True)\n                scores[unit_cls.__name__] = f\"ERROR: {e}\"  # Log error in scores dict\n\n        # Sort scores for clearer logging (highest first)\n        sorted_scores = dict(\n            sorted(scores.items(), key=lambda item: item[1] if isinstance(item[1], (int, float)) else -1, reverse=True)\n        )\n        logger.debug(\n            f\"Scores for {original_type_for_cache} (unwrapped: {type_to_match}, {field_info=}): {sorted_scores}\"\n        )\n        if best_unit:  # Added logging\n            logger.debug(f\"Selected best unit: {best_unit.__name__} with score {highest_score}\")  # Added logging\n        else:  # Added logging\n            logger.debug(\"No best unit found based on scoring.\")  # Added logging\n\n        # --- Handle Fallbacks (Collections/Any) --- #\n        if best_unit is None and highest_score == 0.0:\n            logger.debug(f\"Re-evaluating fallback/handling for {type_to_match}\")\n            unwrapped_origin = get_origin(type_to_match)\n            unwrapped_args = get_args(type_to_match)\n\n            # 1. Check standard collections first (MOVED FROM TOP)\n            if unwrapped_origin in (dict, list, set, tuple) or type_to_match in (dict, list, set, tuple):\n                # Re-check list/set here to ensure it wasn't a list of known models handled above\n                if unwrapped_origin in (list, set) and unwrapped_args:\n                    inner_type = unwrapped_args[0]\n                    try:\n                        is_list_of_known_models_fallback = (\n                            inspect.isclass(inner_type)\n                            and (issubclass(inner_type, BaseModel) or dataclasses.is_dataclass(inner_type))\n                            and self.relationship_accessor.is_source_model_known(inner_type)\n                        )\n                    except TypeError:\n                        is_list_of_known_models_fallback = False\n\n                    if not is_list_of_known_models_fallback:\n                        logger.debug(f\"Type {type_to_match} is a non-model collection, selecting JsonFieldMapping.\")\n                        best_unit = JsonFieldMapping\n                    # else: It was a list of known models, should have been handled earlier. Log warning?\n                    else:\n                        logger.warning(f\"List of known models {type_to_match} reached fallback logic unexpectedly.\")\n                        # Default to M2M as a safe bet?\n                        best_unit = ManyToManyFieldMapping\n                # Handle dict/tuple\n                elif unwrapped_origin in (dict, tuple) or type_to_match in (dict, tuple):\n                    logger.debug(f\"Type {type_to_match} is a dict/tuple collection, selecting JsonFieldMapping.\")\n                    best_unit = JsonFieldMapping\n            # 2. Check for Any\n            elif type_to_match is Any:\n                logger.debug(\"Type is Any, selecting JsonFieldMapping.\")\n                best_unit = JsonFieldMapping\n\n        # Final Logging\n        if best_unit is None:\n            logger.warning(\n                f\"No specific mapping unit found for Python type: {original_type_for_cache} (unwrapped to {type_to_match}) with field_info: {field_info}\"\n            )\n            # Log cache state before potential fallback write\n            logger.debug(f\"Cache keys before fallback write: {list(self._pydantic_cache.keys())}\")\n\n        # Re-enable cache write\n        self._pydantic_cache[cache_key] = best_unit  # Cache using original key\n        return best_unit\n\n    def _find_unit_for_django_field(self, dj_field_type: type[models.Field]) -&gt; Optional[type[TypeMappingUnit]]:\n        \"\"\"Find the most specific mapping unit based on Django field type MRO and registry order.\"\"\"\n        # Revert to simpler single pass using refined registry order.\n        if dj_field_type in self._django_cache:\n            return self._django_cache[dj_field_type]\n\n        # Filter registry to exclude EnumFieldMapping unless it's specifically needed? No, registry order handles it.\n        # Ensure EnumFieldMapping isn't incorrectly picked before Str/Int if choices are present.\n        # The registry order should have Str/Int base mappings *after* EnumFieldMapping if EnumFieldMapping\n        # only maps Enum/Literal python types. But dj_field_type matching is different.\n        # If a CharField has choices, we want EnumFieldMapping logic, not StrFieldMapping.\n        registry_for_django = self._registry  # Use the full registry for now\n\n        for unit_cls in registry_for_django:\n            # Special check: If field has choices, prioritize EnumFieldMapping if applicable type\n            # This is handled by get_pydantic_mapping logic already, not needed here.\n\n            if issubclass(dj_field_type, unit_cls.django_field_type):\n                # Found the first, most specific match based on registry order\n                # Example: PositiveIntegerField is subclass of IntegerField. If PositiveIntFieldMapping\n                # comes first in registry, it will be matched correctly.\n                self._django_cache[dj_field_type] = unit_cls\n                return unit_cls\n\n        # Fallback if no unit explicitly handles it (should be rare)\n        logger.warning(\n            f\"No specific mapping unit found for Django field type: {dj_field_type.__name__}, check registry order.\"\n        )\n        self._django_cache[dj_field_type] = None\n        return None\n\n    def get_django_mapping(\n        self,\n        python_type: Any,\n        field_info: Optional[FieldInfo] = None,\n        parent_pydantic_model: Optional[type[BaseModel]] = None,  # Add parent model for self-ref check\n    ) -&gt; tuple[type[models.Field], dict[str, Any]]:\n        \"\"\"Get the corresponding Django Field type and constructor kwargs for a Python type.\"\"\"\n        processed_type_info = TypeHandler.process_field_type(python_type)\n        original_py_type = python_type\n        is_optional = processed_type_info[\"is_optional\"]\n        is_list = processed_type_info[\"is_list\"]\n\n        unit_cls = None  # Initialize unit_cls\n        base_py_type = original_py_type  # Start with original\n        union_details = None  # Store details if it's a Union[BaseModel,...]\n        gfk_details = None\n\n        # --- Check for M2M case FIRST ---\n        if is_list:\n            # Get the type inside the list, handling Optional[List[T]]\n            list_inner_type = original_py_type\n            if is_optional:\n                args_check = get_args(list_inner_type)\n                list_inner_type = next((arg for arg in args_check if arg is not type(None)), Any)\n\n            # Now get the type *inside* the list\n            list_args = get_args(list_inner_type)  # Should be List[T]\n            inner_type = list_args[0] if list_args else Any\n\n            # --- GFK Check: Is the inner type a Union of known models? ---\n            inner_origin = get_origin(inner_type)\n            inner_args = get_args(inner_type)\n            if inner_origin in (Union, UnionType) and inner_args:\n                union_models = []\n                other_types = [\n                    arg\n                    for arg in inner_args\n                    if not (\n                        inspect.isclass(arg)\n                        and (issubclass(arg, BaseModel) or dataclasses.is_dataclass(arg))\n                        and self.relationship_accessor.is_source_model_known(arg)\n                    )\n                ]\n                union_models = [arg for arg in inner_args if arg not in other_types]\n\n                if union_models and not other_types:\n                    logger.debug(f\"Detected GFK List[Union[...]] with models: {union_models}\")\n                    gfk_details = {\n                        \"type\": \"gfk\",\n                        \"models\": union_models,\n                        \"is_optional\": is_optional,\n                    }\n                    unit_cls = JsonFieldMapping\n                    base_py_type = original_py_type\n\n            if unit_cls is None:\n                # --- M2M Check: Is the inner type a known related BaseModel OR Dataclass? ---\n                if (\n                    inspect.isclass(inner_type)\n                    and (issubclass(inner_type, BaseModel) or dataclasses.is_dataclass(inner_type))\n                    and self.relationship_accessor.is_source_model_known(inner_type)\n                ):\n                    unit_cls = ManyToManyFieldMapping\n                    base_py_type = inner_type\n                    logger.debug(f\"Detected List[RelatedModel] ({inner_type.__name__}), mapping to ManyToManyField.\")\n                else:\n                    # --- Fallback for other lists ---\n                    unit_cls = JsonFieldMapping\n                    base_py_type = original_py_type\n                    logger.debug(f\"Detected List of non-models ({original_py_type}), mapping directly to JSONField.\")\n\n        # --- If not a list, find unit for the base (non-list) type ---\n        if unit_cls is None:\n            # --- Handle Union[BaseModel,...] Signaling FIRST --- #\n            simplified_base_type = processed_type_info[\"type_obj\"]\n            simplified_origin = get_origin(simplified_base_type)\n            simplified_args = get_args(simplified_base_type)\n\n            logger.debug(\n                f\"Checking simplified type for Union[Model,...]: {simplified_base_type!r} (Origin: {simplified_origin})\"\n            )\n            # Log the is_optional flag determined by TypeHandler\n            logger.debug(f\"TypeHandler returned is_optional: {is_optional} for original type: {original_py_type!r}\")\n\n            # Check if the simplified origin is Union[...] or T | U\n            if simplified_origin in (Union, UnionType) and simplified_args:\n                union_models = []\n                other_types_in_union = []\n\n                for arg in simplified_args:\n                    # We already unwrapped Optional, so no need to check for NoneType here\n                    logger.debug(f\"-- Checking simplified Union arg: {arg!r}\")\n\n                    # Check if arg is a known BaseModel or Dataclass\n                    is_class = inspect.isclass(arg)\n                    # Need try-except for issubclass with non-class types\n                    is_pyd_model = False\n                    is_dc = False\n                    is_known_by_accessor = False\n                    if is_class:\n                        try:\n                            is_pyd_model = issubclass(arg, BaseModel)\n                            is_dc = dataclasses.is_dataclass(arg)\n                            # Only check accessor if it's a model type\n                            if is_pyd_model or is_dc:\n                                is_known_by_accessor = self.relationship_accessor.is_source_model_known(arg)\n                        except TypeError:\n                            # issubclass might fail if arg is not a class (e.g., a type alias)\n                            pass  # Keep flags as False\n\n                    logger.debug(\n                        f\"    is_class: {is_class}, is_pyd_model: {is_pyd_model}, is_dc: {is_dc}, is_known_by_accessor: {is_known_by_accessor}\"\n                    )\n\n                    is_known_model_or_dc = is_class and (is_pyd_model or is_dc) and is_known_by_accessor\n\n                    if is_known_model_or_dc:\n                        logger.debug(f\"    -&gt; Added {arg.__name__} to union_models\")  # More specific logging\n                        union_models.append(arg)\n                    else:\n                        # Make sure we don't add NoneType here if Optional wasn't fully handled upstream somehow\n                        if arg is not type(None):\n                            logger.debug(f\"    -&gt; Added {arg!r} to other_types_in_union\")  # More specific logging\n                            other_types_in_union.append(arg)\n\n                # --- EDIT: Only set union_details IF ONLY models were found ---\n                # Add logging just before the check\n                logger.debug(\n                    f\"Finished Union arg loop. union_models: {[m.__name__ for m in union_models]}, other_types: {other_types_in_union}\"\n                )\n                if union_models and not other_types_in_union:\n                    logger.debug(\n                        f\"Detected Union containing ONLY known models: {union_models}. Generating _union_details signal.\"\n                    )\n                    union_details = {\n                        \"type\": \"multi_fk\",\n                        \"models\": union_models,\n                        \"is_optional\": is_optional,  # Use the flag determined earlier\n                    }\n                    # Log the created union_details\n                    logger.debug(f\"Generated union_details: {union_details!r}\")\n                    # Set unit_cls to JsonFieldMapping for model unions\n                    unit_cls = JsonFieldMapping\n                    base_py_type = original_py_type\n                    logger.debug(\"Setting unit_cls to JsonFieldMapping for model union\")\n\n            # --- Now, find the unit for the (potentially complex) base type --- #\n            # Only find unit if not already set (e.g. by model union handling)\n            if unit_cls is None:\n                # Determine the type to use for finding the unit.\n                # If it was M2M or handled List, unit_cls is already set.\n                # Otherwise, use the processed type_obj which handles Optional/Annotated.\n                type_for_unit_finding = processed_type_info[\"type_obj\"]\n                logger.debug(f\"Type used for finding unit (after Union check): {type_for_unit_finding!r}\")\n\n                # Use the simplified base type after processing Optional/Annotated\n                base_py_type = type_for_unit_finding\n                logger.debug(f\"Finding unit for base type: {base_py_type!r} with field_info: {field_info}\")\n                unit_cls = self._find_unit_for_pydantic_type(base_py_type, field_info)\n\n        # --- Check if a unit was found --- #\n        if not unit_cls:\n            # If _find_unit_for_pydantic_type returned None, fallback to JSON\n            logger.warning(\n                f\"No mapping unit found by scoring for base type {base_py_type} \"\n                f\"(derived from {original_py_type}), falling back to JSONField.\"\n            )\n            unit_cls = JsonFieldMapping\n            # Consider raising MappingError if even JSON doesn't fit?\n            # raise MappingError(f\"Could not find mapping unit for Python type: {base_py_type}\")\n\n        # &gt;&gt; Add logging to check selected unit &lt;&lt;\n        logger.info(f\"Selected Unit for {original_py_type}: {unit_cls.__name__ if unit_cls else 'None'}\")\n\n        instance_unit = unit_cls()  # Instantiate to call methods\n\n        # --- Determine Django Field Type ---\n        # Start with the type defined on the selected unit class\n        django_field_type = instance_unit.django_field_type\n\n        # --- Get Kwargs (before potentially overriding field type for Enums) ---\n        kwargs = instance_unit.pydantic_to_django_kwargs(base_py_type, field_info)\n\n        # --- Add Union or GFK Details if applicable --- #\n        if union_details:\n            logger.info(\"Adding _union_details to kwargs.\")\n            kwargs[\"_union_details\"] = union_details\n            kwargs[\"null\"] = union_details.get(\"is_optional\", False)\n            kwargs[\"blank\"] = union_details.get(\"is_optional\", False)\n        elif gfk_details:\n            logger.info(\"Adding _gfk_details to kwargs.\")\n            kwargs[\"_gfk_details\"] = gfk_details\n            # GFK fields are placeholder JSONFields, nullability is based on Optional status\n            kwargs[\"null\"] = is_optional\n            kwargs[\"blank\"] = is_optional\n        else:\n            logger.debug(\"union_details and gfk_details are None, skipping addition to kwargs.\")\n            # --- Special Handling for Enums/Literals (Only if not multi-FK/GFK union) --- #\n            if unit_cls is EnumFieldMapping:\n                field_type_hint = kwargs.pop(\"_field_type_hint\", None)\n                if field_type_hint and isinstance(field_type_hint, type) and issubclass(field_type_hint, models.Field):\n                    # Directly use the hinted field type if valid\n                    logger.debug(\n                        f\"Using hinted field type {field_type_hint.__name__} from EnumFieldMapping for {base_py_type}.\"\n                    )\n                    django_field_type = field_type_hint\n                    # Ensure max_length is removed if type becomes IntegerField\n                    if django_field_type == models.IntegerField:\n                        kwargs.pop(\"max_length\", None)\n                else:\n                    logger.warning(\"EnumFieldMapping selected but failed to get valid field type hint from kwargs.\")\n\n            # --- Handle Relationships (Only if not multi-FK union) --- #\n            # This section needs to run *after* unit selection but *before* final nullability checks\n            if unit_cls in (ForeignKeyMapping, OneToOneFieldMapping, ManyToManyFieldMapping):\n                # Ensure base_py_type is the related model (set during M2M check or found by find_unit for FK/O2O)\n                related_py_model = base_py_type\n\n                # Check if it's a known Pydantic BaseModel OR a known Dataclass\n                is_pyd_or_dc = inspect.isclass(related_py_model) and (\n                    issubclass(related_py_model, BaseModel) or dataclasses.is_dataclass(related_py_model)\n                )\n                if not is_pyd_or_dc:\n                    raise MappingError(\n                        f\"Relationship mapping unit {unit_cls.__name__} selected, but base type {related_py_model} is not a known Pydantic model or Dataclass.\"\n                    )\n\n                # Check for self-reference BEFORE trying to get the Django model\n                is_self_ref = parent_pydantic_model is not None and related_py_model == parent_pydantic_model\n\n                if is_self_ref:\n                    model_ref = \"self\"\n                    # Get the target Django model name for logging/consistency if possible, but use 'self'\n                    # Check if the related model is a Pydantic BaseModel or a dataclass\n                    if inspect.isclass(related_py_model) and issubclass(related_py_model, BaseModel):\n                        target_django_model = self.relationship_accessor.get_django_model_for_pydantic(\n                            cast(type[BaseModel], related_py_model)\n                        )\n                    elif dataclasses.is_dataclass(related_py_model):\n                        target_django_model = self.relationship_accessor.get_django_model_for_dataclass(\n                            related_py_model\n                        )\n                    else:\n                        # This case should ideally not be reached due to earlier checks, but handle defensively\n                        target_django_model = None\n                        logger.warning(\n                            f\"Self-reference check: related_py_model '{related_py_model}' is neither BaseModel nor dataclass.\"\n                        )\n\n                    logger.debug(\n                        f\"Detected self-reference for {related_py_model.__name__ if inspect.isclass(related_py_model) else related_py_model} \"\n                        f\"(Django: {getattr(target_django_model, '__name__', 'N/A')}), using 'self'.\"\n                    )\n                else:\n                    # Get target Django model based on source type (Pydantic or Dataclass)\n                    target_django_model = None\n                    # Ensure related_py_model is actually a type before issubclass check\n                    if inspect.isclass(related_py_model) and issubclass(related_py_model, BaseModel):\n                        # Cast to satisfy type checker, as we've confirmed it's a BaseModel subclass here\n                        target_django_model = self.relationship_accessor.get_django_model_for_pydantic(\n                            cast(type[BaseModel], related_py_model)\n                        )\n                    elif dataclasses.is_dataclass(related_py_model):\n                        target_django_model = self.relationship_accessor.get_django_model_for_dataclass(\n                            related_py_model\n                        )\n\n                    if not target_django_model:\n                        raise MappingError(\n                            f\"Cannot map relationship: No corresponding Django model found for source model \"\n                            f\"{related_py_model.__name__} in RelationshipConversionAccessor.\"\n                        )\n                    # Use lowercase label for internal consistency with existing expectations\n                    model_ref = getattr(target_django_model._meta, \"label_lower\", target_django_model.__name__)\n\n                kwargs[\"to\"] = model_ref\n                django_field_type = unit_cls.django_field_type  # Re-confirm M2MField, FK, O2O type\n                # Set on_delete for FK/O2O based on Optional status\n                if unit_cls in (ForeignKeyMapping, OneToOneFieldMapping):\n                    # Default to CASCADE for non-optional, SET_NULL for optional (matching test expectation)\n                    kwargs[\"on_delete\"] = (\n                        models.SET_NULL if is_optional else models.CASCADE\n                    )  # Changed PROTECT to CASCADE\n\n        # --- Final Adjustments (Nullability, etc.) --- #\n        # Apply nullability. M2M fields cannot be null in Django.\n        # Do not override nullability if it was already forced by a multi-FK union\n        if django_field_type != models.ManyToManyField and not union_details:\n            kwargs[\"null\"] = is_optional\n            # Explicitly set blank based on optionality.\n            # Simplified logic: Mirror the null assignment directly\n            kwargs[\"blank\"] = is_optional\n\n        logger.debug(\n            f\"FINAL RETURN from get_django_mapping: Type={django_field_type}, Kwargs={kwargs}\"\n        )  # Added final state logging\n        return django_field_type, kwargs\n\n    def get_pydantic_mapping(self, dj_field: models.Field) -&gt; tuple[Any, dict[str, Any]]:\n        \"\"\"Get the corresponding Pydantic type hint and FieldInfo kwargs for a Django Field.\"\"\"\n        dj_field_type = type(dj_field)\n        is_optional = dj_field.null\n        is_choices = bool(dj_field.choices)\n\n        # --- Find base unit (ignoring choices for now) ---\n        # Find the mapping unit based on the specific Django field type MRO\n        # This gives us the correct underlying Python type (str, int, etc.)\n        base_unit_cls = self._find_unit_for_django_field(dj_field_type)\n\n        if not base_unit_cls:\n            logger.warning(f\"No base mapping unit for {dj_field_type.__name__}, falling back to Any.\")\n            pydantic_type = Optional[Any] if is_optional else Any\n            return pydantic_type, {}\n\n        base_instance_unit = base_unit_cls()\n        # Get the base Pydantic type from this unit\n        final_pydantic_type = base_instance_unit.python_type\n\n        # --- Determine Final Pydantic Type Adjustments --- #\n        # (Relationships, AutoPK, Optional wrapper)\n\n        # Handle choices FIRST to determine the core type before Optional wrapping\n        if is_choices:\n            # Default to base type, override if valid choices found\n            final_pydantic_type = base_instance_unit.python_type\n            if dj_field.choices:  # Explicit check before iteration\n                try:\n                    choice_values = tuple(choice[0] for choice in dj_field.choices)\n                    if choice_values:  # Ensure the tuple is not empty\n                        final_pydantic_type = Literal[choice_values]  # type: ignore\n                        logger.debug(f\"Mapped choices for '{dj_field.name}' to Pydantic type: {final_pydantic_type}\")\n                    else:\n                        logger.warning(\n                            f\"Field '{dj_field.name}' has choices defined, but extracted values are empty. Falling back.\"\n                        )\n                        # Keep final_pydantic_type as base type\n                except Exception as e:\n                    logger.warning(f\"Failed to extract choices for field '{dj_field.name}'. Error: {e}. Falling back.\")\n                    # Keep final_pydantic_type as base type\n            # If dj_field.choices was None/empty initially, final_pydantic_type remains the base type\n        else:\n            # Get the base Pydantic type from this unit if not choices\n            final_pydantic_type = base_instance_unit.python_type\n\n        # 1. Handle Relationships first, as they determine the core type\n        if base_unit_cls in (ForeignKeyMapping, OneToOneFieldMapping, ManyToManyFieldMapping):\n            related_dj_model = getattr(dj_field, \"related_model\", None)\n            if not related_dj_model:\n                raise MappingError(f\"Cannot determine related Django model for field '{dj_field.name}'\")\n\n            # Resolve 'self' reference\n            if related_dj_model == \"self\":\n                # We need the Django model class that dj_field belongs to.\n                # This info isn't directly passed, so this approach might be limited.\n                # Assuming self-reference points to the same type hierarchy for now.\n                # A better solution might need the model context passed down.\n                logger.warning(\n                    f\"Handling 'self' reference for field '{dj_field.name}'. Mapping might be incomplete without parent model context.\"\n                )\n                # Attempt to get Pydantic model mapped to the field's owner model if possible (heuristically)\n                # This is complex and potentially fragile.\n                # For now, let's use a placeholder or raise an error if needed strictly.\n                # Sticking with the base type (e.g., Any or int for PK) might be safer without context.\n                # Use the base type (likely PK int/uuid) as the fallback type here\n                target_pydantic_model = base_instance_unit.python_type\n                logger.debug(f\"Using Any as placeholder for 'self' reference '{dj_field.name}'\")\n            else:\n                target_pydantic_model = self.relationship_accessor.get_pydantic_model_for_django(related_dj_model)\n\n            if not target_pydantic_model or target_pydantic_model is Any:\n                if related_dj_model != \"self\":  # Avoid redundant warning for self\n                    logger.warning(\n                        f\"Cannot map relationship: No corresponding Pydantic model found for Django model \"\n                        f\"'{related_dj_model._meta.label if hasattr(related_dj_model, '_meta') else related_dj_model.__name__}'. \"\n                        f\"Using placeholder '{final_pydantic_type}'.\"\n                    )\n                # Keep final_pydantic_type as the base unit's python_type (e.g., int for FK)\n            else:\n                if base_unit_cls == ManyToManyFieldMapping:\n                    final_pydantic_type = list[target_pydantic_model]\n                else:  # FK or O2O\n                    # Keep the PK type (e.g., int) if target model not found,\n                    # otherwise use the target Pydantic model type.\n                    final_pydantic_type = target_pydantic_model  # This should now be the related model type\n                logger.debug(f\"Mapped relationship field '{dj_field.name}' to Pydantic type: {final_pydantic_type}\")\n\n        # 2. AutoPK override (after relationship resolution)\n        is_auto_pk = dj_field.primary_key and isinstance(\n            dj_field, (models.AutoField, models.BigAutoField, models.SmallAutoField)\n        )\n        if is_auto_pk:\n            final_pydantic_type = Optional[int]\n            logger.debug(f\"Mapped AutoPK field '{dj_field.name}' to {final_pydantic_type}\")\n            is_optional = True  # AutoPKs are always optional in Pydantic input\n\n        # 3. Apply Optional[...] wrapper if necessary (AFTER relationship/AutoPK)\n        # Do not wrap M2M lists or already Optional AutoPKs in Optional[] again.\n        # Also, don't wrap if the type is already Literal (choices handled Optionality) - NO, wrap Literal too if null=True\n        if is_optional and not is_auto_pk:  # Check if is_choices? No, optional applies to literal too.\n            origin = get_origin(final_pydantic_type)\n            args = get_args(final_pydantic_type)\n            is_already_optional = origin is Optional or origin is UnionType and type(None) in args\n\n            if not is_already_optional:\n                final_pydantic_type = Optional[final_pydantic_type]\n                logger.debug(f\"Wrapped type for '{dj_field.name}' in Optional: {final_pydantic_type}\")\n\n        # --- Generate FieldInfo Kwargs --- #\n        # Use EnumFieldMapping logic for kwargs ONLY if choices exist,\n        # otherwise use the base unit determined earlier. # --&gt; NO, always use base unit for kwargs now. Literal type handles choices.\n        # kwargs_unit_cls = EnumFieldMapping if is_choices else base_unit_cls # OLD logic\n        instance_unit = base_unit_cls()  # Use the base unit (e.g., StrFieldMapping) for base kwargs\n\n        field_info_kwargs = instance_unit.django_to_pydantic_field_info_kwargs(dj_field)\n\n        # --- Explicitly cast title (verbose_name) and description (help_text) --- #\n        if field_info_kwargs.get(\"title\") is not None:\n            field_info_kwargs[\"title\"] = str(field_info_kwargs[\"title\"])\n            logger.debug(f\"Ensured title is str for '{dj_field.name}': {field_info_kwargs['title']}\")\n        if field_info_kwargs.get(\"description\") is not None:\n            field_info_kwargs[\"description\"] = str(field_info_kwargs[\"description\"])\n            logger.debug(f\"Ensured description is str for '{dj_field.name}': {field_info_kwargs['description']}\")\n        # --- End Casting --- #\n\n        # --- Keep choices in json_schema_extra even when using Literal ---\n        # This preserves the (value, label) mapping as metadata alongside the Literal type.\n        if (\n            is_choices\n            and \"json_schema_extra\" in field_info_kwargs\n            and \"choices\" in field_info_kwargs[\"json_schema_extra\"]\n        ):\n            logger.debug(f\"Kept choices in json_schema_extra for Literal field '{dj_field.name}'\")\n        elif is_choices:\n            logger.debug(\n                f\"Field '{dj_field.name}' has choices, but they weren't added to json_schema_extra by the mapping unit.\"\n            )\n\n        # Clean up redundant `default=None` for Optional fields handled by Pydantic v2.\n        # Do not force-add default=None; only keep explicit defaults (e.g., for AutoPK).\n        if is_optional:\n            if field_info_kwargs.get(\"default\") is None and not is_auto_pk:\n                # Remove implicit default=None for Optional fields\n                field_info_kwargs.pop(\"default\", None)\n                logger.debug(f\"Removed redundant default=None for Optional field '{dj_field.name}'\")\n            elif is_auto_pk and \"default\" not in field_info_kwargs:\n                # Keep default=None for AutoPK if not already set\n                field_info_kwargs[\"default\"] = None\n                logger.debug(f\"Set default=None for AutoPK Optional field '{dj_field.name}'\")\n\n        logger.debug(\n            f\"Final Pydantic mapping for '{dj_field.name}': Type={final_pydantic_type}, Kwargs={field_info_kwargs}\"\n        )\n        return final_pydantic_type, field_info_kwargs\n</code></pre>"},{"location":"reference/pydantic2django/core/bidirectional_mapper/#pydantic2django.core.bidirectional_mapper.BidirectionalTypeMapper.get_django_mapping","title":"<code>get_django_mapping(python_type, field_info=None, parent_pydantic_model=None)</code>","text":"<p>Get the corresponding Django Field type and constructor kwargs for a Python type.</p> Source code in <code>src/pydantic2django/core/bidirectional_mapper.py</code> <pre><code>def get_django_mapping(\n    self,\n    python_type: Any,\n    field_info: Optional[FieldInfo] = None,\n    parent_pydantic_model: Optional[type[BaseModel]] = None,  # Add parent model for self-ref check\n) -&gt; tuple[type[models.Field], dict[str, Any]]:\n    \"\"\"Get the corresponding Django Field type and constructor kwargs for a Python type.\"\"\"\n    processed_type_info = TypeHandler.process_field_type(python_type)\n    original_py_type = python_type\n    is_optional = processed_type_info[\"is_optional\"]\n    is_list = processed_type_info[\"is_list\"]\n\n    unit_cls = None  # Initialize unit_cls\n    base_py_type = original_py_type  # Start with original\n    union_details = None  # Store details if it's a Union[BaseModel,...]\n    gfk_details = None\n\n    # --- Check for M2M case FIRST ---\n    if is_list:\n        # Get the type inside the list, handling Optional[List[T]]\n        list_inner_type = original_py_type\n        if is_optional:\n            args_check = get_args(list_inner_type)\n            list_inner_type = next((arg for arg in args_check if arg is not type(None)), Any)\n\n        # Now get the type *inside* the list\n        list_args = get_args(list_inner_type)  # Should be List[T]\n        inner_type = list_args[0] if list_args else Any\n\n        # --- GFK Check: Is the inner type a Union of known models? ---\n        inner_origin = get_origin(inner_type)\n        inner_args = get_args(inner_type)\n        if inner_origin in (Union, UnionType) and inner_args:\n            union_models = []\n            other_types = [\n                arg\n                for arg in inner_args\n                if not (\n                    inspect.isclass(arg)\n                    and (issubclass(arg, BaseModel) or dataclasses.is_dataclass(arg))\n                    and self.relationship_accessor.is_source_model_known(arg)\n                )\n            ]\n            union_models = [arg for arg in inner_args if arg not in other_types]\n\n            if union_models and not other_types:\n                logger.debug(f\"Detected GFK List[Union[...]] with models: {union_models}\")\n                gfk_details = {\n                    \"type\": \"gfk\",\n                    \"models\": union_models,\n                    \"is_optional\": is_optional,\n                }\n                unit_cls = JsonFieldMapping\n                base_py_type = original_py_type\n\n        if unit_cls is None:\n            # --- M2M Check: Is the inner type a known related BaseModel OR Dataclass? ---\n            if (\n                inspect.isclass(inner_type)\n                and (issubclass(inner_type, BaseModel) or dataclasses.is_dataclass(inner_type))\n                and self.relationship_accessor.is_source_model_known(inner_type)\n            ):\n                unit_cls = ManyToManyFieldMapping\n                base_py_type = inner_type\n                logger.debug(f\"Detected List[RelatedModel] ({inner_type.__name__}), mapping to ManyToManyField.\")\n            else:\n                # --- Fallback for other lists ---\n                unit_cls = JsonFieldMapping\n                base_py_type = original_py_type\n                logger.debug(f\"Detected List of non-models ({original_py_type}), mapping directly to JSONField.\")\n\n    # --- If not a list, find unit for the base (non-list) type ---\n    if unit_cls is None:\n        # --- Handle Union[BaseModel,...] Signaling FIRST --- #\n        simplified_base_type = processed_type_info[\"type_obj\"]\n        simplified_origin = get_origin(simplified_base_type)\n        simplified_args = get_args(simplified_base_type)\n\n        logger.debug(\n            f\"Checking simplified type for Union[Model,...]: {simplified_base_type!r} (Origin: {simplified_origin})\"\n        )\n        # Log the is_optional flag determined by TypeHandler\n        logger.debug(f\"TypeHandler returned is_optional: {is_optional} for original type: {original_py_type!r}\")\n\n        # Check if the simplified origin is Union[...] or T | U\n        if simplified_origin in (Union, UnionType) and simplified_args:\n            union_models = []\n            other_types_in_union = []\n\n            for arg in simplified_args:\n                # We already unwrapped Optional, so no need to check for NoneType here\n                logger.debug(f\"-- Checking simplified Union arg: {arg!r}\")\n\n                # Check if arg is a known BaseModel or Dataclass\n                is_class = inspect.isclass(arg)\n                # Need try-except for issubclass with non-class types\n                is_pyd_model = False\n                is_dc = False\n                is_known_by_accessor = False\n                if is_class:\n                    try:\n                        is_pyd_model = issubclass(arg, BaseModel)\n                        is_dc = dataclasses.is_dataclass(arg)\n                        # Only check accessor if it's a model type\n                        if is_pyd_model or is_dc:\n                            is_known_by_accessor = self.relationship_accessor.is_source_model_known(arg)\n                    except TypeError:\n                        # issubclass might fail if arg is not a class (e.g., a type alias)\n                        pass  # Keep flags as False\n\n                logger.debug(\n                    f\"    is_class: {is_class}, is_pyd_model: {is_pyd_model}, is_dc: {is_dc}, is_known_by_accessor: {is_known_by_accessor}\"\n                )\n\n                is_known_model_or_dc = is_class and (is_pyd_model or is_dc) and is_known_by_accessor\n\n                if is_known_model_or_dc:\n                    logger.debug(f\"    -&gt; Added {arg.__name__} to union_models\")  # More specific logging\n                    union_models.append(arg)\n                else:\n                    # Make sure we don't add NoneType here if Optional wasn't fully handled upstream somehow\n                    if arg is not type(None):\n                        logger.debug(f\"    -&gt; Added {arg!r} to other_types_in_union\")  # More specific logging\n                        other_types_in_union.append(arg)\n\n            # --- EDIT: Only set union_details IF ONLY models were found ---\n            # Add logging just before the check\n            logger.debug(\n                f\"Finished Union arg loop. union_models: {[m.__name__ for m in union_models]}, other_types: {other_types_in_union}\"\n            )\n            if union_models and not other_types_in_union:\n                logger.debug(\n                    f\"Detected Union containing ONLY known models: {union_models}. Generating _union_details signal.\"\n                )\n                union_details = {\n                    \"type\": \"multi_fk\",\n                    \"models\": union_models,\n                    \"is_optional\": is_optional,  # Use the flag determined earlier\n                }\n                # Log the created union_details\n                logger.debug(f\"Generated union_details: {union_details!r}\")\n                # Set unit_cls to JsonFieldMapping for model unions\n                unit_cls = JsonFieldMapping\n                base_py_type = original_py_type\n                logger.debug(\"Setting unit_cls to JsonFieldMapping for model union\")\n\n        # --- Now, find the unit for the (potentially complex) base type --- #\n        # Only find unit if not already set (e.g. by model union handling)\n        if unit_cls is None:\n            # Determine the type to use for finding the unit.\n            # If it was M2M or handled List, unit_cls is already set.\n            # Otherwise, use the processed type_obj which handles Optional/Annotated.\n            type_for_unit_finding = processed_type_info[\"type_obj\"]\n            logger.debug(f\"Type used for finding unit (after Union check): {type_for_unit_finding!r}\")\n\n            # Use the simplified base type after processing Optional/Annotated\n            base_py_type = type_for_unit_finding\n            logger.debug(f\"Finding unit for base type: {base_py_type!r} with field_info: {field_info}\")\n            unit_cls = self._find_unit_for_pydantic_type(base_py_type, field_info)\n\n    # --- Check if a unit was found --- #\n    if not unit_cls:\n        # If _find_unit_for_pydantic_type returned None, fallback to JSON\n        logger.warning(\n            f\"No mapping unit found by scoring for base type {base_py_type} \"\n            f\"(derived from {original_py_type}), falling back to JSONField.\"\n        )\n        unit_cls = JsonFieldMapping\n        # Consider raising MappingError if even JSON doesn't fit?\n        # raise MappingError(f\"Could not find mapping unit for Python type: {base_py_type}\")\n\n    # &gt;&gt; Add logging to check selected unit &lt;&lt;\n    logger.info(f\"Selected Unit for {original_py_type}: {unit_cls.__name__ if unit_cls else 'None'}\")\n\n    instance_unit = unit_cls()  # Instantiate to call methods\n\n    # --- Determine Django Field Type ---\n    # Start with the type defined on the selected unit class\n    django_field_type = instance_unit.django_field_type\n\n    # --- Get Kwargs (before potentially overriding field type for Enums) ---\n    kwargs = instance_unit.pydantic_to_django_kwargs(base_py_type, field_info)\n\n    # --- Add Union or GFK Details if applicable --- #\n    if union_details:\n        logger.info(\"Adding _union_details to kwargs.\")\n        kwargs[\"_union_details\"] = union_details\n        kwargs[\"null\"] = union_details.get(\"is_optional\", False)\n        kwargs[\"blank\"] = union_details.get(\"is_optional\", False)\n    elif gfk_details:\n        logger.info(\"Adding _gfk_details to kwargs.\")\n        kwargs[\"_gfk_details\"] = gfk_details\n        # GFK fields are placeholder JSONFields, nullability is based on Optional status\n        kwargs[\"null\"] = is_optional\n        kwargs[\"blank\"] = is_optional\n    else:\n        logger.debug(\"union_details and gfk_details are None, skipping addition to kwargs.\")\n        # --- Special Handling for Enums/Literals (Only if not multi-FK/GFK union) --- #\n        if unit_cls is EnumFieldMapping:\n            field_type_hint = kwargs.pop(\"_field_type_hint\", None)\n            if field_type_hint and isinstance(field_type_hint, type) and issubclass(field_type_hint, models.Field):\n                # Directly use the hinted field type if valid\n                logger.debug(\n                    f\"Using hinted field type {field_type_hint.__name__} from EnumFieldMapping for {base_py_type}.\"\n                )\n                django_field_type = field_type_hint\n                # Ensure max_length is removed if type becomes IntegerField\n                if django_field_type == models.IntegerField:\n                    kwargs.pop(\"max_length\", None)\n            else:\n                logger.warning(\"EnumFieldMapping selected but failed to get valid field type hint from kwargs.\")\n\n        # --- Handle Relationships (Only if not multi-FK union) --- #\n        # This section needs to run *after* unit selection but *before* final nullability checks\n        if unit_cls in (ForeignKeyMapping, OneToOneFieldMapping, ManyToManyFieldMapping):\n            # Ensure base_py_type is the related model (set during M2M check or found by find_unit for FK/O2O)\n            related_py_model = base_py_type\n\n            # Check if it's a known Pydantic BaseModel OR a known Dataclass\n            is_pyd_or_dc = inspect.isclass(related_py_model) and (\n                issubclass(related_py_model, BaseModel) or dataclasses.is_dataclass(related_py_model)\n            )\n            if not is_pyd_or_dc:\n                raise MappingError(\n                    f\"Relationship mapping unit {unit_cls.__name__} selected, but base type {related_py_model} is not a known Pydantic model or Dataclass.\"\n                )\n\n            # Check for self-reference BEFORE trying to get the Django model\n            is_self_ref = parent_pydantic_model is not None and related_py_model == parent_pydantic_model\n\n            if is_self_ref:\n                model_ref = \"self\"\n                # Get the target Django model name for logging/consistency if possible, but use 'self'\n                # Check if the related model is a Pydantic BaseModel or a dataclass\n                if inspect.isclass(related_py_model) and issubclass(related_py_model, BaseModel):\n                    target_django_model = self.relationship_accessor.get_django_model_for_pydantic(\n                        cast(type[BaseModel], related_py_model)\n                    )\n                elif dataclasses.is_dataclass(related_py_model):\n                    target_django_model = self.relationship_accessor.get_django_model_for_dataclass(\n                        related_py_model\n                    )\n                else:\n                    # This case should ideally not be reached due to earlier checks, but handle defensively\n                    target_django_model = None\n                    logger.warning(\n                        f\"Self-reference check: related_py_model '{related_py_model}' is neither BaseModel nor dataclass.\"\n                    )\n\n                logger.debug(\n                    f\"Detected self-reference for {related_py_model.__name__ if inspect.isclass(related_py_model) else related_py_model} \"\n                    f\"(Django: {getattr(target_django_model, '__name__', 'N/A')}), using 'self'.\"\n                )\n            else:\n                # Get target Django model based on source type (Pydantic or Dataclass)\n                target_django_model = None\n                # Ensure related_py_model is actually a type before issubclass check\n                if inspect.isclass(related_py_model) and issubclass(related_py_model, BaseModel):\n                    # Cast to satisfy type checker, as we've confirmed it's a BaseModel subclass here\n                    target_django_model = self.relationship_accessor.get_django_model_for_pydantic(\n                        cast(type[BaseModel], related_py_model)\n                    )\n                elif dataclasses.is_dataclass(related_py_model):\n                    target_django_model = self.relationship_accessor.get_django_model_for_dataclass(\n                        related_py_model\n                    )\n\n                if not target_django_model:\n                    raise MappingError(\n                        f\"Cannot map relationship: No corresponding Django model found for source model \"\n                        f\"{related_py_model.__name__} in RelationshipConversionAccessor.\"\n                    )\n                # Use lowercase label for internal consistency with existing expectations\n                model_ref = getattr(target_django_model._meta, \"label_lower\", target_django_model.__name__)\n\n            kwargs[\"to\"] = model_ref\n            django_field_type = unit_cls.django_field_type  # Re-confirm M2MField, FK, O2O type\n            # Set on_delete for FK/O2O based on Optional status\n            if unit_cls in (ForeignKeyMapping, OneToOneFieldMapping):\n                # Default to CASCADE for non-optional, SET_NULL for optional (matching test expectation)\n                kwargs[\"on_delete\"] = (\n                    models.SET_NULL if is_optional else models.CASCADE\n                )  # Changed PROTECT to CASCADE\n\n    # --- Final Adjustments (Nullability, etc.) --- #\n    # Apply nullability. M2M fields cannot be null in Django.\n    # Do not override nullability if it was already forced by a multi-FK union\n    if django_field_type != models.ManyToManyField and not union_details:\n        kwargs[\"null\"] = is_optional\n        # Explicitly set blank based on optionality.\n        # Simplified logic: Mirror the null assignment directly\n        kwargs[\"blank\"] = is_optional\n\n    logger.debug(\n        f\"FINAL RETURN from get_django_mapping: Type={django_field_type}, Kwargs={kwargs}\"\n    )  # Added final state logging\n    return django_field_type, kwargs\n</code></pre>"},{"location":"reference/pydantic2django/core/bidirectional_mapper/#pydantic2django.core.bidirectional_mapper.BidirectionalTypeMapper.get_pydantic_mapping","title":"<code>get_pydantic_mapping(dj_field)</code>","text":"<p>Get the corresponding Pydantic type hint and FieldInfo kwargs for a Django Field.</p> Source code in <code>src/pydantic2django/core/bidirectional_mapper.py</code> <pre><code>def get_pydantic_mapping(self, dj_field: models.Field) -&gt; tuple[Any, dict[str, Any]]:\n    \"\"\"Get the corresponding Pydantic type hint and FieldInfo kwargs for a Django Field.\"\"\"\n    dj_field_type = type(dj_field)\n    is_optional = dj_field.null\n    is_choices = bool(dj_field.choices)\n\n    # --- Find base unit (ignoring choices for now) ---\n    # Find the mapping unit based on the specific Django field type MRO\n    # This gives us the correct underlying Python type (str, int, etc.)\n    base_unit_cls = self._find_unit_for_django_field(dj_field_type)\n\n    if not base_unit_cls:\n        logger.warning(f\"No base mapping unit for {dj_field_type.__name__}, falling back to Any.\")\n        pydantic_type = Optional[Any] if is_optional else Any\n        return pydantic_type, {}\n\n    base_instance_unit = base_unit_cls()\n    # Get the base Pydantic type from this unit\n    final_pydantic_type = base_instance_unit.python_type\n\n    # --- Determine Final Pydantic Type Adjustments --- #\n    # (Relationships, AutoPK, Optional wrapper)\n\n    # Handle choices FIRST to determine the core type before Optional wrapping\n    if is_choices:\n        # Default to base type, override if valid choices found\n        final_pydantic_type = base_instance_unit.python_type\n        if dj_field.choices:  # Explicit check before iteration\n            try:\n                choice_values = tuple(choice[0] for choice in dj_field.choices)\n                if choice_values:  # Ensure the tuple is not empty\n                    final_pydantic_type = Literal[choice_values]  # type: ignore\n                    logger.debug(f\"Mapped choices for '{dj_field.name}' to Pydantic type: {final_pydantic_type}\")\n                else:\n                    logger.warning(\n                        f\"Field '{dj_field.name}' has choices defined, but extracted values are empty. Falling back.\"\n                    )\n                    # Keep final_pydantic_type as base type\n            except Exception as e:\n                logger.warning(f\"Failed to extract choices for field '{dj_field.name}'. Error: {e}. Falling back.\")\n                # Keep final_pydantic_type as base type\n        # If dj_field.choices was None/empty initially, final_pydantic_type remains the base type\n    else:\n        # Get the base Pydantic type from this unit if not choices\n        final_pydantic_type = base_instance_unit.python_type\n\n    # 1. Handle Relationships first, as they determine the core type\n    if base_unit_cls in (ForeignKeyMapping, OneToOneFieldMapping, ManyToManyFieldMapping):\n        related_dj_model = getattr(dj_field, \"related_model\", None)\n        if not related_dj_model:\n            raise MappingError(f\"Cannot determine related Django model for field '{dj_field.name}'\")\n\n        # Resolve 'self' reference\n        if related_dj_model == \"self\":\n            # We need the Django model class that dj_field belongs to.\n            # This info isn't directly passed, so this approach might be limited.\n            # Assuming self-reference points to the same type hierarchy for now.\n            # A better solution might need the model context passed down.\n            logger.warning(\n                f\"Handling 'self' reference for field '{dj_field.name}'. Mapping might be incomplete without parent model context.\"\n            )\n            # Attempt to get Pydantic model mapped to the field's owner model if possible (heuristically)\n            # This is complex and potentially fragile.\n            # For now, let's use a placeholder or raise an error if needed strictly.\n            # Sticking with the base type (e.g., Any or int for PK) might be safer without context.\n            # Use the base type (likely PK int/uuid) as the fallback type here\n            target_pydantic_model = base_instance_unit.python_type\n            logger.debug(f\"Using Any as placeholder for 'self' reference '{dj_field.name}'\")\n        else:\n            target_pydantic_model = self.relationship_accessor.get_pydantic_model_for_django(related_dj_model)\n\n        if not target_pydantic_model or target_pydantic_model is Any:\n            if related_dj_model != \"self\":  # Avoid redundant warning for self\n                logger.warning(\n                    f\"Cannot map relationship: No corresponding Pydantic model found for Django model \"\n                    f\"'{related_dj_model._meta.label if hasattr(related_dj_model, '_meta') else related_dj_model.__name__}'. \"\n                    f\"Using placeholder '{final_pydantic_type}'.\"\n                )\n            # Keep final_pydantic_type as the base unit's python_type (e.g., int for FK)\n        else:\n            if base_unit_cls == ManyToManyFieldMapping:\n                final_pydantic_type = list[target_pydantic_model]\n            else:  # FK or O2O\n                # Keep the PK type (e.g., int) if target model not found,\n                # otherwise use the target Pydantic model type.\n                final_pydantic_type = target_pydantic_model  # This should now be the related model type\n            logger.debug(f\"Mapped relationship field '{dj_field.name}' to Pydantic type: {final_pydantic_type}\")\n\n    # 2. AutoPK override (after relationship resolution)\n    is_auto_pk = dj_field.primary_key and isinstance(\n        dj_field, (models.AutoField, models.BigAutoField, models.SmallAutoField)\n    )\n    if is_auto_pk:\n        final_pydantic_type = Optional[int]\n        logger.debug(f\"Mapped AutoPK field '{dj_field.name}' to {final_pydantic_type}\")\n        is_optional = True  # AutoPKs are always optional in Pydantic input\n\n    # 3. Apply Optional[...] wrapper if necessary (AFTER relationship/AutoPK)\n    # Do not wrap M2M lists or already Optional AutoPKs in Optional[] again.\n    # Also, don't wrap if the type is already Literal (choices handled Optionality) - NO, wrap Literal too if null=True\n    if is_optional and not is_auto_pk:  # Check if is_choices? No, optional applies to literal too.\n        origin = get_origin(final_pydantic_type)\n        args = get_args(final_pydantic_type)\n        is_already_optional = origin is Optional or origin is UnionType and type(None) in args\n\n        if not is_already_optional:\n            final_pydantic_type = Optional[final_pydantic_type]\n            logger.debug(f\"Wrapped type for '{dj_field.name}' in Optional: {final_pydantic_type}\")\n\n    # --- Generate FieldInfo Kwargs --- #\n    # Use EnumFieldMapping logic for kwargs ONLY if choices exist,\n    # otherwise use the base unit determined earlier. # --&gt; NO, always use base unit for kwargs now. Literal type handles choices.\n    # kwargs_unit_cls = EnumFieldMapping if is_choices else base_unit_cls # OLD logic\n    instance_unit = base_unit_cls()  # Use the base unit (e.g., StrFieldMapping) for base kwargs\n\n    field_info_kwargs = instance_unit.django_to_pydantic_field_info_kwargs(dj_field)\n\n    # --- Explicitly cast title (verbose_name) and description (help_text) --- #\n    if field_info_kwargs.get(\"title\") is not None:\n        field_info_kwargs[\"title\"] = str(field_info_kwargs[\"title\"])\n        logger.debug(f\"Ensured title is str for '{dj_field.name}': {field_info_kwargs['title']}\")\n    if field_info_kwargs.get(\"description\") is not None:\n        field_info_kwargs[\"description\"] = str(field_info_kwargs[\"description\"])\n        logger.debug(f\"Ensured description is str for '{dj_field.name}': {field_info_kwargs['description']}\")\n    # --- End Casting --- #\n\n    # --- Keep choices in json_schema_extra even when using Literal ---\n    # This preserves the (value, label) mapping as metadata alongside the Literal type.\n    if (\n        is_choices\n        and \"json_schema_extra\" in field_info_kwargs\n        and \"choices\" in field_info_kwargs[\"json_schema_extra\"]\n    ):\n        logger.debug(f\"Kept choices in json_schema_extra for Literal field '{dj_field.name}'\")\n    elif is_choices:\n        logger.debug(\n            f\"Field '{dj_field.name}' has choices, but they weren't added to json_schema_extra by the mapping unit.\"\n        )\n\n    # Clean up redundant `default=None` for Optional fields handled by Pydantic v2.\n    # Do not force-add default=None; only keep explicit defaults (e.g., for AutoPK).\n    if is_optional:\n        if field_info_kwargs.get(\"default\") is None and not is_auto_pk:\n            # Remove implicit default=None for Optional fields\n            field_info_kwargs.pop(\"default\", None)\n            logger.debug(f\"Removed redundant default=None for Optional field '{dj_field.name}'\")\n        elif is_auto_pk and \"default\" not in field_info_kwargs:\n            # Keep default=None for AutoPK if not already set\n            field_info_kwargs[\"default\"] = None\n            logger.debug(f\"Set default=None for AutoPK Optional field '{dj_field.name}'\")\n\n    logger.debug(\n        f\"Final Pydantic mapping for '{dj_field.name}': Type={final_pydantic_type}, Kwargs={field_info_kwargs}\"\n    )\n    return final_pydantic_type, field_info_kwargs\n</code></pre>"},{"location":"reference/pydantic2django/core/bidirectional_mapper/#pydantic2django.core.bidirectional_mapper.MappingError","title":"<code>MappingError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Custom exception for mapping errors.</p> Source code in <code>src/pydantic2django/core/bidirectional_mapper.py</code> <pre><code>class MappingError(Exception):\n    \"\"\"Custom exception for mapping errors.\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/pydantic2django/core/context/","title":"pydantic2django.core.context","text":"<p>Context storage system for handling non-serializable fields in Pydantic2Django.</p> <p>This module provides the core functionality for managing context fields and their mapping back to Pydantic objects. It handles the storage and retrieval of context information needed for field reconstruction.</p>"},{"location":"reference/pydantic2django/core/context/#pydantic2django.core.context.ContextClassGenerator","title":"<code>ContextClassGenerator</code>","text":"<p>Utility class for generating context class code from ModelContext objects.</p> Source code in <code>src/pydantic2django/core/context.py</code> <pre><code>class ContextClassGenerator:\n    \"\"\"\n    Utility class for generating context class code from ModelContext objects.\n    \"\"\"\n\n    def __init__(self, jinja_env: Any | None = None) -&gt; None:\n        \"\"\"\n        Initialize the ContextClassGenerator.\n\n        Args:\n            jinja_env: Optional Jinja2 environment to use for template rendering.\n        \"\"\"\n        self.jinja_env = jinja_env\n        # Initialize imports needed for the context class generation\n        self.imports: dict[str, set[str]] = {\"typing\": set(), \"custom\": set()}\n\n    def _load_template(self, template_name: str) -&gt; Any:\n        \"\"\"Load Jinja2 template.\"\"\"\n        if self.jinja_env:\n            return self.jinja_env.get_template(template_name)\n        else:\n            # Fallback to basic string formatting if Jinja2 is not available\n            # Note: This is a simplified fallback and might not handle complex templates\n            # Load template content from file or define as string here\n            # Example using basic string formatting:\n            # template_content = \"... {model_name} ... {field_definitions} ...\"\n            # return template_content\n            raise ImportError(\"Jinja2 environment not provided for template loading.\")\n\n    def _simplify_type_string(self, type_str: str) -&gt; str:\n        \"\"\"\n        Simplifies complex type strings for cleaner code generation.\n        Removes module paths like 'typing.' or full paths for common types.\n        \"\"\"\n        # Basic simplification: remove typing module path\n        simplified = type_str.replace(\"typing.\", \"\")\n\n        # Use TypeHandler to potentially clean further if needed\n        # simplified = TypeHandler.clean_type_string(simplified)\n\n        # Regex to remove full paths for nested standard types like list, dict, etc.\n        # Define common standard types that might appear with full paths\n        standard_types = [\"list\", \"dict\", \"tuple\", \"set\", \"Optional\", \"Union\"]\n\n        def replacer_class(match):\n            full_path = match.group(0)\n            # Extract the class name after the last dot\n            class_name = full_path.split(\".\")[-1]\n            # Check if the extracted class name is a standard type we want to simplify\n            if class_name in standard_types:\n                # If it is, return just the class name\n                return class_name\n            else:\n                # Otherwise, keep the full path (or handle custom imports)\n                # For now, keeping full path for non-standard types\n                # self._maybe_add_type_to_imports(full_path) # Add import for custom type\n                return full_path\n\n        # Pattern to find qualified names (e.g., some.module.ClassName)\n        # This needs careful crafting to avoid unintended replacements\n        # Example: r'\\b([a-zA-Z_][\\w\\.]*\\.)?([A-Z][a-zA-Z0-9_]*)\\b' might be too broad\n        # Focusing on paths likely coming from TypeHandler.get_required_imports might be safer\n        # For now, rely on basic replace and potential TypeHandler cleaning\n\n        return simplified\n\n    def generate_context_class(self, model_context: ModelContext) -&gt; str:\n        \"\"\"\n        Generates the Python code string for a context dataclass.\n        \"\"\"\n        template = self._load_template(\"context_class.py.j2\")\n        self.imports = model_context.get_required_imports()  # Get imports first\n\n        field_definitions = []\n        for field_name, field_context in model_context.context_fields.items():\n            field_type_str = field_context.field_type_str  # field_type is now the string representation\n\n            # Use TypeHandler._get_raw_type_string to get the clean, unquoted type string\n            # --- Corrected import path for TypeHandler ---\n            from .typing import TypeHandler\n\n            clean_type = TypeHandler._get_raw_type_string(field_type_str)\n\n            # Simplify the type string for display\n            simplified_type = self._simplify_type_string(clean_type)\n\n            # Add necessary imports based on the simplified type\n            # (Assuming _simplify_type_string and get_required_imports handle this)\n\n            # Format default value if present\n            default_repr = repr(field_context.value) if field_context.value is not None else \"None\"\n\n            field_def = f\"    {field_name}: {simplified_type} = field(default={default_repr})\"\n            field_definitions.append(field_def)\n\n        # Prepare imports for the template\n        typing_imports_str = \", \".join(sorted(self.imports[\"typing\"]))\n        custom_imports_list = sorted(self.imports[\"custom\"])  # Keep as list of strings\n\n        model_name = self._clean_generic_type(model_context.django_model.__name__)\n        source_class_name = self._clean_generic_type(model_context.source_class.__name__)\n\n        return template.render(\n            model_name=model_name,\n            # Use source_class_name instead of pydantic_class\n            source_class_name=source_class_name,\n            source_module=model_context.source_class.__module__,\n            field_definitions=\"\\n\".join(field_definitions),\n            typing_imports=typing_imports_str,\n            custom_imports=custom_imports_list,\n        )\n\n    def _clean_generic_type(self, name: str) -&gt; str:\n        \"\"\"Remove generic parameters like [T] from class names.\"\"\"\n        return name.split(\"[\")[0]\n</code></pre>"},{"location":"reference/pydantic2django/core/context/#pydantic2django.core.context.ContextClassGenerator.__init__","title":"<code>__init__(jinja_env=None)</code>","text":"<p>Initialize the ContextClassGenerator.</p> <p>Parameters:</p> Name Type Description Default <code>jinja_env</code> <code>Any | None</code> <p>Optional Jinja2 environment to use for template rendering.</p> <code>None</code> Source code in <code>src/pydantic2django/core/context.py</code> <pre><code>def __init__(self, jinja_env: Any | None = None) -&gt; None:\n    \"\"\"\n    Initialize the ContextClassGenerator.\n\n    Args:\n        jinja_env: Optional Jinja2 environment to use for template rendering.\n    \"\"\"\n    self.jinja_env = jinja_env\n    # Initialize imports needed for the context class generation\n    self.imports: dict[str, set[str]] = {\"typing\": set(), \"custom\": set()}\n</code></pre>"},{"location":"reference/pydantic2django/core/context/#pydantic2django.core.context.ContextClassGenerator.generate_context_class","title":"<code>generate_context_class(model_context)</code>","text":"<p>Generates the Python code string for a context dataclass.</p> Source code in <code>src/pydantic2django/core/context.py</code> <pre><code>def generate_context_class(self, model_context: ModelContext) -&gt; str:\n    \"\"\"\n    Generates the Python code string for a context dataclass.\n    \"\"\"\n    template = self._load_template(\"context_class.py.j2\")\n    self.imports = model_context.get_required_imports()  # Get imports first\n\n    field_definitions = []\n    for field_name, field_context in model_context.context_fields.items():\n        field_type_str = field_context.field_type_str  # field_type is now the string representation\n\n        # Use TypeHandler._get_raw_type_string to get the clean, unquoted type string\n        # --- Corrected import path for TypeHandler ---\n        from .typing import TypeHandler\n\n        clean_type = TypeHandler._get_raw_type_string(field_type_str)\n\n        # Simplify the type string for display\n        simplified_type = self._simplify_type_string(clean_type)\n\n        # Add necessary imports based on the simplified type\n        # (Assuming _simplify_type_string and get_required_imports handle this)\n\n        # Format default value if present\n        default_repr = repr(field_context.value) if field_context.value is not None else \"None\"\n\n        field_def = f\"    {field_name}: {simplified_type} = field(default={default_repr})\"\n        field_definitions.append(field_def)\n\n    # Prepare imports for the template\n    typing_imports_str = \", \".join(sorted(self.imports[\"typing\"]))\n    custom_imports_list = sorted(self.imports[\"custom\"])  # Keep as list of strings\n\n    model_name = self._clean_generic_type(model_context.django_model.__name__)\n    source_class_name = self._clean_generic_type(model_context.source_class.__name__)\n\n    return template.render(\n        model_name=model_name,\n        # Use source_class_name instead of pydantic_class\n        source_class_name=source_class_name,\n        source_module=model_context.source_class.__module__,\n        field_definitions=\"\\n\".join(field_definitions),\n        typing_imports=typing_imports_str,\n        custom_imports=custom_imports_list,\n    )\n</code></pre>"},{"location":"reference/pydantic2django/core/context/#pydantic2django.core.context.FieldContext","title":"<code>FieldContext</code>  <code>dataclass</code>","text":"<p>Represents context information for a single field.</p> Source code in <code>src/pydantic2django/core/context.py</code> <pre><code>@dataclass\nclass FieldContext:\n    \"\"\"\n    Represents context information for a single field.\n    \"\"\"\n\n    field_name: str\n    field_type_str: str  # Renamed from field_type for clarity\n    is_optional: bool = False\n    is_list: bool = False\n    additional_metadata: dict[str, Any] = field(default_factory=dict)\n    value: Optional[Any] = None\n</code></pre>"},{"location":"reference/pydantic2django/core/context/#pydantic2django.core.context.ModelContext","title":"<code>ModelContext</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Generic[SourceModelType]</code></p> <p>Base class for model context classes. Stores context information for a Django model's fields that require special handling during conversion back to the source object (Pydantic/Dataclass).</p> Source code in <code>src/pydantic2django/core/context.py</code> <pre><code>@dataclass\nclass ModelContext(Generic[SourceModelType]):  # Make ModelContext generic\n    \"\"\"\n    Base class for model context classes.\n    Stores context information for a Django model's fields that require special handling\n    during conversion back to the source object (Pydantic/Dataclass).\n    \"\"\"\n\n    django_model: type[models.Model]\n    source_class: type[SourceModelType]  # Changed from pydantic_class\n    context_fields: dict[str, FieldContext] = field(default_factory=dict)\n    context_data: dict[str, Any] = field(default_factory=dict)\n\n    @property\n    def required_context_keys(self) -&gt; set[str]:\n        required_fields = {fc.field_name for fc in self.context_fields.values() if not fc.is_optional}\n        return required_fields\n\n    def add_field(\n        self,\n        field_name: str,\n        field_type_str: str,\n        is_optional: bool = False,\n        is_list: bool = False,\n        **kwargs: Any,\n    ) -&gt; None:\n        \"\"\"\n        Add a field to the context storage.\n\n        Args:\n            field_name: Name of the field.\n            field_type_str: String representation of the field's type.\n            is_optional: Whether the field is optional.\n            is_list: Whether the field is a list.\n            **kwargs: Additional metadata for the field.\n        \"\"\"\n        # Pass is_optional, is_list explicitly\n        field_context = FieldContext(\n            field_name=field_name,\n            field_type_str=field_type_str,\n            is_optional=is_optional,\n            is_list=is_list,\n            additional_metadata=kwargs,\n        )\n        self.context_fields[field_name] = field_context\n\n    def validate_context(self, context: dict[str, Any]) -&gt; None:\n        \"\"\"\n        Validate that all required context fields are present.\n\n        Args:\n            context: The context dictionary to validate\n\n        Raises:\n            ValueError: If required context fields are missing\n        \"\"\"\n\n        missing_fields = self.required_context_keys - set(context.keys())\n        if missing_fields:\n            raise ValueError(f\"Missing required context fields: {', '.join(missing_fields)}\")\n\n    def get_field_type_str(self, field_name: str) -&gt; Optional[str]:\n        \"\"\"Get the string representation type of a context field.\"\"\"\n        field_context = self.context_fields.get(field_name)\n        return field_context.field_type_str if field_context else None\n\n    def get_field_by_name(self, field_name: str) -&gt; Optional[FieldContext]:\n        \"\"\"\n        Get a field context by name.\n\n        Args:\n            field_name: Name of the field to find\n\n        Returns:\n            The FieldContext if found, None otherwise\n        \"\"\"\n        return self.context_fields.get(field_name)\n\n    def to_conversion_dict(self) -&gt; dict[str, Any]:\n        \"\"\"Convert context to a dictionary format suitable for conversion back to source object.\"\"\"\n        # Renamed from to_dict to be more generic\n        return {\n            field_name: field_context.value\n            for field_name, field_context in self.context_fields.items()\n            if field_context.value is not None\n        }\n\n    def set_value(self, field_name: str, value: Any) -&gt; None:\n        \"\"\"\n        Set the value for a context field.\n\n        Args:\n            field_name: Name of the field\n            value: Value to set\n\n        Raises:\n            ValueError: If the field doesn't exist in the context\n        \"\"\"\n        field = self.get_field_by_name(field_name)\n        if field is None:\n            raise ValueError(f\"Field {field_name} not found in context\")\n        field.value = value\n\n    def get_value(self, field_name: str) -&gt; Optional[Any]:\n        \"\"\"\n        Get the value of a context field.\n\n        Args:\n            field_name: Name of the field\n\n        Returns:\n            The field value if it exists and has been set, None otherwise\n        \"\"\"\n        field = self.get_field_by_name(field_name)\n        if field is not None:\n            return field.value\n        return None\n\n    def get_required_imports(self) -&gt; dict[str, set[str]]:  # Return sets for auto-dedup\n        \"\"\"\n        Get all required imports for the context class fields using TypeHandler.\n        \"\"\"\n        imports: dict[str, set[str]] = {\"typing\": set(), \"custom\": set()}\n\n        # Process each field\n        for _, field_context in self.context_fields.items():\n            # Use TypeHandler with the stored type string\n            type_imports = TypeHandler.get_required_imports(field_context.field_type_str)\n\n            # Add to our overall imports\n            imports[\"typing\"].update(type_imports.get(\"typing\", []))\n            imports[\"custom\"].update(type_imports.get(\"datetime\", []))  # Example specific types\n            imports[\"custom\"].update(type_imports.get(\"decimal\", []))\n            imports[\"custom\"].update(type_imports.get(\"uuid\", []))\n            # Add any other known modules TypeHandler might return\n\n            # Add Optional/List based on flags\n            if field_context.is_optional:\n                imports[\"typing\"].add(\"Optional\")\n            if field_context.is_list:\n                imports[\"typing\"].add(\"List\")\n\n        # Add base source model import\n        source_module = getattr(self.source_class, \"__module__\", None)\n        source_name = getattr(self.source_class, \"__name__\", None)\n        if source_module and source_name and source_module != \"builtins\":\n            imports[\"custom\"].add(f\"from {source_module} import {source_name}\")\n\n        # Add BaseModel or dataclass import\n        if isinstance(self.source_class, type) and issubclass(self.source_class, BaseModel):\n            imports[\"custom\"].add(\"from pydantic import BaseModel\")\n        elif dataclasses.is_dataclass(self.source_class):\n            imports[\"custom\"].add(\"from dataclasses import dataclass\")\n\n        # Add Any import if needed\n        if any(\"Any\" in fc.field_type_str for fc in self.context_fields.values()):\n            imports[\"typing\"].add(\"Any\")\n\n        return imports\n\n    @classmethod\n    def generate_context_class_code(cls, model_context: \"ModelContext\", jinja_env: Any | None = None) -&gt; str:\n        \"\"\"\n        Generate a string representation of the context class.\n\n        Args:\n            model_context: The ModelContext to generate a class for\n            jinja_env: Optional Jinja2 environment to use for rendering\n\n        Returns:\n            String representation of the context class\n        \"\"\"\n        # Create a ContextClassGenerator and use it to generate the class\n        generator = ContextClassGenerator(jinja_env=jinja_env)\n        return generator.generate_context_class(model_context)\n</code></pre>"},{"location":"reference/pydantic2django/core/context/#pydantic2django.core.context.ModelContext.add_field","title":"<code>add_field(field_name, field_type_str, is_optional=False, is_list=False, **kwargs)</code>","text":"<p>Add a field to the context storage.</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>Name of the field.</p> required <code>field_type_str</code> <code>str</code> <p>String representation of the field's type.</p> required <code>is_optional</code> <code>bool</code> <p>Whether the field is optional.</p> <code>False</code> <code>is_list</code> <code>bool</code> <p>Whether the field is a list.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional metadata for the field.</p> <code>{}</code> Source code in <code>src/pydantic2django/core/context.py</code> <pre><code>def add_field(\n    self,\n    field_name: str,\n    field_type_str: str,\n    is_optional: bool = False,\n    is_list: bool = False,\n    **kwargs: Any,\n) -&gt; None:\n    \"\"\"\n    Add a field to the context storage.\n\n    Args:\n        field_name: Name of the field.\n        field_type_str: String representation of the field's type.\n        is_optional: Whether the field is optional.\n        is_list: Whether the field is a list.\n        **kwargs: Additional metadata for the field.\n    \"\"\"\n    # Pass is_optional, is_list explicitly\n    field_context = FieldContext(\n        field_name=field_name,\n        field_type_str=field_type_str,\n        is_optional=is_optional,\n        is_list=is_list,\n        additional_metadata=kwargs,\n    )\n    self.context_fields[field_name] = field_context\n</code></pre>"},{"location":"reference/pydantic2django/core/context/#pydantic2django.core.context.ModelContext.generate_context_class_code","title":"<code>generate_context_class_code(model_context, jinja_env=None)</code>  <code>classmethod</code>","text":"<p>Generate a string representation of the context class.</p> <p>Parameters:</p> Name Type Description Default <code>model_context</code> <code>ModelContext</code> <p>The ModelContext to generate a class for</p> required <code>jinja_env</code> <code>Any | None</code> <p>Optional Jinja2 environment to use for rendering</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>String representation of the context class</p> Source code in <code>src/pydantic2django/core/context.py</code> <pre><code>@classmethod\ndef generate_context_class_code(cls, model_context: \"ModelContext\", jinja_env: Any | None = None) -&gt; str:\n    \"\"\"\n    Generate a string representation of the context class.\n\n    Args:\n        model_context: The ModelContext to generate a class for\n        jinja_env: Optional Jinja2 environment to use for rendering\n\n    Returns:\n        String representation of the context class\n    \"\"\"\n    # Create a ContextClassGenerator and use it to generate the class\n    generator = ContextClassGenerator(jinja_env=jinja_env)\n    return generator.generate_context_class(model_context)\n</code></pre>"},{"location":"reference/pydantic2django/core/context/#pydantic2django.core.context.ModelContext.get_field_by_name","title":"<code>get_field_by_name(field_name)</code>","text":"<p>Get a field context by name.</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>Name of the field to find</p> required <p>Returns:</p> Type Description <code>Optional[FieldContext]</code> <p>The FieldContext if found, None otherwise</p> Source code in <code>src/pydantic2django/core/context.py</code> <pre><code>def get_field_by_name(self, field_name: str) -&gt; Optional[FieldContext]:\n    \"\"\"\n    Get a field context by name.\n\n    Args:\n        field_name: Name of the field to find\n\n    Returns:\n        The FieldContext if found, None otherwise\n    \"\"\"\n    return self.context_fields.get(field_name)\n</code></pre>"},{"location":"reference/pydantic2django/core/context/#pydantic2django.core.context.ModelContext.get_field_type_str","title":"<code>get_field_type_str(field_name)</code>","text":"<p>Get the string representation type of a context field.</p> Source code in <code>src/pydantic2django/core/context.py</code> <pre><code>def get_field_type_str(self, field_name: str) -&gt; Optional[str]:\n    \"\"\"Get the string representation type of a context field.\"\"\"\n    field_context = self.context_fields.get(field_name)\n    return field_context.field_type_str if field_context else None\n</code></pre>"},{"location":"reference/pydantic2django/core/context/#pydantic2django.core.context.ModelContext.get_required_imports","title":"<code>get_required_imports()</code>","text":"<p>Get all required imports for the context class fields using TypeHandler.</p> Source code in <code>src/pydantic2django/core/context.py</code> <pre><code>def get_required_imports(self) -&gt; dict[str, set[str]]:  # Return sets for auto-dedup\n    \"\"\"\n    Get all required imports for the context class fields using TypeHandler.\n    \"\"\"\n    imports: dict[str, set[str]] = {\"typing\": set(), \"custom\": set()}\n\n    # Process each field\n    for _, field_context in self.context_fields.items():\n        # Use TypeHandler with the stored type string\n        type_imports = TypeHandler.get_required_imports(field_context.field_type_str)\n\n        # Add to our overall imports\n        imports[\"typing\"].update(type_imports.get(\"typing\", []))\n        imports[\"custom\"].update(type_imports.get(\"datetime\", []))  # Example specific types\n        imports[\"custom\"].update(type_imports.get(\"decimal\", []))\n        imports[\"custom\"].update(type_imports.get(\"uuid\", []))\n        # Add any other known modules TypeHandler might return\n\n        # Add Optional/List based on flags\n        if field_context.is_optional:\n            imports[\"typing\"].add(\"Optional\")\n        if field_context.is_list:\n            imports[\"typing\"].add(\"List\")\n\n    # Add base source model import\n    source_module = getattr(self.source_class, \"__module__\", None)\n    source_name = getattr(self.source_class, \"__name__\", None)\n    if source_module and source_name and source_module != \"builtins\":\n        imports[\"custom\"].add(f\"from {source_module} import {source_name}\")\n\n    # Add BaseModel or dataclass import\n    if isinstance(self.source_class, type) and issubclass(self.source_class, BaseModel):\n        imports[\"custom\"].add(\"from pydantic import BaseModel\")\n    elif dataclasses.is_dataclass(self.source_class):\n        imports[\"custom\"].add(\"from dataclasses import dataclass\")\n\n    # Add Any import if needed\n    if any(\"Any\" in fc.field_type_str for fc in self.context_fields.values()):\n        imports[\"typing\"].add(\"Any\")\n\n    return imports\n</code></pre>"},{"location":"reference/pydantic2django/core/context/#pydantic2django.core.context.ModelContext.get_value","title":"<code>get_value(field_name)</code>","text":"<p>Get the value of a context field.</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>Name of the field</p> required <p>Returns:</p> Type Description <code>Optional[Any]</code> <p>The field value if it exists and has been set, None otherwise</p> Source code in <code>src/pydantic2django/core/context.py</code> <pre><code>def get_value(self, field_name: str) -&gt; Optional[Any]:\n    \"\"\"\n    Get the value of a context field.\n\n    Args:\n        field_name: Name of the field\n\n    Returns:\n        The field value if it exists and has been set, None otherwise\n    \"\"\"\n    field = self.get_field_by_name(field_name)\n    if field is not None:\n        return field.value\n    return None\n</code></pre>"},{"location":"reference/pydantic2django/core/context/#pydantic2django.core.context.ModelContext.set_value","title":"<code>set_value(field_name, value)</code>","text":"<p>Set the value for a context field.</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>Name of the field</p> required <code>value</code> <code>Any</code> <p>Value to set</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the field doesn't exist in the context</p> Source code in <code>src/pydantic2django/core/context.py</code> <pre><code>def set_value(self, field_name: str, value: Any) -&gt; None:\n    \"\"\"\n    Set the value for a context field.\n\n    Args:\n        field_name: Name of the field\n        value: Value to set\n\n    Raises:\n        ValueError: If the field doesn't exist in the context\n    \"\"\"\n    field = self.get_field_by_name(field_name)\n    if field is None:\n        raise ValueError(f\"Field {field_name} not found in context\")\n    field.value = value\n</code></pre>"},{"location":"reference/pydantic2django/core/context/#pydantic2django.core.context.ModelContext.to_conversion_dict","title":"<code>to_conversion_dict()</code>","text":"<p>Convert context to a dictionary format suitable for conversion back to source object.</p> Source code in <code>src/pydantic2django/core/context.py</code> <pre><code>def to_conversion_dict(self) -&gt; dict[str, Any]:\n    \"\"\"Convert context to a dictionary format suitable for conversion back to source object.\"\"\"\n    # Renamed from to_dict to be more generic\n    return {\n        field_name: field_context.value\n        for field_name, field_context in self.context_fields.items()\n        if field_context.value is not None\n    }\n</code></pre>"},{"location":"reference/pydantic2django/core/context/#pydantic2django.core.context.ModelContext.validate_context","title":"<code>validate_context(context)</code>","text":"<p>Validate that all required context fields are present.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>dict[str, Any]</code> <p>The context dictionary to validate</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If required context fields are missing</p> Source code in <code>src/pydantic2django/core/context.py</code> <pre><code>def validate_context(self, context: dict[str, Any]) -&gt; None:\n    \"\"\"\n    Validate that all required context fields are present.\n\n    Args:\n        context: The context dictionary to validate\n\n    Raises:\n        ValueError: If required context fields are missing\n    \"\"\"\n\n    missing_fields = self.required_context_keys - set(context.keys())\n    if missing_fields:\n        raise ValueError(f\"Missing required context fields: {', '.join(missing_fields)}\")\n</code></pre>"},{"location":"reference/pydantic2django/core/defs/","title":"pydantic2django.core.defs","text":"<p>Type definitions for pydantic2django.</p>"},{"location":"reference/pydantic2django/core/defs/#pydantic2django.core.defs.TypeMappingDefinition","title":"<code>TypeMappingDefinition</code>  <code>dataclass</code>","text":"<p>Definition of a mapping between a Python/Pydantic type and a Django field type.</p> <p>This class represents a single mapping between a Python type and a Django field type, with optional additional attributes like max_length and relationship info.</p> Source code in <code>src/pydantic2django/core/defs.py</code> <pre><code>@dataclass\nclass TypeMappingDefinition:\n    \"\"\"\n    Definition of a mapping between a Python/Pydantic type and a Django field type.\n\n    This class represents a single mapping between a Python type and a Django field type,\n    with optional additional attributes like max_length and relationship info.\n    \"\"\"\n\n    python_type: PythonType\n    django_field: type[models.Field]\n    max_length: Optional[int] = None\n    is_relationship: bool = False\n    on_delete: Optional[Any] = None  # For ForeignKey relationships\n    field_kwargs: dict[str, Any] = field(default_factory=dict)\n\n    # Post-init logic that depended on get_default_max_length\n    # will be handled in the django.mapping module where that function lives.\n\n    # Property and class methods related to Django field creation\n    # (relationship_type, char_field, text_field, etc.) are moved to django.mapping.\n\n    def matches_type(self, python_type: Any) -&gt; bool:\n        \"\"\"Check if this definition matches the given Python type.\"\"\"\n        actual_type = python_type\n        origin = get_origin(python_type)\n        args = get_args(python_type)\n\n        # Handle Optional[T]\n        if origin is Union and type(None) in args and len(args) == 2:\n            actual_type = next((arg for arg in args if arg is not type(None)), None)\n            if actual_type is None:  # Handle Optional[NoneType]\n                return False\n\n        # 1. Direct type equality check (most common case for simple types)\n        if self.python_type == actual_type:\n            return True\n\n        # 2. Special case for JSONField matching generic collections if not matched directly\n        # Check if this definition is for JSONField and the actual_type is a collection origin\n        if self.django_field == models.JSONField:\n            actual_origin = get_origin(actual_type)  # Use origin from actual_type if unwrapped\n            if actual_origin in (list, dict, set, tuple):\n                # Check if this mapping's python_type matches the collection origin\n                if self.python_type == actual_origin:\n                    return True\n\n        # 3. Subclass check (use sparingly, mainly for things like EmailStr matching str)\n        # Ensure both are actual classes before checking issubclass\n        # Avoid comparing basic types like issubclass(int, str)\n        basic_types = (str, int, float, bool, list, dict, set, tuple, bytes)\n        if (\n            inspect.isclass(self.python_type)\n            and inspect.isclass(actual_type)\n            and self.python_type not in basic_types\n            and actual_type not in basic_types\n        ):\n            try:\n                return issubclass(actual_type, self.python_type)\n            except TypeError:\n                return False  # issubclass fails if args aren't classes\n\n        # If no match after all checks\n        return False\n</code></pre>"},{"location":"reference/pydantic2django/core/defs/#pydantic2django.core.defs.TypeMappingDefinition.matches_type","title":"<code>matches_type(python_type)</code>","text":"<p>Check if this definition matches the given Python type.</p> Source code in <code>src/pydantic2django/core/defs.py</code> <pre><code>def matches_type(self, python_type: Any) -&gt; bool:\n    \"\"\"Check if this definition matches the given Python type.\"\"\"\n    actual_type = python_type\n    origin = get_origin(python_type)\n    args = get_args(python_type)\n\n    # Handle Optional[T]\n    if origin is Union and type(None) in args and len(args) == 2:\n        actual_type = next((arg for arg in args if arg is not type(None)), None)\n        if actual_type is None:  # Handle Optional[NoneType]\n            return False\n\n    # 1. Direct type equality check (most common case for simple types)\n    if self.python_type == actual_type:\n        return True\n\n    # 2. Special case for JSONField matching generic collections if not matched directly\n    # Check if this definition is for JSONField and the actual_type is a collection origin\n    if self.django_field == models.JSONField:\n        actual_origin = get_origin(actual_type)  # Use origin from actual_type if unwrapped\n        if actual_origin in (list, dict, set, tuple):\n            # Check if this mapping's python_type matches the collection origin\n            if self.python_type == actual_origin:\n                return True\n\n    # 3. Subclass check (use sparingly, mainly for things like EmailStr matching str)\n    # Ensure both are actual classes before checking issubclass\n    # Avoid comparing basic types like issubclass(int, str)\n    basic_types = (str, int, float, bool, list, dict, set, tuple, bytes)\n    if (\n        inspect.isclass(self.python_type)\n        and inspect.isclass(actual_type)\n        and self.python_type not in basic_types\n        and actual_type not in basic_types\n    ):\n        try:\n            return issubclass(actual_type, self.python_type)\n        except TypeError:\n            return False  # issubclass fails if args aren't classes\n\n    # If no match after all checks\n    return False\n</code></pre>"},{"location":"reference/pydantic2django/core/defs/#pydantic2django.core.defs.is_pydantic_model","title":"<code>is_pydantic_model(obj)</code>","text":"<p>Check if an object is a Pydantic model class.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>The object to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the object is a Pydantic model class, False otherwise</p> Source code in <code>src/pydantic2django/core/defs.py</code> <pre><code>def is_pydantic_model(obj: Any) -&gt; bool:\n    \"\"\"\n    Check if an object is a Pydantic model class.\n\n    Args:\n        obj: The object to check\n\n    Returns:\n        True if the object is a Pydantic model class, False otherwise\n    \"\"\"\n    try:\n        return issubclass(obj, BaseModel)\n    except TypeError:\n        return False\n</code></pre>"},{"location":"reference/pydantic2django/core/defs/#pydantic2django.core.defs.is_serializable_type","title":"<code>is_serializable_type(field_type)</code>","text":"<p>Check if a type is serializable (can be stored in the database).</p> <p>A type is considered serializable if: 1. It's a basic Python type (str, int, float, bool, dict, list, set, NoneType) 2. It's a collection (list, dict, set) of serializable types 3. It's a Pydantic model 4. It's an Enum 5. It has get_pydantic_core_schema defined 6. It has a serialization method (to_json, to_dict, etc.)</p> <p>Parameters:</p> Name Type Description Default <code>field_type</code> <code>Any</code> <p>The type to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the type is serializable, False otherwise</p> Source code in <code>src/pydantic2django/core/defs.py</code> <pre><code>def is_serializable_type(field_type: Any) -&gt; bool:\n    \"\"\"\n    Check if a type is serializable (can be stored in the database).\n\n    A type is considered serializable if:\n    1. It's a basic Python type (str, int, float, bool, dict, list, set, NoneType)\n    2. It's a collection (list, dict, set) of serializable types\n    3. It's a Pydantic model\n    4. It's an Enum\n    5. It has __get_pydantic_core_schema__ defined\n    6. It has a serialization method (to_json, to_dict, etc.)\n\n    Args:\n        field_type: The type to check\n\n    Returns:\n        True if the type is serializable, False otherwise\n    \"\"\"\n    # Handle typing.Any specially - it's not serializable\n    if field_type is Any:\n        return False\n\n    # Handle NoneType (type(None)) specially - it is serializable\n    if field_type is type(None):\n        return True\n\n    # Handle Optional types\n    origin = get_origin(field_type)\n    args = get_args(field_type)\n\n    if origin is Union and type(None) in args:\n        # For Optional types, check the inner type\n        inner_type = next(arg for arg in args if arg is not type(None))\n        return is_serializable_type(inner_type)\n\n    # Basic Python types that are always serializable\n    basic_types = (str, int, float, bool, dict, list, set)\n    if field_type in basic_types:\n        return True\n\n    # Handle collection types\n    if origin in (list, dict, set):\n        # For collections, check if all type arguments are serializable\n        return all(is_serializable_type(arg) for arg in args)\n\n    # Check if the type has __get_pydantic_core_schema__ (can be serialized)\n    if hasattr(field_type, \"__get_pydantic_core_schema__\"):\n        return True\n\n    # Handle Pydantic models (they can be serialized to JSON)\n    try:\n        if inspect.isclass(field_type) and issubclass(field_type, BaseModel):\n            return True\n    except TypeError:\n        # field_type might not be a class, which is fine\n        pass\n\n    # Handle Enums (they can be serialized)\n    try:\n        if inspect.isclass(field_type) and issubclass(field_type, Enum):\n            return True\n    except TypeError:\n        # field_type might not be a class, which is fine\n        pass\n\n    # For class types, check if they have a serialization method\n    if inspect.isclass(field_type):\n        # Create a dummy instance to test serialization\n        try:\n            instance = object.__new__(field_type)\n            return hasattr(instance, \"to_json\") or hasattr(instance, \"to_dict\") or hasattr(instance, \"__json__\")\n        except Exception:\n            # If we can't create an instance, assume it's not serializable\n            return False\n\n    # If none of the above conditions are met, it's not serializable\n    return False\n</code></pre>"},{"location":"reference/pydantic2django/core/discovery/","title":"pydantic2django.core.discovery","text":""},{"location":"reference/pydantic2django/core/discovery/#pydantic2django.core.discovery.BaseDiscovery","title":"<code>BaseDiscovery</code>","text":"<p>               Bases: <code>ABC</code>, <code>Generic[TModel]</code></p> <p>Abstract base class for discovering models (e.g., Pydantic, Dataclasses).</p> Source code in <code>src/pydantic2django/core/discovery.py</code> <pre><code>class BaseDiscovery(abc.ABC, Generic[TModel]):\n    \"\"\"Abstract base class for discovering models (e.g., Pydantic, Dataclasses).\"\"\"\n\n    def __init__(self):\n        self.all_models: dict[str, type[TModel]] = {}  # All discovered models before any filtering\n        self.filtered_models: dict[str, type[TModel]] = {}  # Models after all filters\n        self.dependencies: dict[type[TModel], set[type[TModel]]] = {}  # Dependencies between filtered models\n\n    @abc.abstractmethod\n    def _is_target_model(self, obj: Any) -&gt; bool:\n        \"\"\"Check if an object is the type of model this discovery class handles.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def _default_eligibility_filter(self, model: type[TModel]) -&gt; bool:\n        \"\"\"\n        Apply default filtering logic inherent to the model type (e.g., exclude abstract classes).\n        Return True if the model is eligible, False otherwise.\n        \"\"\"\n        pass\n\n    def discover_models(\n        self,\n        packages: list[str],\n        app_label: str,  # Keep for potential use in filters or subclasses\n        user_filters: Optional[Union[Callable[[type[TModel]], bool], list[Callable[[type[TModel]], bool]]]] = None,\n    ) -&gt; None:\n        \"\"\"Discover target models in the specified packages, applying default and user filters.\"\"\"\n        self.all_models = {}\n        self.filtered_models = {}\n        self.dependencies = {}\n\n        # Normalize user_filters to always be a list\n        if user_filters is None:\n            filters = []\n        elif isinstance(user_filters, list):\n            filters = user_filters\n        else:  # It's a single callable\n            filters = [user_filters]\n\n        model_type_name = getattr(self, \"__name__\", \"TargetModel\")  # Get class name for logging\n\n        logger.info(f\"Starting {model_type_name} discovery in packages: {packages}\")\n        for package_name in packages:\n            try:\n                package = importlib.import_module(package_name)\n                logger.debug(f\"Scanning package: {package_name}\")\n\n                for _importer, modname, _ispkg in pkgutil.walk_packages(\n                    path=package.__path__ if hasattr(package, \"__path__\") else None,\n                    prefix=package.__name__ + \".\",\n                    onerror=lambda name: logger.warning(f\"Error accessing module {name}\"),\n                ):\n                    try:\n                        module = importlib.import_module(modname)\n                        for name, obj in inspect.getmembers(module):\n                            # Use the subclass implementation to check if it's the right model type\n                            if self._is_target_model(obj):\n                                model_qualname = f\"{modname}.{name}\"\n                                if model_qualname not in self.all_models:\n                                    self.all_models[model_qualname] = obj\n                                    logger.debug(f\"Discovered potential {model_type_name}: {model_qualname}\")\n\n                                    # Apply filters sequentially using subclass implementation\n                                    is_eligible = self._default_eligibility_filter(obj)\n                                    if is_eligible:\n                                        for user_filter in filters:\n                                            try:\n                                                if not user_filter(obj):\n                                                    is_eligible = False\n                                                    logger.debug(\n                                                        f\"Filtered out {model_type_name} by user filter: {model_qualname}\"\n                                                    )\n                                                    break  # No need to check other filters\n                                            except Exception as filter_exc:\n                                                # Attempt to get filter name, default to repr\n                                                filter_name = getattr(user_filter, \"__name__\", repr(user_filter))\n                                                logger.error(\n                                                    f\"Error applying user filter {filter_name} to {model_qualname}: {filter_exc}\",\n                                                    exc_info=True,\n                                                )\n                                                is_eligible = False  # Exclude on filter error\n                                                break\n\n                                    if is_eligible:\n                                        self.filtered_models[model_qualname] = obj\n                                        logger.debug(f\"Added eligible {model_type_name}: {model_qualname}\")\n\n                    except ImportError as e:\n                        logger.warning(f\"Could not import module {modname}: {e}\")\n                    except Exception as e:\n                        logger.error(f\"Error inspecting module {modname} for {model_type_name}s: {e}\", exc_info=True)\n\n            except ImportError:\n                logger.error(f\"Package {package_name} not found.\")\n            except Exception as e:\n                logger.error(f\"Error discovering {model_type_name}s in package {package_name}: {e}\", exc_info=True)\n\n        logger.info(\n            f\"{model_type_name} discovery complete. Found {len(self.all_models)} total models, {len(self.filtered_models)} after filtering.\"\n        )\n\n        # Hooks for subclass-specific post-processing if needed\n        self._post_discovery_hook()\n\n        # Resolve forward references if applicable (might be subclass specific)\n        self._resolve_forward_refs()\n\n        # Build dependency graph for filtered models\n        self.analyze_dependencies()\n\n    @abc.abstractmethod\n    def analyze_dependencies(self) -&gt; None:\n        \"\"\"Analyze dependencies between the filtered models.\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def get_models_in_registration_order(self) -&gt; list[type[TModel]]:\n        \"\"\"Return filtered models sorted topologically based on dependencies.\"\"\"\n        pass\n\n    # Optional hook for subclasses to run code after discovery loop but before analyze\n    def _post_discovery_hook(self) -&gt; None:\n        pass\n\n    # Keep placeholder, subclasses might override if needed\n    def _resolve_forward_refs(self) -&gt; None:\n        \"\"\"Placeholder for resolving forward references if needed.\"\"\"\n        logger.debug(\"Base _resolve_forward_refs called (if applicable).\")\n        pass\n</code></pre>"},{"location":"reference/pydantic2django/core/discovery/#pydantic2django.core.discovery.BaseDiscovery.analyze_dependencies","title":"<code>analyze_dependencies()</code>  <code>abstractmethod</code>","text":"<p>Analyze dependencies between the filtered models.</p> Source code in <code>src/pydantic2django/core/discovery.py</code> <pre><code>@abc.abstractmethod\ndef analyze_dependencies(self) -&gt; None:\n    \"\"\"Analyze dependencies between the filtered models.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/pydantic2django/core/discovery/#pydantic2django.core.discovery.BaseDiscovery.discover_models","title":"<code>discover_models(packages, app_label, user_filters=None)</code>","text":"<p>Discover target models in the specified packages, applying default and user filters.</p> Source code in <code>src/pydantic2django/core/discovery.py</code> <pre><code>def discover_models(\n    self,\n    packages: list[str],\n    app_label: str,  # Keep for potential use in filters or subclasses\n    user_filters: Optional[Union[Callable[[type[TModel]], bool], list[Callable[[type[TModel]], bool]]]] = None,\n) -&gt; None:\n    \"\"\"Discover target models in the specified packages, applying default and user filters.\"\"\"\n    self.all_models = {}\n    self.filtered_models = {}\n    self.dependencies = {}\n\n    # Normalize user_filters to always be a list\n    if user_filters is None:\n        filters = []\n    elif isinstance(user_filters, list):\n        filters = user_filters\n    else:  # It's a single callable\n        filters = [user_filters]\n\n    model_type_name = getattr(self, \"__name__\", \"TargetModel\")  # Get class name for logging\n\n    logger.info(f\"Starting {model_type_name} discovery in packages: {packages}\")\n    for package_name in packages:\n        try:\n            package = importlib.import_module(package_name)\n            logger.debug(f\"Scanning package: {package_name}\")\n\n            for _importer, modname, _ispkg in pkgutil.walk_packages(\n                path=package.__path__ if hasattr(package, \"__path__\") else None,\n                prefix=package.__name__ + \".\",\n                onerror=lambda name: logger.warning(f\"Error accessing module {name}\"),\n            ):\n                try:\n                    module = importlib.import_module(modname)\n                    for name, obj in inspect.getmembers(module):\n                        # Use the subclass implementation to check if it's the right model type\n                        if self._is_target_model(obj):\n                            model_qualname = f\"{modname}.{name}\"\n                            if model_qualname not in self.all_models:\n                                self.all_models[model_qualname] = obj\n                                logger.debug(f\"Discovered potential {model_type_name}: {model_qualname}\")\n\n                                # Apply filters sequentially using subclass implementation\n                                is_eligible = self._default_eligibility_filter(obj)\n                                if is_eligible:\n                                    for user_filter in filters:\n                                        try:\n                                            if not user_filter(obj):\n                                                is_eligible = False\n                                                logger.debug(\n                                                    f\"Filtered out {model_type_name} by user filter: {model_qualname}\"\n                                                )\n                                                break  # No need to check other filters\n                                        except Exception as filter_exc:\n                                            # Attempt to get filter name, default to repr\n                                            filter_name = getattr(user_filter, \"__name__\", repr(user_filter))\n                                            logger.error(\n                                                f\"Error applying user filter {filter_name} to {model_qualname}: {filter_exc}\",\n                                                exc_info=True,\n                                            )\n                                            is_eligible = False  # Exclude on filter error\n                                            break\n\n                                if is_eligible:\n                                    self.filtered_models[model_qualname] = obj\n                                    logger.debug(f\"Added eligible {model_type_name}: {model_qualname}\")\n\n                except ImportError as e:\n                    logger.warning(f\"Could not import module {modname}: {e}\")\n                except Exception as e:\n                    logger.error(f\"Error inspecting module {modname} for {model_type_name}s: {e}\", exc_info=True)\n\n        except ImportError:\n            logger.error(f\"Package {package_name} not found.\")\n        except Exception as e:\n            logger.error(f\"Error discovering {model_type_name}s in package {package_name}: {e}\", exc_info=True)\n\n    logger.info(\n        f\"{model_type_name} discovery complete. Found {len(self.all_models)} total models, {len(self.filtered_models)} after filtering.\"\n    )\n\n    # Hooks for subclass-specific post-processing if needed\n    self._post_discovery_hook()\n\n    # Resolve forward references if applicable (might be subclass specific)\n    self._resolve_forward_refs()\n\n    # Build dependency graph for filtered models\n    self.analyze_dependencies()\n</code></pre>"},{"location":"reference/pydantic2django/core/discovery/#pydantic2django.core.discovery.BaseDiscovery.get_models_in_registration_order","title":"<code>get_models_in_registration_order()</code>  <code>abstractmethod</code>","text":"<p>Return filtered models sorted topologically based on dependencies.</p> Source code in <code>src/pydantic2django/core/discovery.py</code> <pre><code>@abc.abstractmethod\ndef get_models_in_registration_order(self) -&gt; list[type[TModel]]:\n    \"\"\"Return filtered models sorted topologically based on dependencies.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/pydantic2django/core/factories/","title":"pydantic2django.core.factories","text":""},{"location":"reference/pydantic2django/core/factories/#pydantic2django.core.factories.BaseFieldFactory","title":"<code>BaseFieldFactory</code>","text":"<p>               Bases: <code>ABC</code>, <code>Generic[SourceFieldType]</code></p> <p>Abstract base class for field factories.</p> Source code in <code>src/pydantic2django/core/factories.py</code> <pre><code>class BaseFieldFactory(ABC, Generic[SourceFieldType]):\n    \"\"\"Abstract base class for field factories.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        # Allow subclasses to accept necessary dependencies (e.g., relationship accessors)\n        pass\n\n    @abstractmethod\n    def create_field(\n        self, field_info: SourceFieldType, model_name: str, carrier: ConversionCarrier\n    ) -&gt; FieldConversionResult:\n        \"\"\"\n        Convert a source field type into a Django Field.\n\n        Args:\n            field_info: The field information object from the source (Pydantic/Dataclass).\n            model_name: The name of the source model containing the field.\n            carrier: The conversion carrier for context (e.g., app_label, relationships).\n\n        Returns:\n            A FieldConversionResult containing the generated Django field or context/error info.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/pydantic2django/core/factories/#pydantic2django.core.factories.BaseFieldFactory.create_field","title":"<code>create_field(field_info, model_name, carrier)</code>  <code>abstractmethod</code>","text":"<p>Convert a source field type into a Django Field.</p> <p>Parameters:</p> Name Type Description Default <code>field_info</code> <code>SourceFieldType</code> <p>The field information object from the source (Pydantic/Dataclass).</p> required <code>model_name</code> <code>str</code> <p>The name of the source model containing the field.</p> required <code>carrier</code> <code>ConversionCarrier</code> <p>The conversion carrier for context (e.g., app_label, relationships).</p> required <p>Returns:</p> Type Description <code>FieldConversionResult</code> <p>A FieldConversionResult containing the generated Django field or context/error info.</p> Source code in <code>src/pydantic2django/core/factories.py</code> <pre><code>@abstractmethod\ndef create_field(\n    self, field_info: SourceFieldType, model_name: str, carrier: ConversionCarrier\n) -&gt; FieldConversionResult:\n    \"\"\"\n    Convert a source field type into a Django Field.\n\n    Args:\n        field_info: The field information object from the source (Pydantic/Dataclass).\n        model_name: The name of the source model containing the field.\n        carrier: The conversion carrier for context (e.g., app_label, relationships).\n\n    Returns:\n        A FieldConversionResult containing the generated Django field or context/error info.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/pydantic2django/core/factories/#pydantic2django.core.factories.BaseModelFactory","title":"<code>BaseModelFactory</code>","text":"<p>               Bases: <code>ABC</code>, <code>Generic[SourceModelType, SourceFieldType]</code></p> <p>Abstract base class for model factories.</p> Source code in <code>src/pydantic2django/core/factories.py</code> <pre><code>class BaseModelFactory(ABC, Generic[SourceModelType, SourceFieldType]):\n    \"\"\"Abstract base class for model factories.\"\"\"\n\n    field_factory: BaseFieldFactory[SourceFieldType]\n\n    def __init__(self, field_factory: BaseFieldFactory[SourceFieldType], *args, **kwargs):\n        \"\"\"\n        Initializes the model factory with a compatible field factory.\n        Allows subclasses to accept additional dependencies.\n        \"\"\"\n        self.field_factory = field_factory\n\n    @abstractmethod\n    def _process_source_fields(self, carrier: ConversionCarrier[SourceModelType]):\n        \"\"\"Abstract method for subclasses to implement field processing.\"\"\"\n        pass\n\n    # Common logic moved from subclasses\n    def _handle_field_collisions(self, carrier: ConversionCarrier[SourceModelType]):\n        \"\"\"Check for field name collisions with the base Django model.\"\"\"\n        base_model = carrier.base_django_model\n        if not base_model or not hasattr(base_model, \"_meta\"):\n            return\n\n        try:\n            base_fields = base_model._meta.get_fields(include_parents=True, include_hidden=False)\n            base_field_names = {f.name for f in base_fields if not f.name.endswith(\"+\")}\n        except Exception as e:\n            logger.warning(f\"Could not get fields from base model {base_model.__name__} for collision check: {e}\")\n            return\n\n        all_new_fields = set(carrier.django_fields.keys()) | set(carrier.relationship_fields.keys())\n        collision_fields = all_new_fields &amp; base_field_names\n\n        if collision_fields:\n            source_name = getattr(carrier.source_model, \"__name__\", \"?\")\n            msg = f\"Field collision detected between {source_name} and base model {base_model.__name__}: {collision_fields}.\"\n            if carrier.strict:\n                logger.error(msg + \" Raising error due to strict=True.\")\n                raise ValueError(msg + \" Use strict=False or rename fields.\")\n            else:\n                logger.warning(msg + \" Removing colliding fields from generated model (strict=False).\")\n                for field_name in collision_fields:\n                    carrier.django_fields.pop(field_name, None)\n                    carrier.relationship_fields.pop(field_name, None)\n\n    def _create_django_meta(self, carrier: ConversionCarrier[SourceModelType]):\n        \"\"\"Create the Meta class for the generated Django model.\"\"\"\n        source_name = getattr(carrier.source_model, \"__name__\", \"UnknownSourceModel\")\n        source_model_name_cleaned = source_name.replace(\"_\", \" \")\n        meta_attrs = {\n            \"app_label\": carrier.meta_app_label,\n            \"db_table\": f\"{carrier.meta_app_label}_{source_name.lower()}\",\n            # Keep dynamic models abstract so Django does not register them in the\n            # global app registry. This avoids conflicts with concrete generated models\n            # imported later (e.g., during makemigrations in tests).\n            \"abstract\": True,\n            \"managed\": True,\n            \"verbose_name\": source_model_name_cleaned,\n            \"verbose_name_plural\": source_model_name_cleaned + \"s\",\n            \"ordering\": [\"pk\"],\n        }\n\n        # Create Meta, not inheriting from base Meta to ensure abstract stays True\n        logger.debug(\"Creating new Meta class for dynamic model (abstract=True)\")\n        carrier.django_meta_class = type(\"Meta\", (), meta_attrs)\n\n    def _assemble_django_model_class(self, carrier: ConversionCarrier[SourceModelType]):\n        \"\"\"Assemble the final Django model class using type().\"\"\"\n        source_name = getattr(carrier.source_model, \"__name__\", \"UnknownSourceModel\")\n        source_module = getattr(carrier.source_model, \"__module__\", None)\n\n        model_attrs: dict[str, Any] = {\n            **carrier.django_fields,\n            **carrier.relationship_fields,\n            # Set __module__ for where the model appears to live\n            \"__module__\": source_module or f\"{carrier.meta_app_label}.models\",\n        }\n        if carrier.django_meta_class:\n            model_attrs[\"Meta\"] = carrier.django_meta_class\n\n        # Add a reference back to the source model (generic attribute name)\n        model_attrs[\"_pydantic2django_source\"] = carrier.source_model\n\n        bases = (carrier.base_django_model,) if carrier.base_django_model else (models.Model,)\n\n        # Even if no fields were generated (e.g., collisions with base removed them),\n        # we still assemble the model class so that later phases (e.g., relationship finalization)\n        # can inject fields and meta indexes onto the carrier.\n        if not carrier.django_fields and not carrier.relationship_fields:\n            logger.info(\n                f\"No Django fields generated for {source_name}, assembling bare model class to allow later injections.\"\n            )\n\n        model_name = f\"{carrier.class_name_prefix}{source_name}\"\n        logger.debug(f\"Assembling model class '{model_name}' with bases {bases} and attrs: {list(model_attrs.keys())}\")\n\n        try:\n            # Use type() to dynamically create the class\n            carrier.django_model = cast(type[models.Model], type(model_name, bases, model_attrs))\n            logger.info(f\"Successfully assembled Django model class: {model_name}\")\n        except Exception as e:\n            logger.error(f\"Failed to assemble Django model class {model_name}: {e}\", exc_info=True)\n            carrier.invalid_fields.append((\"_assembly\", f\"Failed to create model type: {e}\"))\n            carrier.django_model = None\n\n    @abstractmethod\n    def _build_model_context(self, carrier: ConversionCarrier[SourceModelType]):\n        \"\"\"Abstract method for subclasses to build the specific ModelContext.\"\"\"\n        pass\n\n    # Main orchestration method\n    def make_django_model(self, carrier: ConversionCarrier[SourceModelType]) -&gt; None:\n        \"\"\"\n        Orchestrates the Django model creation process.\n        Subclasses implement _process_source_fields and _build_model_context.\n        Handles caching.\n        \"\"\"\n        model_key = carrier.model_key\n        logger.debug(f\"Attempting to create Django model for {model_key}\")\n\n        # TODO: Cache handling needs refinement - how to access subclass cache?\n        # For now, skipping cache check in base class.\n        # if model_key in self._converted_models and not carrier.existing_model:\n        #     # ... update carrier from cache ...\n        #     return\n\n        # Reset results on carrier\n        carrier.django_fields = {}\n        carrier.relationship_fields = {}\n        carrier.context_fields = {}\n        carrier.invalid_fields = []\n        carrier.django_meta_class = None\n        carrier.django_model = None\n        carrier.model_context = None\n        carrier.django_field_definitions = {}  # Reset definitions dict\n\n        # Core Steps\n        self._process_source_fields(carrier)\n        self._handle_field_collisions(carrier)\n        self._create_django_meta(carrier)\n        self._assemble_django_model_class(carrier)\n\n        # Build context only if model assembly succeeded\n        if carrier.django_model:\n            self._build_model_context(carrier)\n</code></pre>"},{"location":"reference/pydantic2django/core/factories/#pydantic2django.core.factories.BaseModelFactory.__init__","title":"<code>__init__(field_factory, *args, **kwargs)</code>","text":"<p>Initializes the model factory with a compatible field factory. Allows subclasses to accept additional dependencies.</p> Source code in <code>src/pydantic2django/core/factories.py</code> <pre><code>def __init__(self, field_factory: BaseFieldFactory[SourceFieldType], *args, **kwargs):\n    \"\"\"\n    Initializes the model factory with a compatible field factory.\n    Allows subclasses to accept additional dependencies.\n    \"\"\"\n    self.field_factory = field_factory\n</code></pre>"},{"location":"reference/pydantic2django/core/factories/#pydantic2django.core.factories.BaseModelFactory.make_django_model","title":"<code>make_django_model(carrier)</code>","text":"<p>Orchestrates the Django model creation process. Subclasses implement _process_source_fields and _build_model_context. Handles caching.</p> Source code in <code>src/pydantic2django/core/factories.py</code> <pre><code>def make_django_model(self, carrier: ConversionCarrier[SourceModelType]) -&gt; None:\n    \"\"\"\n    Orchestrates the Django model creation process.\n    Subclasses implement _process_source_fields and _build_model_context.\n    Handles caching.\n    \"\"\"\n    model_key = carrier.model_key\n    logger.debug(f\"Attempting to create Django model for {model_key}\")\n\n    # TODO: Cache handling needs refinement - how to access subclass cache?\n    # For now, skipping cache check in base class.\n    # if model_key in self._converted_models and not carrier.existing_model:\n    #     # ... update carrier from cache ...\n    #     return\n\n    # Reset results on carrier\n    carrier.django_fields = {}\n    carrier.relationship_fields = {}\n    carrier.context_fields = {}\n    carrier.invalid_fields = []\n    carrier.django_meta_class = None\n    carrier.django_model = None\n    carrier.model_context = None\n    carrier.django_field_definitions = {}  # Reset definitions dict\n\n    # Core Steps\n    self._process_source_fields(carrier)\n    self._handle_field_collisions(carrier)\n    self._create_django_meta(carrier)\n    self._assemble_django_model_class(carrier)\n\n    # Build context only if model assembly succeeded\n    if carrier.django_model:\n        self._build_model_context(carrier)\n</code></pre>"},{"location":"reference/pydantic2django/core/factories/#pydantic2django.core.factories.ConversionCarrier","title":"<code>ConversionCarrier</code>  <code>dataclass</code>","text":"<p>               Bases: <code>Generic[SourceModelType]</code></p> <p>Carrier class for converting a source model (Pydantic/Dataclass) to a Django model. Holds configuration and accumulates results during the conversion process. Generalized from the original DjangoModelFactoryCarrier.</p> Source code in <code>src/pydantic2django/core/factories.py</code> <pre><code>@dataclass\nclass ConversionCarrier(Generic[SourceModelType]):\n    \"\"\"\n    Carrier class for converting a source model (Pydantic/Dataclass) to a Django model.\n    Holds configuration and accumulates results during the conversion process.\n    Generalized from the original DjangoModelFactoryCarrier.\n    \"\"\"\n\n    source_model: type[SourceModelType]\n    meta_app_label: str\n    base_django_model: type[models.Model]  # Base Django model to inherit from\n    existing_model: Optional[type[models.Model]] = None  # For updating existing models\n    class_name_prefix: str = \"Django\"  # Prefix for generated Django model name\n    strict: bool = False  # Strict mode for field collisions\n    used_related_names_per_target: dict[str, set[str]] = field(default_factory=dict)\n    django_field_definitions: dict[str, str] = field(default_factory=dict)  # Added field defs\n\n    # --- Result fields (populated during conversion) ---\n    django_fields: dict[str, models.Field] = field(default_factory=dict)\n    relationship_fields: dict[str, models.Field] = field(default_factory=dict)\n    context_fields: dict[str, Any] = field(default_factory=dict)  # Store original source field info\n    context_data: dict[str, Any] = field(default_factory=dict)\n    # Stores (original_field_name, union_details_dict) for multi-FK unions\n    pending_multi_fk_unions: list[tuple[str, dict]] = field(default_factory=list)\n    # --- GFK mode support ---\n    enable_gfk: bool = False\n    gfk_policy: str | None = None\n    gfk_threshold_children: int | None = None\n    gfk_value_mode: str | None = None\n    gfk_normalize_common_attrs: bool = False\n    # Mark children that should be represented as GenericEntry rows\n    pending_gfk_children: list[dict[str, Any]] = field(default_factory=list)\n    invalid_fields: list[tuple[str, str]] = field(default_factory=list)\n    django_meta_class: Optional[type] = None\n    django_model: Optional[type[models.Model]] = None  # Changed from DjangoModelType\n    model_context: Optional[ModelContext] = None\n    # Removed import_handler from carrier\n\n    def model_key(self) -&gt; str:\n        \"\"\"Generate a unique key for the source model.\"\"\"\n        module = getattr(self.source_model, \"__module__\", \"?\")\n        name = getattr(self.source_model, \"__name__\", \"UnknownModel\")\n        return f\"{module}.{name}\"\n\n    def __str__(self):\n        source_name = getattr(self.source_model, \"__name__\", \"UnknownSource\")\n        django_name = getattr(self.django_model, \"__name__\", \"None\") if self.django_model else \"None\"\n        return f\"{source_name} -&gt; {django_name}\"\n</code></pre>"},{"location":"reference/pydantic2django/core/factories/#pydantic2django.core.factories.ConversionCarrier.model_key","title":"<code>model_key()</code>","text":"<p>Generate a unique key for the source model.</p> Source code in <code>src/pydantic2django/core/factories.py</code> <pre><code>def model_key(self) -&gt; str:\n    \"\"\"Generate a unique key for the source model.\"\"\"\n    module = getattr(self.source_model, \"__module__\", \"?\")\n    name = getattr(self.source_model, \"__name__\", \"UnknownModel\")\n    return f\"{module}.{name}\"\n</code></pre>"},{"location":"reference/pydantic2django/core/factories/#pydantic2django.core.factories.FieldConversionResult","title":"<code>FieldConversionResult</code>  <code>dataclass</code>","text":"<p>Data structure holding the result of attempting to convert a single source field.</p> Source code in <code>src/pydantic2django/core/factories.py</code> <pre><code>@dataclass\nclass FieldConversionResult:\n    \"\"\"Data structure holding the result of attempting to convert a single source field.\"\"\"\n\n    field_info: Any  # Original source field info (FieldInfo, dataclasses.Field)\n    field_name: str\n    # type_mapping_definition: Optional[TypeMappingDefinition] = None # Keep mapping info internal?\n    field_kwargs: dict[str, Any] = field(default_factory=dict)\n    django_field: Optional[models.Field] = None\n    context_field: Optional[Any] = None  # Holds original field_info if handled by context\n    error_str: Optional[str] = None\n    field_definition_str: Optional[str] = None  # Added field definition string\n    # Added required_imports dictionary\n    required_imports: dict[str, list[str]] = field(default_factory=dict)\n    # Store the raw kwargs returned by the mapper\n    raw_mapper_kwargs: dict[str, Any] = field(default_factory=dict)\n\n    def add_import(self, module: str, name: str):\n        \"\"\"Helper to add an import to this result.\"\"\"\n        if not module or module == \"builtins\":\n            return\n        current_names = self.required_imports.setdefault(module, [])\n        if name not in current_names:\n            current_names.append(name)\n\n    def add_import_for_obj(self, obj: Any):\n        \"\"\"Helper to add an import for a given object (class, function, etc.).\"\"\"\n        if hasattr(obj, \"__module__\") and hasattr(obj, \"__name__\"):\n            module = obj.__module__\n            name = obj.__name__\n            self.add_import(module, name)\n        else:\n            logger.warning(f\"Could not determine import for object: {obj!r}\")\n\n    def __str__(self):\n        status = \"Success\" if self.django_field else (\"Context\" if self.context_field else f\"Error: {self.error_str}\")\n        field_type = type(self.django_field).__name__ if self.django_field else \"N/A\"\n        return f\"FieldConversionResult(field={self.field_name}, status={status}, django_type={field_type})\"\n</code></pre>"},{"location":"reference/pydantic2django/core/factories/#pydantic2django.core.factories.FieldConversionResult.add_import","title":"<code>add_import(module, name)</code>","text":"<p>Helper to add an import to this result.</p> Source code in <code>src/pydantic2django/core/factories.py</code> <pre><code>def add_import(self, module: str, name: str):\n    \"\"\"Helper to add an import to this result.\"\"\"\n    if not module or module == \"builtins\":\n        return\n    current_names = self.required_imports.setdefault(module, [])\n    if name not in current_names:\n        current_names.append(name)\n</code></pre>"},{"location":"reference/pydantic2django/core/factories/#pydantic2django.core.factories.FieldConversionResult.add_import_for_obj","title":"<code>add_import_for_obj(obj)</code>","text":"<p>Helper to add an import for a given object (class, function, etc.).</p> Source code in <code>src/pydantic2django/core/factories.py</code> <pre><code>def add_import_for_obj(self, obj: Any):\n    \"\"\"Helper to add an import for a given object (class, function, etc.).\"\"\"\n    if hasattr(obj, \"__module__\") and hasattr(obj, \"__name__\"):\n        module = obj.__module__\n        name = obj.__name__\n        self.add_import(module, name)\n    else:\n        logger.warning(f\"Could not determine import for object: {obj!r}\")\n</code></pre>"},{"location":"reference/pydantic2django/core/filter_helpers/","title":"pydantic2django.core.filter_helpers","text":""},{"location":"reference/pydantic2django/core/filter_helpers/#pydantic2django.core.filter_helpers.always_include","title":"<code>always_include(_model_name, _model_class)</code>","text":"<p>A stub filter function that always returns True.</p> <p>This can be used as a starting point for creating custom filter functions for the discover_models function.</p> <p>Parameters:</p> Name Type Description Default <code>_model_name</code> <code>str</code> <p>The normalized name of the model</p> required <code>_model_class</code> <code>type[BaseModel]</code> <p>The Pydantic model class</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True, always including the model</p> Source code in <code>src/pydantic2django/core/filter_helpers.py</code> <pre><code>def always_include(_model_name: str, _model_class: type[BaseModel]) -&gt; bool:\n    \"\"\"\n    A stub filter function that always returns True.\n\n    This can be used as a starting point for creating custom filter functions\n    for the discover_models function.\n\n    Args:\n        _model_name: The normalized name of the model\n        _model_class: The Pydantic model class\n\n    Returns:\n        True, always including the model\n    \"\"\"\n    return True\n</code></pre>"},{"location":"reference/pydantic2django/core/filter_helpers/#pydantic2django.core.filter_helpers.exclude_models","title":"<code>exclude_models(model_names)</code>","text":"<p>Create a filter function that excludes models with specific names.</p> <p>Parameters:</p> Name Type Description Default <code>model_names</code> <code>list[str]</code> <p>List of model names to exclude</p> required <p>Returns:</p> Type Description <code>Callable[[str, type[BaseModel]], bool]</code> <p>A filter function that returns False for models in the exclusion list</p> Source code in <code>src/pydantic2django/core/filter_helpers.py</code> <pre><code>def exclude_models(model_names: list[str]) -&gt; Callable[[str, type[BaseModel]], bool]:\n    \"\"\"\n    Create a filter function that excludes models with specific names.\n\n    Args:\n        model_names: List of model names to exclude\n\n    Returns:\n        A filter function that returns False for models in the exclusion list\n    \"\"\"\n    return lambda model_name, _: model_name not in model_names\n</code></pre>"},{"location":"reference/pydantic2django/core/filter_helpers/#pydantic2django.core.filter_helpers.has_field","title":"<code>has_field(field_name)</code>","text":"<p>Create a filter function that includes only models with a specific field.</p> <p>Parameters:</p> Name Type Description Default <code>field_name</code> <code>str</code> <p>The name of the field that models must have</p> required <p>Returns:</p> Type Description <code>Callable[[str, type[BaseModel]], bool]</code> <p>A filter function that returns True only for models with the specified field</p> Source code in <code>src/pydantic2django/core/filter_helpers.py</code> <pre><code>def has_field(field_name: str) -&gt; Callable[[str, type[BaseModel]], bool]:\n    \"\"\"\n    Create a filter function that includes only models with a specific field.\n\n    Args:\n        field_name: The name of the field that models must have\n\n    Returns:\n        A filter function that returns True only for models with the specified field\n    \"\"\"\n    return lambda _, model_class: field_name in model_class.model_fields\n</code></pre>"},{"location":"reference/pydantic2django/core/filter_helpers/#pydantic2django.core.filter_helpers.include_models","title":"<code>include_models(model_names)</code>","text":"<p>Create a filter function that includes only models with specific names.</p> <p>Parameters:</p> Name Type Description Default <code>model_names</code> <code>list[str]</code> <p>List of model names to include</p> required <p>Returns:</p> Type Description <code>Callable[[str, type[BaseModel]], bool]</code> <p>A filter function that returns True only for models in the inclusion list</p> Source code in <code>src/pydantic2django/core/filter_helpers.py</code> <pre><code>def include_models(model_names: list[str]) -&gt; Callable[[str, type[BaseModel]], bool]:\n    \"\"\"\n    Create a filter function that includes only models with specific names.\n\n    Args:\n        model_names: List of model names to include\n\n    Returns:\n        A filter function that returns True only for models in the inclusion list\n    \"\"\"\n    return lambda model_name, _: model_name in model_names\n</code></pre>"},{"location":"reference/pydantic2django/core/imports/","title":"pydantic2django.core.imports","text":""},{"location":"reference/pydantic2django/core/imports/#pydantic2django.core.imports.ImportHandler","title":"<code>ImportHandler</code>","text":"<p>Handles import statements for generated Django models and their context classes. Tracks and deduplicates imports from multiple sources while ensuring all necessary dependencies are included.</p> Source code in <code>src/pydantic2django/core/imports.py</code> <pre><code>class ImportHandler:\n    \"\"\"\n    Handles import statements for generated Django models and their context classes.\n    Tracks and deduplicates imports from multiple sources while ensuring all necessary\n    dependencies are included.\n    \"\"\"\n\n    def __init__(self, module_mappings: Optional[dict[str, str]] = None):\n        \"\"\"\n        Initialize empty collections for different types of imports.\n\n        Args:\n            module_mappings: Optional mapping of modules to remap (e.g. {\"__main__\": \"my_app.models\"})\n        \"\"\"\n        # Track imports by category\n        self.extra_type_imports: set[str] = set()  # For typing and other utility imports\n        self.pydantic_imports: set[str] = set()  # For Pydantic model imports\n        self.context_class_imports: set[str] = set()  # For context class and field type imports\n\n        # For tracking imported names to avoid duplicates\n        self.imported_names: dict[str, str] = {}  # Maps type name to its module\n\n        # For tracking field type dependencies we've already processed\n        self.processed_field_types: set[str] = set()\n\n        # Module mappings to remap imports (e.g. \"__main__\" -&gt; \"my_app.models\")\n        self.module_mappings = module_mappings or {}\n\n        logger.info(\"ImportHandler initialized\")\n        if self.module_mappings:\n            logger.info(f\"Using module mappings: {self.module_mappings}\")\n\n    def add_import(self, module: str, name: str):\n        \"\"\"Adds a single import based on module and name strings.\"\"\"\n        if not module or module == \"builtins\":\n            return\n\n        # Apply module mappings\n        if module in self.module_mappings:\n            module = self.module_mappings[module]\n\n        # Clean name (e.g., remove generics for import statement)\n        clean_name = self._clean_generic_type(name)\n\n        # Check if already imported\n        if name in self.imported_names:\n            # Could verify module matches, but usually name is unique enough\n            logger.debug(f\"Skipping already imported name: {name} (from module {module})\")\n            return\n        if clean_name != name and clean_name in self.imported_names:\n            logger.debug(f\"Skipping already imported clean name: {clean_name} (from module {module})\")\n            return\n\n        # Determine category\n        # Simplistic: If module is known Pydantic, Django, or common stdlib -&gt; context\n        # Otherwise, if it's 'typing' -&gt; extra_type\n        # TODO: Refine categorization if needed (e.g., dedicated django_imports set)\n        import_statement = f\"from {module} import {clean_name}\"\n        if module == \"typing\":\n            self.extra_type_imports.add(clean_name)  # Add only name to typing imports set\n            logger.debug(f\"Adding typing import: {clean_name}\")\n        # elif module.startswith(\"django.\"):\n        # Add to a dedicated django set if we create one\n        #    self.context_class_imports.add(import_statement)\n        #    logger.info(f\"Adding Django import: {import_statement}\")\n        else:\n            # Default to context imports for non-typing\n            self.context_class_imports.add(import_statement)\n            logger.info(f\"Adding context class import: {import_statement}\")\n\n        # Mark as imported\n        self.imported_names[name] = module\n        if clean_name != name:\n            self.imported_names[clean_name] = module\n\n    def add_pydantic_model_import(self, model_class: type) -&gt; None:\n        \"\"\"\n        Add an import statement for a Pydantic model.\n\n        Args:\n            model_class: The Pydantic model class to import\n        \"\"\"\n        if not hasattr(model_class, \"__module__\") or not hasattr(model_class, \"__name__\"):\n            logger.warning(f\"Cannot add import for {model_class}: missing __module__ or __name__\")\n            return\n\n        module_path = model_class.__module__\n        model_name = self._clean_generic_type(model_class.__name__)\n\n        # Apply module mappings if needed\n        if module_path in self.module_mappings:\n            actual_module = self.module_mappings[module_path]\n            logger.debug(f\"Remapping module import: {module_path} -&gt; {actual_module}\")\n            module_path = actual_module\n\n        logger.debug(f\"Processing Pydantic model import: {model_name} from {module_path}\")\n\n        # Skip if already imported\n        if model_name in self.imported_names:\n            logger.debug(f\"Skipping already imported model: {model_name}\")\n            return\n\n        import_statement = f\"from {module_path} import {model_name}\"\n        logger.info(f\"Adding Pydantic import: {import_statement}\")\n        self.pydantic_imports.add(import_statement)\n        self.imported_names[model_name] = module_path\n\n    def add_context_field_type_import(self, field_type: Any) -&gt; None:\n        \"\"\"\n        Add an import statement for a context field type with recursive dependency detection.\n\n        Args:\n            field_type: The field type to import\n        \"\"\"\n        # Skip if we've already processed this field type\n        field_type_str = str(field_type)\n        if field_type_str in self.processed_field_types:\n            logger.debug(f\"Skipping already processed field type: {field_type_str}\")\n            return\n\n        logger.info(f\"Processing context field type: {field_type_str}\")\n        self.processed_field_types.add(field_type_str)\n\n        # Try to add direct import for the field type if it's a class\n        self._add_type_import(field_type)\n\n        # Handle nested types in generics, unions, etc.\n        self._process_nested_types(field_type)\n\n        # Add typing imports based on the field type string\n        self._add_typing_imports(field_type_str)\n\n    def _add_type_import(self, field_type: Any) -&gt; None:\n        \"\"\"\n        Add an import for a single type object if it has module and name attributes.\n\n        Args:\n            field_type: The type to import\n        \"\"\"\n        try:\n            if hasattr(field_type, \"__module__\") and hasattr(field_type, \"__name__\"):\n                type_module = field_type.__module__\n                type_name = field_type.__name__\n\n                # Apply module mappings if needed\n                if type_module in self.module_mappings:\n                    actual_module = self.module_mappings[type_module]\n                    logger.debug(f\"Remapping module import: {type_module} -&gt; {actual_module}\")\n                    type_module = actual_module\n\n                logger.debug(f\"Examining type: {type_name} from module {type_module}\")\n\n                # Skip built-in types and typing module types\n                if (\n                    type_module.startswith(\"typing\")\n                    or type_module == \"builtins\"\n                    or type_name in [\"str\", \"int\", \"float\", \"bool\", \"dict\", \"list\"]\n                ):\n                    logger.debug(f\"Skipping built-in or typing type: {type_name}\")\n                    return\n\n                # Skip TypeVar definitions to avoid conflicts\n                if type_name == \"T\" or type_name == \"TypeVar\":\n                    logger.debug(f\"Skipping TypeVar definition: {type_name} - will be defined locally\")\n                    return\n\n                # Clean up any parametrized generic types for the import statement\n                clean_type_name = self._clean_generic_type(type_name)\n\n                # Use the original type_name (potentially with generics) for the imported_names check\n                if type_name in self.imported_names:\n                    logger.debug(f\"Skipping already imported type: {type_name}\")\n                    return\n\n                # Add to context class imports *before* marking as imported\n                # Use the clean name for the import statement itself\n                import_statement = f\"from {type_module} import {clean_type_name}\"\n                logger.info(f\"Adding context class import: {import_statement}\")\n                self.context_class_imports.add(import_statement)\n\n                # Add the original type name to imported_names to prevent re-processing\n                self.imported_names[type_name] = type_module\n                # Also add the cleaned name in case it's encountered separately\n                if clean_type_name != type_name:\n                    self.imported_names[clean_type_name] = type_module\n\n        except (AttributeError, TypeError) as e:\n            logger.warning(f\"Error processing type import for {field_type}: {e}\")\n\n    def _process_nested_types(self, field_type: Any) -&gt; None:\n        \"\"\"\n        Recursively process nested types in generics, unions, etc.\n\n        Args:\n            field_type: The type that might contain nested types\n        \"\"\"\n        # Handle __args__ for generic types, unions, etc.\n        if hasattr(field_type, \"__args__\"):\n            logger.debug(f\"Processing nested types for {field_type}\")\n            for arg_type in field_type.__args__:\n                logger.debug(f\"Found nested type argument: {arg_type}\")\n                # Recursively process each argument type\n                self.add_context_field_type_import(arg_type)\n\n        # Handle __origin__ for generic types (like List, Dict, etc.)\n        if hasattr(field_type, \"__origin__\"):\n            logger.debug(f\"Processing origin type for {field_type}: {field_type.__origin__}\")\n            self.add_context_field_type_import(field_type.__origin__)\n\n    def _add_typing_imports(self, field_type_str: str) -&gt; None:\n        \"\"\"\n        Add required typing imports based on the string representation of the field type.\n\n        Args:\n            field_type_str: String representation of the field type\n        \"\"\"\n        # Check for common typing constructs\n        if \"List[\" in field_type_str or \"list[\" in field_type_str:\n            logger.debug(f\"Adding List import from {field_type_str}\")\n            self.extra_type_imports.add(\"List\")\n\n        if \"Dict[\" in field_type_str or \"dict[\" in field_type_str:\n            logger.debug(f\"Adding Dict import from {field_type_str}\")\n            self.extra_type_imports.add(\"Dict\")\n\n        if \"Tuple[\" in field_type_str or \"tuple[\" in field_type_str:\n            logger.debug(f\"Adding Tuple import from {field_type_str}\")\n            self.extra_type_imports.add(\"Tuple\")\n\n        if \"Optional[\" in field_type_str or \"Union[\" in field_type_str or \"None\" in field_type_str:\n            logger.debug(f\"Adding Optional import from {field_type_str}\")\n            self.extra_type_imports.add(\"Optional\")\n\n        if \"Union[\" in field_type_str:\n            logger.debug(f\"Adding Union import from {field_type_str}\")\n            self.extra_type_imports.add(\"Union\")\n\n        if \"Callable[\" in field_type_str:\n            logger.debug(f\"Adding Callable import from {field_type_str}\")\n            self.extra_type_imports.add(\"Callable\")\n\n        if \"Any\" in field_type_str:\n            logger.debug(f\"Adding Any import from {field_type_str}\")\n            self.extra_type_imports.add(\"Any\")\n\n        # Extract custom types from the field type string\n        self._extract_custom_types_from_string(field_type_str)\n\n    def _extract_custom_types_from_string(self, field_type_str: str) -&gt; None:\n        \"\"\"\n        Extract custom type names from a string representation of a field type.\n\n        Args:\n            field_type_str: String representation of the field type\n        \"\"\"\n        # Extract potential type names from the string\n        # This regex looks for capitalized words that might be type names\n        type_names = re.findall(r\"[A-Z][a-zA-Z0-9]*\", field_type_str)\n\n        logger.debug(f\"Extracted potential type names from string {field_type_str}: {type_names}\")\n\n        for type_name in type_names:\n            # Skip common type names that are already handled\n            if type_name in [\"List\", \"Dict\", \"Optional\", \"Union\", \"Tuple\", \"Callable\", \"Any\"]:\n                logger.debug(f\"Skipping common typing name: {type_name}\")\n                continue\n\n            # Skip if already in imported names\n            if type_name in self.imported_names:\n                logger.debug(f\"Skipping already imported name: {type_name}\")\n                continue\n\n            # Log potential custom type\n            logger.info(f\"Adding potential custom type to extra_type_imports: {type_name}\")\n\n            # Add to extra type imports - these are types that we couldn't resolve to a module\n            # They'll need to be imported elsewhere or we might generate an error\n            self.extra_type_imports.add(type_name)\n\n    def get_required_imports(self, field_type_str: str) -&gt; dict[str, list[str]]:\n        \"\"\"\n        Get typing and custom type imports required for a field type.\n\n        Args:\n            field_type_str: String representation of a field type\n\n        Returns:\n            Dictionary with \"typing\" and \"custom\" import lists\n        \"\"\"\n        logger.debug(f\"Getting required imports for: {field_type_str}\")\n        self._add_typing_imports(field_type_str)\n\n        # Get custom types (non-typing types)\n        custom_types = [\n            name\n            for name in self.extra_type_imports\n            if name not in [\"List\", \"Dict\", \"Tuple\", \"Set\", \"Optional\", \"Union\", \"Any\", \"Callable\"]\n        ]\n\n        logger.debug(f\"Found custom types: {custom_types}\")\n\n        # Return the latest state of imports\n        return {\n            \"typing\": list(self.extra_type_imports),\n            \"custom\": custom_types,\n        }\n\n    def deduplicate_imports(self) -&gt; dict[str, set[str]]:\n        \"\"\"\n        De-duplicate imports between Pydantic models and context field types.\n\n        Returns:\n            Dict with de-duplicated import sets\n        \"\"\"\n        logger.info(\"Deduplicating imports\")\n        logger.debug(f\"Current pydantic imports: {self.pydantic_imports}\")\n        logger.debug(f\"Current context imports: {self.context_class_imports}\")\n\n        # Extract class names and modules from import statements\n        pydantic_classes = {}\n        context_classes = {}\n\n        # Handle special case for TypeVar imports\n        typevars = set()\n\n        for import_stmt in self.pydantic_imports:\n            if import_stmt.startswith(\"from \") and \" import \" in import_stmt:\n                module, classes = import_stmt.split(\" import \")\n                module = module.replace(\"from \", \"\")\n\n                # Skip __main__ and rewrite to real module paths if possible\n                if module == \"__main__\":\n                    logger.warning(f\"Skipping __main__ import: {import_stmt} - these won't work when imported\")\n                    continue\n\n                for cls in classes.split(\", \"):\n                    # Check if it's a TypeVar to handle duplicate definitions\n                    if cls == \"T\" or cls == \"TypeVar\":\n                        typevars.add(cls)\n                        continue\n\n                    # Clean up any parameterized generic types in class names\n                    cls = self._clean_generic_type(cls)\n                    pydantic_classes[cls] = module\n\n        for import_stmt in self.context_class_imports:\n            if import_stmt.startswith(\"from \") and \" import \" in import_stmt:\n                module, classes = import_stmt.split(\" import \")\n                module = module.replace(\"from \", \"\")\n\n                # Skip __main__ imports or rewrite to real module paths if possible\n                if module == \"__main__\":\n                    logger.warning(f\"Skipping __main__ import: {import_stmt} - these won't work when imported\")\n                    continue\n\n                for cls in classes.split(\", \"):\n                    # Check if it's a TypeVar to handle duplicate definitions\n                    if cls == \"T\" or cls == \"TypeVar\":\n                        typevars.add(cls)\n                        continue\n\n                    # Clean up any parameterized generic types in class names\n                    cls = self._clean_generic_type(cls)\n                    # If this class is already imported in pydantic imports, skip it\n                    if cls in pydantic_classes:\n                        logger.debug(f\"Skipping duplicate context import for {cls}, already in pydantic imports\")\n                        continue\n                    context_classes[cls] = module\n\n        # Rebuild import statements\n        module_to_classes = {}\n        for cls, module in pydantic_classes.items():\n            if module not in module_to_classes:\n                module_to_classes[module] = []\n            module_to_classes[module].append(cls)\n\n        deduplicated_pydantic_imports = set()\n        for module, classes in module_to_classes.items():\n            deduplicated_pydantic_imports.add(f\"from {module} import {', '.join(sorted(classes))}\")\n\n        # Same for context imports\n        module_to_classes = {}\n        for cls, module in context_classes.items():\n            if module not in module_to_classes:\n                module_to_classes[module] = []\n            module_to_classes[module].append(cls)\n\n        deduplicated_context_imports = set()\n        for module, classes in module_to_classes.items():\n            deduplicated_context_imports.add(f\"from {module} import {', '.join(sorted(classes))}\")\n\n        logger.info(f\"Final pydantic imports: {deduplicated_pydantic_imports}\")\n        logger.info(f\"Final context imports: {deduplicated_context_imports}\")\n\n        # Log any TypeVar names we're skipping\n        if typevars:\n            logger.info(f\"Skipping TypeVar imports: {typevars} - these will be defined locally\")\n\n        return {\"pydantic\": deduplicated_pydantic_imports, \"context\": deduplicated_context_imports}\n\n    def _clean_generic_type(self, name: str) -&gt; str:\n        \"\"\"\n        Clean generic parameters from a type name.\n\n        Args:\n            name: The type name to clean\n\n        Returns:\n            The cleaned type name without generic parameters\n        \"\"\"\n        if \"[\" in name or \"&lt;\" in name:\n            cleaned = re.sub(r\"\\[.*\\]\", \"\", name)\n            logger.debug(f\"Cleaned generic type {name} to {cleaned}\")\n            return cleaned\n        return name\n</code></pre>"},{"location":"reference/pydantic2django/core/imports/#pydantic2django.core.imports.ImportHandler.__init__","title":"<code>__init__(module_mappings=None)</code>","text":"<p>Initialize empty collections for different types of imports.</p> <p>Parameters:</p> Name Type Description Default <code>module_mappings</code> <code>Optional[dict[str, str]]</code> <p>Optional mapping of modules to remap (e.g. {\"main\": \"my_app.models\"})</p> <code>None</code> Source code in <code>src/pydantic2django/core/imports.py</code> <pre><code>def __init__(self, module_mappings: Optional[dict[str, str]] = None):\n    \"\"\"\n    Initialize empty collections for different types of imports.\n\n    Args:\n        module_mappings: Optional mapping of modules to remap (e.g. {\"__main__\": \"my_app.models\"})\n    \"\"\"\n    # Track imports by category\n    self.extra_type_imports: set[str] = set()  # For typing and other utility imports\n    self.pydantic_imports: set[str] = set()  # For Pydantic model imports\n    self.context_class_imports: set[str] = set()  # For context class and field type imports\n\n    # For tracking imported names to avoid duplicates\n    self.imported_names: dict[str, str] = {}  # Maps type name to its module\n\n    # For tracking field type dependencies we've already processed\n    self.processed_field_types: set[str] = set()\n\n    # Module mappings to remap imports (e.g. \"__main__\" -&gt; \"my_app.models\")\n    self.module_mappings = module_mappings or {}\n\n    logger.info(\"ImportHandler initialized\")\n    if self.module_mappings:\n        logger.info(f\"Using module mappings: {self.module_mappings}\")\n</code></pre>"},{"location":"reference/pydantic2django/core/imports/#pydantic2django.core.imports.ImportHandler.add_context_field_type_import","title":"<code>add_context_field_type_import(field_type)</code>","text":"<p>Add an import statement for a context field type with recursive dependency detection.</p> <p>Parameters:</p> Name Type Description Default <code>field_type</code> <code>Any</code> <p>The field type to import</p> required Source code in <code>src/pydantic2django/core/imports.py</code> <pre><code>def add_context_field_type_import(self, field_type: Any) -&gt; None:\n    \"\"\"\n    Add an import statement for a context field type with recursive dependency detection.\n\n    Args:\n        field_type: The field type to import\n    \"\"\"\n    # Skip if we've already processed this field type\n    field_type_str = str(field_type)\n    if field_type_str in self.processed_field_types:\n        logger.debug(f\"Skipping already processed field type: {field_type_str}\")\n        return\n\n    logger.info(f\"Processing context field type: {field_type_str}\")\n    self.processed_field_types.add(field_type_str)\n\n    # Try to add direct import for the field type if it's a class\n    self._add_type_import(field_type)\n\n    # Handle nested types in generics, unions, etc.\n    self._process_nested_types(field_type)\n\n    # Add typing imports based on the field type string\n    self._add_typing_imports(field_type_str)\n</code></pre>"},{"location":"reference/pydantic2django/core/imports/#pydantic2django.core.imports.ImportHandler.add_import","title":"<code>add_import(module, name)</code>","text":"<p>Adds a single import based on module and name strings.</p> Source code in <code>src/pydantic2django/core/imports.py</code> <pre><code>def add_import(self, module: str, name: str):\n    \"\"\"Adds a single import based on module and name strings.\"\"\"\n    if not module or module == \"builtins\":\n        return\n\n    # Apply module mappings\n    if module in self.module_mappings:\n        module = self.module_mappings[module]\n\n    # Clean name (e.g., remove generics for import statement)\n    clean_name = self._clean_generic_type(name)\n\n    # Check if already imported\n    if name in self.imported_names:\n        # Could verify module matches, but usually name is unique enough\n        logger.debug(f\"Skipping already imported name: {name} (from module {module})\")\n        return\n    if clean_name != name and clean_name in self.imported_names:\n        logger.debug(f\"Skipping already imported clean name: {clean_name} (from module {module})\")\n        return\n\n    # Determine category\n    # Simplistic: If module is known Pydantic, Django, or common stdlib -&gt; context\n    # Otherwise, if it's 'typing' -&gt; extra_type\n    # TODO: Refine categorization if needed (e.g., dedicated django_imports set)\n    import_statement = f\"from {module} import {clean_name}\"\n    if module == \"typing\":\n        self.extra_type_imports.add(clean_name)  # Add only name to typing imports set\n        logger.debug(f\"Adding typing import: {clean_name}\")\n    # elif module.startswith(\"django.\"):\n    # Add to a dedicated django set if we create one\n    #    self.context_class_imports.add(import_statement)\n    #    logger.info(f\"Adding Django import: {import_statement}\")\n    else:\n        # Default to context imports for non-typing\n        self.context_class_imports.add(import_statement)\n        logger.info(f\"Adding context class import: {import_statement}\")\n\n    # Mark as imported\n    self.imported_names[name] = module\n    if clean_name != name:\n        self.imported_names[clean_name] = module\n</code></pre>"},{"location":"reference/pydantic2django/core/imports/#pydantic2django.core.imports.ImportHandler.add_pydantic_model_import","title":"<code>add_pydantic_model_import(model_class)</code>","text":"<p>Add an import statement for a Pydantic model.</p> <p>Parameters:</p> Name Type Description Default <code>model_class</code> <code>type</code> <p>The Pydantic model class to import</p> required Source code in <code>src/pydantic2django/core/imports.py</code> <pre><code>def add_pydantic_model_import(self, model_class: type) -&gt; None:\n    \"\"\"\n    Add an import statement for a Pydantic model.\n\n    Args:\n        model_class: The Pydantic model class to import\n    \"\"\"\n    if not hasattr(model_class, \"__module__\") or not hasattr(model_class, \"__name__\"):\n        logger.warning(f\"Cannot add import for {model_class}: missing __module__ or __name__\")\n        return\n\n    module_path = model_class.__module__\n    model_name = self._clean_generic_type(model_class.__name__)\n\n    # Apply module mappings if needed\n    if module_path in self.module_mappings:\n        actual_module = self.module_mappings[module_path]\n        logger.debug(f\"Remapping module import: {module_path} -&gt; {actual_module}\")\n        module_path = actual_module\n\n    logger.debug(f\"Processing Pydantic model import: {model_name} from {module_path}\")\n\n    # Skip if already imported\n    if model_name in self.imported_names:\n        logger.debug(f\"Skipping already imported model: {model_name}\")\n        return\n\n    import_statement = f\"from {module_path} import {model_name}\"\n    logger.info(f\"Adding Pydantic import: {import_statement}\")\n    self.pydantic_imports.add(import_statement)\n    self.imported_names[model_name] = module_path\n</code></pre>"},{"location":"reference/pydantic2django/core/imports/#pydantic2django.core.imports.ImportHandler.deduplicate_imports","title":"<code>deduplicate_imports()</code>","text":"<p>De-duplicate imports between Pydantic models and context field types.</p> <p>Returns:</p> Type Description <code>dict[str, set[str]]</code> <p>Dict with de-duplicated import sets</p> Source code in <code>src/pydantic2django/core/imports.py</code> <pre><code>def deduplicate_imports(self) -&gt; dict[str, set[str]]:\n    \"\"\"\n    De-duplicate imports between Pydantic models and context field types.\n\n    Returns:\n        Dict with de-duplicated import sets\n    \"\"\"\n    logger.info(\"Deduplicating imports\")\n    logger.debug(f\"Current pydantic imports: {self.pydantic_imports}\")\n    logger.debug(f\"Current context imports: {self.context_class_imports}\")\n\n    # Extract class names and modules from import statements\n    pydantic_classes = {}\n    context_classes = {}\n\n    # Handle special case for TypeVar imports\n    typevars = set()\n\n    for import_stmt in self.pydantic_imports:\n        if import_stmt.startswith(\"from \") and \" import \" in import_stmt:\n            module, classes = import_stmt.split(\" import \")\n            module = module.replace(\"from \", \"\")\n\n            # Skip __main__ and rewrite to real module paths if possible\n            if module == \"__main__\":\n                logger.warning(f\"Skipping __main__ import: {import_stmt} - these won't work when imported\")\n                continue\n\n            for cls in classes.split(\", \"):\n                # Check if it's a TypeVar to handle duplicate definitions\n                if cls == \"T\" or cls == \"TypeVar\":\n                    typevars.add(cls)\n                    continue\n\n                # Clean up any parameterized generic types in class names\n                cls = self._clean_generic_type(cls)\n                pydantic_classes[cls] = module\n\n    for import_stmt in self.context_class_imports:\n        if import_stmt.startswith(\"from \") and \" import \" in import_stmt:\n            module, classes = import_stmt.split(\" import \")\n            module = module.replace(\"from \", \"\")\n\n            # Skip __main__ imports or rewrite to real module paths if possible\n            if module == \"__main__\":\n                logger.warning(f\"Skipping __main__ import: {import_stmt} - these won't work when imported\")\n                continue\n\n            for cls in classes.split(\", \"):\n                # Check if it's a TypeVar to handle duplicate definitions\n                if cls == \"T\" or cls == \"TypeVar\":\n                    typevars.add(cls)\n                    continue\n\n                # Clean up any parameterized generic types in class names\n                cls = self._clean_generic_type(cls)\n                # If this class is already imported in pydantic imports, skip it\n                if cls in pydantic_classes:\n                    logger.debug(f\"Skipping duplicate context import for {cls}, already in pydantic imports\")\n                    continue\n                context_classes[cls] = module\n\n    # Rebuild import statements\n    module_to_classes = {}\n    for cls, module in pydantic_classes.items():\n        if module not in module_to_classes:\n            module_to_classes[module] = []\n        module_to_classes[module].append(cls)\n\n    deduplicated_pydantic_imports = set()\n    for module, classes in module_to_classes.items():\n        deduplicated_pydantic_imports.add(f\"from {module} import {', '.join(sorted(classes))}\")\n\n    # Same for context imports\n    module_to_classes = {}\n    for cls, module in context_classes.items():\n        if module not in module_to_classes:\n            module_to_classes[module] = []\n        module_to_classes[module].append(cls)\n\n    deduplicated_context_imports = set()\n    for module, classes in module_to_classes.items():\n        deduplicated_context_imports.add(f\"from {module} import {', '.join(sorted(classes))}\")\n\n    logger.info(f\"Final pydantic imports: {deduplicated_pydantic_imports}\")\n    logger.info(f\"Final context imports: {deduplicated_context_imports}\")\n\n    # Log any TypeVar names we're skipping\n    if typevars:\n        logger.info(f\"Skipping TypeVar imports: {typevars} - these will be defined locally\")\n\n    return {\"pydantic\": deduplicated_pydantic_imports, \"context\": deduplicated_context_imports}\n</code></pre>"},{"location":"reference/pydantic2django/core/imports/#pydantic2django.core.imports.ImportHandler.get_required_imports","title":"<code>get_required_imports(field_type_str)</code>","text":"<p>Get typing and custom type imports required for a field type.</p> <p>Parameters:</p> Name Type Description Default <code>field_type_str</code> <code>str</code> <p>String representation of a field type</p> required <p>Returns:</p> Type Description <code>dict[str, list[str]]</code> <p>Dictionary with \"typing\" and \"custom\" import lists</p> Source code in <code>src/pydantic2django/core/imports.py</code> <pre><code>def get_required_imports(self, field_type_str: str) -&gt; dict[str, list[str]]:\n    \"\"\"\n    Get typing and custom type imports required for a field type.\n\n    Args:\n        field_type_str: String representation of a field type\n\n    Returns:\n        Dictionary with \"typing\" and \"custom\" import lists\n    \"\"\"\n    logger.debug(f\"Getting required imports for: {field_type_str}\")\n    self._add_typing_imports(field_type_str)\n\n    # Get custom types (non-typing types)\n    custom_types = [\n        name\n        for name in self.extra_type_imports\n        if name not in [\"List\", \"Dict\", \"Tuple\", \"Set\", \"Optional\", \"Union\", \"Any\", \"Callable\"]\n    ]\n\n    logger.debug(f\"Found custom types: {custom_types}\")\n\n    # Return the latest state of imports\n    return {\n        \"typing\": list(self.extra_type_imports),\n        \"custom\": custom_types,\n    }\n</code></pre>"},{"location":"reference/pydantic2django/core/mapping_units/","title":"pydantic2django.core.mapping_units","text":"<p>Defines the individual mapping units used by the BidirectionalTypeMapper.</p> <p>Each class maps a specific Python/Pydantic type to a Django Field type and handles the conversion of relevant attributes between FieldInfo and Django kwargs.</p>"},{"location":"reference/pydantic2django/core/mapping_units/#pydantic2django.core.mapping_units.EmailFieldMapping","title":"<code>EmailFieldMapping</code>","text":"<p>               Bases: <code>StrFieldMapping</code></p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>class EmailFieldMapping(StrFieldMapping):\n    python_type = EmailStr\n    django_field_type = models.EmailField\n\n    @classmethod\n    def matches(cls, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; float:\n        \"\"\"Strongly prefer EmailStr.\"\"\"\n        if py_type == cls.python_type:\n            return 1.2\n        return 0.0\n\n    def pydantic_to_django_kwargs(self, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; dict[str, Any]:\n        kwargs = super().pydantic_to_django_kwargs(py_type, field_info)\n        # Ensure default max_length for EmailField if not specified via metadata\n        pyd_max_length = None\n        if field_info and hasattr(field_info, \"metadata\"):\n            for item in field_info.metadata:\n                if hasattr(item, \"max_length\"):\n                    pyd_max_length = item.max_length\n                    break\n        if pyd_max_length is None and \"max_length\" not in kwargs:\n            # Django's default max_length for EmailField\n            kwargs[\"max_length\"] = 254\n        return kwargs\n</code></pre>"},{"location":"reference/pydantic2django/core/mapping_units/#pydantic2django.core.mapping_units.EmailFieldMapping.matches","title":"<code>matches(py_type, field_info=None)</code>  <code>classmethod</code>","text":"<p>Strongly prefer EmailStr.</p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>@classmethod\ndef matches(cls, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; float:\n    \"\"\"Strongly prefer EmailStr.\"\"\"\n    if py_type == cls.python_type:\n        return 1.2\n    return 0.0\n</code></pre>"},{"location":"reference/pydantic2django/core/mapping_units/#pydantic2django.core.mapping_units.EnumFieldMapping","title":"<code>EnumFieldMapping</code>","text":"<p>               Bases: <code>TypeMappingUnit</code></p> <p>Handles fields with choices, mapping to Literal or Enum based on context. Django Field: Any field with <code>choices</code> set. Pydantic Type: Literal[...] or Enum.</p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>class EnumFieldMapping(TypeMappingUnit):\n    \"\"\"Handles fields with choices, mapping to Literal or Enum based on context.\n    Django Field: Any field with `choices` set.\n    Pydantic Type: Literal[...] or Enum.\n    \"\"\"\n\n    # Use a generic Field as the base, as choices can be on various types\n    django_field_type = models.Field\n    # Base python type depends on the underlying field type (str, int, etc.)\n    # This will be overridden dynamically based on the actual field.\n    python_type = Any\n\n    @classmethod\n    def matches(cls, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; float:\n        # Check if it's a Literal type\n        origin = get_origin(py_type)\n        if origin is Literal:\n            return 2.0  # High score for direct Literal match\n        # Check if it's an Enum\n        if inspect.isclass(py_type) and issubclass(py_type, Enum):\n            return 1.9  # Slightly lower than Literal, but high\n        return 0.0  # Don't match other types directly\n\n    def django_to_pydantic_field_info_kwargs(self, dj_field: models.Field) -&gt; dict[str, Any]:\n        \"\"\"Generate FieldInfo kwargs, adding choices to json_schema_extra.\"\"\"\n        # Start with base kwargs (title, description, default handling)\n        kwargs = super().django_to_pydantic_field_info_kwargs(dj_field)\n\n        # EnumFieldMapping specific logic (if any) could go here\n        # For now, just rely on the base method\n        logger.debug(f\"EnumFieldMapping using base kwargs for '{dj_field.name}': {kwargs}\")\n        return kwargs\n\n    def pydantic_to_django_kwargs(self, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; dict[str, Any]:\n        kwargs = super().pydantic_to_django_kwargs(py_type, field_info)\n        origin_type = get_origin(py_type)\n        values = []\n        field_type_hint = models.CharField  # Default hint\n\n        if origin_type is Literal:\n            literal_args = get_args(py_type)\n            if not literal_args:\n                logger.warning(\"Literal type has no arguments.\")\n                return kwargs\n            values = [arg for arg in literal_args]\n            kwargs[\"choices\"] = [(str(v), str(v)) for v in values]\n        elif py_type and inspect.isclass(py_type) and issubclass(py_type, Enum):\n            members = list(py_type)\n            values = [member.value for member in members]\n            kwargs[\"choices\"] = [(member.value, member.name) for member in members]\n        else:\n            logger.warning(\"Enum mapping used but type is not an Enum or Literal?\")\n            return kwargs  # Return base kwargs if type is somehow wrong\n\n        if not values:\n            logger.warning(f\"No values found for Enum/Literal type: {py_type}\")\n            return kwargs\n\n        # Determine field type and calculate max_length if needed\n        if all(isinstance(val, int) for val in values):\n            field_type_hint = models.IntegerField\n            kwargs.pop(\"max_length\", None)  # No max_length for IntegerField\n            logger.debug(f\"Enum/Literal {py_type} maps to IntegerField.\")\n        else:\n            field_type_hint = models.CharField\n            try:\n                max_length = max(len(str(val)) for val in values)\n            except TypeError:\n                # Handle cases where values might be mixed types without good str representation?\n                logger.warning(\n                    f\"Could not determine max_length for Enum/Literal {py_type} values: {values}. Defaulting to 255.\"\n                )\n                max_length = 255\n            kwargs[\"max_length\"] = max_length\n            logger.debug(f\"Enum/Literal {py_type} maps to CharField with max_length={max_length}.\")\n\n        # Add the hint to the kwargs to be used by the caller\n        kwargs[\"_field_type_hint\"] = field_type_hint\n\n        return kwargs\n</code></pre>"},{"location":"reference/pydantic2django/core/mapping_units/#pydantic2django.core.mapping_units.EnumFieldMapping.django_to_pydantic_field_info_kwargs","title":"<code>django_to_pydantic_field_info_kwargs(dj_field)</code>","text":"<p>Generate FieldInfo kwargs, adding choices to json_schema_extra.</p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>def django_to_pydantic_field_info_kwargs(self, dj_field: models.Field) -&gt; dict[str, Any]:\n    \"\"\"Generate FieldInfo kwargs, adding choices to json_schema_extra.\"\"\"\n    # Start with base kwargs (title, description, default handling)\n    kwargs = super().django_to_pydantic_field_info_kwargs(dj_field)\n\n    # EnumFieldMapping specific logic (if any) could go here\n    # For now, just rely on the base method\n    logger.debug(f\"EnumFieldMapping using base kwargs for '{dj_field.name}': {kwargs}\")\n    return kwargs\n</code></pre>"},{"location":"reference/pydantic2django/core/mapping_units/#pydantic2django.core.mapping_units.FileFieldMapping","title":"<code>FileFieldMapping</code>","text":"<p>               Bases: <code>StrFieldMapping</code></p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>class FileFieldMapping(StrFieldMapping):\n    # Map File/Image fields to str (URL/path) by default in Pydantic\n    # Optionality should be determined by the field's null status, not the base mapping.\n    python_type = str\n    django_field_type = models.FileField\n\n    @classmethod\n    def matches(cls, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; float:\n        \"\"\"Prefer str if FieldInfo hints suggest a generic file/binary.\"\"\"\n        if py_type == str:\n            # Check for format hint in json_schema_extra\n            has_file_hint = False\n            schema_extra = getattr(field_info, \"json_schema_extra\", None)\n            if isinstance(schema_extra, dict):\n                format_hint = schema_extra.get(\"format\")\n                # Prioritize if format is explicitly binary or file-like\n                if format_hint in (\"binary\", \"byte\", \"file\"):  # Add other relevant hints?\n                    has_file_hint = True\n\n            if has_file_hint:\n                return 1.15  # Higher than StrField/TextField/Slug\n            else:\n                # If str but no file hint, this unit should not match strongly.\n                # It shouldn't override StrFieldMapping if max_length is present.\n                # Return 0.0 to prevent interference.\n                return 0.0\n                # return StrFieldMapping.matches(py_type, field_info) # Incorrect, causes tie\n\n        # Don't match non-str types\n        return 0.0\n\n    def pydantic_to_django_kwargs(self, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; dict[str, Any]:\n        kwargs = super().pydantic_to_django_kwargs(py_type, field_info)\n        kwargs.pop(\"max_length\", None)  # File paths/URLs don't use Pydantic max_length\n        return kwargs\n\n    def django_to_pydantic_field_info_kwargs(self, dj_field: models.Field) -&gt; dict[str, Any]:\n        # Start with base kwargs (which includes StrFieldMapping -&gt; TypeMappingUnit)\n        kwargs = super().django_to_pydantic_field_info_kwargs(dj_field)\n        # Add OpenAPI format for file uploads\n        kwargs.setdefault(\"json_schema_extra\", {})[\"format\"] = \"binary\"\n        # File/Image fields typically map to str (URL/path), max_length isn't relevant\n        kwargs.pop(\"max_length\", None)\n        logger.debug(f\"FileFieldMapping kwargs for '{getattr(dj_field, 'name', 'unknown')}': {kwargs}\")\n        return kwargs\n</code></pre>"},{"location":"reference/pydantic2django/core/mapping_units/#pydantic2django.core.mapping_units.FileFieldMapping.matches","title":"<code>matches(py_type, field_info=None)</code>  <code>classmethod</code>","text":"<p>Prefer str if FieldInfo hints suggest a generic file/binary.</p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>@classmethod\ndef matches(cls, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; float:\n    \"\"\"Prefer str if FieldInfo hints suggest a generic file/binary.\"\"\"\n    if py_type == str:\n        # Check for format hint in json_schema_extra\n        has_file_hint = False\n        schema_extra = getattr(field_info, \"json_schema_extra\", None)\n        if isinstance(schema_extra, dict):\n            format_hint = schema_extra.get(\"format\")\n            # Prioritize if format is explicitly binary or file-like\n            if format_hint in (\"binary\", \"byte\", \"file\"):  # Add other relevant hints?\n                has_file_hint = True\n\n        if has_file_hint:\n            return 1.15  # Higher than StrField/TextField/Slug\n        else:\n            # If str but no file hint, this unit should not match strongly.\n            # It shouldn't override StrFieldMapping if max_length is present.\n            # Return 0.0 to prevent interference.\n            return 0.0\n            # return StrFieldMapping.matches(py_type, field_info) # Incorrect, causes tie\n\n    # Don't match non-str types\n    return 0.0\n</code></pre>"},{"location":"reference/pydantic2django/core/mapping_units/#pydantic2django.core.mapping_units.FilePathFieldMapping","title":"<code>FilePathFieldMapping</code>","text":"<p>               Bases: <code>StrFieldMapping</code></p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>class FilePathFieldMapping(StrFieldMapping):\n    python_type = Path\n    django_field_type = models.FilePathField\n\n    @classmethod\n    def matches(cls, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; float:\n        \"\"\"Strongly prefer Path.\"\"\"\n        if py_type == cls.python_type:\n            return 1.2  # Very high score for exact type\n        # Don't match plain str\n        return 0.0\n        # return super().matches(py_type, field_info) # Default base matching - Incorrect\n\n    # Add default max_length for FilePathField\n    def pydantic_to_django_kwargs(self, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; dict[str, Any]:\n        kwargs = super().pydantic_to_django_kwargs(py_type, field_info)\n        if \"max_length\" not in kwargs:\n            kwargs[\"max_length\"] = 100  # Default for FilePathField\n        return kwargs\n\n    def django_to_pydantic_field_info_kwargs(self, dj_field: models.Field) -&gt; dict[str, Any]:\n        # FilePathField specific logic if any, else default\n        return super().django_to_pydantic_field_info_kwargs(dj_field)\n</code></pre>"},{"location":"reference/pydantic2django/core/mapping_units/#pydantic2django.core.mapping_units.FilePathFieldMapping.matches","title":"<code>matches(py_type, field_info=None)</code>  <code>classmethod</code>","text":"<p>Strongly prefer Path.</p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>@classmethod\ndef matches(cls, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; float:\n    \"\"\"Strongly prefer Path.\"\"\"\n    if py_type == cls.python_type:\n        return 1.2  # Very high score for exact type\n    # Don't match plain str\n    return 0.0\n</code></pre>"},{"location":"reference/pydantic2django/core/mapping_units/#pydantic2django.core.mapping_units.GenericForeignKeyMappingUnit","title":"<code>GenericForeignKeyMappingUnit</code>","text":"<p>               Bases: <code>TypeMappingUnit</code></p> <p>Represents a Generic Foreign Key relationship concept.</p> <p>This unit doesn't map directly to a single Django field type on the parent model or a single Pydantic type in the traditional sense. It's identified structurally (e.g., List[Union[ModelA, ModelB]]) by the BidirectionalTypeMapper, which signals the generator to create the necessary intermediate model and GFK fields (ContentType FK, object_id, parent FK).</p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>class GenericForeignKeyMappingUnit(TypeMappingUnit):\n    \"\"\"Represents a Generic Foreign Key relationship concept.\n\n    This unit doesn't map directly to a single Django field type on the parent\n    model or a single Pydantic type in the traditional sense. It's identified\n    structurally (e.g., List[Union[ModelA, ModelB]]) by the BidirectionalTypeMapper,\n    which signals the generator to create the necessary intermediate model and GFK fields\n    (ContentType FK, object_id, parent FK).\n    \"\"\"\n\n    # Use placeholders that are unlikely to be matched directly\n    python_type = list[Any]  # Conceptual representation (List[Union[...]])\n    django_field_type = models.Field  # Placeholder, not directly used\n\n    @classmethod\n    def matches(cls, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; float:\n        \"\"\"GFK relationships are typically detected structurally, not via simple type matching.\n        This unit should generally not be selected via scoring.\n        \"\"\"\n        return 0.0  # Do not participate in standard scoring selection\n\n    def pydantic_to_django_kwargs(self, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; dict[str, Any]:\n        \"\"\"Returns base kwargs. GFK details are signaled separately.\n        Actual GFK fields (ContentType, object_id) are handled by the generator.\n        \"\"\"\n        # Return base attributes like verbose_name/help_text if applicable\n        kwargs = super().pydantic_to_django_kwargs(py_type, field_info)\n        # Remove attributes not relevant to the GFK *placeholder* on the parent\n        kwargs.pop(\"default\", None)\n        kwargs.pop(\"max_length\", None)\n        kwargs.pop(\"choices\", None)\n        return kwargs\n\n    def django_to_pydantic_field_info_kwargs(self, dj_field: models.Field) -&gt; dict[str, Any]:\n        \"\"\"Mapping *from* a GFK conceptual field back to Pydantic is complex.\n        Typically results in a List[Union[...]]. Return base kwargs.\n        \"\"\"\n        kwargs = super().django_to_pydantic_field_info_kwargs(dj_field)\n        # Represent as a list, likely defaulting to an empty list\n        kwargs[\"default_factory\"] = list\n        # Remove attributes not relevant to the Pydantic List[Union[...]] representation\n        kwargs.pop(\"max_length\", None)\n        kwargs.pop(\"max_digits\", None)\n        kwargs.pop(\"decimal_places\", None)\n        kwargs.pop(\"ge\", None)\n        kwargs.pop(\"le\", None)\n        # Choices might be relevant if the Union members have constraints, but complex to map here.\n        # Keep title/description if they exist.\n        return kwargs\n</code></pre>"},{"location":"reference/pydantic2django/core/mapping_units/#pydantic2django.core.mapping_units.GenericForeignKeyMappingUnit.django_to_pydantic_field_info_kwargs","title":"<code>django_to_pydantic_field_info_kwargs(dj_field)</code>","text":"<p>Mapping from a GFK conceptual field back to Pydantic is complex. Typically results in a List[Union[...]]. Return base kwargs.</p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>def django_to_pydantic_field_info_kwargs(self, dj_field: models.Field) -&gt; dict[str, Any]:\n    \"\"\"Mapping *from* a GFK conceptual field back to Pydantic is complex.\n    Typically results in a List[Union[...]]. Return base kwargs.\n    \"\"\"\n    kwargs = super().django_to_pydantic_field_info_kwargs(dj_field)\n    # Represent as a list, likely defaulting to an empty list\n    kwargs[\"default_factory\"] = list\n    # Remove attributes not relevant to the Pydantic List[Union[...]] representation\n    kwargs.pop(\"max_length\", None)\n    kwargs.pop(\"max_digits\", None)\n    kwargs.pop(\"decimal_places\", None)\n    kwargs.pop(\"ge\", None)\n    kwargs.pop(\"le\", None)\n    # Choices might be relevant if the Union members have constraints, but complex to map here.\n    # Keep title/description if they exist.\n    return kwargs\n</code></pre>"},{"location":"reference/pydantic2django/core/mapping_units/#pydantic2django.core.mapping_units.GenericForeignKeyMappingUnit.matches","title":"<code>matches(py_type, field_info=None)</code>  <code>classmethod</code>","text":"<p>GFK relationships are typically detected structurally, not via simple type matching. This unit should generally not be selected via scoring.</p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>@classmethod\ndef matches(cls, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; float:\n    \"\"\"GFK relationships are typically detected structurally, not via simple type matching.\n    This unit should generally not be selected via scoring.\n    \"\"\"\n    return 0.0  # Do not participate in standard scoring selection\n</code></pre>"},{"location":"reference/pydantic2django/core/mapping_units/#pydantic2django.core.mapping_units.GenericForeignKeyMappingUnit.pydantic_to_django_kwargs","title":"<code>pydantic_to_django_kwargs(py_type, field_info=None)</code>","text":"<p>Returns base kwargs. GFK details are signaled separately. Actual GFK fields (ContentType, object_id) are handled by the generator.</p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>def pydantic_to_django_kwargs(self, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; dict[str, Any]:\n    \"\"\"Returns base kwargs. GFK details are signaled separately.\n    Actual GFK fields (ContentType, object_id) are handled by the generator.\n    \"\"\"\n    # Return base attributes like verbose_name/help_text if applicable\n    kwargs = super().pydantic_to_django_kwargs(py_type, field_info)\n    # Remove attributes not relevant to the GFK *placeholder* on the parent\n    kwargs.pop(\"default\", None)\n    kwargs.pop(\"max_length\", None)\n    kwargs.pop(\"choices\", None)\n    return kwargs\n</code></pre>"},{"location":"reference/pydantic2django/core/mapping_units/#pydantic2django.core.mapping_units.IPAddressFieldMapping","title":"<code>IPAddressFieldMapping</code>","text":"<p>               Bases: <code>StrFieldMapping</code></p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>class IPAddressFieldMapping(StrFieldMapping):\n    python_type = IPvAnyAddress  # Pydantic's type covers v4/v6\n    django_field_type = models.GenericIPAddressField\n\n    @classmethod\n    def matches(cls, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; float:\n        \"\"\"Strongly prefer IPvAnyAddress.\"\"\"\n        if py_type == cls.python_type:\n            return 1.2  # Very high score for exact type\n        # Don't match plain str\n        return 0.0\n        # return super().matches(py_type, field_info) # Default base matching - Incorrect\n\n    def pydantic_to_django_kwargs(self, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; dict[str, Any]:\n        return super().pydantic_to_django_kwargs(py_type, field_info)\n\n    def django_to_pydantic_field_info_kwargs(self, dj_field: models.Field) -&gt; dict[str, Any]:\n        kwargs = super().django_to_pydantic_field_info_kwargs(dj_field)\n        # IP addresses don't have max_length in Pydantic equivalent\n        kwargs.pop(\"max_length\", None)\n        return kwargs\n</code></pre>"},{"location":"reference/pydantic2django/core/mapping_units/#pydantic2django.core.mapping_units.IPAddressFieldMapping.matches","title":"<code>matches(py_type, field_info=None)</code>  <code>classmethod</code>","text":"<p>Strongly prefer IPvAnyAddress.</p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>@classmethod\ndef matches(cls, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; float:\n    \"\"\"Strongly prefer IPvAnyAddress.\"\"\"\n    if py_type == cls.python_type:\n        return 1.2  # Very high score for exact type\n    # Don't match plain str\n    return 0.0\n</code></pre>"},{"location":"reference/pydantic2django/core/mapping_units/#pydantic2django.core.mapping_units.ImageFieldMapping","title":"<code>ImageFieldMapping</code>","text":"<p>               Bases: <code>FileFieldMapping</code></p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>class ImageFieldMapping(FileFieldMapping):\n    django_field_type = models.ImageField\n\n    @classmethod\n    def matches(cls, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; float:\n        \"\"\"Prefer str if FieldInfo hints suggest an image.\"\"\"\n        if py_type == str:\n            # Check for format hint in json_schema_extra\n            schema_extra = getattr(field_info, \"json_schema_extra\", None)\n            if isinstance(schema_extra, dict):\n                format_hint = schema_extra.get(\"format\")\n                if format_hint == \"image\":\n                    return 1.16  # Slightly higher than FileFieldMapping\n            # If str but no image hint, this unit should not match.\n            return 0.0\n            # return super().matches(py_type, field_info) # Incorrect: Hits FileFieldMapping logic\n        return 0.0  # Don't match non-str types\n\n    def pydantic_to_django_kwargs(self, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; dict[str, Any]:\n        return super().pydantic_to_django_kwargs(py_type, field_info)\n\n    def django_to_pydantic_field_info_kwargs(self, dj_field: models.Field) -&gt; dict[str, Any]:\n        # Start with FileFieldMapping's kwargs\n        kwargs = super().django_to_pydantic_field_info_kwargs(dj_field)\n        # Override format to 'image' if possible\n        if \"json_schema_extra\" in kwargs:\n            kwargs[\"json_schema_extra\"][\"format\"] = \"image\"\n        else:  # Should not happen if FileFieldMapping worked correctly\n            kwargs.setdefault(\"json_schema_extra\", {})[\n                \"format\"\n            ] = \"image\"  # Use setdefault here too for consistency/safety\n        logger.debug(f\"ImageFieldMapping kwargs for '{getattr(dj_field, 'name', 'unknown')}': {kwargs}\")\n        return kwargs\n</code></pre>"},{"location":"reference/pydantic2django/core/mapping_units/#pydantic2django.core.mapping_units.ImageFieldMapping.matches","title":"<code>matches(py_type, field_info=None)</code>  <code>classmethod</code>","text":"<p>Prefer str if FieldInfo hints suggest an image.</p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>@classmethod\ndef matches(cls, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; float:\n    \"\"\"Prefer str if FieldInfo hints suggest an image.\"\"\"\n    if py_type == str:\n        # Check for format hint in json_schema_extra\n        schema_extra = getattr(field_info, \"json_schema_extra\", None)\n        if isinstance(schema_extra, dict):\n            format_hint = schema_extra.get(\"format\")\n            if format_hint == \"image\":\n                return 1.16  # Slightly higher than FileFieldMapping\n        # If str but no image hint, this unit should not match.\n        return 0.0\n        # return super().matches(py_type, field_info) # Incorrect: Hits FileFieldMapping logic\n    return 0.0  # Don't match non-str types\n</code></pre>"},{"location":"reference/pydantic2django/core/mapping_units/#pydantic2django.core.mapping_units.IntFieldMapping","title":"<code>IntFieldMapping</code>","text":"<p>               Bases: <code>TypeMappingUnit</code></p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>class IntFieldMapping(TypeMappingUnit):\n    python_type = int\n    django_field_type = models.IntegerField\n\n    @classmethod\n    def matches(cls, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; float:\n        \"\"\"Prefer base IntField for plain int over AutoFields etc.\"\"\"\n        if py_type == int:\n            # Slightly higher score than subclasses that also map int\n            return 1.01\n        return super().matches(py_type, field_info)\n\n    def pydantic_to_django_kwargs(self, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; dict[str, Any]:\n        return super().pydantic_to_django_kwargs(py_type, field_info)\n</code></pre>"},{"location":"reference/pydantic2django/core/mapping_units/#pydantic2django.core.mapping_units.IntFieldMapping.matches","title":"<code>matches(py_type, field_info=None)</code>  <code>classmethod</code>","text":"<p>Prefer base IntField for plain int over AutoFields etc.</p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>@classmethod\ndef matches(cls, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; float:\n    \"\"\"Prefer base IntField for plain int over AutoFields etc.\"\"\"\n    if py_type == int:\n        # Slightly higher score than subclasses that also map int\n        return 1.01\n    return super().matches(py_type, field_info)\n</code></pre>"},{"location":"reference/pydantic2django/core/mapping_units/#pydantic2django.core.mapping_units.JsonFieldMapping","title":"<code>JsonFieldMapping</code>","text":"<p>               Bases: <code>TypeMappingUnit</code></p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>class JsonFieldMapping(TypeMappingUnit):\n    # Map complex Python types (dict, list, tuple, set, Json) to JSONField\n    # Map Django JSONField to Pydantic Any (or Json for stricter validation)\n    python_type = Any  # Or could use Json type: from pydantic import Json\n    django_field_type = models.JSONField\n\n    @classmethod\n    def matches(cls, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; float:\n        \"\"\"Match collection types and Any as a fallback.\"\"\"\n        origin = get_origin(py_type)\n        # Give a moderate score for common collection types (using origin OR type itself)\n        if origin in (dict, list, set, tuple) or py_type in (dict, list, set, tuple):\n            # TODO: Check if list contains BaseModels? M2M should handle that.\n            # For now, assume non-model lists/collections map here.\n            return 0.8\n        # Give a slightly higher score for Any, acting as a preferred fallback over relationships\n        if py_type == Any:\n            return 0.2  # Higher than base match (0.1?), lower than specific types\n        # If pydantic.Json is used, match it\n        # Need to import Json from pydantic for this check\n        # try:\n        #     from pydantic import Json\n        #     if py_type is Json:\n        #          return 1.0\n        # except ImportError:\n        #     pass\n\n        # Use super().matches() to handle potential subclass checks if needed?\n        # No, JsonField is usually a fallback, direct type checks are sufficient.\n        return 0.0  # Explicitly return 0.0 if not a collection or Any\n        # return super().matches(py_type, field_info) # Default base matching (0.0 for most)\n\n    def pydantic_to_django_kwargs(self, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; dict[str, Any]:\n        return super().pydantic_to_django_kwargs(py_type, field_info)\n\n    def django_to_pydantic_field_info_kwargs(self, dj_field: models.Field) -&gt; dict[str, Any]:\n        # Get base kwargs (handles title, default_factory for dict/list)\n        kwargs = super().django_to_pydantic_field_info_kwargs(dj_field)\n        # Remove max_length if present, JSON doesn't use it\n        kwargs.pop(\"max_length\", None)\n        return kwargs\n</code></pre>"},{"location":"reference/pydantic2django/core/mapping_units/#pydantic2django.core.mapping_units.JsonFieldMapping.matches","title":"<code>matches(py_type, field_info=None)</code>  <code>classmethod</code>","text":"<p>Match collection types and Any as a fallback.</p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>@classmethod\ndef matches(cls, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; float:\n    \"\"\"Match collection types and Any as a fallback.\"\"\"\n    origin = get_origin(py_type)\n    # Give a moderate score for common collection types (using origin OR type itself)\n    if origin in (dict, list, set, tuple) or py_type in (dict, list, set, tuple):\n        # TODO: Check if list contains BaseModels? M2M should handle that.\n        # For now, assume non-model lists/collections map here.\n        return 0.8\n    # Give a slightly higher score for Any, acting as a preferred fallback over relationships\n    if py_type == Any:\n        return 0.2  # Higher than base match (0.1?), lower than specific types\n    # If pydantic.Json is used, match it\n    # Need to import Json from pydantic for this check\n    # try:\n    #     from pydantic import Json\n    #     if py_type is Json:\n    #          return 1.0\n    # except ImportError:\n    #     pass\n\n    # Use super().matches() to handle potential subclass checks if needed?\n    # No, JsonField is usually a fallback, direct type checks are sufficient.\n    return 0.0  # Explicitly return 0.0 if not a collection or Any\n</code></pre>"},{"location":"reference/pydantic2django/core/mapping_units/#pydantic2django.core.mapping_units.PositiveBigIntFieldMapping","title":"<code>PositiveBigIntFieldMapping</code>","text":"<p>               Bases: <code>PositiveIntFieldMapping</code></p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>class PositiveBigIntFieldMapping(PositiveIntFieldMapping):\n    django_field_type = models.PositiveBigIntegerField\n\n    @classmethod\n    def matches(cls, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; float:\n        \"\"\"Check for ge=0 constraint, but score slightly lower than base PositiveIntFieldMapping.\"\"\"\n        base_score = super().matches(\n            py_type, field_info\n        )  # Gets score from PositiveIntFieldMapping (e.g., 1.05 if constraint matches)\n        # Reduce score slightly if constraint matched to allow base PositiveIntFieldMapping to win\n        if base_score &gt;= 1.05:\n            logger.debug(\"PositiveBigIntFieldMapping: Reducing score slightly from base Positive match.\")\n            return 1.04\n        # If base score is low (no constraint match or not int), return that score.\n        return base_score\n\n    def pydantic_to_django_kwargs(self, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; dict[str, Any]:\n        return super().pydantic_to_django_kwargs(py_type, field_info)\n</code></pre>"},{"location":"reference/pydantic2django/core/mapping_units/#pydantic2django.core.mapping_units.PositiveBigIntFieldMapping.matches","title":"<code>matches(py_type, field_info=None)</code>  <code>classmethod</code>","text":"<p>Check for ge=0 constraint, but score slightly lower than base PositiveIntFieldMapping.</p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>@classmethod\ndef matches(cls, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; float:\n    \"\"\"Check for ge=0 constraint, but score slightly lower than base PositiveIntFieldMapping.\"\"\"\n    base_score = super().matches(\n        py_type, field_info\n    )  # Gets score from PositiveIntFieldMapping (e.g., 1.05 if constraint matches)\n    # Reduce score slightly if constraint matched to allow base PositiveIntFieldMapping to win\n    if base_score &gt;= 1.05:\n        logger.debug(\"PositiveBigIntFieldMapping: Reducing score slightly from base Positive match.\")\n        return 1.04\n    # If base score is low (no constraint match or not int), return that score.\n    return base_score\n</code></pre>"},{"location":"reference/pydantic2django/core/mapping_units/#pydantic2django.core.mapping_units.PositiveIntFieldMapping","title":"<code>PositiveIntFieldMapping</code>","text":"<p>               Bases: <code>TypeMappingUnit</code></p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>class PositiveIntFieldMapping(TypeMappingUnit):\n    python_type = int\n    django_field_type = models.PositiveIntegerField\n\n    @classmethod\n    def matches(cls, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; float:\n        \"\"\"Prefer mapping `int` to PositiveIntegerField when `ge=0` is suggested.\"\"\"\n        base_score = super().matches(py_type, field_info)  # Should be 0.0 or 0.5 normally\n        if py_type != int:\n            return base_score\n\n        has_ge0_constraint = False\n        if field_info and hasattr(field_info, \"metadata\"):\n            for item in field_info.metadata:\n                # Check for ge/gt constraints\n                ge_value = getattr(item, \"ge\", None)\n                gt_value = getattr(item, \"gt\", None)\n                if ge_value == 0 or (gt_value is not None and gt_value &gt;= -1):\n                    # Consider gt &gt; -1 (i.e. &gt;=0) as positive indication too\n                    has_ge0_constraint = True\n                    break\n        # Also check FieldInfo directly (older Pydantic?)\n        if not has_ge0_constraint and field_info:\n            ge_value = getattr(field_info, \"ge\", None)\n            gt_value = getattr(field_info, \"gt\", None)\n            if ge_value == 0 or (gt_value is not None and gt_value &gt;= -1):\n                has_ge0_constraint = True\n\n        if has_ge0_constraint:\n            # Higher score than IntFieldMapping (1.01) if ge=0 constraint present\n            logger.debug(\"PositiveIntFieldMapping: Found ge=0 constraint, score -&gt; 1.05\")\n            return 1.05\n        else:\n            # If no constraint, give a lower score than IntFieldMapping\n            # Base score (0.5) for int is already lower than IntFieldMapping (1.01)\n            logger.debug(\"PositiveIntFieldMapping: No ge=0 constraint, relying on base score.\")\n            # Return a slightly lower score than plain IntField to ensure IntField wins by default\n            return 0.4  # Ensure it doesn't accidentally win over IntField (1.01)\n\n    def pydantic_to_django_kwargs(self, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; dict[str, Any]:\n        return super().pydantic_to_django_kwargs(py_type, field_info)\n\n    def django_to_pydantic_field_info_kwargs(self, dj_field: models.Field) -&gt; dict[str, Any]:\n        kwargs = super().django_to_pydantic_field_info_kwargs(dj_field)\n        kwargs[\"ge\"] = 0\n        return kwargs\n</code></pre>"},{"location":"reference/pydantic2django/core/mapping_units/#pydantic2django.core.mapping_units.PositiveIntFieldMapping.matches","title":"<code>matches(py_type, field_info=None)</code>  <code>classmethod</code>","text":"<p>Prefer mapping <code>int</code> to PositiveIntegerField when <code>ge=0</code> is suggested.</p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>@classmethod\ndef matches(cls, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; float:\n    \"\"\"Prefer mapping `int` to PositiveIntegerField when `ge=0` is suggested.\"\"\"\n    base_score = super().matches(py_type, field_info)  # Should be 0.0 or 0.5 normally\n    if py_type != int:\n        return base_score\n\n    has_ge0_constraint = False\n    if field_info and hasattr(field_info, \"metadata\"):\n        for item in field_info.metadata:\n            # Check for ge/gt constraints\n            ge_value = getattr(item, \"ge\", None)\n            gt_value = getattr(item, \"gt\", None)\n            if ge_value == 0 or (gt_value is not None and gt_value &gt;= -1):\n                # Consider gt &gt; -1 (i.e. &gt;=0) as positive indication too\n                has_ge0_constraint = True\n                break\n    # Also check FieldInfo directly (older Pydantic?)\n    if not has_ge0_constraint and field_info:\n        ge_value = getattr(field_info, \"ge\", None)\n        gt_value = getattr(field_info, \"gt\", None)\n        if ge_value == 0 or (gt_value is not None and gt_value &gt;= -1):\n            has_ge0_constraint = True\n\n    if has_ge0_constraint:\n        # Higher score than IntFieldMapping (1.01) if ge=0 constraint present\n        logger.debug(\"PositiveIntFieldMapping: Found ge=0 constraint, score -&gt; 1.05\")\n        return 1.05\n    else:\n        # If no constraint, give a lower score than IntFieldMapping\n        # Base score (0.5) for int is already lower than IntFieldMapping (1.01)\n        logger.debug(\"PositiveIntFieldMapping: No ge=0 constraint, relying on base score.\")\n        # Return a slightly lower score than plain IntField to ensure IntField wins by default\n        return 0.4  # Ensure it doesn't accidentally win over IntField (1.01)\n</code></pre>"},{"location":"reference/pydantic2django/core/mapping_units/#pydantic2django.core.mapping_units.PositiveSmallIntFieldMapping","title":"<code>PositiveSmallIntFieldMapping</code>","text":"<p>               Bases: <code>PositiveIntFieldMapping</code></p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>class PositiveSmallIntFieldMapping(PositiveIntFieldMapping):\n    django_field_type = models.PositiveSmallIntegerField\n\n    @classmethod\n    def matches(cls, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; float:\n        \"\"\"Check for ge=0 constraint, but score slightly lower than base PositiveIntFieldMapping.\"\"\"\n        base_score = super().matches(\n            py_type, field_info\n        )  # Gets score from PositiveIntFieldMapping (e.g., 1.05 if constraint matches)\n        # Reduce score slightly if constraint matched to allow base PositiveIntFieldMapping to win\n        if base_score &gt;= 1.05:\n            logger.debug(\"PositiveSmallIntFieldMapping: Reducing score slightly from base Positive match.\")\n            return 1.04\n        # If base score is low (no constraint match or not int), return that score.\n        return base_score\n\n    def pydantic_to_django_kwargs(self, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; dict[str, Any]:\n        return super().pydantic_to_django_kwargs(py_type, field_info)\n</code></pre>"},{"location":"reference/pydantic2django/core/mapping_units/#pydantic2django.core.mapping_units.PositiveSmallIntFieldMapping.matches","title":"<code>matches(py_type, field_info=None)</code>  <code>classmethod</code>","text":"<p>Check for ge=0 constraint, but score slightly lower than base PositiveIntFieldMapping.</p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>@classmethod\ndef matches(cls, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; float:\n    \"\"\"Check for ge=0 constraint, but score slightly lower than base PositiveIntFieldMapping.\"\"\"\n    base_score = super().matches(\n        py_type, field_info\n    )  # Gets score from PositiveIntFieldMapping (e.g., 1.05 if constraint matches)\n    # Reduce score slightly if constraint matched to allow base PositiveIntFieldMapping to win\n    if base_score &gt;= 1.05:\n        logger.debug(\"PositiveSmallIntFieldMapping: Reducing score slightly from base Positive match.\")\n        return 1.04\n    # If base score is low (no constraint match or not int), return that score.\n    return base_score\n</code></pre>"},{"location":"reference/pydantic2django/core/mapping_units/#pydantic2django.core.mapping_units.StrFieldMapping","title":"<code>StrFieldMapping</code>","text":"<p>               Bases: <code>TypeMappingUnit</code></p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>class StrFieldMapping(TypeMappingUnit):  # Base for Char/Text\n    python_type = str\n    django_field_type = models.CharField  # Default to CharField\n\n    @classmethod\n    def matches(cls, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; float:\n        \"\"\"Prefer mapping `str` to CharField ONLY when `max_length` is suggested.\"\"\"\n        base_score = super().matches(py_type, field_info)\n        if py_type == str:\n            has_max_length = False\n            if field_info and hasattr(field_info, \"metadata\"):\n                for item in field_info.metadata:\n                    if hasattr(item, \"max_length\") and item.max_length is not None:\n                        has_max_length = True\n                        break\n\n            if has_max_length:\n                # High score if max_length specified\n                return 1.1\n            else:\n                # Lower score than TextFieldMapping (1.0) if no max_length hint\n                return 0.9\n        return base_score\n\n    def pydantic_to_django_kwargs(self, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; dict[str, Any]:\n        kwargs = super().pydantic_to_django_kwargs(py_type, field_info)\n        pyd_max_length = None\n        if field_info and hasattr(field_info, \"metadata\"):\n            for item in field_info.metadata:\n                if hasattr(item, \"max_length\"):\n                    pyd_max_length = item.max_length\n                    break\n\n        if pyd_max_length is not None:\n            kwargs[\"max_length\"] = pyd_max_length\n        elif \"max_length\" not in kwargs:\n            if self.django_field_type == models.CharField:\n                # Apply default only if CharField is selected and no length was specified\n                kwargs[\"max_length\"] = 255\n        return kwargs\n\n    def django_to_pydantic_field_info_kwargs(self, dj_field: models.Field) -&gt; dict[str, Any]:\n        kwargs = super().django_to_pydantic_field_info_kwargs(dj_field)\n        if isinstance(dj_field, models.CharField) and dj_field.max_length is not None:\n            kwargs[\"max_length\"] = dj_field.max_length\n        return kwargs\n</code></pre>"},{"location":"reference/pydantic2django/core/mapping_units/#pydantic2django.core.mapping_units.StrFieldMapping.matches","title":"<code>matches(py_type, field_info=None)</code>  <code>classmethod</code>","text":"<p>Prefer mapping <code>str</code> to CharField ONLY when <code>max_length</code> is suggested.</p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>@classmethod\ndef matches(cls, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; float:\n    \"\"\"Prefer mapping `str` to CharField ONLY when `max_length` is suggested.\"\"\"\n    base_score = super().matches(py_type, field_info)\n    if py_type == str:\n        has_max_length = False\n        if field_info and hasattr(field_info, \"metadata\"):\n            for item in field_info.metadata:\n                if hasattr(item, \"max_length\") and item.max_length is not None:\n                    has_max_length = True\n                    break\n\n        if has_max_length:\n            # High score if max_length specified\n            return 1.1\n        else:\n            # Lower score than TextFieldMapping (1.0) if no max_length hint\n            return 0.9\n    return base_score\n</code></pre>"},{"location":"reference/pydantic2django/core/mapping_units/#pydantic2django.core.mapping_units.TextFieldMapping","title":"<code>TextFieldMapping</code>","text":"<p>               Bases: <code>StrFieldMapping</code></p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>class TextFieldMapping(StrFieldMapping):\n    django_field_type = models.TextField\n\n    @classmethod\n    def matches(cls, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; float:\n        \"\"\"Prefer mapping `str` to TextField when `max_length` is NOT suggested.\"\"\"\n        base_score = super(StrFieldMapping, cls).matches(py_type, field_info)\n\n        if py_type == str:\n            has_max_length = False\n            if field_info and hasattr(field_info, \"metadata\"):\n                for item in field_info.metadata:\n                    if hasattr(item, \"max_length\") and item.max_length is not None:\n                        has_max_length = True\n                        break\n\n            if not has_max_length:\n                # Prefer TextField (1.0) over CharField (0.9) if no length constraint\n                return 1.0\n            else:\n                # Low score if max_length *is* specified\n                return 0.4\n        return base_score\n\n    def pydantic_to_django_kwargs(self, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; dict[str, Any]:\n        kwargs = super().pydantic_to_django_kwargs(py_type, field_info)\n        pyd_max_length = None\n        if field_info and hasattr(field_info, \"metadata\"):\n            for item in field_info.metadata:\n                if hasattr(item, \"max_length\"):\n                    pyd_max_length = item.max_length\n                    break\n        if pyd_max_length is None:\n            kwargs.pop(\"max_length\", None)\n        return kwargs\n</code></pre>"},{"location":"reference/pydantic2django/core/mapping_units/#pydantic2django.core.mapping_units.TextFieldMapping.matches","title":"<code>matches(py_type, field_info=None)</code>  <code>classmethod</code>","text":"<p>Prefer mapping <code>str</code> to TextField when <code>max_length</code> is NOT suggested.</p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>@classmethod\ndef matches(cls, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; float:\n    \"\"\"Prefer mapping `str` to TextField when `max_length` is NOT suggested.\"\"\"\n    base_score = super(StrFieldMapping, cls).matches(py_type, field_info)\n\n    if py_type == str:\n        has_max_length = False\n        if field_info and hasattr(field_info, \"metadata\"):\n            for item in field_info.metadata:\n                if hasattr(item, \"max_length\") and item.max_length is not None:\n                    has_max_length = True\n                    break\n\n        if not has_max_length:\n            # Prefer TextField (1.0) over CharField (0.9) if no length constraint\n            return 1.0\n        else:\n            # Low score if max_length *is* specified\n            return 0.4\n    return base_score\n</code></pre>"},{"location":"reference/pydantic2django/core/mapping_units/#pydantic2django.core.mapping_units.TypeMappingUnit","title":"<code>TypeMappingUnit</code>","text":"<p>Base class defining a bidirectional mapping between a Python type and a Django Field.</p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>class TypeMappingUnit:\n    \"\"\"Base class defining a bidirectional mapping between a Python type and a Django Field.\"\"\"\n\n    python_type: type[T_PydanticType]\n    django_field_type: type[models.Field]  # Use base class here\n\n    def __init_subclass__(cls, **kwargs):\n        \"\"\"Ensure subclasses define the required types.\"\"\"\n        super().__init_subclass__(**kwargs)\n        if not hasattr(cls, \"python_type\") or not hasattr(cls, \"django_field_type\"):\n            raise NotImplementedError(\n                \"Subclasses of TypeMappingUnit must define 'python_type' and 'django_field_type' class attributes.\"\n            )\n\n    @classmethod\n    def matches(cls, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; float:\n        \"\"\"\n        Calculate a score indicating how well this unit matches the given Python type and FieldInfo.\n\n        Args:\n            py_type: The Python type to match against.\n            field_info: Optional Pydantic FieldInfo for context.\n\n        Returns:\n            A float score (0.0 = no match, higher = better match).\n            Base implementation scores:\n            - 1.0 for exact type match (cls.python_type == py_type)\n            - 0.5 for subclass match (issubclass(py_type, cls.python_type))\n            - 0.0 otherwise\n        \"\"\"\n        target_py_type = cls.python_type\n        if py_type == target_py_type:\n            return 1.0\n        try:\n            # Check issubclass only if both are actual classes and py_type is not Any\n            if (\n                py_type is not Any\n                and inspect.isclass(py_type)\n                and inspect.isclass(target_py_type)\n                and issubclass(py_type, target_py_type)\n            ):\n                # Don't match if it's the same type (already handled by exact match)\n                if py_type is not target_py_type:\n                    return 0.5\n        except TypeError:\n            # issubclass fails on non-classes (like Any, List[int], etc.)\n            pass\n        return 0.0\n\n    def pydantic_to_django_kwargs(self, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; dict[str, Any]:\n        \"\"\"Generate Django field constructor kwargs from Pydantic FieldInfo.\"\"\"\n        kwargs = {}\n        if field_info:\n            # Map common attributes\n            if field_info.title:\n                kwargs[\"verbose_name\"] = field_info.title\n            if field_info.description:\n                kwargs[\"help_text\"] = field_info.description\n\n            # Only consider `default` if `default_factory` is None\n            if field_info.default_factory is None:\n                if field_info.default is not PydanticUndefined and field_info.default is not None:\n                    # Django doesn't handle callable defaults easily here\n                    if not callable(field_info.default):\n                        kwargs[\"default\"] = field_info.default\n                elif field_info.default is None:  # Explicitly check for None default\n                    kwargs[\"default\"] = None  # Add default=None if present in FieldInfo\n            # else: If default_factory is present, do not add a 'default' kwarg.\n            # No warning needed as this is now expected behavior.\n\n            # Note: Frozen, ge, le etc. are validation rules, map separately if needed\n        return kwargs\n\n    def django_to_pydantic_field_info_kwargs(self, dj_field: models.Field) -&gt; dict[str, Any]:\n        \"\"\"Generate Pydantic FieldInfo kwargs from a Django field instance.\"\"\"\n        kwargs = {}\n        field_name = getattr(dj_field, \"name\", \"unknown_field\")  # Get field name for logging\n\n        # Title: Use verbose_name or generate from field name\n        verbose_name = getattr(dj_field, \"verbose_name\", None)\n        logger.debug(f\"Processing field '{field_name}': verbose_name='{verbose_name}'\")\n        if verbose_name:\n            # Ensure verbose_name is a string, handling lazy proxies\n            kwargs[\"title\"] = force_str(verbose_name).capitalize()\n        elif field_name != \"unknown_field\" and isinstance(field_name, str):\n            # Generate title from name if verbose_name is missing and name is a string\n            generated_title = field_name.replace(\"_\", \" \").capitalize()\n            kwargs[\"title\"] = generated_title\n            logger.debug(f\"Generated title for '{field_name}': '{generated_title}'\")\n        # else: field name is None or 'unknown_field', no title generated by default\n\n        # Description\n        if dj_field.help_text:\n            # Ensure help_text is a string, handling lazy proxies\n            kwargs[\"description\"] = force_str(dj_field.help_text)\n\n        # Default value/factory handling\n        if dj_field.has_default():\n            dj_default = dj_field.get_default()\n            if dj_default is not models.fields.NOT_PROVIDED:\n                if callable(dj_default):\n                    factory_set = False\n                    if dj_default is dict:\n                        kwargs[\"default_factory\"] = dict\n                        factory_set = True\n                    elif dj_default is list:\n                        kwargs[\"default_factory\"] = list\n                        factory_set = True\n                    # Add other known callable mappings if needed\n                    else:\n                        logger.debug(\n                            f\"Django field '{dj_field.name}' has an unmapped callable default ({dj_default}), \"\n                            \"not mapping to Pydantic default/default_factory.\"\n                        )\n                    if factory_set:\n                        kwargs.pop(\"default\", None)\n                # Handle non-callable defaults\n                # Map default={} back to default_factory=dict for JSONField\n                elif dj_default == {}:\n                    kwargs[\"default_factory\"] = dict\n                    kwargs.pop(\"default\", None)\n                elif dj_default == []:\n                    kwargs[\"default_factory\"] = list\n                    kwargs.pop(\"default\", None)\n                elif dj_default is not None:\n                    # Add non-None, non-callable, non-empty-collection defaults\n                    logger.debug(\n                        f\"Processing non-callable default for '{field_name}'. Type: {type(dj_default)}, Value: {dj_default!r}\"\n                    )\n                    # Apply force_str ONLY if the default value's type suggests it might be a lazy proxy string.\n                    # A simple check is if 'proxy' is in the type name.\n                    processed_default = dj_default\n                    if \"proxy\" in type(dj_default).__name__:\n                        try:\n                            processed_default = force_str(dj_default)\n                            logger.debug(\n                                f\"Applied force_str to potential lazy default for '{field_name}'. New value: {processed_default!r}\"\n                            )\n                        except Exception as e:\n                            logger.error(\n                                f\"Failed to apply force_str to default value for '{field_name}': {e}. Assigning raw default.\"\n                            )\n                            processed_default = dj_default  # Keep original on error\n\n                    kwargs[\"default\"] = processed_default\n                    logger.debug(f\"Assigned final default for '{field_name}': {kwargs.get('default')!r}\")\n\n        # Handle AutoField PKs -&gt; frozen=True, default=None\n        is_auto_pk = dj_field.primary_key and isinstance(\n            dj_field, (models.AutoField, models.BigAutoField, models.SmallAutoField)\n        )\n        if is_auto_pk:\n            kwargs[\"frozen\"] = True\n            kwargs[\"default\"] = None\n\n        # Handle choices (including processing labels and limiting)\n        # Log choices *before* calling handle_choices\n        if hasattr(dj_field, \"choices\") and dj_field.choices:\n            try:\n                # Log the raw choices from the Django field\n                raw_choices_repr = repr(list(dj_field.choices))  # Materialize and get repr\n                logger.debug(f\"Field '{field_name}': Raw choices before handle_choices: {raw_choices_repr}\")\n            except Exception as log_err:\n                logger.warning(f\"Field '{field_name}': Error logging raw choices: {log_err}\")\n\n            self.handle_choices(dj_field, kwargs)\n            # Log choices *after* handle_choices modified kwargs\n            processed_choices_repr = repr(kwargs.get(\"json_schema_extra\", {}).get(\"choices\"))\n            logger.debug(f\"Field '{field_name}': Choices in kwargs after handle_choices: {processed_choices_repr}\")\n\n        # Handle non-choice max_length only if choices were NOT processed\n        elif dj_field.max_length is not None:\n            # Only add max_length if not choices - specific units can override\n            kwargs[\"max_length\"] = dj_field.max_length\n\n        logger.debug(f\"Base kwargs generated for '{field_name}': {kwargs}\")\n        return kwargs\n\n    def handle_choices(self, dj_field: models.Field, kwargs: dict[str, Any]) -&gt; None:\n        \"\"\"\n        Handles Django field choices, ensuring lazy translation proxies are resolved.\n\n        It processes the choices, forces string conversion on labels within an\n        active translation context, limits the number of choices added to the schema,\n        and stores them in `json_schema_extra`.\n        \"\"\"\n        field_name = getattr(dj_field, \"name\", \"unknown_field\")\n        processed_choices = []\n        MAX_CHOICES_IN_SCHEMA = 30  # TODO: Make configurable\n        limited_choices = []\n        default_value = kwargs.get(\"default\")  # Use potentially processed default\n        default_included = False\n\n        # --- Ensure Translation Context --- #\n        active_translation = None\n        if translation:\n            try:\n                # Get the currently active language to restore later\n                current_language = translation.get_language()\n                # Activate the default language (or a specific one like 'en')\n                # This forces lazy objects to resolve using a consistent language.\n                # Using settings.LANGUAGE_CODE assumes it's set correctly.\n                default_language = getattr(settings, \"LANGUAGE_CODE\", \"en\")  # Fallback to 'en'\n                active_translation = translation.override(default_language)\n                logger.debug(\n                    f\"Activated translation override ('{default_language}') for processing choices of '{field_name}'\"\n                )\n                active_translation.__enter__()  # Manually enter context\n            except Exception as trans_err:\n                logger.warning(f\"Failed to activate translation context for '{field_name}': {trans_err}\")\n                active_translation = None  # Ensure it's None if activation failed\n        else:\n            logger.warning(\"Django translation module not available. Lazy choices might not resolve correctly.\")\n\n        # --- Process Choices (within potential translation context) --- #\n        try:\n            all_choices = list(dj_field.choices or [])\n            for value, label in all_choices:\n                logger.debug(\n                    f\"  Processing choice for '{field_name}': Value={value!r}, Label={label!r} (Type: {type(label)})\"\n                )\n                try:\n                    # Apply force_str defensively; should resolve lazy proxies if context is active\n                    processed_label = force_str(label)\n                    logger.debug(\n                        f\"  Processed label for '{field_name}': Value={value!r}, Label={processed_label!r} (Type: {type(processed_label)})\"\n                    )\n                except Exception as force_str_err:\n                    logger.error(\n                        f\"Error using force_str on label for '{field_name}' (value: {value!r}): {force_str_err}\"\n                    )\n                    # Fallback: use repr or a placeholder if force_str fails completely\n                    processed_label = f\"&lt;unresolved: {repr(label)}&gt;\"\n                processed_choices.append((value, processed_label))\n\n            # --- Limit Choices --- #\n            if len(processed_choices) &gt; MAX_CHOICES_IN_SCHEMA:\n                logger.warning(\n                    f\"Limiting choices for '{field_name}' from {len(processed_choices)} to {MAX_CHOICES_IN_SCHEMA}\"\n                )\n                if default_value is not None:\n                    for val, lbl in processed_choices:\n                        if val == default_value:\n                            limited_choices.append((val, lbl))\n                            default_included = True\n                            break\n                remaining_slots = MAX_CHOICES_IN_SCHEMA - len(limited_choices)\n                if remaining_slots &gt; 0:\n                    for val, lbl in processed_choices:\n                        if len(limited_choices) &gt;= MAX_CHOICES_IN_SCHEMA:\n                            break\n                        if not (default_included and val == default_value):\n                            limited_choices.append((val, lbl))\n                final_choices_list = limited_choices\n            else:\n                final_choices_list = processed_choices\n\n            # --- Store Choices --- #\n            kwargs.setdefault(\"json_schema_extra\", {})[\"choices\"] = final_choices_list\n            kwargs.pop(\"max_length\", None)  # Remove max_length if choices are present\n            logger.debug(f\"Stored final choices in json_schema_extra for '{field_name}'\")\n\n        except Exception as e:\n            logger.error(f\"Error processing or limiting choices for field '{field_name}': {e}\", exc_info=True)\n            kwargs.pop(\"json_schema_extra\", None)\n\n        finally:\n            # --- Deactivate Translation Context --- #\n            if active_translation:\n                try:\n                    active_translation.__exit__(None, None, None)  # Manually exit context\n                    logger.debug(f\"Deactivated translation override for '{field_name}'\")\n                except Exception as trans_exit_err:\n                    logger.warning(f\"Error deactivating translation context for '{field_name}': {trans_exit_err}\")\n</code></pre>"},{"location":"reference/pydantic2django/core/mapping_units/#pydantic2django.core.mapping_units.TypeMappingUnit.__init_subclass__","title":"<code>__init_subclass__(**kwargs)</code>","text":"<p>Ensure subclasses define the required types.</p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>def __init_subclass__(cls, **kwargs):\n    \"\"\"Ensure subclasses define the required types.\"\"\"\n    super().__init_subclass__(**kwargs)\n    if not hasattr(cls, \"python_type\") or not hasattr(cls, \"django_field_type\"):\n        raise NotImplementedError(\n            \"Subclasses of TypeMappingUnit must define 'python_type' and 'django_field_type' class attributes.\"\n        )\n</code></pre>"},{"location":"reference/pydantic2django/core/mapping_units/#pydantic2django.core.mapping_units.TypeMappingUnit.django_to_pydantic_field_info_kwargs","title":"<code>django_to_pydantic_field_info_kwargs(dj_field)</code>","text":"<p>Generate Pydantic FieldInfo kwargs from a Django field instance.</p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>def django_to_pydantic_field_info_kwargs(self, dj_field: models.Field) -&gt; dict[str, Any]:\n    \"\"\"Generate Pydantic FieldInfo kwargs from a Django field instance.\"\"\"\n    kwargs = {}\n    field_name = getattr(dj_field, \"name\", \"unknown_field\")  # Get field name for logging\n\n    # Title: Use verbose_name or generate from field name\n    verbose_name = getattr(dj_field, \"verbose_name\", None)\n    logger.debug(f\"Processing field '{field_name}': verbose_name='{verbose_name}'\")\n    if verbose_name:\n        # Ensure verbose_name is a string, handling lazy proxies\n        kwargs[\"title\"] = force_str(verbose_name).capitalize()\n    elif field_name != \"unknown_field\" and isinstance(field_name, str):\n        # Generate title from name if verbose_name is missing and name is a string\n        generated_title = field_name.replace(\"_\", \" \").capitalize()\n        kwargs[\"title\"] = generated_title\n        logger.debug(f\"Generated title for '{field_name}': '{generated_title}'\")\n    # else: field name is None or 'unknown_field', no title generated by default\n\n    # Description\n    if dj_field.help_text:\n        # Ensure help_text is a string, handling lazy proxies\n        kwargs[\"description\"] = force_str(dj_field.help_text)\n\n    # Default value/factory handling\n    if dj_field.has_default():\n        dj_default = dj_field.get_default()\n        if dj_default is not models.fields.NOT_PROVIDED:\n            if callable(dj_default):\n                factory_set = False\n                if dj_default is dict:\n                    kwargs[\"default_factory\"] = dict\n                    factory_set = True\n                elif dj_default is list:\n                    kwargs[\"default_factory\"] = list\n                    factory_set = True\n                # Add other known callable mappings if needed\n                else:\n                    logger.debug(\n                        f\"Django field '{dj_field.name}' has an unmapped callable default ({dj_default}), \"\n                        \"not mapping to Pydantic default/default_factory.\"\n                    )\n                if factory_set:\n                    kwargs.pop(\"default\", None)\n            # Handle non-callable defaults\n            # Map default={} back to default_factory=dict for JSONField\n            elif dj_default == {}:\n                kwargs[\"default_factory\"] = dict\n                kwargs.pop(\"default\", None)\n            elif dj_default == []:\n                kwargs[\"default_factory\"] = list\n                kwargs.pop(\"default\", None)\n            elif dj_default is not None:\n                # Add non-None, non-callable, non-empty-collection defaults\n                logger.debug(\n                    f\"Processing non-callable default for '{field_name}'. Type: {type(dj_default)}, Value: {dj_default!r}\"\n                )\n                # Apply force_str ONLY if the default value's type suggests it might be a lazy proxy string.\n                # A simple check is if 'proxy' is in the type name.\n                processed_default = dj_default\n                if \"proxy\" in type(dj_default).__name__:\n                    try:\n                        processed_default = force_str(dj_default)\n                        logger.debug(\n                            f\"Applied force_str to potential lazy default for '{field_name}'. New value: {processed_default!r}\"\n                        )\n                    except Exception as e:\n                        logger.error(\n                            f\"Failed to apply force_str to default value for '{field_name}': {e}. Assigning raw default.\"\n                        )\n                        processed_default = dj_default  # Keep original on error\n\n                kwargs[\"default\"] = processed_default\n                logger.debug(f\"Assigned final default for '{field_name}': {kwargs.get('default')!r}\")\n\n    # Handle AutoField PKs -&gt; frozen=True, default=None\n    is_auto_pk = dj_field.primary_key and isinstance(\n        dj_field, (models.AutoField, models.BigAutoField, models.SmallAutoField)\n    )\n    if is_auto_pk:\n        kwargs[\"frozen\"] = True\n        kwargs[\"default\"] = None\n\n    # Handle choices (including processing labels and limiting)\n    # Log choices *before* calling handle_choices\n    if hasattr(dj_field, \"choices\") and dj_field.choices:\n        try:\n            # Log the raw choices from the Django field\n            raw_choices_repr = repr(list(dj_field.choices))  # Materialize and get repr\n            logger.debug(f\"Field '{field_name}': Raw choices before handle_choices: {raw_choices_repr}\")\n        except Exception as log_err:\n            logger.warning(f\"Field '{field_name}': Error logging raw choices: {log_err}\")\n\n        self.handle_choices(dj_field, kwargs)\n        # Log choices *after* handle_choices modified kwargs\n        processed_choices_repr = repr(kwargs.get(\"json_schema_extra\", {}).get(\"choices\"))\n        logger.debug(f\"Field '{field_name}': Choices in kwargs after handle_choices: {processed_choices_repr}\")\n\n    # Handle non-choice max_length only if choices were NOT processed\n    elif dj_field.max_length is not None:\n        # Only add max_length if not choices - specific units can override\n        kwargs[\"max_length\"] = dj_field.max_length\n\n    logger.debug(f\"Base kwargs generated for '{field_name}': {kwargs}\")\n    return kwargs\n</code></pre>"},{"location":"reference/pydantic2django/core/mapping_units/#pydantic2django.core.mapping_units.TypeMappingUnit.handle_choices","title":"<code>handle_choices(dj_field, kwargs)</code>","text":"<p>Handles Django field choices, ensuring lazy translation proxies are resolved.</p> <p>It processes the choices, forces string conversion on labels within an active translation context, limits the number of choices added to the schema, and stores them in <code>json_schema_extra</code>.</p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>def handle_choices(self, dj_field: models.Field, kwargs: dict[str, Any]) -&gt; None:\n    \"\"\"\n    Handles Django field choices, ensuring lazy translation proxies are resolved.\n\n    It processes the choices, forces string conversion on labels within an\n    active translation context, limits the number of choices added to the schema,\n    and stores them in `json_schema_extra`.\n    \"\"\"\n    field_name = getattr(dj_field, \"name\", \"unknown_field\")\n    processed_choices = []\n    MAX_CHOICES_IN_SCHEMA = 30  # TODO: Make configurable\n    limited_choices = []\n    default_value = kwargs.get(\"default\")  # Use potentially processed default\n    default_included = False\n\n    # --- Ensure Translation Context --- #\n    active_translation = None\n    if translation:\n        try:\n            # Get the currently active language to restore later\n            current_language = translation.get_language()\n            # Activate the default language (or a specific one like 'en')\n            # This forces lazy objects to resolve using a consistent language.\n            # Using settings.LANGUAGE_CODE assumes it's set correctly.\n            default_language = getattr(settings, \"LANGUAGE_CODE\", \"en\")  # Fallback to 'en'\n            active_translation = translation.override(default_language)\n            logger.debug(\n                f\"Activated translation override ('{default_language}') for processing choices of '{field_name}'\"\n            )\n            active_translation.__enter__()  # Manually enter context\n        except Exception as trans_err:\n            logger.warning(f\"Failed to activate translation context for '{field_name}': {trans_err}\")\n            active_translation = None  # Ensure it's None if activation failed\n    else:\n        logger.warning(\"Django translation module not available. Lazy choices might not resolve correctly.\")\n\n    # --- Process Choices (within potential translation context) --- #\n    try:\n        all_choices = list(dj_field.choices or [])\n        for value, label in all_choices:\n            logger.debug(\n                f\"  Processing choice for '{field_name}': Value={value!r}, Label={label!r} (Type: {type(label)})\"\n            )\n            try:\n                # Apply force_str defensively; should resolve lazy proxies if context is active\n                processed_label = force_str(label)\n                logger.debug(\n                    f\"  Processed label for '{field_name}': Value={value!r}, Label={processed_label!r} (Type: {type(processed_label)})\"\n                )\n            except Exception as force_str_err:\n                logger.error(\n                    f\"Error using force_str on label for '{field_name}' (value: {value!r}): {force_str_err}\"\n                )\n                # Fallback: use repr or a placeholder if force_str fails completely\n                processed_label = f\"&lt;unresolved: {repr(label)}&gt;\"\n            processed_choices.append((value, processed_label))\n\n        # --- Limit Choices --- #\n        if len(processed_choices) &gt; MAX_CHOICES_IN_SCHEMA:\n            logger.warning(\n                f\"Limiting choices for '{field_name}' from {len(processed_choices)} to {MAX_CHOICES_IN_SCHEMA}\"\n            )\n            if default_value is not None:\n                for val, lbl in processed_choices:\n                    if val == default_value:\n                        limited_choices.append((val, lbl))\n                        default_included = True\n                        break\n            remaining_slots = MAX_CHOICES_IN_SCHEMA - len(limited_choices)\n            if remaining_slots &gt; 0:\n                for val, lbl in processed_choices:\n                    if len(limited_choices) &gt;= MAX_CHOICES_IN_SCHEMA:\n                        break\n                    if not (default_included and val == default_value):\n                        limited_choices.append((val, lbl))\n            final_choices_list = limited_choices\n        else:\n            final_choices_list = processed_choices\n\n        # --- Store Choices --- #\n        kwargs.setdefault(\"json_schema_extra\", {})[\"choices\"] = final_choices_list\n        kwargs.pop(\"max_length\", None)  # Remove max_length if choices are present\n        logger.debug(f\"Stored final choices in json_schema_extra for '{field_name}'\")\n\n    except Exception as e:\n        logger.error(f\"Error processing or limiting choices for field '{field_name}': {e}\", exc_info=True)\n        kwargs.pop(\"json_schema_extra\", None)\n\n    finally:\n        # --- Deactivate Translation Context --- #\n        if active_translation:\n            try:\n                active_translation.__exit__(None, None, None)  # Manually exit context\n                logger.debug(f\"Deactivated translation override for '{field_name}'\")\n            except Exception as trans_exit_err:\n                logger.warning(f\"Error deactivating translation context for '{field_name}': {trans_exit_err}\")\n</code></pre>"},{"location":"reference/pydantic2django/core/mapping_units/#pydantic2django.core.mapping_units.TypeMappingUnit.matches","title":"<code>matches(py_type, field_info=None)</code>  <code>classmethod</code>","text":"<p>Calculate a score indicating how well this unit matches the given Python type and FieldInfo.</p> <p>Parameters:</p> Name Type Description Default <code>py_type</code> <code>Any</code> <p>The Python type to match against.</p> required <code>field_info</code> <code>Optional[FieldInfo]</code> <p>Optional Pydantic FieldInfo for context.</p> <code>None</code> <p>Returns:</p> Type Description <code>float</code> <p>A float score (0.0 = no match, higher = better match).</p> <code>float</code> <p>Base implementation scores:</p> <code>float</code> <ul> <li>1.0 for exact type match (cls.python_type == py_type)</li> </ul> <code>float</code> <ul> <li>0.5 for subclass match (issubclass(py_type, cls.python_type))</li> </ul> <code>float</code> <ul> <li>0.0 otherwise</li> </ul> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>@classmethod\ndef matches(cls, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; float:\n    \"\"\"\n    Calculate a score indicating how well this unit matches the given Python type and FieldInfo.\n\n    Args:\n        py_type: The Python type to match against.\n        field_info: Optional Pydantic FieldInfo for context.\n\n    Returns:\n        A float score (0.0 = no match, higher = better match).\n        Base implementation scores:\n        - 1.0 for exact type match (cls.python_type == py_type)\n        - 0.5 for subclass match (issubclass(py_type, cls.python_type))\n        - 0.0 otherwise\n    \"\"\"\n    target_py_type = cls.python_type\n    if py_type == target_py_type:\n        return 1.0\n    try:\n        # Check issubclass only if both are actual classes and py_type is not Any\n        if (\n            py_type is not Any\n            and inspect.isclass(py_type)\n            and inspect.isclass(target_py_type)\n            and issubclass(py_type, target_py_type)\n        ):\n            # Don't match if it's the same type (already handled by exact match)\n            if py_type is not target_py_type:\n                return 0.5\n    except TypeError:\n        # issubclass fails on non-classes (like Any, List[int], etc.)\n        pass\n    return 0.0\n</code></pre>"},{"location":"reference/pydantic2django/core/mapping_units/#pydantic2django.core.mapping_units.TypeMappingUnit.pydantic_to_django_kwargs","title":"<code>pydantic_to_django_kwargs(py_type, field_info=None)</code>","text":"<p>Generate Django field constructor kwargs from Pydantic FieldInfo.</p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>def pydantic_to_django_kwargs(self, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; dict[str, Any]:\n    \"\"\"Generate Django field constructor kwargs from Pydantic FieldInfo.\"\"\"\n    kwargs = {}\n    if field_info:\n        # Map common attributes\n        if field_info.title:\n            kwargs[\"verbose_name\"] = field_info.title\n        if field_info.description:\n            kwargs[\"help_text\"] = field_info.description\n\n        # Only consider `default` if `default_factory` is None\n        if field_info.default_factory is None:\n            if field_info.default is not PydanticUndefined and field_info.default is not None:\n                # Django doesn't handle callable defaults easily here\n                if not callable(field_info.default):\n                    kwargs[\"default\"] = field_info.default\n            elif field_info.default is None:  # Explicitly check for None default\n                kwargs[\"default\"] = None  # Add default=None if present in FieldInfo\n        # else: If default_factory is present, do not add a 'default' kwarg.\n        # No warning needed as this is now expected behavior.\n\n        # Note: Frozen, ge, le etc. are validation rules, map separately if needed\n    return kwargs\n</code></pre>"},{"location":"reference/pydantic2django/core/mapping_units/#pydantic2django.core.mapping_units.URLFieldMapping","title":"<code>URLFieldMapping</code>","text":"<p>               Bases: <code>StrFieldMapping</code></p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>class URLFieldMapping(StrFieldMapping):\n    python_type = HttpUrl\n    django_field_type = models.URLField\n\n    @classmethod\n    def matches(cls, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; float:\n        \"\"\"Strongly prefer HttpUrl.\"\"\"\n        if py_type == cls.python_type:\n            return 1.2  # Very high score for exact type\n        # Don't match plain str\n        return 0.0\n        # return super().matches(py_type, field_info) # Default base matching - Incorrect\n\n    # Add default max_length for URLField\n    def pydantic_to_django_kwargs(self, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; dict[str, Any]:\n        kwargs = super().pydantic_to_django_kwargs(py_type, field_info)\n        # Ensure default max_length for URLField if not specified via metadata\n        pyd_max_length = None\n        if field_info and hasattr(field_info, \"metadata\"):\n            for item in field_info.metadata:\n                if hasattr(item, \"max_length\"):\n                    pyd_max_length = item.max_length\n                    break\n        if pyd_max_length is None and \"max_length\" not in kwargs:\n            kwargs[\"max_length\"] = 200  # Default for URLField matching test\n        return kwargs\n\n    def django_to_pydantic_field_info_kwargs(self, dj_field: models.Field) -&gt; dict[str, Any]:\n        # Let super handle title/desc/max_length from Django field\n        return super().django_to_pydantic_field_info_kwargs(dj_field)\n</code></pre>"},{"location":"reference/pydantic2django/core/mapping_units/#pydantic2django.core.mapping_units.URLFieldMapping.matches","title":"<code>matches(py_type, field_info=None)</code>  <code>classmethod</code>","text":"<p>Strongly prefer HttpUrl.</p> Source code in <code>src/pydantic2django/core/mapping_units.py</code> <pre><code>@classmethod\ndef matches(cls, py_type: Any, field_info: Optional[FieldInfo] = None) -&gt; float:\n    \"\"\"Strongly prefer HttpUrl.\"\"\"\n    if py_type == cls.python_type:\n        return 1.2  # Very high score for exact type\n    # Don't match plain str\n    return 0.0\n</code></pre>"},{"location":"reference/pydantic2django/core/relationships/","title":"pydantic2django.core.relationships","text":""},{"location":"reference/pydantic2django/core/relationships/#pydantic2django.core.relationships.RelationshipConversionAccessor","title":"<code>RelationshipConversionAccessor</code>  <code>dataclass</code>","text":"Source code in <code>src/pydantic2django/core/relationships.py</code> <pre><code>@dataclass\nclass RelationshipConversionAccessor:\n    available_relationships: list[RelationshipMapper] = field(default_factory=list)\n    # dependencies: Optional[dict[str, set[str]]] = field(default=None) # Keep if used\n\n    @classmethod\n    def from_dict(cls, relationship_mapping_dict: dict) -&gt; \"RelationshipConversionAccessor\":\n        \"\"\"\n        Convert a dictionary of strings representing model qualified names to a RelationshipConversionAccessor\n\n        The dictionary should be of the form:\n        {\n            \"pydantic_model_qualified_name\": \"django_model_qualified_name\",\n            ...\n        }\n        \"\"\"\n        available_relationships = []\n        for pydantic_mqn, django_mqn in relationship_mapping_dict.items():\n            try:\n                # Split the module path and class name\n                pydantic_module_path, pydantic_class_name = pydantic_mqn.rsplit(\".\", 1)\n                django_module_path, django_class_name = django_mqn.rsplit(\".\", 1)\n\n                # Import the modules\n                pydantic_module = importlib.import_module(pydantic_module_path)\n                django_module = importlib.import_module(django_module_path)\n\n                # Get the actual class objects\n                pydantic_model = getattr(pydantic_module, pydantic_class_name)\n                django_model = getattr(django_module, django_class_name)\n\n                available_relationships.append(RelationshipMapper(pydantic_model, django_model, context=None))\n            except Exception as e:\n                logger.warning(f\"Error importing model {pydantic_mqn} or {django_mqn}: {e}\")\n                continue\n        return cls(available_relationships)\n\n    def to_dict(self) -&gt; dict:\n        \"\"\"\n        Convert the relationships to a dictionary of strings representing\n        model qualified names for bidirectional conversion.\n\n        Can be stored in a JSON field, and used to reconstruct the relationships.\n        \"\"\"\n        relationship_mapping_dict = {}\n        for relationship in self.available_relationships:\n            # Skip relationships where either model is None\n            if relationship.pydantic_model is None or relationship.django_model is None:\n                continue\n\n            pydantic_mqn = self._get_pydantic_model_qualified_name(relationship.pydantic_model)\n            django_mqn = self._get_django_model_qualified_name(relationship.django_model)\n            relationship_mapping_dict[pydantic_mqn] = django_mqn\n\n        return relationship_mapping_dict\n\n    def _get_pydantic_model_qualified_name(self, model: type[BaseModel] | None) -&gt; str:\n        \"\"\"Get the fully qualified name of a Pydantic model as module.class_name\"\"\"\n        if model is None:\n            return \"\"\n        return f\"{model.__module__}.{model.__name__}\"\n\n    def _get_django_model_qualified_name(self, model: type[models.Model] | None) -&gt; str:\n        \"\"\"Get the fully qualified name of a Django model as app_label.model_name\"\"\"\n        if model is None:\n            return \"\"\n        return f\"{model._meta.app_label}.{model.__name__}\"\n\n    @property\n    def available_source_models(self) -&gt; list[type]:\n        \"\"\"Get a list of all source models (Pydantic or Dataclass).\"\"\"\n        models = []\n        for r in self.available_relationships:\n            if r.pydantic_model:\n                models.append(r.pydantic_model)\n            if r.dataclass_model:\n                models.append(r.dataclass_model)\n        return models\n\n    @property\n    def available_django_models(self) -&gt; list[type[models.Model]]:\n        \"\"\"Get a list of all Django models in the relationship accessor\"\"\"\n        return [r.django_model for r in self.available_relationships if r.django_model is not None]\n\n    def add_pydantic_model(self, model: type[BaseModel]) -&gt; None:\n        \"\"\"Add a Pydantic model to the relationship accessor\"\"\"\n        # Check if the model is already in available_pydantic_models by comparing class names\n        model_name = model.__name__\n        existing_models = [m.__name__ for m in self.available_source_models]\n\n        if model_name not in existing_models:\n            self.available_relationships.append(RelationshipMapper(model, None, context=None))\n\n    def add_dataclass_model(self, model: type) -&gt; None:\n        \"\"\"Add a Dataclass model to the relationship accessor\"\"\"\n        # Check if the model is already mapped\n        if any(r.dataclass_model == model for r in self.available_relationships):\n            return  # Already exists\n        # Check if a Pydantic model with the same name is already mapped (potential conflict)\n        if any(r.pydantic_model and r.pydantic_model.__name__ == model.__name__ for r in self.available_relationships):\n            logger.warning(f\"Adding dataclass {model.__name__}, but a Pydantic model with the same name exists.\")\n\n        self.available_relationships.append(RelationshipMapper(dataclass_model=model))\n\n    def add_django_model(self, model: type[models.Model]) -&gt; None:\n        \"\"\"Add a Django model to the relationship accessor\"\"\"\n        # Check if the model is already in available_django_models by comparing class names\n        model_name = model.__name__\n        existing_models = [m.__name__ for m in self.available_django_models]\n\n        if model_name not in existing_models:\n            self.available_relationships.append(RelationshipMapper(None, None, model, context=None))\n\n    def get_django_model_for_pydantic(self, pydantic_model: type[BaseModel]) -&gt; Optional[type[models.Model]]:\n        \"\"\"\n        Find the corresponding Django model for a given Pydantic model\n\n        Returns None if no matching Django model is found\n        \"\"\"\n        for relationship in self.available_relationships:\n            if relationship.pydantic_model == pydantic_model and relationship.django_model is not None:\n                return relationship.django_model\n        return None\n\n    def get_pydantic_model_for_django(self, django_model: type[models.Model]) -&gt; Optional[type[BaseModel]]:\n        \"\"\"\n        Find the corresponding Pydantic model for a given Django model\n\n        Returns None if no matching Pydantic model is found\n        \"\"\"\n        for relationship in self.available_relationships:\n            if relationship.django_model == django_model and relationship.pydantic_model is not None:\n                return relationship.pydantic_model\n        return None\n\n    def get_django_model_for_dataclass(self, dataclass_model: type) -&gt; Optional[type[models.Model]]:\n        \"\"\"Find the corresponding Django model for a given Dataclass model.\"\"\"\n        logger.debug(f\"Searching for Django model matching dataclass: {dataclass_model.__name__}\")\n        for relationship in self.available_relationships:\n            # Check if this mapper holds the target dataclass and has a linked Django model\n            if relationship.dataclass_model == dataclass_model and relationship.django_model is not None:\n                logger.debug(f\"  Found match: {relationship.django_model.__name__}\")\n                return relationship.django_model\n        logger.debug(f\"  No match found for dataclass {dataclass_model.__name__}\")\n        return None\n\n    def map_relationship(self, source_model: type, django_model: type[models.Model]) -&gt; None:\n        \"\"\"\n        Create or update a mapping between a source model (Pydantic/Dataclass) and a Django model.\n        \"\"\"\n        source_type = (\n            \"pydantic\"\n            if isinstance(source_model, type) and issubclass(source_model, BaseModel)\n            else \"dataclass\"\n            if dataclasses.is_dataclass(source_model)\n            else \"unknown\"\n        )\n\n        if source_type == \"unknown\":\n            logger.warning(f\"Cannot map relationship for unknown source type: {source_model}\")\n            return\n\n        # Check if either model already exists in a relationship\n        for relationship in self.available_relationships:\n            if source_type == \"pydantic\" and relationship.pydantic_model == source_model:\n                relationship.django_model = django_model\n                # Ensure dataclass_model is None if we map pydantic\n                relationship.dataclass_model = None\n                logger.debug(f\"Updated mapping: Pydantic {source_model.__name__} -&gt; Django {django_model.__name__}\")\n                return\n            if source_type == \"dataclass\" and relationship.dataclass_model == source_model:\n                relationship.django_model = django_model\n                # Ensure pydantic_model is None\n                relationship.pydantic_model = None\n                logger.debug(f\"Updated mapping: Dataclass {source_model.__name__} -&gt; Django {django_model.__name__}\")\n                return\n            if relationship.django_model == django_model:\n                # Map the source model based on its type\n                if source_type == \"pydantic\":\n                    relationship.pydantic_model = cast(type[BaseModel], source_model)\n                    relationship.dataclass_model = None\n                    logger.debug(\n                        f\"Updated mapping: Pydantic {source_model.__name__} -&gt; Django {django_model.__name__} (found via Django model)\"\n                    )\n                elif source_type == \"dataclass\":\n                    relationship.dataclass_model = cast(type, source_model)\n                    relationship.pydantic_model = None\n                    logger.debug(\n                        f\"Updated mapping: Dataclass {source_model.__name__} -&gt; Django {django_model.__name__} (found via Django model)\"\n                    )\n                return\n\n        # If no existing relationship found, create a new one\n        logger.debug(\n            f\"Creating new mapping: {source_type.capitalize()} {source_model.__name__} -&gt; Django {django_model.__name__}\"\n        )\n        if source_type == \"pydantic\":\n            self.available_relationships.append(\n                RelationshipMapper(pydantic_model=cast(type[BaseModel], source_model), django_model=django_model)\n            )\n        elif source_type == \"dataclass\":\n            self.available_relationships.append(\n                RelationshipMapper(dataclass_model=cast(type, source_model), django_model=django_model)\n            )\n\n    def is_source_model_known(self, model: type) -&gt; bool:\n        \"\"\"Check if a specific source model (Pydantic or Dataclass) is known.\"\"\"\n        is_pydantic = isinstance(model, type) and issubclass(model, BaseModel)\n        is_dataclass = dataclasses.is_dataclass(model)\n\n        for r in self.available_relationships:\n            if is_pydantic and r.pydantic_model == model:\n                return True\n            if is_dataclass and r.dataclass_model == model:\n                return True\n        return False\n\n    # Add a method to lookup source type by name\n    def get_source_model_by_name(self, model_name: str) -&gt; Optional[type]:\n        \"\"\"Find a known source model (Pydantic or Dataclass) by its class name.\"\"\"\n        for r in self.available_relationships:\n            if r.pydantic_model and r.pydantic_model.__name__ == model_name:\n                return r.pydantic_model\n            if r.dataclass_model and r.dataclass_model.__name__ == model_name:\n                return r.dataclass_model\n        return None\n</code></pre>"},{"location":"reference/pydantic2django/core/relationships/#pydantic2django.core.relationships.RelationshipConversionAccessor.available_django_models","title":"<code>available_django_models</code>  <code>property</code>","text":"<p>Get a list of all Django models in the relationship accessor</p>"},{"location":"reference/pydantic2django/core/relationships/#pydantic2django.core.relationships.RelationshipConversionAccessor.available_source_models","title":"<code>available_source_models</code>  <code>property</code>","text":"<p>Get a list of all source models (Pydantic or Dataclass).</p>"},{"location":"reference/pydantic2django/core/relationships/#pydantic2django.core.relationships.RelationshipConversionAccessor.add_dataclass_model","title":"<code>add_dataclass_model(model)</code>","text":"<p>Add a Dataclass model to the relationship accessor</p> Source code in <code>src/pydantic2django/core/relationships.py</code> <pre><code>def add_dataclass_model(self, model: type) -&gt; None:\n    \"\"\"Add a Dataclass model to the relationship accessor\"\"\"\n    # Check if the model is already mapped\n    if any(r.dataclass_model == model for r in self.available_relationships):\n        return  # Already exists\n    # Check if a Pydantic model with the same name is already mapped (potential conflict)\n    if any(r.pydantic_model and r.pydantic_model.__name__ == model.__name__ for r in self.available_relationships):\n        logger.warning(f\"Adding dataclass {model.__name__}, but a Pydantic model with the same name exists.\")\n\n    self.available_relationships.append(RelationshipMapper(dataclass_model=model))\n</code></pre>"},{"location":"reference/pydantic2django/core/relationships/#pydantic2django.core.relationships.RelationshipConversionAccessor.add_django_model","title":"<code>add_django_model(model)</code>","text":"<p>Add a Django model to the relationship accessor</p> Source code in <code>src/pydantic2django/core/relationships.py</code> <pre><code>def add_django_model(self, model: type[models.Model]) -&gt; None:\n    \"\"\"Add a Django model to the relationship accessor\"\"\"\n    # Check if the model is already in available_django_models by comparing class names\n    model_name = model.__name__\n    existing_models = [m.__name__ for m in self.available_django_models]\n\n    if model_name not in existing_models:\n        self.available_relationships.append(RelationshipMapper(None, None, model, context=None))\n</code></pre>"},{"location":"reference/pydantic2django/core/relationships/#pydantic2django.core.relationships.RelationshipConversionAccessor.add_pydantic_model","title":"<code>add_pydantic_model(model)</code>","text":"<p>Add a Pydantic model to the relationship accessor</p> Source code in <code>src/pydantic2django/core/relationships.py</code> <pre><code>def add_pydantic_model(self, model: type[BaseModel]) -&gt; None:\n    \"\"\"Add a Pydantic model to the relationship accessor\"\"\"\n    # Check if the model is already in available_pydantic_models by comparing class names\n    model_name = model.__name__\n    existing_models = [m.__name__ for m in self.available_source_models]\n\n    if model_name not in existing_models:\n        self.available_relationships.append(RelationshipMapper(model, None, context=None))\n</code></pre>"},{"location":"reference/pydantic2django/core/relationships/#pydantic2django.core.relationships.RelationshipConversionAccessor.from_dict","title":"<code>from_dict(relationship_mapping_dict)</code>  <code>classmethod</code>","text":"<p>Convert a dictionary of strings representing model qualified names to a RelationshipConversionAccessor</p> <p>The dictionary should be of the form: {     \"pydantic_model_qualified_name\": \"django_model_qualified_name\",     ... }</p> Source code in <code>src/pydantic2django/core/relationships.py</code> <pre><code>@classmethod\ndef from_dict(cls, relationship_mapping_dict: dict) -&gt; \"RelationshipConversionAccessor\":\n    \"\"\"\n    Convert a dictionary of strings representing model qualified names to a RelationshipConversionAccessor\n\n    The dictionary should be of the form:\n    {\n        \"pydantic_model_qualified_name\": \"django_model_qualified_name\",\n        ...\n    }\n    \"\"\"\n    available_relationships = []\n    for pydantic_mqn, django_mqn in relationship_mapping_dict.items():\n        try:\n            # Split the module path and class name\n            pydantic_module_path, pydantic_class_name = pydantic_mqn.rsplit(\".\", 1)\n            django_module_path, django_class_name = django_mqn.rsplit(\".\", 1)\n\n            # Import the modules\n            pydantic_module = importlib.import_module(pydantic_module_path)\n            django_module = importlib.import_module(django_module_path)\n\n            # Get the actual class objects\n            pydantic_model = getattr(pydantic_module, pydantic_class_name)\n            django_model = getattr(django_module, django_class_name)\n\n            available_relationships.append(RelationshipMapper(pydantic_model, django_model, context=None))\n        except Exception as e:\n            logger.warning(f\"Error importing model {pydantic_mqn} or {django_mqn}: {e}\")\n            continue\n    return cls(available_relationships)\n</code></pre>"},{"location":"reference/pydantic2django/core/relationships/#pydantic2django.core.relationships.RelationshipConversionAccessor.get_django_model_for_dataclass","title":"<code>get_django_model_for_dataclass(dataclass_model)</code>","text":"<p>Find the corresponding Django model for a given Dataclass model.</p> Source code in <code>src/pydantic2django/core/relationships.py</code> <pre><code>def get_django_model_for_dataclass(self, dataclass_model: type) -&gt; Optional[type[models.Model]]:\n    \"\"\"Find the corresponding Django model for a given Dataclass model.\"\"\"\n    logger.debug(f\"Searching for Django model matching dataclass: {dataclass_model.__name__}\")\n    for relationship in self.available_relationships:\n        # Check if this mapper holds the target dataclass and has a linked Django model\n        if relationship.dataclass_model == dataclass_model and relationship.django_model is not None:\n            logger.debug(f\"  Found match: {relationship.django_model.__name__}\")\n            return relationship.django_model\n    logger.debug(f\"  No match found for dataclass {dataclass_model.__name__}\")\n    return None\n</code></pre>"},{"location":"reference/pydantic2django/core/relationships/#pydantic2django.core.relationships.RelationshipConversionAccessor.get_django_model_for_pydantic","title":"<code>get_django_model_for_pydantic(pydantic_model)</code>","text":"<p>Find the corresponding Django model for a given Pydantic model</p> <p>Returns None if no matching Django model is found</p> Source code in <code>src/pydantic2django/core/relationships.py</code> <pre><code>def get_django_model_for_pydantic(self, pydantic_model: type[BaseModel]) -&gt; Optional[type[models.Model]]:\n    \"\"\"\n    Find the corresponding Django model for a given Pydantic model\n\n    Returns None if no matching Django model is found\n    \"\"\"\n    for relationship in self.available_relationships:\n        if relationship.pydantic_model == pydantic_model and relationship.django_model is not None:\n            return relationship.django_model\n    return None\n</code></pre>"},{"location":"reference/pydantic2django/core/relationships/#pydantic2django.core.relationships.RelationshipConversionAccessor.get_pydantic_model_for_django","title":"<code>get_pydantic_model_for_django(django_model)</code>","text":"<p>Find the corresponding Pydantic model for a given Django model</p> <p>Returns None if no matching Pydantic model is found</p> Source code in <code>src/pydantic2django/core/relationships.py</code> <pre><code>def get_pydantic_model_for_django(self, django_model: type[models.Model]) -&gt; Optional[type[BaseModel]]:\n    \"\"\"\n    Find the corresponding Pydantic model for a given Django model\n\n    Returns None if no matching Pydantic model is found\n    \"\"\"\n    for relationship in self.available_relationships:\n        if relationship.django_model == django_model and relationship.pydantic_model is not None:\n            return relationship.pydantic_model\n    return None\n</code></pre>"},{"location":"reference/pydantic2django/core/relationships/#pydantic2django.core.relationships.RelationshipConversionAccessor.get_source_model_by_name","title":"<code>get_source_model_by_name(model_name)</code>","text":"<p>Find a known source model (Pydantic or Dataclass) by its class name.</p> Source code in <code>src/pydantic2django/core/relationships.py</code> <pre><code>def get_source_model_by_name(self, model_name: str) -&gt; Optional[type]:\n    \"\"\"Find a known source model (Pydantic or Dataclass) by its class name.\"\"\"\n    for r in self.available_relationships:\n        if r.pydantic_model and r.pydantic_model.__name__ == model_name:\n            return r.pydantic_model\n        if r.dataclass_model and r.dataclass_model.__name__ == model_name:\n            return r.dataclass_model\n    return None\n</code></pre>"},{"location":"reference/pydantic2django/core/relationships/#pydantic2django.core.relationships.RelationshipConversionAccessor.is_source_model_known","title":"<code>is_source_model_known(model)</code>","text":"<p>Check if a specific source model (Pydantic or Dataclass) is known.</p> Source code in <code>src/pydantic2django/core/relationships.py</code> <pre><code>def is_source_model_known(self, model: type) -&gt; bool:\n    \"\"\"Check if a specific source model (Pydantic or Dataclass) is known.\"\"\"\n    is_pydantic = isinstance(model, type) and issubclass(model, BaseModel)\n    is_dataclass = dataclasses.is_dataclass(model)\n\n    for r in self.available_relationships:\n        if is_pydantic and r.pydantic_model == model:\n            return True\n        if is_dataclass and r.dataclass_model == model:\n            return True\n    return False\n</code></pre>"},{"location":"reference/pydantic2django/core/relationships/#pydantic2django.core.relationships.RelationshipConversionAccessor.map_relationship","title":"<code>map_relationship(source_model, django_model)</code>","text":"<p>Create or update a mapping between a source model (Pydantic/Dataclass) and a Django model.</p> Source code in <code>src/pydantic2django/core/relationships.py</code> <pre><code>def map_relationship(self, source_model: type, django_model: type[models.Model]) -&gt; None:\n    \"\"\"\n    Create or update a mapping between a source model (Pydantic/Dataclass) and a Django model.\n    \"\"\"\n    source_type = (\n        \"pydantic\"\n        if isinstance(source_model, type) and issubclass(source_model, BaseModel)\n        else \"dataclass\"\n        if dataclasses.is_dataclass(source_model)\n        else \"unknown\"\n    )\n\n    if source_type == \"unknown\":\n        logger.warning(f\"Cannot map relationship for unknown source type: {source_model}\")\n        return\n\n    # Check if either model already exists in a relationship\n    for relationship in self.available_relationships:\n        if source_type == \"pydantic\" and relationship.pydantic_model == source_model:\n            relationship.django_model = django_model\n            # Ensure dataclass_model is None if we map pydantic\n            relationship.dataclass_model = None\n            logger.debug(f\"Updated mapping: Pydantic {source_model.__name__} -&gt; Django {django_model.__name__}\")\n            return\n        if source_type == \"dataclass\" and relationship.dataclass_model == source_model:\n            relationship.django_model = django_model\n            # Ensure pydantic_model is None\n            relationship.pydantic_model = None\n            logger.debug(f\"Updated mapping: Dataclass {source_model.__name__} -&gt; Django {django_model.__name__}\")\n            return\n        if relationship.django_model == django_model:\n            # Map the source model based on its type\n            if source_type == \"pydantic\":\n                relationship.pydantic_model = cast(type[BaseModel], source_model)\n                relationship.dataclass_model = None\n                logger.debug(\n                    f\"Updated mapping: Pydantic {source_model.__name__} -&gt; Django {django_model.__name__} (found via Django model)\"\n                )\n            elif source_type == \"dataclass\":\n                relationship.dataclass_model = cast(type, source_model)\n                relationship.pydantic_model = None\n                logger.debug(\n                    f\"Updated mapping: Dataclass {source_model.__name__} -&gt; Django {django_model.__name__} (found via Django model)\"\n                )\n            return\n\n    # If no existing relationship found, create a new one\n    logger.debug(\n        f\"Creating new mapping: {source_type.capitalize()} {source_model.__name__} -&gt; Django {django_model.__name__}\"\n    )\n    if source_type == \"pydantic\":\n        self.available_relationships.append(\n            RelationshipMapper(pydantic_model=cast(type[BaseModel], source_model), django_model=django_model)\n        )\n    elif source_type == \"dataclass\":\n        self.available_relationships.append(\n            RelationshipMapper(dataclass_model=cast(type, source_model), django_model=django_model)\n        )\n</code></pre>"},{"location":"reference/pydantic2django/core/relationships/#pydantic2django.core.relationships.RelationshipConversionAccessor.to_dict","title":"<code>to_dict()</code>","text":"<p>Convert the relationships to a dictionary of strings representing model qualified names for bidirectional conversion.</p> <p>Can be stored in a JSON field, and used to reconstruct the relationships.</p> Source code in <code>src/pydantic2django/core/relationships.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"\n    Convert the relationships to a dictionary of strings representing\n    model qualified names for bidirectional conversion.\n\n    Can be stored in a JSON field, and used to reconstruct the relationships.\n    \"\"\"\n    relationship_mapping_dict = {}\n    for relationship in self.available_relationships:\n        # Skip relationships where either model is None\n        if relationship.pydantic_model is None or relationship.django_model is None:\n            continue\n\n        pydantic_mqn = self._get_pydantic_model_qualified_name(relationship.pydantic_model)\n        django_mqn = self._get_django_model_qualified_name(relationship.django_model)\n        relationship_mapping_dict[pydantic_mqn] = django_mqn\n\n    return relationship_mapping_dict\n</code></pre>"},{"location":"reference/pydantic2django/core/relationships/#pydantic2django.core.relationships.RelationshipMapper","title":"<code>RelationshipMapper</code>  <code>dataclass</code>","text":"<p>Bidirectional mapper between source models (Pydantic/Dataclass) and Django models.</p> Source code in <code>src/pydantic2django/core/relationships.py</code> <pre><code>@dataclass\nclass RelationshipMapper:\n    \"\"\"\n    Bidirectional mapper between source models (Pydantic/Dataclass) and Django models.\n    \"\"\"\n\n    # Allow storing either source type\n    pydantic_model: Optional[type[BaseModel]] = None\n    dataclass_model: Optional[type] = None\n    django_model: Optional[type[models.Model]] = None\n    context: Optional[ModelContext] = None  # Keep context if needed later\n\n    @property\n    def source_model(self) -&gt; Optional[type]:\n        \"\"\"Return the source model (either Pydantic or Dataclass).\"\"\"\n        return self.pydantic_model or self.dataclass_model\n</code></pre>"},{"location":"reference/pydantic2django/core/relationships/#pydantic2django.core.relationships.RelationshipMapper.source_model","title":"<code>source_model</code>  <code>property</code>","text":"<p>Return the source model (either Pydantic or Dataclass).</p>"},{"location":"reference/pydantic2django/core/serialization/","title":"pydantic2django.core.serialization","text":""},{"location":"reference/pydantic2django/core/serialization/#pydantic2django.core.serialization.SerializationMethod","title":"<code>SerializationMethod</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration of available serialization methods.</p> Source code in <code>src/pydantic2django/core/serialization.py</code> <pre><code>class SerializationMethod(Enum):\n    \"\"\"Enumeration of available serialization methods.\"\"\"\n\n    MODEL_DUMP = \"model_dump\"  # Pydantic's model_dump\n    TO_JSON = \"to_json\"  # Custom to_json method\n    TO_DICT = \"to_dict\"  # Custom to_dict method\n    STR = \"str\"  # __str__ method\n    DICT = \"dict\"  # __dict__ attribute\n    NONE = \"none\"  # No serialization method found\n</code></pre>"},{"location":"reference/pydantic2django/core/serialization/#pydantic2django.core.serialization.get_serialization_method","title":"<code>get_serialization_method(obj)</code>","text":"<p>Get the appropriate serialization method for an object.</p> <p>This function checks for various serialization methods in order of preference: 1. model_dump (Pydantic) 2. to_json 3. to_dict 4. str (if overridden) 5. dict</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>The object to check for serialization methods</p> required <p>Returns:</p> Type Description <code>tuple[SerializationMethod, Optional[Callable[[], Any]]]</code> <p>A tuple of (SerializationMethod, Optional[Callable]). The callable is None if no method is found.</p> Source code in <code>src/pydantic2django/core/serialization.py</code> <pre><code>def get_serialization_method(\n    obj: Any,\n) -&gt; tuple[SerializationMethod, Optional[Callable[[], Any]]]:\n    \"\"\"\n    Get the appropriate serialization method for an object.\n\n    This function checks for various serialization methods in order of preference:\n    1. model_dump (Pydantic)\n    2. to_json\n    3. to_dict\n    4. __str__ (if overridden)\n    5. __dict__\n\n    Args:\n        obj: The object to check for serialization methods\n\n    Returns:\n        A tuple of (SerializationMethod, Optional[Callable]). The callable is None if no method is found.\n    \"\"\"\n    # Check for Pydantic model_dump\n    if isinstance(obj, BaseModel):\n        return SerializationMethod.MODEL_DUMP, obj.model_dump\n\n    # Check for to_json method\n    if hasattr(obj, \"to_json\"):\n        return SerializationMethod.TO_JSON, obj.to_json\n\n    # Check for to_dict method\n    if hasattr(obj, \"to_dict\"):\n        return SerializationMethod.TO_DICT, obj.to_dict\n\n    # Check for overridden __str__ method\n    if hasattr(obj, \"__str__\") and obj.__class__.__str__ is not object.__str__:\n        return SerializationMethod.STR, obj.__str__\n\n    # Check for __dict__ attribute\n    if hasattr(obj, \"__dict__\"):\n\n        def dict_serializer():\n            return {\n                \"__class__\": obj.__class__.__name__,\n                \"__module__\": obj.__class__.__module__,\n                \"data\": obj.__dict__,\n            }\n\n        return SerializationMethod.DICT, dict_serializer\n\n    return SerializationMethod.NONE, None\n</code></pre>"},{"location":"reference/pydantic2django/core/serialization/#pydantic2django.core.serialization.is_serializable","title":"<code>is_serializable(obj)</code>","text":"<p>Check if an object is serializable.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>The object to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the object has a valid serialization method, False otherwise</p> Source code in <code>src/pydantic2django/core/serialization.py</code> <pre><code>def is_serializable(obj: Any) -&gt; bool:\n    \"\"\"\n    Check if an object is serializable.\n\n    Args:\n        obj: The object to check\n\n    Returns:\n        True if the object has a valid serialization method, False otherwise\n    \"\"\"\n    method, _ = get_serialization_method(obj)\n    return method != SerializationMethod.NONE\n</code></pre>"},{"location":"reference/pydantic2django/core/serialization/#pydantic2django.core.serialization.serialize_value","title":"<code>serialize_value(value)</code>","text":"<p>Serialize a value using the most appropriate method.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>The value to serialize</p> required <p>Returns:</p> Type Description <code>Any</code> <p>The serialized value</p> Source code in <code>src/pydantic2django/core/serialization.py</code> <pre><code>def serialize_value(value: Any) -&gt; Any:\n    \"\"\"\n    Serialize a value using the most appropriate method.\n\n    Args:\n        value: The value to serialize\n\n    Returns:\n        The serialized value\n    \"\"\"\n    method, serializer = get_serialization_method(value)\n\n    if method == SerializationMethod.NONE:\n        # If no serialization method is found, return the value as is\n        return value\n\n    if serializer is None:\n        return value\n\n    try:\n        return serializer()\n    except Exception:\n        # If serialization fails, return the string representation\n        return str(value)\n</code></pre>"},{"location":"reference/pydantic2django/core/typing/","title":"pydantic2django.core.typing","text":""},{"location":"reference/pydantic2django/core/typing/#pydantic2django.core.typing.TypeHandler","title":"<code>TypeHandler</code>","text":"Source code in <code>src/pydantic2django/core/typing.py</code> <pre><code>class TypeHandler:\n    PATTERNS = {\n        \"angle_bracket_class\": re.compile(r\"&lt;class '([^']+)'&gt;\"),\n    }\n\n    @staticmethod\n    def _add_import(imports: dict[str, list[str]], module: str, name: str):\n        \"\"\"Safely add an import to the dictionary.\"\"\"\n        if not module or module == \"builtins\":\n            return\n        # Avoid adding the module itself if name matches module (e.g., import datetime)\n        # if name == module.split('.')[-1]:\n        #     name = module # This logic might be too simplistic, revert for now\n        current_names = imports.setdefault(module, [])\n        if name not in current_names:\n            current_names.append(name)\n\n    @staticmethod\n    def _merge_imports(dict1: dict, dict2: dict) -&gt; dict:\n        \"\"\"Merge two import dictionaries.\"\"\"\n        merged = dict1.copy()\n        for module, names in dict2.items():\n            current_names = merged.setdefault(module, [])\n            for name in names:\n                if name not in current_names:\n                    current_names.append(name)\n        # Sort names within each module for consistency\n        for module in merged:\n            merged[module].sort()\n        return merged\n\n    @staticmethod\n    def get_class_name(type_obj: Any) -&gt; str:\n        \"\"\"Extract a simple, usable class name from a type object.\"\"\"\n        origin = get_origin(type_obj)\n        args = get_args(type_obj)\n\n        # Check for Optional[T] specifically first (Union[T, NoneType])\n        if origin in (Union, UnionType) and len(args) == 2 and type(None) in args:\n            return \"Optional\"\n\n        if origin:\n            # Now check for other origins\n            if origin in (Union, UnionType):  # Handles Union[A, B, ...]\n                return \"Union\"\n            if origin is list:\n                return \"List\"  # Use capital L consistently\n            if origin is dict:\n                return \"Dict\"  # Use capital D consistently\n            if origin is tuple:\n                return \"Tuple\"  # Use capital T consistently\n            if origin is set:\n                return \"Set\"  # Use capital S consistently\n            if origin is Callable:\n                return \"Callable\"\n            if origin is type:\n                return \"Type\"\n            # Fallback for other generic types\n            return getattr(origin, \"__name__\", str(origin))\n\n        # Handle non-generic types\n        if hasattr(type_obj, \"__name__\"):\n            return type_obj.__name__\n\n        type_str = str(type_obj)\n        match = TypeHandler.PATTERNS[\"angle_bracket_class\"].match(type_str)\n        if match:\n            return match.group(1).split(\".\")[-1]\n\n        return str(type_obj)\n\n    @staticmethod\n    def get_required_imports(type_obj: Any) -&gt; dict[str, list[str]]:\n        \"\"\"Determine necessary imports by traversing a type object.\"\"\"\n        imports: dict[str, list[str]] = {}\n        processed_types = set()\n\n        # Define modules for known Pydantic types that might need explicit import\n        pydantic_module_map = {\n            \"EmailStr\": \"pydantic\",\n            \"IPvAnyAddress\": \"pydantic\",\n            \"Json\": \"pydantic\",\n            \"BaseModel\": \"pydantic\",\n            # Add others if needed (e.g., SecretStr, UrlStr)\n        }\n\n        def _traverse(current_type: Any):\n            nonlocal imports\n            try:\n                type_repr = repr(current_type)\n                if type_repr in processed_types:\n                    return\n                processed_types.add(type_repr)\n            except TypeError:\n                # Handle unhashable types if necessary, e.g., log a warning\n                pass\n\n            origin = get_origin(current_type)\n            args = get_args(current_type)\n\n            if origin:\n                # Handle Generic Alias (List, Dict, Union, Optional, Callable, Type)\n                origin_module = getattr(origin, \"__module__\", \"\")\n                origin_name = getattr(origin, \"__name__\", \"\")\n\n                # Determine the canonical name used in 'typing' imports (e.g., List, Dict, Callable)\n                typing_name = None\n                if origin is list:\n                    typing_name = \"List\"\n                elif origin is dict:\n                    typing_name = \"Dict\"\n                elif origin is tuple:\n                    typing_name = \"Tuple\"\n                elif origin is set:\n                    typing_name = \"Set\"\n                elif origin in (Union, UnionType):  # Handle types.UnionType for Python 3.10+\n                    # We don't need to add Union or Optional imports anymore with | syntax\n                    typing_name = None\n                elif origin is type:\n                    typing_name = \"Type\"\n                # Check both typing.Callable and collections.abc.Callable\n                elif origin_module == \"typing\" and origin_name == \"Callable\":\n                    typing_name = \"Callable\"\n                elif origin_module == \"collections.abc\" and origin_name == \"Callable\":\n                    typing_name = \"Callable\"\n                # Add more specific checks if needed (e.g., Sequence, Mapping)\n\n                # Add import if we identified a standard typing construct\n                if typing_name:\n                    TypeHandler._add_import(imports, \"typing\", typing_name)\n\n                # Traverse arguments regardless of origin's module\n                for arg in args:\n                    if arg is not type(None):  # Skip NoneType in Optional/Union\n                        if isinstance(arg, TypeVar):\n                            # Handle TypeVar by traversing its constraints/bound\n                            constraints = getattr(arg, \"__constraints__\", ())\n                            bound = getattr(arg, \"__bound__\", None)\n                            if bound:\n                                _traverse(bound)\n                            for constraint in constraints:\n                                _traverse(constraint)\n                        else:\n                            _traverse(arg)  # Recursively traverse arguments\n            # Handle Base Types or Classes (int, str, MyClass, etc.)\n            elif isinstance(current_type, type):\n                module_name = getattr(current_type, \"__module__\", \"\")\n                type_name = getattr(current_type, \"__name__\", \"\")\n\n                if not type_name or module_name == \"builtins\":\n                    pass  # Skip builtins or types without names\n                elif module_name == \"typing\" and type_name not in (\"NoneType\", \"Generic\"):\n                    # Catch Any, etc. used directly\n                    TypeHandler._add_import(imports, \"typing\", type_name)\n                # Check for dataclasses and Pydantic models specifically\n                elif is_dataclass(current_type) or (\n                    inspect.isclass(current_type) and issubclass(current_type, BaseModel)\n                ):\n                    actual_module = inspect.getmodule(current_type)\n                    if actual_module and actual_module.__name__ != \"__main__\":\n                        TypeHandler._add_import(imports, actual_module.__name__, type_name)\n                    # Add specific imports if needed (e.g., dataclasses.dataclass, pydantic.BaseModel)\n                    if is_dataclass(current_type):\n                        TypeHandler._add_import(imports, \"dataclasses\", \"dataclass\")\n                    # No need to add BaseModel here usually, handled by pydantic_module_map or direct usage\n                elif module_name:\n                    # Handle known standard library modules explicitly\n                    known_stdlib = {\"datetime\", \"decimal\", \"uuid\", \"pathlib\"}\n                    if module_name in known_stdlib:\n                        TypeHandler._add_import(imports, module_name, type_name)\n                    # Handle known Pydantic types explicitly (redundant with BaseModel check?)\n                    elif type_name in pydantic_module_map:\n                        TypeHandler._add_import(imports, pydantic_module_map[type_name], type_name)\n                    # Assume other types defined in modules need importing\n                    elif module_name != \"__main__\":  # Avoid importing from main script context\n                        TypeHandler._add_import(imports, module_name, type_name)\n\n            elif current_type is Any:\n                TypeHandler._add_import(imports, \"typing\", \"Any\")\n            elif isinstance(current_type, TypeVar):\n                # Handle TypeVar used directly\n                constraints = getattr(current_type, \"__constraints__\", ())\n                bound = getattr(current_type, \"__bound__\", None)\n                if bound:\n                    _traverse(bound)\n                for c in constraints:\n                    _traverse(c)\n            # Consider adding ForwardRef handling if needed:\n            # elif isinstance(current_type, typing.ForwardRef):\n            #     # Potentially add logic to resolve/import forward refs\n            #     pass\n\n        _traverse(type_obj)\n\n        # Clean up imports (unique, sorted)\n        final_imports = {}\n        for module, names in imports.items():\n            unique_names = sorted(set(names))\n            if unique_names:\n                final_imports[module] = unique_names\n        return final_imports\n\n    @staticmethod\n    def process_field_type(field_type: Any) -&gt; dict[str, Any]:\n        \"\"\"Process a field type to get name, flags, imports, and contained dataclasses.\"\"\"\n        logger.debug(f\"[TypeHandler] Processing type: {field_type!r}\")\n        is_optional = False\n        is_list = False\n        metadata: tuple[Any, ...] | None = None  # Initialize metadata with type hint\n        imports = set()\n        contained_dataclasses = set()\n        current_type = field_type  # Keep track of the potentially unwrapped type\n\n        # Helper function (remains the same)\n        def _is_potential_dataclass(t: Any) -&gt; bool:\n            return inspect.isclass(t) and is_dataclass(t)\n\n        def _find_contained_dataclasses(current_type: Any):\n            origin = get_origin(current_type)\n            args = get_args(current_type)\n            if origin:\n                for arg in args:\n                    if arg is not type(None):\n                        _find_contained_dataclasses(arg)\n            elif _is_potential_dataclass(current_type):\n                contained_dataclasses.add(current_type)\n\n        _find_contained_dataclasses(field_type)\n        if contained_dataclasses:\n            logger.debug(f\"  Found potential contained dataclasses: {[dc.__name__ for dc in contained_dataclasses]}\")\n\n        # --- Simplification Loop ---\n        # Repeatedly unwrap until we hit a base type or Any\n        processed = True\n        while processed:\n            processed = False\n            origin = get_origin(current_type)\n            args = get_args(current_type)\n\n            # 0. Unwrap Annotated[T, ...]\n            # Check if the origin exists and has the name 'Annotated'\n            # This check is more robust than `origin is Annotated` across Python versions\n            if origin is Annotated:\n                if args:\n                    core_type = args[0]\n                    metadata = args[1:]\n                    current_type = core_type\n                    logger.debug(f\"  Unwrapped Annotated, current type: {current_type!r}, metadata: {metadata!r}\")\n                    processed = True\n                    continue  # Restart loop with unwrapped type\n                else:\n                    logger.warning(\"  Found Annotated without arguments? Treating as Any.\")\n                    current_type = Any\n                    processed = True\n                    continue\n\n            # 1. Unwrap Optional[T] (Union[T, NoneType])\n            if origin in (Union, UnionType) and type(None) in args:\n                is_optional = True  # Flag it\n                # Rebuild the Union without NoneType\n                non_none_args = tuple(arg for arg in args if arg is not type(None))\n                if len(non_none_args) == 1:\n                    current_type = non_none_args[0]  # Simplify Union[T, None] to T\n                elif len(non_none_args) &gt; 1:\n                    # Use UnionType to rebuild\n                    current_type = reduce(lambda x, y: x | y, non_none_args)\n                else:  # pragma: no cover\n                    # Should not happen if NoneType was in args\n                    current_type = Any\n                logger.debug(f\"  Unwrapped Union with None, current type: {current_type!r}\")\n                processed = True\n                continue  # Restart loop with the non-optional type\n\n            # 2. Unwrap List[T] or Sequence[T]\n            if origin in (list, Sequence):\n                is_list = True  # Flag it\n                if args:\n                    current_type = args[0]\n                    logger.debug(f\"  Unwrapped List/Sequence, current element type: {current_type!r}\")\n                else:\n                    current_type = Any  # List without args -&gt; List[Any]\n                    logger.debug(\"  Unwrapped List/Sequence without args, assuming Any\")\n                processed = True\n                continue  # Restart loop with unwrapped element type\n\n            # 3. Unwrap Literal[...]\n            if origin is Literal:\n                # Keep the Literal origin, but simplify args if possible?\n                # No, the mapper needs the original Literal to extract choices.\n                # Just log and break the loop for Literal.\n                logger.debug(\"  Hit Literal origin, stopping simplification loop.\")\n                break  # Stop simplification here, keep Literal type\n\n        # --- Post-Loop Handling ---\n        # At this point, current_type should be the base type (int, str, datetime, Any, etc.)\n        # or a complex type we don't simplify further (like a raw Union or a specific class)\n        base_type_obj = current_type\n\n        # --- FIX: If the original type was a list, ensure base_type_obj reflects the *List* --- #\n        # The simplification loop above sets current_type to the *inner* type of the list.\n        # We need the actual List type for the mapper logic.\n        if is_list:\n            # Determine the simplified inner type from the end of the loop\n            simplified_inner_type = base_type_obj\n\n            # Check if the original type involved Optional wrapping the list\n            # A simple check: was is_optional also flagged?\n            if is_optional:\n                # Reconstruct Optional[List[SimplifiedInner]]\n                reconstructed_type = list[simplified_inner_type] | None\n                logger.debug(\n                    f\"  Original was Optional[List-like]. Reconstructing List[...] | None \"\n                    f\"around simplified inner type {simplified_inner_type!r} -&gt; {reconstructed_type!r}\"\n                )\n            else:\n                # Reconstruct List[SimplifiedInner]\n                reconstructed_type = list[simplified_inner_type]\n                logger.debug(\n                    f\"  Original was List-like (non-optional). Reconstructing List[...] \"\n                    f\"around simplified inner type {simplified_inner_type!r} -&gt; {reconstructed_type!r}\"\n                )\n\n            # Check against original type structure (might be more robust but complex?)\n            # original_origin = get_origin(field_type)\n            # if original_origin is Optional and get_origin(get_args(field_type)[0]) in (list, Sequence):\n            #     # Handle Optional[List[...]] structure\n            # elif original_origin in (list, Sequence):\n            #     # Handle List[...] structure\n            # else:\n            #     # Handle complex cases like Annotated[Optional[List[...]]]\n\n            base_type_obj = reconstructed_type\n\n        # --- End FIX --- #\n\n        # Add check for Callable simplification\n        origin = get_origin(base_type_obj)\n        if origin is Callable or (\n            hasattr(base_type_obj, \"__module__\")\n            and base_type_obj.__module__ == \"collections.abc\"\n            and base_type_obj.__name__ == \"Callable\"\n        ):\n            logger.debug(\n                f\"  Final type is complex Callable {base_type_obj!r}, simplifying base object to Callable origin.\"\n            )\n            base_type_obj = Callable\n\n        # --- Result Assembly ---\n        imports = TypeHandler.get_required_imports(field_type)  # Imports based on original\n        type_string = TypeHandler.format_type_string(field_type)  # Formatting based on original\n\n        result = {\n            \"type_str\": type_string,\n            \"type_obj\": base_type_obj,  # THIS is the crucial simplified type object\n            \"is_optional\": is_optional,\n            \"is_list\": is_list,\n            \"imports\": imports,\n            \"contained_dataclasses\": contained_dataclasses,\n            \"metadata\": metadata,\n        }\n        logger.debug(f\"[TypeHandler] Processed result: {result!r}\")\n        return result\n\n    @staticmethod\n    def format_type_string(type_obj: Any) -&gt; str:\n        \"\"\"Return a string representation suitable for generated code.\"\"\"\n        # --- Simplified version to break recursion ---\n        # Get the raw string representation first\n        raw_repr = TypeHandler._get_raw_type_string(type_obj)\n\n        # Basic cleanup for common typing constructs\n        base_name = raw_repr.replace(\"typing.\", \"\")\n\n        # Attempt to refine based on origin/args if needed (optional)\n        origin = get_origin(type_obj)\n        args = get_args(type_obj)\n\n        if origin in (Union, UnionType) and len(args) == 2 and type(None) in args:\n            # Handle Optional[T]\n            inner_type_str = TypeHandler.format_type_string(next(arg for arg in args if arg is not type(None)))\n            return f\"{inner_type_str} | None\"\n        elif origin in (list, Sequence):\n            # Handle List[T] / Sequence[T]\n            if args:\n                inner_type_str = TypeHandler.format_type_string(args[0])\n                return f\"List[{inner_type_str}]\"  # Prefer List for generated code\n            else:\n                return \"List[Any]\"\n        elif origin is dict:\n            if args and len(args) == 2:\n                key_type_str = TypeHandler.format_type_string(args[0])\n                value_type_str = TypeHandler.format_type_string(args[1])\n                return f\"Dict[{key_type_str}, {value_type_str}]\"\n            else:\n                return \"dict\"\n        elif origin is Callable:\n            if args:\n                # For Callable[[A, B], R], args is ([A, B], R) in Py3.9+\n                # For Callable[A, R], args is (A, R)\n                # For Callable[[], R], args is ([], R)\n                param_part = args[0]\n                return_part = args[-1]\n\n                if param_part is ...:\n                    param_str = \"...\"\n                elif isinstance(param_part, list):\n                    param_types = [TypeHandler.format_type_string(p) for p in param_part]\n                    param_str = f'[{\", \".join(param_types)}]'\n                else:  # Single argument\n                    param_str = f\"[{TypeHandler.format_type_string(param_part)}]\"\n\n                return_type_str = TypeHandler.format_type_string(return_part)\n                return f\"Callable[{param_str}, {return_type_str}]\"\n            else:\n                return \"Callable\"\n        elif origin in (Union, UnionType):  # Non-optional Union\n            inner_types = [TypeHandler.format_type_string(arg) for arg in args]\n            return \" | \".join(inner_types)\n        elif origin is Literal:\n            inner_values = [repr(arg) for arg in args]\n            return f\"Literal[{', '.join(inner_values)}]\"\n        # Add other origins like Dict, Tuple, Callable if needed\n\n        # Fallback to the cleaned raw representation\n        return base_name.replace(\"collections.abc.\", \"\")\n\n    @staticmethod\n    def _get_raw_type_string(type_obj: Any) -&gt; str:\n        module = getattr(type_obj, \"__module__\", \"\")\n        if module == \"typing\":\n            return repr(type_obj).replace(\"typing.\", \"\")\n        # Use name for classes/dataclasses\n        if hasattr(type_obj, \"__name__\") and isinstance(type_obj, type):\n            return type_obj.__name__\n        # Fallback to str\n        return str(type_obj)\n</code></pre>"},{"location":"reference/pydantic2django/core/typing/#pydantic2django.core.typing.TypeHandler.format_type_string","title":"<code>format_type_string(type_obj)</code>  <code>staticmethod</code>","text":"<p>Return a string representation suitable for generated code.</p> Source code in <code>src/pydantic2django/core/typing.py</code> <pre><code>@staticmethod\ndef format_type_string(type_obj: Any) -&gt; str:\n    \"\"\"Return a string representation suitable for generated code.\"\"\"\n    # --- Simplified version to break recursion ---\n    # Get the raw string representation first\n    raw_repr = TypeHandler._get_raw_type_string(type_obj)\n\n    # Basic cleanup for common typing constructs\n    base_name = raw_repr.replace(\"typing.\", \"\")\n\n    # Attempt to refine based on origin/args if needed (optional)\n    origin = get_origin(type_obj)\n    args = get_args(type_obj)\n\n    if origin in (Union, UnionType) and len(args) == 2 and type(None) in args:\n        # Handle Optional[T]\n        inner_type_str = TypeHandler.format_type_string(next(arg for arg in args if arg is not type(None)))\n        return f\"{inner_type_str} | None\"\n    elif origin in (list, Sequence):\n        # Handle List[T] / Sequence[T]\n        if args:\n            inner_type_str = TypeHandler.format_type_string(args[0])\n            return f\"List[{inner_type_str}]\"  # Prefer List for generated code\n        else:\n            return \"List[Any]\"\n    elif origin is dict:\n        if args and len(args) == 2:\n            key_type_str = TypeHandler.format_type_string(args[0])\n            value_type_str = TypeHandler.format_type_string(args[1])\n            return f\"Dict[{key_type_str}, {value_type_str}]\"\n        else:\n            return \"dict\"\n    elif origin is Callable:\n        if args:\n            # For Callable[[A, B], R], args is ([A, B], R) in Py3.9+\n            # For Callable[A, R], args is (A, R)\n            # For Callable[[], R], args is ([], R)\n            param_part = args[0]\n            return_part = args[-1]\n\n            if param_part is ...:\n                param_str = \"...\"\n            elif isinstance(param_part, list):\n                param_types = [TypeHandler.format_type_string(p) for p in param_part]\n                param_str = f'[{\", \".join(param_types)}]'\n            else:  # Single argument\n                param_str = f\"[{TypeHandler.format_type_string(param_part)}]\"\n\n            return_type_str = TypeHandler.format_type_string(return_part)\n            return f\"Callable[{param_str}, {return_type_str}]\"\n        else:\n            return \"Callable\"\n    elif origin in (Union, UnionType):  # Non-optional Union\n        inner_types = [TypeHandler.format_type_string(arg) for arg in args]\n        return \" | \".join(inner_types)\n    elif origin is Literal:\n        inner_values = [repr(arg) for arg in args]\n        return f\"Literal[{', '.join(inner_values)}]\"\n    # Add other origins like Dict, Tuple, Callable if needed\n\n    # Fallback to the cleaned raw representation\n    return base_name.replace(\"collections.abc.\", \"\")\n</code></pre>"},{"location":"reference/pydantic2django/core/typing/#pydantic2django.core.typing.TypeHandler.get_class_name","title":"<code>get_class_name(type_obj)</code>  <code>staticmethod</code>","text":"<p>Extract a simple, usable class name from a type object.</p> Source code in <code>src/pydantic2django/core/typing.py</code> <pre><code>@staticmethod\ndef get_class_name(type_obj: Any) -&gt; str:\n    \"\"\"Extract a simple, usable class name from a type object.\"\"\"\n    origin = get_origin(type_obj)\n    args = get_args(type_obj)\n\n    # Check for Optional[T] specifically first (Union[T, NoneType])\n    if origin in (Union, UnionType) and len(args) == 2 and type(None) in args:\n        return \"Optional\"\n\n    if origin:\n        # Now check for other origins\n        if origin in (Union, UnionType):  # Handles Union[A, B, ...]\n            return \"Union\"\n        if origin is list:\n            return \"List\"  # Use capital L consistently\n        if origin is dict:\n            return \"Dict\"  # Use capital D consistently\n        if origin is tuple:\n            return \"Tuple\"  # Use capital T consistently\n        if origin is set:\n            return \"Set\"  # Use capital S consistently\n        if origin is Callable:\n            return \"Callable\"\n        if origin is type:\n            return \"Type\"\n        # Fallback for other generic types\n        return getattr(origin, \"__name__\", str(origin))\n\n    # Handle non-generic types\n    if hasattr(type_obj, \"__name__\"):\n        return type_obj.__name__\n\n    type_str = str(type_obj)\n    match = TypeHandler.PATTERNS[\"angle_bracket_class\"].match(type_str)\n    if match:\n        return match.group(1).split(\".\")[-1]\n\n    return str(type_obj)\n</code></pre>"},{"location":"reference/pydantic2django/core/typing/#pydantic2django.core.typing.TypeHandler.get_required_imports","title":"<code>get_required_imports(type_obj)</code>  <code>staticmethod</code>","text":"<p>Determine necessary imports by traversing a type object.</p> Source code in <code>src/pydantic2django/core/typing.py</code> <pre><code>@staticmethod\ndef get_required_imports(type_obj: Any) -&gt; dict[str, list[str]]:\n    \"\"\"Determine necessary imports by traversing a type object.\"\"\"\n    imports: dict[str, list[str]] = {}\n    processed_types = set()\n\n    # Define modules for known Pydantic types that might need explicit import\n    pydantic_module_map = {\n        \"EmailStr\": \"pydantic\",\n        \"IPvAnyAddress\": \"pydantic\",\n        \"Json\": \"pydantic\",\n        \"BaseModel\": \"pydantic\",\n        # Add others if needed (e.g., SecretStr, UrlStr)\n    }\n\n    def _traverse(current_type: Any):\n        nonlocal imports\n        try:\n            type_repr = repr(current_type)\n            if type_repr in processed_types:\n                return\n            processed_types.add(type_repr)\n        except TypeError:\n            # Handle unhashable types if necessary, e.g., log a warning\n            pass\n\n        origin = get_origin(current_type)\n        args = get_args(current_type)\n\n        if origin:\n            # Handle Generic Alias (List, Dict, Union, Optional, Callable, Type)\n            origin_module = getattr(origin, \"__module__\", \"\")\n            origin_name = getattr(origin, \"__name__\", \"\")\n\n            # Determine the canonical name used in 'typing' imports (e.g., List, Dict, Callable)\n            typing_name = None\n            if origin is list:\n                typing_name = \"List\"\n            elif origin is dict:\n                typing_name = \"Dict\"\n            elif origin is tuple:\n                typing_name = \"Tuple\"\n            elif origin is set:\n                typing_name = \"Set\"\n            elif origin in (Union, UnionType):  # Handle types.UnionType for Python 3.10+\n                # We don't need to add Union or Optional imports anymore with | syntax\n                typing_name = None\n            elif origin is type:\n                typing_name = \"Type\"\n            # Check both typing.Callable and collections.abc.Callable\n            elif origin_module == \"typing\" and origin_name == \"Callable\":\n                typing_name = \"Callable\"\n            elif origin_module == \"collections.abc\" and origin_name == \"Callable\":\n                typing_name = \"Callable\"\n            # Add more specific checks if needed (e.g., Sequence, Mapping)\n\n            # Add import if we identified a standard typing construct\n            if typing_name:\n                TypeHandler._add_import(imports, \"typing\", typing_name)\n\n            # Traverse arguments regardless of origin's module\n            for arg in args:\n                if arg is not type(None):  # Skip NoneType in Optional/Union\n                    if isinstance(arg, TypeVar):\n                        # Handle TypeVar by traversing its constraints/bound\n                        constraints = getattr(arg, \"__constraints__\", ())\n                        bound = getattr(arg, \"__bound__\", None)\n                        if bound:\n                            _traverse(bound)\n                        for constraint in constraints:\n                            _traverse(constraint)\n                    else:\n                        _traverse(arg)  # Recursively traverse arguments\n        # Handle Base Types or Classes (int, str, MyClass, etc.)\n        elif isinstance(current_type, type):\n            module_name = getattr(current_type, \"__module__\", \"\")\n            type_name = getattr(current_type, \"__name__\", \"\")\n\n            if not type_name or module_name == \"builtins\":\n                pass  # Skip builtins or types without names\n            elif module_name == \"typing\" and type_name not in (\"NoneType\", \"Generic\"):\n                # Catch Any, etc. used directly\n                TypeHandler._add_import(imports, \"typing\", type_name)\n            # Check for dataclasses and Pydantic models specifically\n            elif is_dataclass(current_type) or (\n                inspect.isclass(current_type) and issubclass(current_type, BaseModel)\n            ):\n                actual_module = inspect.getmodule(current_type)\n                if actual_module and actual_module.__name__ != \"__main__\":\n                    TypeHandler._add_import(imports, actual_module.__name__, type_name)\n                # Add specific imports if needed (e.g., dataclasses.dataclass, pydantic.BaseModel)\n                if is_dataclass(current_type):\n                    TypeHandler._add_import(imports, \"dataclasses\", \"dataclass\")\n                # No need to add BaseModel here usually, handled by pydantic_module_map or direct usage\n            elif module_name:\n                # Handle known standard library modules explicitly\n                known_stdlib = {\"datetime\", \"decimal\", \"uuid\", \"pathlib\"}\n                if module_name in known_stdlib:\n                    TypeHandler._add_import(imports, module_name, type_name)\n                # Handle known Pydantic types explicitly (redundant with BaseModel check?)\n                elif type_name in pydantic_module_map:\n                    TypeHandler._add_import(imports, pydantic_module_map[type_name], type_name)\n                # Assume other types defined in modules need importing\n                elif module_name != \"__main__\":  # Avoid importing from main script context\n                    TypeHandler._add_import(imports, module_name, type_name)\n\n        elif current_type is Any:\n            TypeHandler._add_import(imports, \"typing\", \"Any\")\n        elif isinstance(current_type, TypeVar):\n            # Handle TypeVar used directly\n            constraints = getattr(current_type, \"__constraints__\", ())\n            bound = getattr(current_type, \"__bound__\", None)\n            if bound:\n                _traverse(bound)\n            for c in constraints:\n                _traverse(c)\n        # Consider adding ForwardRef handling if needed:\n        # elif isinstance(current_type, typing.ForwardRef):\n        #     # Potentially add logic to resolve/import forward refs\n        #     pass\n\n    _traverse(type_obj)\n\n    # Clean up imports (unique, sorted)\n    final_imports = {}\n    for module, names in imports.items():\n        unique_names = sorted(set(names))\n        if unique_names:\n            final_imports[module] = unique_names\n    return final_imports\n</code></pre>"},{"location":"reference/pydantic2django/core/typing/#pydantic2django.core.typing.TypeHandler.process_field_type","title":"<code>process_field_type(field_type)</code>  <code>staticmethod</code>","text":"<p>Process a field type to get name, flags, imports, and contained dataclasses.</p> Source code in <code>src/pydantic2django/core/typing.py</code> <pre><code>@staticmethod\ndef process_field_type(field_type: Any) -&gt; dict[str, Any]:\n    \"\"\"Process a field type to get name, flags, imports, and contained dataclasses.\"\"\"\n    logger.debug(f\"[TypeHandler] Processing type: {field_type!r}\")\n    is_optional = False\n    is_list = False\n    metadata: tuple[Any, ...] | None = None  # Initialize metadata with type hint\n    imports = set()\n    contained_dataclasses = set()\n    current_type = field_type  # Keep track of the potentially unwrapped type\n\n    # Helper function (remains the same)\n    def _is_potential_dataclass(t: Any) -&gt; bool:\n        return inspect.isclass(t) and is_dataclass(t)\n\n    def _find_contained_dataclasses(current_type: Any):\n        origin = get_origin(current_type)\n        args = get_args(current_type)\n        if origin:\n            for arg in args:\n                if arg is not type(None):\n                    _find_contained_dataclasses(arg)\n        elif _is_potential_dataclass(current_type):\n            contained_dataclasses.add(current_type)\n\n    _find_contained_dataclasses(field_type)\n    if contained_dataclasses:\n        logger.debug(f\"  Found potential contained dataclasses: {[dc.__name__ for dc in contained_dataclasses]}\")\n\n    # --- Simplification Loop ---\n    # Repeatedly unwrap until we hit a base type or Any\n    processed = True\n    while processed:\n        processed = False\n        origin = get_origin(current_type)\n        args = get_args(current_type)\n\n        # 0. Unwrap Annotated[T, ...]\n        # Check if the origin exists and has the name 'Annotated'\n        # This check is more robust than `origin is Annotated` across Python versions\n        if origin is Annotated:\n            if args:\n                core_type = args[0]\n                metadata = args[1:]\n                current_type = core_type\n                logger.debug(f\"  Unwrapped Annotated, current type: {current_type!r}, metadata: {metadata!r}\")\n                processed = True\n                continue  # Restart loop with unwrapped type\n            else:\n                logger.warning(\"  Found Annotated without arguments? Treating as Any.\")\n                current_type = Any\n                processed = True\n                continue\n\n        # 1. Unwrap Optional[T] (Union[T, NoneType])\n        if origin in (Union, UnionType) and type(None) in args:\n            is_optional = True  # Flag it\n            # Rebuild the Union without NoneType\n            non_none_args = tuple(arg for arg in args if arg is not type(None))\n            if len(non_none_args) == 1:\n                current_type = non_none_args[0]  # Simplify Union[T, None] to T\n            elif len(non_none_args) &gt; 1:\n                # Use UnionType to rebuild\n                current_type = reduce(lambda x, y: x | y, non_none_args)\n            else:  # pragma: no cover\n                # Should not happen if NoneType was in args\n                current_type = Any\n            logger.debug(f\"  Unwrapped Union with None, current type: {current_type!r}\")\n            processed = True\n            continue  # Restart loop with the non-optional type\n\n        # 2. Unwrap List[T] or Sequence[T]\n        if origin in (list, Sequence):\n            is_list = True  # Flag it\n            if args:\n                current_type = args[0]\n                logger.debug(f\"  Unwrapped List/Sequence, current element type: {current_type!r}\")\n            else:\n                current_type = Any  # List without args -&gt; List[Any]\n                logger.debug(\"  Unwrapped List/Sequence without args, assuming Any\")\n            processed = True\n            continue  # Restart loop with unwrapped element type\n\n        # 3. Unwrap Literal[...]\n        if origin is Literal:\n            # Keep the Literal origin, but simplify args if possible?\n            # No, the mapper needs the original Literal to extract choices.\n            # Just log and break the loop for Literal.\n            logger.debug(\"  Hit Literal origin, stopping simplification loop.\")\n            break  # Stop simplification here, keep Literal type\n\n    # --- Post-Loop Handling ---\n    # At this point, current_type should be the base type (int, str, datetime, Any, etc.)\n    # or a complex type we don't simplify further (like a raw Union or a specific class)\n    base_type_obj = current_type\n\n    # --- FIX: If the original type was a list, ensure base_type_obj reflects the *List* --- #\n    # The simplification loop above sets current_type to the *inner* type of the list.\n    # We need the actual List type for the mapper logic.\n    if is_list:\n        # Determine the simplified inner type from the end of the loop\n        simplified_inner_type = base_type_obj\n\n        # Check if the original type involved Optional wrapping the list\n        # A simple check: was is_optional also flagged?\n        if is_optional:\n            # Reconstruct Optional[List[SimplifiedInner]]\n            reconstructed_type = list[simplified_inner_type] | None\n            logger.debug(\n                f\"  Original was Optional[List-like]. Reconstructing List[...] | None \"\n                f\"around simplified inner type {simplified_inner_type!r} -&gt; {reconstructed_type!r}\"\n            )\n        else:\n            # Reconstruct List[SimplifiedInner]\n            reconstructed_type = list[simplified_inner_type]\n            logger.debug(\n                f\"  Original was List-like (non-optional). Reconstructing List[...] \"\n                f\"around simplified inner type {simplified_inner_type!r} -&gt; {reconstructed_type!r}\"\n            )\n\n        # Check against original type structure (might be more robust but complex?)\n        # original_origin = get_origin(field_type)\n        # if original_origin is Optional and get_origin(get_args(field_type)[0]) in (list, Sequence):\n        #     # Handle Optional[List[...]] structure\n        # elif original_origin in (list, Sequence):\n        #     # Handle List[...] structure\n        # else:\n        #     # Handle complex cases like Annotated[Optional[List[...]]]\n\n        base_type_obj = reconstructed_type\n\n    # --- End FIX --- #\n\n    # Add check for Callable simplification\n    origin = get_origin(base_type_obj)\n    if origin is Callable or (\n        hasattr(base_type_obj, \"__module__\")\n        and base_type_obj.__module__ == \"collections.abc\"\n        and base_type_obj.__name__ == \"Callable\"\n    ):\n        logger.debug(\n            f\"  Final type is complex Callable {base_type_obj!r}, simplifying base object to Callable origin.\"\n        )\n        base_type_obj = Callable\n\n    # --- Result Assembly ---\n    imports = TypeHandler.get_required_imports(field_type)  # Imports based on original\n    type_string = TypeHandler.format_type_string(field_type)  # Formatting based on original\n\n    result = {\n        \"type_str\": type_string,\n        \"type_obj\": base_type_obj,  # THIS is the crucial simplified type object\n        \"is_optional\": is_optional,\n        \"is_list\": is_list,\n        \"imports\": imports,\n        \"contained_dataclasses\": contained_dataclasses,\n        \"metadata\": metadata,\n    }\n    logger.debug(f\"[TypeHandler] Processed result: {result!r}\")\n    return result\n</code></pre>"},{"location":"reference/pydantic2django/core/typing/#pydantic2django.core.typing.configure_core_typing_logging","title":"<code>configure_core_typing_logging(level=logging.WARNING, format_str='%Y-%m-%d %H:%M:%S - %(name)s - %(levelname)s - %(message)s')</code>","text":"<p>Configure the logging for core typing module.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>The logging level (e.g., logging.DEBUG, logging.INFO)</p> <code>WARNING</code> <code>format_str</code> <code>str</code> <p>The format string for log messages</p> <code>'%Y-%m-%d %H:%M:%S - %(name)s - %(levelname)s - %(message)s'</code> Source code in <code>src/pydantic2django/core/typing.py</code> <pre><code>def configure_core_typing_logging(\n    level: int = logging.WARNING,\n    format_str: str = \"%Y-%m-%d %H:%M:%S - %(name)s - %(levelname)s - %(message)s\",\n) -&gt; None:\n    \"\"\"\n    Configure the logging for core typing module.\n\n    Args:\n        level: The logging level (e.g., logging.DEBUG, logging.INFO)\n        format_str: The format string for log messages\n    \"\"\"\n    handler = logging.StreamHandler()\n    formatter = logging.Formatter(format_str)\n    handler.setFormatter(formatter)\n\n    logger.setLevel(level)\n    logger.addHandler(handler)\n    logger.propagate = False  # Prevent duplicate logging\n\n    logger.debug(\"Core typing logging configured\")\n</code></pre>"},{"location":"reference/pydantic2django/core/utils/","title":"pydantic2django.core.utils","text":""},{"location":"reference/pydantic2django/core/utils/naming/","title":"pydantic2django.core.utils.naming","text":""},{"location":"reference/pydantic2django/core/utils/naming/#pydantic2django.core.utils.naming.enum_class_name_from_field","title":"<code>enum_class_name_from_field(field_name)</code>","text":"<p>Derive a PascalCase enum class name from a field name.</p> <ul> <li>Replace separators with underscores, then title-case and remove underscores</li> <li>Example: \"type\" -&gt; \"Type\", \"xlink:type\" -&gt; \"XlinkType\"</li> </ul> Source code in <code>src/pydantic2django/core/utils/naming.py</code> <pre><code>def enum_class_name_from_field(field_name: str) -&gt; str:\n    \"\"\"\n    Derive a PascalCase enum class name from a field name.\n\n    - Replace separators with underscores, then title-case and remove underscores\n    - Example: \"type\" -&gt; \"Type\", \"xlink:type\" -&gt; \"XlinkType\"\n    \"\"\"\n    base = normalize_namespace_separators(field_name)\n    return base.replace(\"_\", \" \").title().replace(\" \", \"\")\n</code></pre>"},{"location":"reference/pydantic2django/core/utils/naming/#pydantic2django.core.utils.naming.normalize_namespace_separators","title":"<code>normalize_namespace_separators(name)</code>","text":"<p>Replace common namespace and punctuation separators with underscores.</p> <p>Examples: - \"xlink:type\" -&gt; \"xlink_type\" - \"ns.element-name\" -&gt; \"ns_element_name\" - \"foo bar\" -&gt; \"foo_bar\"</p> Source code in <code>src/pydantic2django/core/utils/naming.py</code> <pre><code>def normalize_namespace_separators(name: str) -&gt; str:\n    \"\"\"\n    Replace common namespace and punctuation separators with underscores.\n\n    Examples:\n    - \"xlink:type\" -&gt; \"xlink_type\"\n    - \"ns.element-name\" -&gt; \"ns_element_name\"\n    - \"foo bar\" -&gt; \"foo_bar\"\n    \"\"\"\n    return re.sub(r\"[:\\.\\-\\s]+\", \"_\", str(name))\n</code></pre>"},{"location":"reference/pydantic2django/core/utils/naming/#pydantic2django.core.utils.naming.sanitize_field_identifier","title":"<code>sanitize_field_identifier(name)</code>","text":"<p>Produce a valid Python/Django field identifier from an arbitrary source name.</p> <p>Rules: - Replace namespace/punctuation separators (\":\", \".\", \"-\", spaces) with \"\" - Convert CamelCase to snake_case - Remove invalid characters (keep only [A-Za-z0-9]) - Ensure starts with a letter or underscore; if not, prefix with \"_\" - Lowercase and collapse multiple underscores</p> <p>Note: Does not suffix Python keywords/builtins; callers decide policy.</p> Source code in <code>src/pydantic2django/core/utils/naming.py</code> <pre><code>def sanitize_field_identifier(name: str) -&gt; str:\n    \"\"\"\n    Produce a valid Python/Django field identifier from an arbitrary source name.\n\n    Rules:\n    - Replace namespace/punctuation separators (\":\", \".\", \"-\", spaces) with \"_\"\n    - Convert CamelCase to snake_case\n    - Remove invalid characters (keep only [A-Za-z0-9_])\n    - Ensure starts with a letter or underscore; if not, prefix with \"_\"\n    - Lowercase and collapse multiple underscores\n\n    Note: Does not suffix Python keywords/builtins; callers decide policy.\n    \"\"\"\n    # Normalize separators\n    replaced = normalize_namespace_separators(name)\n    # Convert to snake\n    snake = to_snake_case(replaced)\n    # Keep valid identifier chars only\n    cleaned = re.sub(r\"[^0-9a-zA-Z_]\", \"_\", snake)\n    # Ensure starts with letter or underscore\n    if not cleaned or not (cleaned[0].isalpha() or cleaned[0] == \"_\"):\n        cleaned = f\"_{cleaned}\" if cleaned else \"_\"\n    # Lowercase and collapse underscores\n    lowered = cleaned.lower()\n    return re.sub(r\"__+\", \"_\", lowered)\n</code></pre>"},{"location":"reference/pydantic2django/core/utils/naming/#pydantic2django.core.utils.naming.to_snake_case","title":"<code>to_snake_case(name)</code>","text":"<p>Convert a possibly CamelCase or mixedCase string into snake_case.</p> <ul> <li>Inserts underscores between lower-&gt;Upper boundaries</li> <li>Handles multi-capital sequences reasonably (ABCTest -&gt; abc_test)</li> </ul> Source code in <code>src/pydantic2django/core/utils/naming.py</code> <pre><code>def to_snake_case(name: str) -&gt; str:\n    \"\"\"\n    Convert a possibly CamelCase or mixedCase string into snake_case.\n\n    - Inserts underscores between lower-&gt;Upper boundaries\n    - Handles multi-capital sequences reasonably (ABCTest -&gt; abc_test)\n    \"\"\"\n    # First handle boundaries like \"ABCd\" -&gt; \"ABC_d\"\n    s1 = re.sub(r\"(.)([A-Z][a-z]+)\", r\"\\1_\\2\", str(name))\n    # Then handle lower/number to upper boundary: \"fooBAR\" -&gt; \"foo_BAR\"\n    return re.sub(r\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", s1)\n</code></pre>"},{"location":"reference/pydantic2django/core/utils/relationships/","title":"pydantic2django.core.utils.relationships","text":""},{"location":"reference/pydantic2django/core/utils/relationships/#pydantic2django.core.utils.relationships.get_relationship_metadata","title":"<code>get_relationship_metadata(field_type)</code>","text":"<p>Analyze a type hint to extract relationship metadata (if any).</p> <p>Determines if the type represents a relationship (e.g., Optional[OtherModel], List[OtherModel]) and extracts the target model type.</p> <p>Parameters:</p> Name Type Description Default <code>field_type</code> <code>Any</code> <p>The type annotation to analyze.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>A dictionary containing metadata: { 'is_relationship': bool, 'target_model': type | None, 'is_many': bool }</p> Source code in <code>src/pydantic2django/core/utils/relationships.py</code> <pre><code>def get_relationship_metadata(field_type: Any) -&gt; dict[str, Any]:\n    \"\"\"\n    Analyze a type hint to extract relationship metadata (if any).\n\n    Determines if the type represents a relationship (e.g., Optional[OtherModel], List[OtherModel])\n    and extracts the target model type.\n\n    Args:\n        field_type: The type annotation to analyze.\n\n    Returns:\n        A dictionary containing metadata: { 'is_relationship': bool, 'target_model': type | None, 'is_many': bool }\n    \"\"\"\n    metadata = {\"is_relationship\": False, \"target_model\": None, \"is_many\": False}\n    origin = get_origin(field_type)\n    args = get_args(field_type)\n\n    target_type = None\n\n    # Handle Optional[T]\n    if origin is Union and type(None) in args and len(args) == 2:\n        target_type = next(arg for arg in args if arg is not type(None))\n    # Handle List[T] or list[T]\n    elif origin is list and args:\n        target_type = args[0]\n        metadata[\"is_many\"] = True\n    # Handle direct type T\n    elif origin is None and isinstance(field_type, type):\n        target_type = field_type\n\n    # Check if the target type is a Pydantic model\n    if target_type and isinstance(target_type, type):\n        try:\n            if issubclass(target_type, BaseModel):\n                metadata[\"is_relationship\"] = True\n                metadata[\"target_model\"] = target_type\n        except TypeError:\n            pass  # issubclass fails if target_type is not a class\n\n    return metadata\n</code></pre>"},{"location":"reference/pydantic2django/core/utils/strings/","title":"pydantic2django.core.utils.strings","text":""},{"location":"reference/pydantic2django/core/utils/strings/#pydantic2django.core.utils.strings.balanced","title":"<code>balanced(s)</code>","text":"<p>Check if a string containing brackets, parentheses, or braces is balanced.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The string to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if balanced, False otherwise.</p> Source code in <code>src/pydantic2django/core/utils/strings.py</code> <pre><code>def balanced(s: str) -&gt; bool:\n    \"\"\"\n    Check if a string containing brackets, parentheses, or braces is balanced.\n\n    Args:\n        s: The string to check.\n\n    Returns:\n        True if balanced, False otherwise.\n    \"\"\"\n    bracket_map = {\")\": \"(\", \"]\": \"[\", \"}\": \"{\"}\n    opening_brackets = set(bracket_map.values())\n    stack = []\n\n    for char in s:\n        if char in opening_brackets:\n            stack.append(char)\n        elif char in bracket_map:\n            if not stack or stack[-1] != bracket_map[char]:\n                return False\n            stack.pop()\n\n    return not stack  # Stack should be empty if balanced\n</code></pre>"},{"location":"reference/pydantic2django/core/utils/strings/#pydantic2django.core.utils.strings.sanitize_string","title":"<code>sanitize_string(value)</code>","text":"<p>Sanitize a string value, handling Django's lazy translation Promises.</p> <p>Ensures the output is a clean string, escaping potential issues for templates or code generation.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Union[str, Promise, Any]</code> <p>The value to sanitize (can be str, Promise, or other types)</p> required <p>Returns:</p> Type Description <code>str</code> <p>A sanitized string representation.</p> Source code in <code>src/pydantic2django/core/utils/strings.py</code> <pre><code>def sanitize_string(value: Union[str, Promise, Any]) -&gt; str:\n    \"\"\"\n    Sanitize a string value, handling Django's lazy translation Promises.\n\n    Ensures the output is a clean string, escaping potential issues for templates or code generation.\n\n    Args:\n        value: The value to sanitize (can be str, Promise, or other types)\n\n    Returns:\n        A sanitized string representation.\n    \"\"\"\n    processed_value: str\n    # Handle lazy translation objects first\n    if isinstance(value, Promise):\n        try:\n            # Force the promise to evaluate to a string\n            processed_value = str(value)\n        except Exception:\n            # Fallback if evaluation fails\n            processed_value = \"\"  # Or some default placeholder\n            # Consider logging this error\n            # logger.error(f\"Could not resolve Django Promise: {e}\")\n    elif isinstance(value, str):\n        processed_value = value\n    else:\n        # Handle Any other type by converting to string\n        try:\n            processed_value = str(value)\n        except Exception:\n            # If conversion to string fails, return a default empty string\n            # logger.warning(f\"Could not convert value of type {type(value)} to string in sanitize_string\")\n            processed_value = \"\"\n\n    # Basic sanitization: escape backslashes and quotes, and normalize newlines\n    # This is crucial for safely embedding the string in generated Python code\n    sanitized = processed_value.replace(\"\\\\\", \"\\\\\\\\\")  # Escape backslashes first\n    sanitized = sanitized.replace(\"\\n\", \"\\\\n\").replace(\"\\r\", \"\")  # Normalize newlines\n    sanitized = sanitized.replace(\"'\", \"\\\\'\")  # Escape single quotes\n    sanitized = sanitized.replace('\"', '\\\\\"')  # Escape double quotes for double-quoted strings\n\n    return sanitized\n</code></pre>"},{"location":"reference/pydantic2django/core/utils/timescale/","title":"pydantic2django.core.utils.timescale","text":""},{"location":"reference/pydantic2django/core/utils/timescale/#pydantic2django.core.utils.timescale.TimeseriesTimestampMissingError","title":"<code>TimeseriesTimestampMissingError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when a Timescale-enabled model requires 'time' but no source timestamp is found.</p> Source code in <code>src/pydantic2django/core/utils/timescale.py</code> <pre><code>class TimeseriesTimestampMissingError(Exception):\n    \"\"\"Raised when a Timescale-enabled model requires 'time' but no source timestamp is found.\"\"\"\n\n    def __init__(self, model_name: str, *, attempted_aliases: list[str]) -&gt; None:\n        message = (\n            f\"Timescale model '{model_name}' requires a non-null 'time' value, \"\n            f\"but none of the expected timestamp attributes were found: {attempted_aliases}. \"\n            f\"Provide one of these attributes in the source payload or customize mapping.\"\n        )\n        super().__init__(message)\n</code></pre>"},{"location":"reference/pydantic2django/core/utils/timescale/#pydantic2django.core.utils.timescale.ensure_time_or_raise","title":"<code>ensure_time_or_raise(model_name, field_values, model_field_names, *, aliases=TIMESERIES_TIME_ALIASES)</code>","text":"<p>If the model declares a 'time' field, ensure it is present in field_values, attempting to remap aliases first; raise a clear error if still missing.</p> Source code in <code>src/pydantic2django/core/utils/timescale.py</code> <pre><code>def ensure_time_or_raise(\n    model_name: str,\n    field_values: dict,\n    model_field_names: set[str],\n    *,\n    aliases: Iterable[str] = TIMESERIES_TIME_ALIASES,\n) -&gt; None:\n    \"\"\"\n    If the model declares a 'time' field, ensure it is present in field_values,\n    attempting to remap aliases first; raise a clear error if still missing.\n    \"\"\"\n    if not has_timescale_time_field(model_field_names):\n        return\n    mapped, attempted = map_time_alias_into_time(field_values, aliases=aliases)\n    if not mapped:\n        raise TimeseriesTimestampMissingError(model_name, attempted_aliases=list(attempted))\n</code></pre>"},{"location":"reference/pydantic2django/core/utils/timescale/#pydantic2django.core.utils.timescale.has_timescale_time_field","title":"<code>has_timescale_time_field(model_field_names)</code>","text":"<p>Return True if the model declares a 'time' field.</p> Source code in <code>src/pydantic2django/core/utils/timescale.py</code> <pre><code>def has_timescale_time_field(model_field_names: set[str]) -&gt; bool:\n    \"\"\"Return True if the model declares a 'time' field.\"\"\"\n    return \"time\" in model_field_names\n</code></pre>"},{"location":"reference/pydantic2django/core/utils/timescale/#pydantic2django.core.utils.timescale.map_time_alias_into_time","title":"<code>map_time_alias_into_time(field_values, *, aliases=TIMESERIES_TIME_ALIASES)</code>","text":"<p>Map the first present alias from 'aliases' into the canonical 'time' key.</p> <p>Returns (mapped, attempted_aliases)</p> Source code in <code>src/pydantic2django/core/utils/timescale.py</code> <pre><code>def map_time_alias_into_time(\n    field_values: dict, *, aliases: Iterable[str] = TIMESERIES_TIME_ALIASES\n) -&gt; tuple[bool, list[str]]:\n    \"\"\"\n    Map the first present alias from 'aliases' into the canonical 'time' key.\n\n    Returns (mapped, attempted_aliases)\n    \"\"\"\n    attempted = list(aliases)\n    if \"time\" in field_values:\n        return True, attempted\n    for name in attempted:\n        if name in field_values:\n            field_values[\"time\"] = field_values.pop(name)\n            return True, attempted\n    return False, attempted\n</code></pre>"},{"location":"reference/pydantic2django/dataclass/","title":"pydantic2django.dataclass","text":""},{"location":"reference/pydantic2django/dataclass/discovery/","title":"pydantic2django.dataclass.discovery","text":""},{"location":"reference/pydantic2django/dataclass/discovery/#pydantic2django.dataclass.discovery.DataclassDiscovery","title":"<code>DataclassDiscovery</code>","text":"<p>               Bases: <code>BaseDiscovery[DataclassType]</code></p> <p>Discovers Python dataclasses within specified packages.</p> Source code in <code>src/pydantic2django/dataclass/discovery.py</code> <pre><code>class DataclassDiscovery(BaseDiscovery[DataclassType]):\n    \"\"\"Discovers Python dataclasses within specified packages.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        # Dataclass specific attributes (all_models now in base)\n\n    def _is_target_model(self, obj: Any) -&gt; bool:\n        \"\"\"Check if an object is a dataclass type.\"\"\"\n        return inspect.isclass(obj) and dataclasses.is_dataclass(obj)\n\n    def _default_eligibility_filter(self, model: DataclassType) -&gt; bool:\n        \"\"\"Check default eligibility for dataclasses (e.g., not inheriting directly from ABC).\"\"\"\n        # Skip models that directly inherit from ABC\n        if abc.ABC in model.__bases__:\n            logger.debug(f\"Filtering out dataclass {model.__name__} (inherits directly from ABC)\")\n            return False\n        # Dataclasses don't have a standard __abstract__ marker like Pydantic\n        # Add other default checks if needed for dataclasses\n        return True\n\n    # discover_models is now implemented in the BaseDiscovery class\n    # It will call the _is_target_model and _default_eligibility_filter defined above.\n\n    # --- analyze_dependencies and get_models_in_registration_order remain ---\n\n    def analyze_dependencies(self) -&gt; None:\n        \"\"\"Build the dependency graph for the filtered dataclasses.\"\"\"\n        logger.info(\"Analyzing dependencies between filtered dataclasses...\")\n        self.dependencies: dict[DataclassType, set[DataclassType]] = {}\n\n        filtered_model_qualnames = set(self.filtered_models.keys())\n\n        def _find_and_add_dependency(model_type: DataclassType, potential_dep_type: Any):\n            if not self._is_target_model(potential_dep_type):\n                return\n\n            dep_qualname = f\"{potential_dep_type.__module__}.{potential_dep_type.__name__}\"\n\n            if dep_qualname in filtered_model_qualnames and potential_dep_type is not model_type:\n                dep_model_obj = self.filtered_models.get(dep_qualname)\n                if dep_model_obj:\n                    if model_type in self.dependencies:\n                        self.dependencies[model_type].add(dep_model_obj)\n                    else:\n                        logger.warning(\n                            f\"Model {model_type.__name__} wasn't pre-initialized in dependencies dict during analysis. Initializing now.\"\n                        )\n                        self.dependencies[model_type] = {dep_model_obj}\n                else:\n                    logger.warning(\n                        f\"Inconsistency: Dependency '{dep_qualname}' for dataclass '{model_type.__name__}' found by name but not object in filtered set.\"\n                    )\n\n        # Initialize keys based on filtered models\n        for model_type in self.filtered_models.values():\n            self.dependencies[model_type] = set()\n\n        # Analyze fields using dataclasses.fields\n        for model_type in self.filtered_models.values():\n            assert dataclasses.is_dataclass(model_type), f\"Expected {model_type} to be a dataclass\"\n            for field in dataclasses.fields(model_type):\n                annotation = field.type\n                if annotation is None:\n                    continue\n\n                origin = get_origin(annotation)\n                args = get_args(annotation)\n\n                if origin is Union and type(None) in args and len(args) == 2:\n                    annotation = next(arg for arg in args if arg is not type(None))\n                    origin = get_origin(annotation)\n                    args = get_args(annotation)\n\n                _find_and_add_dependency(model_type, annotation)\n\n                if origin in (list, dict, set, tuple):\n                    for arg in args:\n                        arg_origin = get_origin(arg)\n                        arg_args = get_args(arg)\n\n                        if arg_origin is Union and type(None) in arg_args and len(arg_args) == 2:\n                            nested_type = next(t for t in arg_args if t is not type(None))\n                            _find_and_add_dependency(model_type, nested_type)\n                        else:\n                            _find_and_add_dependency(model_type, arg)\n\n        logger.info(\"Dataclass dependency analysis complete.\")\n        # Debug logging moved inside BaseDiscovery\n\n    def get_models_in_registration_order(self) -&gt; list[DataclassType]:\n        \"\"\"\n        Return dataclasses sorted topologically based on dependencies.\n        (Largely similar to Pydantic version, uses DataclassType)\n        \"\"\"\n        if not self.dependencies:\n            logger.warning(\"No dependencies found or analyzed, returning dataclasses in arbitrary order.\")\n            return list(self.filtered_models.values())\n\n        sorted_models = []\n        visited: set[DataclassType] = set()\n        visiting: set[DataclassType] = set()\n        filtered_model_objects = set(self.filtered_models.values())\n\n        def visit(model: DataclassType):\n            if model in visited:\n                return\n            if model in visiting:\n                logger.error(f\"Circular dependency detected involving dataclass {model.__name__}\")\n                # Option: raise TypeError(...)\n                return  # Break cycle\n\n            visiting.add(model)\n\n            if model in self.dependencies:\n                # Use .get for safety, ensure deps are also in filtered set\n                for dep in self.dependencies.get(model, set()):\n                    if dep in filtered_model_objects:\n                        visit(dep)\n\n            visiting.remove(model)\n            visited.add(model)\n            sorted_models.append(model)\n\n        all_target_models = list(self.filtered_models.values())\n        for model in all_target_models:\n            if model not in visited:\n                visit(model)\n\n        logger.info(f\"Dataclasses sorted for registration: {[m.__name__ for m in sorted_models]}\")\n        return sorted_models\n</code></pre>"},{"location":"reference/pydantic2django/dataclass/discovery/#pydantic2django.dataclass.discovery.DataclassDiscovery.analyze_dependencies","title":"<code>analyze_dependencies()</code>","text":"<p>Build the dependency graph for the filtered dataclasses.</p> Source code in <code>src/pydantic2django/dataclass/discovery.py</code> <pre><code>def analyze_dependencies(self) -&gt; None:\n    \"\"\"Build the dependency graph for the filtered dataclasses.\"\"\"\n    logger.info(\"Analyzing dependencies between filtered dataclasses...\")\n    self.dependencies: dict[DataclassType, set[DataclassType]] = {}\n\n    filtered_model_qualnames = set(self.filtered_models.keys())\n\n    def _find_and_add_dependency(model_type: DataclassType, potential_dep_type: Any):\n        if not self._is_target_model(potential_dep_type):\n            return\n\n        dep_qualname = f\"{potential_dep_type.__module__}.{potential_dep_type.__name__}\"\n\n        if dep_qualname in filtered_model_qualnames and potential_dep_type is not model_type:\n            dep_model_obj = self.filtered_models.get(dep_qualname)\n            if dep_model_obj:\n                if model_type in self.dependencies:\n                    self.dependencies[model_type].add(dep_model_obj)\n                else:\n                    logger.warning(\n                        f\"Model {model_type.__name__} wasn't pre-initialized in dependencies dict during analysis. Initializing now.\"\n                    )\n                    self.dependencies[model_type] = {dep_model_obj}\n            else:\n                logger.warning(\n                    f\"Inconsistency: Dependency '{dep_qualname}' for dataclass '{model_type.__name__}' found by name but not object in filtered set.\"\n                )\n\n    # Initialize keys based on filtered models\n    for model_type in self.filtered_models.values():\n        self.dependencies[model_type] = set()\n\n    # Analyze fields using dataclasses.fields\n    for model_type in self.filtered_models.values():\n        assert dataclasses.is_dataclass(model_type), f\"Expected {model_type} to be a dataclass\"\n        for field in dataclasses.fields(model_type):\n            annotation = field.type\n            if annotation is None:\n                continue\n\n            origin = get_origin(annotation)\n            args = get_args(annotation)\n\n            if origin is Union and type(None) in args and len(args) == 2:\n                annotation = next(arg for arg in args if arg is not type(None))\n                origin = get_origin(annotation)\n                args = get_args(annotation)\n\n            _find_and_add_dependency(model_type, annotation)\n\n            if origin in (list, dict, set, tuple):\n                for arg in args:\n                    arg_origin = get_origin(arg)\n                    arg_args = get_args(arg)\n\n                    if arg_origin is Union and type(None) in arg_args and len(arg_args) == 2:\n                        nested_type = next(t for t in arg_args if t is not type(None))\n                        _find_and_add_dependency(model_type, nested_type)\n                    else:\n                        _find_and_add_dependency(model_type, arg)\n\n    logger.info(\"Dataclass dependency analysis complete.\")\n</code></pre>"},{"location":"reference/pydantic2django/dataclass/discovery/#pydantic2django.dataclass.discovery.DataclassDiscovery.get_models_in_registration_order","title":"<code>get_models_in_registration_order()</code>","text":"<p>Return dataclasses sorted topologically based on dependencies. (Largely similar to Pydantic version, uses DataclassType)</p> Source code in <code>src/pydantic2django/dataclass/discovery.py</code> <pre><code>def get_models_in_registration_order(self) -&gt; list[DataclassType]:\n    \"\"\"\n    Return dataclasses sorted topologically based on dependencies.\n    (Largely similar to Pydantic version, uses DataclassType)\n    \"\"\"\n    if not self.dependencies:\n        logger.warning(\"No dependencies found or analyzed, returning dataclasses in arbitrary order.\")\n        return list(self.filtered_models.values())\n\n    sorted_models = []\n    visited: set[DataclassType] = set()\n    visiting: set[DataclassType] = set()\n    filtered_model_objects = set(self.filtered_models.values())\n\n    def visit(model: DataclassType):\n        if model in visited:\n            return\n        if model in visiting:\n            logger.error(f\"Circular dependency detected involving dataclass {model.__name__}\")\n            # Option: raise TypeError(...)\n            return  # Break cycle\n\n        visiting.add(model)\n\n        if model in self.dependencies:\n            # Use .get for safety, ensure deps are also in filtered set\n            for dep in self.dependencies.get(model, set()):\n                if dep in filtered_model_objects:\n                    visit(dep)\n\n        visiting.remove(model)\n        visited.add(model)\n        sorted_models.append(model)\n\n    all_target_models = list(self.filtered_models.values())\n    for model in all_target_models:\n        if model not in visited:\n            visit(model)\n\n    logger.info(f\"Dataclasses sorted for registration: {[m.__name__ for m in sorted_models]}\")\n    return sorted_models\n</code></pre>"},{"location":"reference/pydantic2django/dataclass/factory/","title":"pydantic2django.dataclass.factory","text":""},{"location":"reference/pydantic2django/dataclass/factory/#pydantic2django.dataclass.factory.DataclassFieldFactory","title":"<code>DataclassFieldFactory</code>","text":"<p>               Bases: <code>BaseFieldFactory[Field]</code></p> <p>Creates Django model fields from dataclass fields.</p> Source code in <code>src/pydantic2django/dataclass/factory.py</code> <pre><code>class DataclassFieldFactory(BaseFieldFactory[dataclasses.Field]):\n    \"\"\"Creates Django model fields from dataclass fields.\"\"\"\n\n    relationship_accessor: RelationshipConversionAccessor  # Changed Optional to required\n    bidirectional_mapper: BidirectionalTypeMapper  # Added mapper\n\n    def __init__(\n        self, relationship_accessor: RelationshipConversionAccessor, bidirectional_mapper: BidirectionalTypeMapper\n    ):\n        \"\"\"Initializes with dependencies.\"\"\"\n        self.relationship_accessor = relationship_accessor\n        self.bidirectional_mapper = bidirectional_mapper\n        # No super().__init__() needed if BaseFieldFactory.__init__ is empty or handles this\n\n    def create_field(\n        self, field_info: dataclasses.Field, model_name: str, carrier: ConversionCarrier[DataclassType]\n    ) -&gt; FieldConversionResult:\n        \"\"\"\n        Convert a dataclasses.Field to a Django field instance.\n        Uses BidirectionalTypeMapper and local instantiation.\n        Relies on field_info.metadata['django'] for specific overrides.\n        Adds required imports to the result.\n        \"\"\"\n        field_name = field_info.name\n        original_field_type = field_info.type\n        metadata = field_info.metadata or {}  # Ensure metadata is a dict\n        django_meta_options = metadata.get(\"django\", {})\n\n        # --- Resolve Forward Reference String if necessary --- #\n        type_to_map = original_field_type\n        result = FieldConversionResult(field_info=field_info, field_name=field_name)\n\n        if isinstance(original_field_type, str):\n            logger.debug(\n                f\"Field '{field_name}' has string type '{original_field_type}'. Attempting resolution via RelationshipAccessor.\"\n            )\n            # Assume string type is a model name known to the accessor\n            # Use the newly added method:\n            resolved_source_model = self.relationship_accessor.get_source_model_by_name(original_field_type)\n            if resolved_source_model:\n                logger.debug(f\"Resolved string '{original_field_type}' to type {resolved_source_model}\")\n                type_to_map = resolved_source_model\n            else:\n                # Critical Error: If it's a string but not in accessor, mapping will fail.\n                logger.error(\n                    f\"Field '{field_name}' type is string '{original_field_type}' but was not found in RelationshipAccessor. Cannot map.\"\n                )\n                result.error_str = f\"Unresolved forward reference or unknown model name: {original_field_type}\"\n                result.context_field = field_info\n                return result\n        # --- End Forward Reference Resolution ---\n\n        logger.debug(\n            f\"Processing dataclass field {model_name}.{field_name}: Type={original_field_type}, Metadata={metadata}\"\n        )\n\n        try:\n            # --- Use BidirectionalTypeMapper --- #\n            try:\n                # Pass field_info.type, but no Pydantic FieldInfo equivalent for metadata\n                # The mapper primarily relies on the type itself.\n                django_field_class, constructor_kwargs = self.bidirectional_mapper.get_django_mapping(\n                    python_type=type_to_map,\n                    field_info=None,  # Pass None for field_info\n                )\n                # Add import for the Django field class itself using the result's helper\n                result.add_import_for_obj(django_field_class)\n\n            except MappingError as e:\n                logger.error(f\"Mapping error for '{model_name}.{field_name}' (type: {type_to_map}): {e}\")\n                result.error_str = str(e)\n                result.context_field = field_info\n                return result\n            except Exception as e:\n                logger.error(\n                    f\"Unexpected error getting Django mapping for '{model_name}.{field_name}': {e}\", exc_info=True\n                )\n                result.error_str = f\"Unexpected mapping error: {e}\"\n                result.context_field = field_info\n                return result\n\n            # --- Handle GFK signal from mapper (List[Union[...]] of models) --- #\n            gfk_details = constructor_kwargs.pop(\"_gfk_details\", None)\n            if gfk_details and isinstance(gfk_details, dict):\n                if getattr(carrier, \"enable_gfk\", False):\n                    logger.info(\n                        f\"[GFK] (dataclass) Mapper signaled GFK for '{field_name}' on '{model_name}'. Recording as pending GFK child.\"\n                    )\n                    carrier.pending_gfk_children.append(\n                        {\"field_name\": field_name, \"gfk_details\": gfk_details, \"model_name\": model_name}\n                    )\n                    # Do not generate a concrete field\n                    return result\n                else:\n                    logger.warning(\n                        f\"Received _gfk_details for '{field_name}' but enable_gfk is False. Falling back to JSON field.\"\n                    )\n\n            # --- Merge Dataclass Metadata Overrides --- #\n            # Apply explicit options from metadata *after* getting defaults from mapper\n            constructor_kwargs.update(django_meta_options)\n\n            # --- Apply Dataclass Defaults --- #\n            # This logic now handles both `default` and `default_factory`.\n            if \"default\" not in constructor_kwargs:\n                default_value = field_info.default\n                if default_value is dataclasses.MISSING and field_info.default_factory is not dataclasses.MISSING:\n                    default_value = field_info.default_factory\n\n                if default_value is not dataclasses.MISSING:\n                    if callable(default_value):\n                        try:\n                            # Handle regular importable functions\n                            if (\n                                hasattr(default_value, \"__module__\")\n                                and hasattr(default_value, \"__name__\")\n                                and not default_value.__name__ == \"&lt;lambda&gt;\"\n                            ):\n                                module_name = default_value.__module__\n                                func_name = default_value.__name__\n                                # Avoid importing builtins\n                                if module_name != \"builtins\":\n                                    result.add_import(module_name, func_name)\n                                constructor_kwargs[\"default\"] = func_name\n                            # Handle lambdas\n                            else:\n                                source = inspect.getsource(default_value).strip()\n                                # Remove comma if it's trailing in a lambda definition in a list/dict\n                                if source.endswith(\",\"):\n                                    source = source[:-1]\n                                constructor_kwargs[\"default\"] = RawCode(source)\n                        except (TypeError, OSError) as e:\n                            logger.warning(\n                                f\"Could not introspect callable default for '{model_name}.{field_name}': {e}. \"\n                                \"Falling back to `None`.\"\n                            )\n                            constructor_kwargs[\"default\"] = None\n                            constructor_kwargs[\"null\"] = True\n                            constructor_kwargs[\"blank\"] = True\n                    elif not isinstance(default_value, (list, dict, set)):\n                        constructor_kwargs[\"default\"] = default_value\n                    else:\n                        logger.warning(\n                            f\"Field {model_name}.{field_name} has mutable default {default_value}. Skipping Django default.\"\n                        )\n\n            # --- Handle Relationships Specifically (Adjust Kwargs) --- #\n            is_relationship = issubclass(\n                django_field_class, (models.ForeignKey, models.OneToOneField, models.ManyToManyField)\n            )\n\n            if is_relationship:\n                if \"to\" not in constructor_kwargs:\n                    result.error_str = f\"Mapper failed to determine 'to' for relationship field '{field_name}'.\"\n                    logger.error(result.error_str)\n                    result.context_field = field_info\n                    return result\n\n                # Sanitize and ensure unique related_name\n                user_related_name = django_meta_options.get(\"related_name\")  # Check override from metadata\n                target_django_model_str = constructor_kwargs[\"to\"]\n\n                target_model_cls = None\n                target_model_cls_name_only = target_django_model_str\n                try:\n                    app_label, model_cls_name = target_django_model_str.split(\".\")\n                    target_model_cls = apps.get_model(app_label, model_cls_name)\n                    target_model_cls_name_only = model_cls_name\n                    # Add import for the target model using result helper\n                    result.add_import_for_obj(target_model_cls)\n                except Exception:\n                    logger.warning(\n                        f\"Could not get target model class for '{target_django_model_str}' when generating related_name for '{field_name}'. Using model name string.\"\n                    )\n                    target_model_cls_name_only = target_django_model_str.split(\".\")[-1]\n\n                related_name_base = (\n                    user_related_name\n                    if user_related_name\n                    # Use carrier.source_model.__name__ for default related name base\n                    else f\"{carrier.source_model.__name__.lower()}_{field_name}_set\"\n                )\n                final_related_name_base = sanitize_related_name(\n                    str(related_name_base),\n                    target_model_cls.__name__ if target_model_cls else target_model_cls_name_only,\n                    field_name,\n                )\n\n                # Ensure uniqueness using carrier's tracker\n                target_model_key_for_tracker = (\n                    target_model_cls.__name__ if target_model_cls else target_django_model_str\n                )\n                target_related_names = carrier.used_related_names_per_target.setdefault(\n                    target_model_key_for_tracker, set()\n                )\n                unique_related_name = final_related_name_base\n                counter = 1\n                while unique_related_name in target_related_names:\n                    unique_related_name = f\"{final_related_name_base}_{counter}\"\n                    counter += 1\n                target_related_names.add(unique_related_name)\n                constructor_kwargs[\"related_name\"] = unique_related_name\n                logger.debug(f\"[REL] Dataclass Field '{field_name}': Assigning related_name='{unique_related_name}'\")\n\n                # Re-confirm on_delete (mapper sets default based on Optional, but metadata might override)\n                # Need to check optionality of the original type here\n                origin = get_origin(original_field_type)\n                args = get_args(original_field_type)\n                is_optional = origin is Union and type(None) in args\n\n                if (\n                    django_field_class in (models.ForeignKey, models.OneToOneField)\n                    and \"on_delete\" not in constructor_kwargs  # Only set if not specified in metadata\n                ):\n                    constructor_kwargs[\"on_delete\"] = models.SET_NULL if is_optional else models.CASCADE\n                    # Add import using result helper\n                    result.add_import(\"django.db.models\", \"SET_NULL\" if is_optional else \"CASCADE\")\n                elif django_field_class == models.ManyToManyField:\n                    constructor_kwargs.pop(\"on_delete\", None)\n                    constructor_kwargs.pop(\"null\", None)  # M2M cannot be null\n                    if \"blank\" not in constructor_kwargs:  # Default M2M to blank=True if not set\n                        constructor_kwargs[\"blank\"] = True\n\n            # --- Perform Instantiation Locally --- #\n            try:\n                logger.debug(\n                    f\"Instantiating {django_field_class.__name__} for dataclass field '{field_name}' with kwargs: {constructor_kwargs}\"\n                )\n                result.django_field = django_field_class(**constructor_kwargs)\n                result.field_kwargs = constructor_kwargs  # Store final kwargs\n            except Exception as e:\n                error_msg = f\"Failed to instantiate Django field '{field_name}' (type: {django_field_class.__name__}) with kwargs {constructor_kwargs}: {e}\"\n                logger.error(error_msg, exc_info=True)\n                result.error_str = error_msg\n                result.context_field = field_info\n                return result\n\n            # --- Generate Field Definition String --- #\n            result.field_definition_str = self._generate_field_def_string(result, carrier.meta_app_label)\n\n            return result  # Success\n\n        except Exception as e:\n            # Catch-all for unexpected errors during conversion\n            error_msg = f\"Unexpected error converting dataclass field '{model_name}.{field_name}': {e}\"\n            logger.error(error_msg, exc_info=True)\n            result.error_str = error_msg\n            result.context_field = field_info\n            return result\n\n    def _generate_field_def_string(self, result: FieldConversionResult, app_label: str) -&gt; str:\n        \"\"\"Generates the field definition string safely.\"\"\"\n        if not result.django_field:\n            return \"# Field generation failed\"\n        try:\n            # Use stored final kwargs if available\n            if result.field_kwargs:\n                # Pass the result's required_imports to the serialization function\n                return generate_field_definition_string(\n                    type(result.django_field),\n                    result.field_kwargs,\n                    app_label,\n                )\n            else:\n                # Fallback: Basic serialization if final kwargs weren't stored for some reason\n                logger.warning(\n                    f\"Could not generate definition string for '{result.field_name}': final kwargs not found in result. Using basic serialization.\"\n                )\n                return FieldSerializer.serialize_field(result.django_field)\n        except Exception as e:\n            logger.error(\n                f\"Failed to generate field definition string for '{result.field_name}': {e}\",\n                exc_info=True,\n            )\n            return f\"# Error generating definition: {e}\"\n</code></pre>"},{"location":"reference/pydantic2django/dataclass/factory/#pydantic2django.dataclass.factory.DataclassFieldFactory.__init__","title":"<code>__init__(relationship_accessor, bidirectional_mapper)</code>","text":"<p>Initializes with dependencies.</p> Source code in <code>src/pydantic2django/dataclass/factory.py</code> <pre><code>def __init__(\n    self, relationship_accessor: RelationshipConversionAccessor, bidirectional_mapper: BidirectionalTypeMapper\n):\n    \"\"\"Initializes with dependencies.\"\"\"\n    self.relationship_accessor = relationship_accessor\n    self.bidirectional_mapper = bidirectional_mapper\n</code></pre>"},{"location":"reference/pydantic2django/dataclass/factory/#pydantic2django.dataclass.factory.DataclassFieldFactory.create_field","title":"<code>create_field(field_info, model_name, carrier)</code>","text":"<p>Convert a dataclasses.Field to a Django field instance. Uses BidirectionalTypeMapper and local instantiation. Relies on field_info.metadata['django'] for specific overrides. Adds required imports to the result.</p> Source code in <code>src/pydantic2django/dataclass/factory.py</code> <pre><code>def create_field(\n    self, field_info: dataclasses.Field, model_name: str, carrier: ConversionCarrier[DataclassType]\n) -&gt; FieldConversionResult:\n    \"\"\"\n    Convert a dataclasses.Field to a Django field instance.\n    Uses BidirectionalTypeMapper and local instantiation.\n    Relies on field_info.metadata['django'] for specific overrides.\n    Adds required imports to the result.\n    \"\"\"\n    field_name = field_info.name\n    original_field_type = field_info.type\n    metadata = field_info.metadata or {}  # Ensure metadata is a dict\n    django_meta_options = metadata.get(\"django\", {})\n\n    # --- Resolve Forward Reference String if necessary --- #\n    type_to_map = original_field_type\n    result = FieldConversionResult(field_info=field_info, field_name=field_name)\n\n    if isinstance(original_field_type, str):\n        logger.debug(\n            f\"Field '{field_name}' has string type '{original_field_type}'. Attempting resolution via RelationshipAccessor.\"\n        )\n        # Assume string type is a model name known to the accessor\n        # Use the newly added method:\n        resolved_source_model = self.relationship_accessor.get_source_model_by_name(original_field_type)\n        if resolved_source_model:\n            logger.debug(f\"Resolved string '{original_field_type}' to type {resolved_source_model}\")\n            type_to_map = resolved_source_model\n        else:\n            # Critical Error: If it's a string but not in accessor, mapping will fail.\n            logger.error(\n                f\"Field '{field_name}' type is string '{original_field_type}' but was not found in RelationshipAccessor. Cannot map.\"\n            )\n            result.error_str = f\"Unresolved forward reference or unknown model name: {original_field_type}\"\n            result.context_field = field_info\n            return result\n    # --- End Forward Reference Resolution ---\n\n    logger.debug(\n        f\"Processing dataclass field {model_name}.{field_name}: Type={original_field_type}, Metadata={metadata}\"\n    )\n\n    try:\n        # --- Use BidirectionalTypeMapper --- #\n        try:\n            # Pass field_info.type, but no Pydantic FieldInfo equivalent for metadata\n            # The mapper primarily relies on the type itself.\n            django_field_class, constructor_kwargs = self.bidirectional_mapper.get_django_mapping(\n                python_type=type_to_map,\n                field_info=None,  # Pass None for field_info\n            )\n            # Add import for the Django field class itself using the result's helper\n            result.add_import_for_obj(django_field_class)\n\n        except MappingError as e:\n            logger.error(f\"Mapping error for '{model_name}.{field_name}' (type: {type_to_map}): {e}\")\n            result.error_str = str(e)\n            result.context_field = field_info\n            return result\n        except Exception as e:\n            logger.error(\n                f\"Unexpected error getting Django mapping for '{model_name}.{field_name}': {e}\", exc_info=True\n            )\n            result.error_str = f\"Unexpected mapping error: {e}\"\n            result.context_field = field_info\n            return result\n\n        # --- Handle GFK signal from mapper (List[Union[...]] of models) --- #\n        gfk_details = constructor_kwargs.pop(\"_gfk_details\", None)\n        if gfk_details and isinstance(gfk_details, dict):\n            if getattr(carrier, \"enable_gfk\", False):\n                logger.info(\n                    f\"[GFK] (dataclass) Mapper signaled GFK for '{field_name}' on '{model_name}'. Recording as pending GFK child.\"\n                )\n                carrier.pending_gfk_children.append(\n                    {\"field_name\": field_name, \"gfk_details\": gfk_details, \"model_name\": model_name}\n                )\n                # Do not generate a concrete field\n                return result\n            else:\n                logger.warning(\n                    f\"Received _gfk_details for '{field_name}' but enable_gfk is False. Falling back to JSON field.\"\n                )\n\n        # --- Merge Dataclass Metadata Overrides --- #\n        # Apply explicit options from metadata *after* getting defaults from mapper\n        constructor_kwargs.update(django_meta_options)\n\n        # --- Apply Dataclass Defaults --- #\n        # This logic now handles both `default` and `default_factory`.\n        if \"default\" not in constructor_kwargs:\n            default_value = field_info.default\n            if default_value is dataclasses.MISSING and field_info.default_factory is not dataclasses.MISSING:\n                default_value = field_info.default_factory\n\n            if default_value is not dataclasses.MISSING:\n                if callable(default_value):\n                    try:\n                        # Handle regular importable functions\n                        if (\n                            hasattr(default_value, \"__module__\")\n                            and hasattr(default_value, \"__name__\")\n                            and not default_value.__name__ == \"&lt;lambda&gt;\"\n                        ):\n                            module_name = default_value.__module__\n                            func_name = default_value.__name__\n                            # Avoid importing builtins\n                            if module_name != \"builtins\":\n                                result.add_import(module_name, func_name)\n                            constructor_kwargs[\"default\"] = func_name\n                        # Handle lambdas\n                        else:\n                            source = inspect.getsource(default_value).strip()\n                            # Remove comma if it's trailing in a lambda definition in a list/dict\n                            if source.endswith(\",\"):\n                                source = source[:-1]\n                            constructor_kwargs[\"default\"] = RawCode(source)\n                    except (TypeError, OSError) as e:\n                        logger.warning(\n                            f\"Could not introspect callable default for '{model_name}.{field_name}': {e}. \"\n                            \"Falling back to `None`.\"\n                        )\n                        constructor_kwargs[\"default\"] = None\n                        constructor_kwargs[\"null\"] = True\n                        constructor_kwargs[\"blank\"] = True\n                elif not isinstance(default_value, (list, dict, set)):\n                    constructor_kwargs[\"default\"] = default_value\n                else:\n                    logger.warning(\n                        f\"Field {model_name}.{field_name} has mutable default {default_value}. Skipping Django default.\"\n                    )\n\n        # --- Handle Relationships Specifically (Adjust Kwargs) --- #\n        is_relationship = issubclass(\n            django_field_class, (models.ForeignKey, models.OneToOneField, models.ManyToManyField)\n        )\n\n        if is_relationship:\n            if \"to\" not in constructor_kwargs:\n                result.error_str = f\"Mapper failed to determine 'to' for relationship field '{field_name}'.\"\n                logger.error(result.error_str)\n                result.context_field = field_info\n                return result\n\n            # Sanitize and ensure unique related_name\n            user_related_name = django_meta_options.get(\"related_name\")  # Check override from metadata\n            target_django_model_str = constructor_kwargs[\"to\"]\n\n            target_model_cls = None\n            target_model_cls_name_only = target_django_model_str\n            try:\n                app_label, model_cls_name = target_django_model_str.split(\".\")\n                target_model_cls = apps.get_model(app_label, model_cls_name)\n                target_model_cls_name_only = model_cls_name\n                # Add import for the target model using result helper\n                result.add_import_for_obj(target_model_cls)\n            except Exception:\n                logger.warning(\n                    f\"Could not get target model class for '{target_django_model_str}' when generating related_name for '{field_name}'. Using model name string.\"\n                )\n                target_model_cls_name_only = target_django_model_str.split(\".\")[-1]\n\n            related_name_base = (\n                user_related_name\n                if user_related_name\n                # Use carrier.source_model.__name__ for default related name base\n                else f\"{carrier.source_model.__name__.lower()}_{field_name}_set\"\n            )\n            final_related_name_base = sanitize_related_name(\n                str(related_name_base),\n                target_model_cls.__name__ if target_model_cls else target_model_cls_name_only,\n                field_name,\n            )\n\n            # Ensure uniqueness using carrier's tracker\n            target_model_key_for_tracker = (\n                target_model_cls.__name__ if target_model_cls else target_django_model_str\n            )\n            target_related_names = carrier.used_related_names_per_target.setdefault(\n                target_model_key_for_tracker, set()\n            )\n            unique_related_name = final_related_name_base\n            counter = 1\n            while unique_related_name in target_related_names:\n                unique_related_name = f\"{final_related_name_base}_{counter}\"\n                counter += 1\n            target_related_names.add(unique_related_name)\n            constructor_kwargs[\"related_name\"] = unique_related_name\n            logger.debug(f\"[REL] Dataclass Field '{field_name}': Assigning related_name='{unique_related_name}'\")\n\n            # Re-confirm on_delete (mapper sets default based on Optional, but metadata might override)\n            # Need to check optionality of the original type here\n            origin = get_origin(original_field_type)\n            args = get_args(original_field_type)\n            is_optional = origin is Union and type(None) in args\n\n            if (\n                django_field_class in (models.ForeignKey, models.OneToOneField)\n                and \"on_delete\" not in constructor_kwargs  # Only set if not specified in metadata\n            ):\n                constructor_kwargs[\"on_delete\"] = models.SET_NULL if is_optional else models.CASCADE\n                # Add import using result helper\n                result.add_import(\"django.db.models\", \"SET_NULL\" if is_optional else \"CASCADE\")\n            elif django_field_class == models.ManyToManyField:\n                constructor_kwargs.pop(\"on_delete\", None)\n                constructor_kwargs.pop(\"null\", None)  # M2M cannot be null\n                if \"blank\" not in constructor_kwargs:  # Default M2M to blank=True if not set\n                    constructor_kwargs[\"blank\"] = True\n\n        # --- Perform Instantiation Locally --- #\n        try:\n            logger.debug(\n                f\"Instantiating {django_field_class.__name__} for dataclass field '{field_name}' with kwargs: {constructor_kwargs}\"\n            )\n            result.django_field = django_field_class(**constructor_kwargs)\n            result.field_kwargs = constructor_kwargs  # Store final kwargs\n        except Exception as e:\n            error_msg = f\"Failed to instantiate Django field '{field_name}' (type: {django_field_class.__name__}) with kwargs {constructor_kwargs}: {e}\"\n            logger.error(error_msg, exc_info=True)\n            result.error_str = error_msg\n            result.context_field = field_info\n            return result\n\n        # --- Generate Field Definition String --- #\n        result.field_definition_str = self._generate_field_def_string(result, carrier.meta_app_label)\n\n        return result  # Success\n\n    except Exception as e:\n        # Catch-all for unexpected errors during conversion\n        error_msg = f\"Unexpected error converting dataclass field '{model_name}.{field_name}': {e}\"\n        logger.error(error_msg, exc_info=True)\n        result.error_str = error_msg\n        result.context_field = field_info\n        return result\n</code></pre>"},{"location":"reference/pydantic2django/dataclass/factory/#pydantic2django.dataclass.factory.DataclassModelFactory","title":"<code>DataclassModelFactory</code>","text":"<p>               Bases: <code>BaseModelFactory[DataclassType, Field]</code></p> <p>Dynamically creates Django model classes from dataclasses.</p> Source code in <code>src/pydantic2django/dataclass/factory.py</code> <pre><code>class DataclassModelFactory(BaseModelFactory[DataclassType, dataclasses.Field]):\n    \"\"\"Dynamically creates Django model classes from dataclasses.\"\"\"\n\n    # Cache specific to Dataclass models\n    _converted_models: dict[str, ConversionCarrier[DataclassType]] = {}\n\n    relationship_accessor: RelationshipConversionAccessor  # Changed Optional to required\n    import_handler: ImportHandler  # Added import handler\n\n    def __init__(\n        self,\n        field_factory: DataclassFieldFactory,\n        relationship_accessor: RelationshipConversionAccessor,  # Now required\n        import_handler: Optional[ImportHandler] = None,  # Accept optionally\n    ):\n        \"\"\"Initialize with field factory, relationship accessor, and import handler.\"\"\"\n        self.relationship_accessor = relationship_accessor\n        self.import_handler = import_handler or ImportHandler()\n        # Call super init\n        super().__init__(field_factory)\n\n    def make_django_model(self, carrier: ConversionCarrier[DataclassType]) -&gt; None:\n        \"\"\"\n        Orchestrates the Django model creation process.\n        Subclasses implement _process_source_fields and _build_model_context.\n        Handles caching.\n        Passes import handler down.\n        \"\"\"\n        # --- Pass import handler via carrier --- (or could add to factory state)\n        # Need to set import handler on carrier if passed during init\n        # NOTE: BaseModelFactory.make_django_model does this now.\n        # carrier.import_handler = self.import_handler\n        super().make_django_model(carrier)\n        # Register relationship after successful model creation (moved from original)\n        if carrier.source_model and carrier.django_model:\n            logger.debug(\n                f\"Mapping relationship in accessor: {carrier.source_model.__name__} -&gt; {carrier.django_model.__name__}\"\n            )\n            self.relationship_accessor.map_relationship(\n                source_model=carrier.source_model, django_model=carrier.django_model\n            )\n        # Cache result (moved from original)\n        model_key = carrier.model_key()\n        if carrier.django_model and not carrier.existing_model:\n            self._converted_models[model_key] = carrier\n\n    def _process_source_fields(self, carrier: ConversionCarrier[DataclassType]):\n        \"\"\"Iterate through source dataclass fields, resolve types, create Django fields, and store results.\"\"\"\n        source_model = carrier.source_model\n        if not source_model:\n            logger.error(\n                f\"Cannot process fields: source model missing in carrier for {getattr(carrier, 'target_model_name', '?')}\"\n            )  # Safely access target_model_name\n            carrier.invalid_fields.append((\"_source_model\", \"Source model missing.\"))  # Use invalid_fields\n            return\n\n        # --- Add check: Ensure source_model is a type ---\n        if not isinstance(source_model, type):\n            error_msg = f\"Cannot process fields: expected source_model to be a type, but got {type(source_model)} ({source_model!r}). Problem likely upstream in model discovery/ordering.\"\n            logger.error(error_msg)\n            carrier.invalid_fields.append((\"_source_model\", error_msg))\n            return\n        # --- End Add check ---\n\n        # --- Use dataclasses.fields for introspection ---\n        try:\n            # Resolve type hints first to handle forward references (strings)\n            # Need globals and potentially locals from the source model's module\n            source_module = sys.modules.get(source_model.__module__)\n            globalns = getattr(source_module, \"__dict__\", None)\n            # Revert: Use only globals, assuming types are resolvable in module scope\n\n            # resolved_types = get_type_hints(source_model, globalns=globalns, localns=localns)\n            # logger.debug(f\"Resolved types for {source_model.__name__} using {globalns=}, {localns=}: {resolved_types}\")\n            # Use updated call without potentially incorrect locals:\n            resolved_types = get_type_hints(source_model, globalns=globalns, localns=None)\n            logger.debug(f\"Resolved types for {source_model.__name__} using module globals: {resolved_types}\")\n\n            dataclass_fields = dataclasses.fields(source_model)\n        except (TypeError, NameError) as e:  # Catch errors during type hint resolution or fields() call\n            error_msg = f\"Could not introspect fields or resolve types for {source_model.__name__}: {e}\"\n            logger.error(error_msg, exc_info=True)\n            carrier.invalid_fields.append((\"_introspection\", error_msg))  # Use invalid_fields\n            return\n\n        # Use field definitions directly from carrier\n        # field_definitions: dict[str, str] = {}\n        # context_field_definitions: dict[str, str] = {} # Dataclasses likely won't use this\n\n        for field_info in dataclass_fields:\n            field_name = field_info.name\n\n            # Normalize output Django field identifier\n            try:\n                from ..core.utils.naming import sanitize_field_identifier\n\n                normalized_field_name = sanitize_field_identifier(field_name)\n            except Exception:\n                normalized_field_name = field_name\n\n            # Get the *resolved* type for this field\n            resolved_type = resolved_types.get(field_name)\n            if resolved_type is None:\n                logger.warning(\n                    f\"Could not resolve type hint for field '{field_name}' in {source_model.__name__}. Using original: {field_info.type!r}\"\n                )\n                # Fallback to original, which might be a string\n                resolved_type = field_info.type\n\n            # --- Prepare field type for create_field --- #\n            type_for_create_field = resolved_type  # Start with the result from get_type_hints\n\n            # If the original type annotation was a string (ForwardRef)\n            # and get_type_hints failed to resolve it (resolved_type is None or still the string),\n            # try to find the resolved type from the dict populated earlier.\n            # This specifically handles nested dataclasses defined in local scopes like fixtures.\n            if isinstance(field_info.type, str):\n                explicitly_resolved = resolved_types.get(field_name)\n                if explicitly_resolved and not isinstance(explicitly_resolved, str):\n                    logger.debug(\n                        f\"Using explicitly resolved type {explicitly_resolved!r} for forward ref '{field_info.type}'\"\n                    )\n                    type_for_create_field = explicitly_resolved\n                elif resolved_type is field_info.type:  # Check if resolved_type is still the unresolved string\n                    logger.error(\n                        f\"Type hint for '{field_name}' is string '{field_info.type}' but was not resolved by get_type_hints. Skipping field.\"\n                    )\n                    carrier.invalid_fields.append(\n                        (field_name, f\"Could not resolve forward reference: {field_info.type}\")\n                    )\n                    continue  # Skip this field\n\n            # Temporarily modify a copy of field_info or pass type directly if possible.\n            # Modifying field_info directly is simpler for now.\n            original_type_attr = field_info.type\n            try:\n                field_info.type = type_for_create_field  # Use the determined type\n                logger.debug(f\"Calling create_field for '{field_name}' with type: {field_info.type!r}\")\n\n                field_result = self.field_factory.create_field(\n                    field_info=field_info, model_name=source_model.__name__, carrier=carrier\n                )\n            finally:\n                # Restore original type attribute\n                field_info.type = original_type_attr\n                logger.debug(\"Restored original field_info.type attribute\")\n\n            # Process the result (errors, definitions)\n            if field_result.error_str:\n                carrier.invalid_fields.append((normalized_field_name, field_result.error_str))\n            else:\n                # Store the definition string if available\n                if field_result.field_definition_str:\n                    carrier.django_field_definitions[normalized_field_name] = field_result.field_definition_str\n                else:\n                    logger.warning(\n                        f\"Field '{normalized_field_name}' processing yielded no error and no definition string.\"\n                    )\n\n                # Store the actual field instance in the correct carrier dict\n                if field_result.django_field:\n                    if isinstance(\n                        field_result.django_field, (models.ForeignKey, models.OneToOneField, models.ManyToManyField)\n                    ):\n                        carrier.relationship_fields[normalized_field_name] = field_result.django_field\n                    else:\n                        carrier.django_fields[normalized_field_name] = field_result.django_field\n                elif field_result.context_field:\n                    # Handle context fields if needed (currently seems unused based on logs)\n                    carrier.context_fields[normalized_field_name] = field_result.context_field\n\n                # Merge imports from result into the factory's import handler\n                if field_result.required_imports:\n                    # Use the new add_import method\n                    for module, names in field_result.required_imports.items():\n                        for name in names:\n                            self.import_handler.add_import(module=module, name=name)\n\n        logger.debug(f\"Finished processing fields for {source_model.__name__}. Errors: {len(carrier.invalid_fields)}\")\n\n    # Actual implementation of the abstract method _build_model_context\n    def _build_model_context(self, carrier: ConversionCarrier[DataclassType]):\n        \"\"\"Builds the ModelContext specifically for dataclass source models.\"\"\"\n        if not carrier.source_model or not carrier.django_model:\n            logger.debug(\"Skipping context build: missing source or django model.\")\n            return\n\n        try:\n            # Remove generic type hint if ModelContext is not generic or if causing issues\n            # Assuming ModelContext base class handles the source type appropriately\n            model_context = ModelContext(django_model=carrier.django_model, source_class=carrier.source_model)\n            for field_name, field_info in carrier.context_fields.items():\n                if isinstance(field_info, dataclasses.Field):\n                    # Calculate necessary info for ModelContext.add_field\n                    origin = get_origin(field_info.type)\n                    args = get_args(field_info.type)\n                    is_optional = origin is Union and type(None) in args\n                    field_type_str = repr(field_info.type)  # Use repr for the type string\n\n                    # Call add_field with expected signature\n                    model_context.add_field(\n                        field_name=field_name,\n                        field_type_str=field_type_str,\n                        is_optional=is_optional,\n                        # Pass original annotation if ModelContext uses it\n                        annotation=field_info.type,\n                    )\n                else:\n                    # Log if context field is not the expected type\n                    logger.warning(\n                        f\"Context field '{field_name}' is not a dataclasses.Field ({type(field_info)}), cannot add to ModelContext.\"\n                    )\n            carrier.model_context = model_context\n            logger.debug(f\"Successfully built ModelContext for {carrier.model_key()}\")  # Use method call\n        except Exception as e:\n            logger.error(f\"Failed to build ModelContext for {carrier.model_key()}: {e}\", exc_info=True)\n            carrier.model_context = None\n</code></pre>"},{"location":"reference/pydantic2django/dataclass/factory/#pydantic2django.dataclass.factory.DataclassModelFactory.__init__","title":"<code>__init__(field_factory, relationship_accessor, import_handler=None)</code>","text":"<p>Initialize with field factory, relationship accessor, and import handler.</p> Source code in <code>src/pydantic2django/dataclass/factory.py</code> <pre><code>def __init__(\n    self,\n    field_factory: DataclassFieldFactory,\n    relationship_accessor: RelationshipConversionAccessor,  # Now required\n    import_handler: Optional[ImportHandler] = None,  # Accept optionally\n):\n    \"\"\"Initialize with field factory, relationship accessor, and import handler.\"\"\"\n    self.relationship_accessor = relationship_accessor\n    self.import_handler = import_handler or ImportHandler()\n    # Call super init\n    super().__init__(field_factory)\n</code></pre>"},{"location":"reference/pydantic2django/dataclass/factory/#pydantic2django.dataclass.factory.DataclassModelFactory.make_django_model","title":"<code>make_django_model(carrier)</code>","text":"<p>Orchestrates the Django model creation process. Subclasses implement _process_source_fields and _build_model_context. Handles caching. Passes import handler down.</p> Source code in <code>src/pydantic2django/dataclass/factory.py</code> <pre><code>def make_django_model(self, carrier: ConversionCarrier[DataclassType]) -&gt; None:\n    \"\"\"\n    Orchestrates the Django model creation process.\n    Subclasses implement _process_source_fields and _build_model_context.\n    Handles caching.\n    Passes import handler down.\n    \"\"\"\n    # --- Pass import handler via carrier --- (or could add to factory state)\n    # Need to set import handler on carrier if passed during init\n    # NOTE: BaseModelFactory.make_django_model does this now.\n    # carrier.import_handler = self.import_handler\n    super().make_django_model(carrier)\n    # Register relationship after successful model creation (moved from original)\n    if carrier.source_model and carrier.django_model:\n        logger.debug(\n            f\"Mapping relationship in accessor: {carrier.source_model.__name__} -&gt; {carrier.django_model.__name__}\"\n        )\n        self.relationship_accessor.map_relationship(\n            source_model=carrier.source_model, django_model=carrier.django_model\n        )\n    # Cache result (moved from original)\n    model_key = carrier.model_key()\n    if carrier.django_model and not carrier.existing_model:\n        self._converted_models[model_key] = carrier\n</code></pre>"},{"location":"reference/pydantic2django/dataclass/factory/#pydantic2django.dataclass.factory.create_dataclass_factory","title":"<code>create_dataclass_factory(relationship_accessor, bidirectional_mapper)</code>","text":"<p>Helper function to create a DataclassModelFactory with dependencies.</p> Source code in <code>src/pydantic2django/dataclass/factory.py</code> <pre><code>def create_dataclass_factory(\n    relationship_accessor: RelationshipConversionAccessor, bidirectional_mapper: BidirectionalTypeMapper\n) -&gt; DataclassModelFactory:\n    \"\"\"Helper function to create a DataclassModelFactory with dependencies.\"\"\"\n    field_factory = DataclassFieldFactory(relationship_accessor, bidirectional_mapper)\n    # Create ImportHandler instance here for the factory\n    import_handler = ImportHandler()\n    return DataclassModelFactory(\n        field_factory=field_factory,\n        relationship_accessor=relationship_accessor,\n        import_handler=import_handler,\n    )\n</code></pre>"},{"location":"reference/pydantic2django/dataclass/generator/","title":"pydantic2django.dataclass.generator","text":""},{"location":"reference/pydantic2django/dataclass/generator/#pydantic2django.dataclass.generator.DataclassDjangoModelGenerator","title":"<code>DataclassDjangoModelGenerator</code>","text":"<p>               Bases: <code>BaseStaticGenerator[DataclassType, DataclassFieldInfo]</code></p> <p>Generates Django models.py file content from Python dataclasses.</p> Source code in <code>src/pydantic2django/dataclass/generator.py</code> <pre><code>class DataclassDjangoModelGenerator(\n    BaseStaticGenerator[DataclassType, DataclassFieldInfo]  # Inherit from BaseStaticGenerator\n):\n    \"\"\"Generates Django models.py file content from Python dataclasses.\"\"\"\n\n    def __init__(\n        self,\n        output_path: str,\n        app_label: str,\n        filter_function: Optional[Callable[[DataclassType], bool]],\n        verbose: bool,\n        # Accept specific discovery and factories, or create defaults\n        packages: list[str] | None = None,\n        discovery_instance: Optional[DataclassDiscovery] = None,\n        model_factory_instance: Optional[DataclassModelFactory] = None,\n        field_factory_instance: Optional[DataclassFieldFactory] = None,  # Add field factory param\n        relationship_accessor: Optional[RelationshipConversionAccessor] = None,  # Accept accessor\n        module_mappings: Optional[dict[str, str]] = None,\n        enable_timescale: bool = True,\n        # --- GFK flags ---\n        enable_gfk: bool = True,\n        gfk_policy: str | None = \"threshold_by_children\",\n        gfk_threshold_children: int | None = 8,\n        gfk_value_mode: str | None = \"typed_columns\",\n        gfk_normalize_common_attrs: bool = False,\n    ):\n        # 1. Initialize Dataclass-specific discovery\n        self.dataclass_discovery_instance = discovery_instance or DataclassDiscovery()\n\n        # 2. Initialize Dataclass-specific factories\n        # Dataclass factories might not need RelationshipAccessor, check their definitions\n        # Assuming they don't for now.\n        # --- Correction: They DO need them now ---\n        # Use provided accessor or create a new one\n        self.relationship_accessor = relationship_accessor or RelationshipConversionAccessor()\n        # Create mapper using the (potentially provided) accessor\n        self.bidirectional_mapper = BidirectionalTypeMapper(relationship_accessor=self.relationship_accessor)\n\n        self.dataclass_field_factory = field_factory_instance or DataclassFieldFactory(\n            relationship_accessor=self.relationship_accessor,\n            bidirectional_mapper=self.bidirectional_mapper,\n        )\n        self.dataclass_model_factory = model_factory_instance or DataclassModelFactory(\n            field_factory=self.dataclass_field_factory,\n            relationship_accessor=self.relationship_accessor,  # Pass only accessor\n        )\n\n        # 3. Call the base class __init__\n        super().__init__(\n            output_path=output_path,\n            packages=packages,\n            app_label=app_label,\n            filter_function=filter_function,\n            verbose=verbose,\n            discovery_instance=self.dataclass_discovery_instance,\n            model_factory_instance=self.dataclass_model_factory,\n            module_mappings=module_mappings,\n            base_model_class=self._get_default_base_model_class(),\n            enable_timescale=enable_timescale,\n            enable_gfk=enable_gfk,\n            gfk_policy=gfk_policy,\n            gfk_threshold_children=gfk_threshold_children,\n            gfk_value_mode=gfk_value_mode,\n            gfk_normalize_common_attrs=gfk_normalize_common_attrs,\n        )\n        logger.info(\"DataclassDjangoModelGenerator initialized using BaseStaticGenerator.\")\n        # Timescale classification results cached per run (name -&gt; role)\n        self._timescale_roles: dict[str, TimescaleRole] = {}\n\n    # --- Implement abstract methods from BaseStaticGenerator ---\n\n    def _get_source_model_name(self, carrier: ConversionCarrier[DataclassType]) -&gt; str:\n        \"\"\"Get the name of the original dataclass from the carrier.\"\"\"\n        # Use carrier.source_model (consistent with Base class)\n        if carrier.source_model:\n            return carrier.source_model.__name__\n        # Fallback if source model somehow missing\n        # Check if carrier has pydantic_model attribute as a legacy fallback?\n        legacy_model = getattr(carrier, \"pydantic_model\", None)  # Safely check old attribute\n        if legacy_model:\n            return legacy_model.__name__\n        return \"UnknownDataclass\"\n\n    def _add_source_model_import(self, carrier: ConversionCarrier[DataclassType]):\n        \"\"\"Add the necessary import for the original dataclass.\"\"\"\n        # Use carrier.source_model\n        model_to_import = carrier.source_model\n        if not model_to_import:\n            # Legacy fallback check\n            model_to_import = getattr(carrier, \"pydantic_model\", None)\n\n        if model_to_import:\n            # Use add_pydantic_model_import for consistency? Or add_context_field_type_import?\n            # Let's assume add_context_field_type_import handles dataclasses too.\n            # A dedicated add_dataclass_import or add_general_import would be clearer.\n            self.import_handler.add_context_field_type_import(model_to_import)\n        else:\n            logger.warning(\"Cannot add source model import: source model missing in carrier.\")\n\n    def _prepare_template_context(self, unique_model_definitions, django_model_names, imports) -&gt; dict:\n        \"\"\"Prepare the context specific to dataclasses for the main models_file.py.j2 template.\"\"\"\n        # Base context items are passed in.\n        # Add Dataclass-specific items.\n        base_context = {\n            \"model_definitions\": unique_model_definitions,  # Already joined by base class\n            \"django_model_names\": django_model_names,  # Already list of quoted names\n            # Pass the structured imports dict\n            \"imports\": imports,\n            # --- Dataclass Specific ---\n            \"generation_source_type\": \"dataclass\",  # Flag for template logic\n            # --- Keep compatibility if templates expect these --- (review templates later)\n            # \"django_imports\": sorted(imports.get(\"django\", [])), # Provided by imports dict\n            # \"pydantic_imports\": sorted(imports.get(\"pydantic\", [])), # Likely empty for dataclass\n            # \"general_imports\": sorted(imports.get(\"general\", [])),\n            # \"context_imports\": sorted(imports.get(\"context\", [])),\n            # Add other dataclass specific flags/lists if needed by the template\n            \"context_definitions\": [],  # Dataclasses don't have separate context classes? Assume empty.\n            \"context_class_names\": [],\n            \"model_has_context\": {},  # Assume no context model mapping needed\n        }\n        # Common items added by base class generate_models_file after this call.\n        return base_context\n\n    def _get_models_in_processing_order(self) -&gt; list[DataclassType]:\n        \"\"\"Return dataclasses in dependency order using the discovery instance.\"\"\"\n        # Add assertion for type checker clarity\n        assert isinstance(\n            self.discovery_instance, DataclassDiscovery\n        ), \"Discovery instance must be DataclassDiscovery for this generator\"\n        # Dependencies analyzed by base class discover_models call\n        return self.discovery_instance.get_models_in_registration_order()\n\n    def _get_model_definition_extra_context(self, carrier: ConversionCarrier[DataclassType]) -&gt; dict:\n        \"\"\"Provide extra context specific to dataclasses for model_definition.py.j2.\"\"\"\n        # Removed problematic metadata access from original\n        # Add flags for template conditional logic\n        return {\n            \"is_dataclass_source\": True,\n            \"is_pydantic_source\": False,\n            \"has_context\": False,  # Dataclasses likely don't generate separate context fields/classes\n            # Pass the field definitions dictionary from the carrier\n            \"field_definitions\": carrier.django_field_definitions,\n            # Add other specific details if needed, ensuring they access carrier correctly\n            # Example: \"source_model_module\": carrier.source_model.__module__ if carrier.source_model else \"\"\n        }\n\n    # Choose Timescale base per model (lazy roles computation)\n    def setup_django_model(self, source_model: DataclassType) -&gt; ConversionCarrier | None:  # type: ignore[override]\n        try:\n            from pydantic2django.django.timescale.bases import DataclassTimescaleBase\n            from pydantic2django.django.timescale.heuristics import (\n                classify_dataclass_types,\n                should_use_timescale_base,\n            )\n        except Exception:\n            classify_dataclass_types = None  # type: ignore\n            should_use_timescale_base = None  # type: ignore\n            DataclassTimescaleBase = None  # type: ignore\n\n        # Compute roles lazily if not present\n        if self.enable_timescale and not getattr(self, \"_timescale_roles\", None):\n            roles: dict[str, TimescaleRole] = {}\n            try:\n                models_to_score = []\n                try:\n                    models_to_score = self._get_models_in_processing_order() or []\n                except Exception:\n                    pass\n                if not models_to_score:\n                    models_to_score = [source_model]\n                if classify_dataclass_types:\n                    roles = classify_dataclass_types(models_to_score)\n            except Exception:\n                roles = {}\n            self._timescale_roles = roles\n\n        # Select base class\n        base_cls: type[models.Model] = self.base_model_class\n        if self.enable_timescale:\n            try:\n                name = source_model.__name__\n                if should_use_timescale_base and DataclassTimescaleBase:\n                    if should_use_timescale_base(name, self._timescale_roles):  # type: ignore[arg-type]\n                        base_cls = DataclassTimescaleBase\n            except Exception:\n                pass\n\n        prev_base = self.base_model_class\n        self.base_model_class = base_cls\n        try:\n            carrier = super().setup_django_model(source_model)\n        finally:\n            self.base_model_class = prev_base\n\n        if carrier is not None:\n            carrier.context_data[\"_timescale_roles\"] = getattr(self, \"_timescale_roles\", {})\n        return carrier\n\n    def _get_default_base_model_class(self) -&gt; type[models.Model]:\n        \"\"\"Return the default Django base model for Dataclass conversion.\"\"\"\n        if not django_apps.ready:\n            raise AppRegistryNotReady(\n                \"Django apps are not loaded. Call django.setup() or run within a configured Django context before \"\n                \"instantiating DataclassDjangoModelGenerator.\"\n            )\n        try:\n            from typed2django.django.models import Dataclass2DjangoBaseClass as _Base\n\n            return _Base\n        except Exception as exc:  # pragma: no cover - defensive\n            raise ImportError(\n                \"typed2django.django.models.Dataclass2DjangoBaseClass is required for Dataclass generation.\"\n            ) from exc\n\n    def generate_models_file(self) -&gt; str:\n        \"\"\"Generate models for dataclasses with GFK finalize hook.\"\"\"\n        self.discover_models()\n        models_to_process = self._get_models_in_processing_order()\n\n        # Reset imports and state\n        self.carriers = []\n        self.import_handler.extra_type_imports.clear()\n        self.import_handler.pydantic_imports.clear()\n        self.import_handler.context_class_imports.clear()\n        self.import_handler.imported_names.clear()\n        self.import_handler.processed_field_types.clear()\n        self.import_handler._add_type_import(self.base_model_class)\n\n        # Setup carriers\n        for source_model in models_to_process:\n            self.setup_django_model(source_model)\n\n        # GFK finalize: inject GenericRelation on parents\n        gfk_used = False\n        for carrier in self.carriers:\n            try:\n                if getattr(carrier, \"enable_gfk\", False) and getattr(carrier, \"pending_gfk_children\", None):\n                    self.import_handler.add_import(\"django.contrib.contenttypes.fields\", \"GenericRelation\")\n                    self.import_handler.add_import(\"django.contrib.contenttypes.fields\", \"GenericForeignKey\")\n                    self.import_handler.add_import(\"django.contrib.contenttypes.models\", \"ContentType\")\n                    carrier.django_field_definitions[\n                        \"entries\"\n                    ] = \"GenericRelation('GenericEntry', related_query_name='entries')\"\n                    gfk_used = True\n            except Exception:\n                pass\n\n        # Render model definitions\n        model_definitions: list[str] = []\n        django_model_names: list[str] = []\n        for carrier in self.carriers:\n            if carrier.django_model:\n                try:\n                    model_def = self.generate_model_definition(carrier)\n                    if model_def:\n                        model_definitions.append(model_def)\n                        django_model_names.append(f\"'{self._clean_generic_type(carrier.django_model.__name__)}'\")\n                except Exception:\n                    pass\n\n        if gfk_used:\n            model_definitions.append(self._build_generic_entry_model_definition())\n            django_model_names.append(\"'GenericEntry'\")\n\n        unique_model_definitions = self._deduplicate_definitions(model_definitions)\n        imports = self.import_handler.deduplicate_imports()\n        template_context = self._prepare_template_context(unique_model_definitions, django_model_names, imports)\n        template_context.update(\n            {\n                \"generation_timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n                \"base_model_module\": self.base_model_class.__module__,\n                \"base_model_name\": self.base_model_class.__name__,\n                \"extra_type_imports\": sorted(self.import_handler.extra_type_imports),\n            }\n        )\n        template = self.jinja_env.get_template(\"models_file.py.j2\")\n        return template.render(**template_context)\n\n    def _build_generic_entry_model_definition(self) -&gt; str:\n        \"\"\"Build the GenericEntry model definition for dataclass generation.\"\"\"\n        self.import_handler.add_import(\"django.contrib.contenttypes.fields\", \"GenericForeignKey\")\n        self.import_handler.add_import(\"django.contrib.contenttypes.fields\", \"GenericRelation\")\n        self.import_handler.add_import(\"django.contrib.contenttypes.models\", \"ContentType\")\n\n        fields: list[str] = []\n        fields.append(\"content_type = models.ForeignKey('contenttypes.ContentType', on_delete=models.CASCADE)\")\n        fields.append(\"object_id = models.PositiveIntegerField()\")\n        fields.append(\"content_object = GenericForeignKey('content_type', 'object_id')\")\n        fields.append(\"element_qname = models.CharField(max_length=255)\")\n        fields.append(\"type_qname = models.CharField(max_length=255, null=True, blank=True)\")\n        fields.append(\"attrs_json = models.JSONField(default=dict, blank=True)\")\n        if getattr(self, \"gfk_value_mode\", None) == \"typed_columns\":\n            fields.append(\"text_value = models.TextField(null=True, blank=True)\")\n            fields.append(\"num_value = models.DecimalField(max_digits=20, decimal_places=6, null=True, blank=True)\")\n            fields.append(\"time_value = models.DateTimeField(null=True, blank=True)\")\n        fields.append(\"order_index = models.IntegerField(default=0)\")\n        fields.append(\"path_hint = models.CharField(max_length=255, null=True, blank=True)\")\n\n        indexes_lines = [\"models.Index(fields=['content_type', 'object_id'])\"]\n        if getattr(self, \"gfk_value_mode\", None) == \"typed_columns\":\n            indexes_lines.append(\"models.Index(fields=['element_qname'])\")\n            indexes_lines.append(\"models.Index(fields=['type_qname'])\")\n            indexes_lines.append(\"models.Index(fields=['time_value'])\")\n            indexes_lines.append(\"models.Index(fields=['content_type', 'object_id', '-time_value'])\")\n\n        lines: list[str] = []\n        lines.append(f\"class GenericEntry({self.base_model_class.__name__}):\")\n        for f in fields:\n            lines.append(f\"    {f}\")\n        lines.append(\"\")\n        lines.append(\"    class Meta:\")\n        lines.append(f\"        app_label = '{self.app_label}'\")\n        lines.append(\"        abstract = False\")\n        lines.append(\"        indexes = [\")\n        for idx in indexes_lines:\n            lines.append(f\"            {idx},\")\n        lines.append(\"        ]\")\n        lines.append(\"\")\n        return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/pydantic2django/dataclass/generator/#pydantic2django.dataclass.generator.DataclassDjangoModelGenerator.generate_models_file","title":"<code>generate_models_file()</code>","text":"<p>Generate models for dataclasses with GFK finalize hook.</p> Source code in <code>src/pydantic2django/dataclass/generator.py</code> <pre><code>def generate_models_file(self) -&gt; str:\n    \"\"\"Generate models for dataclasses with GFK finalize hook.\"\"\"\n    self.discover_models()\n    models_to_process = self._get_models_in_processing_order()\n\n    # Reset imports and state\n    self.carriers = []\n    self.import_handler.extra_type_imports.clear()\n    self.import_handler.pydantic_imports.clear()\n    self.import_handler.context_class_imports.clear()\n    self.import_handler.imported_names.clear()\n    self.import_handler.processed_field_types.clear()\n    self.import_handler._add_type_import(self.base_model_class)\n\n    # Setup carriers\n    for source_model in models_to_process:\n        self.setup_django_model(source_model)\n\n    # GFK finalize: inject GenericRelation on parents\n    gfk_used = False\n    for carrier in self.carriers:\n        try:\n            if getattr(carrier, \"enable_gfk\", False) and getattr(carrier, \"pending_gfk_children\", None):\n                self.import_handler.add_import(\"django.contrib.contenttypes.fields\", \"GenericRelation\")\n                self.import_handler.add_import(\"django.contrib.contenttypes.fields\", \"GenericForeignKey\")\n                self.import_handler.add_import(\"django.contrib.contenttypes.models\", \"ContentType\")\n                carrier.django_field_definitions[\n                    \"entries\"\n                ] = \"GenericRelation('GenericEntry', related_query_name='entries')\"\n                gfk_used = True\n        except Exception:\n            pass\n\n    # Render model definitions\n    model_definitions: list[str] = []\n    django_model_names: list[str] = []\n    for carrier in self.carriers:\n        if carrier.django_model:\n            try:\n                model_def = self.generate_model_definition(carrier)\n                if model_def:\n                    model_definitions.append(model_def)\n                    django_model_names.append(f\"'{self._clean_generic_type(carrier.django_model.__name__)}'\")\n            except Exception:\n                pass\n\n    if gfk_used:\n        model_definitions.append(self._build_generic_entry_model_definition())\n        django_model_names.append(\"'GenericEntry'\")\n\n    unique_model_definitions = self._deduplicate_definitions(model_definitions)\n    imports = self.import_handler.deduplicate_imports()\n    template_context = self._prepare_template_context(unique_model_definitions, django_model_names, imports)\n    template_context.update(\n        {\n            \"generation_timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n            \"base_model_module\": self.base_model_class.__module__,\n            \"base_model_name\": self.base_model_class.__name__,\n            \"extra_type_imports\": sorted(self.import_handler.extra_type_imports),\n        }\n    )\n    template = self.jinja_env.get_template(\"models_file.py.j2\")\n    return template.render(**template_context)\n</code></pre>"},{"location":"reference/pydantic2django/django/","title":"pydantic2django.django","text":""},{"location":"reference/pydantic2django/django/admin/","title":"pydantic2django.django.admin","text":"<p>Admin interface generation for pydantic2django.</p> <p>This module provides functionality for automatically generating Django admin interfaces for dynamically created models.</p>"},{"location":"reference/pydantic2django/django/admin/#pydantic2django.django.admin.DynamicModelAdmin","title":"<code>DynamicModelAdmin</code>","text":"<p>               Bases: <code>ModelAdmin</code></p> <p>Dynamic admin interface for discovered models.</p> Source code in <code>src/pydantic2django/django/admin.py</code> <pre><code>class DynamicModelAdmin(admin.ModelAdmin):\n    \"\"\"Dynamic admin interface for discovered models.\"\"\"\n\n    list_display = (\"id\",)  # Start with basic fields\n    search_fields = (\"id\",)\n\n    def get_list_display(self, request):\n        \"\"\"Dynamically determine which fields to display.\"\"\"\n        model = self.model\n        # Start with id field\n        fields = [\"id\"]\n        # Add other fields that might be interesting\n        for field in model._meta.fields:\n            if field.name != \"id\":\n                fields.append(field.name)\n        return fields\n</code></pre>"},{"location":"reference/pydantic2django/django/admin/#pydantic2django.django.admin.DynamicModelAdmin.get_list_display","title":"<code>get_list_display(request)</code>","text":"<p>Dynamically determine which fields to display.</p> Source code in <code>src/pydantic2django/django/admin.py</code> <pre><code>def get_list_display(self, request):\n    \"\"\"Dynamically determine which fields to display.\"\"\"\n    model = self.model\n    # Start with id field\n    fields = [\"id\"]\n    # Add other fields that might be interesting\n    for field in model._meta.fields:\n        if field.name != \"id\":\n            fields.append(field.name)\n    return fields\n</code></pre>"},{"location":"reference/pydantic2django/django/admin/#pydantic2django.django.admin.register_model_admin","title":"<code>register_model_admin(model, model_name)</code>","text":"<p>Register a single model with the Django admin interface.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>type[Model]</code> <p>The Django model to register</p> required <code>model_name</code> <code>str</code> <p>The name of the model for logging purposes</p> required Source code in <code>src/pydantic2django/django/admin.py</code> <pre><code>def register_model_admin(model: type[models.Model], model_name: str) -&gt; None:\n    \"\"\"Register a single model with the Django admin interface.\n\n    Args:\n        model: The Django model to register\n        model_name: The name of the model for logging purposes\n    \"\"\"\n    try:\n        admin.site.register(model, DynamicModelAdmin)\n        logger.info(f\"Registered {model_name} with admin\")\n    except admin.sites.AlreadyRegistered:\n        logger.debug(f\"Admin interface for {model_name} already registered\")\n</code></pre>"},{"location":"reference/pydantic2django/django/admin/#pydantic2django.django.admin.register_model_admins","title":"<code>register_model_admins(app_label)</code>","text":"<p>Register admin interfaces for all models in the app.</p> <p>This should be called after Django is fully initialized and migrations are complete.</p> <p>Parameters:</p> Name Type Description Default <code>app_label</code> <code>str</code> <p>The Django app label the models are registered under</p> required Source code in <code>src/pydantic2django/django/admin.py</code> <pre><code>def register_model_admins(app_label: str) -&gt; None:\n    \"\"\"Register admin interfaces for all models in the app.\n\n    This should be called after Django is fully initialized and migrations are complete.\n\n    Args:\n        app_label: The Django app label the models are registered under\n    \"\"\"\n    logger.info(f\"Registering admin interfaces for {app_label}...\")\n\n    for model in apps.get_app_config(app_label).get_models():\n        try:\n            admin.site.register(model, DynamicModelAdmin)\n            logger.info(f\"Registered admin interface for {model.__name__}\")\n        except admin.sites.AlreadyRegistered:\n            logger.info(f\"Admin interface for {model.__name__} already registered\")\n</code></pre>"},{"location":"reference/pydantic2django/django/conversion/","title":"pydantic2django.django.conversion","text":"<p>Provides functionality to convert Django model instances to Pydantic models.</p>"},{"location":"reference/pydantic2django/django/conversion/#pydantic2django.django.conversion.DjangoPydanticConverter","title":"<code>DjangoPydanticConverter</code>","text":"<p>               Bases: <code>Generic[DjangoModelT]</code></p> <p>Manages the conversion lifecycle between a Django model instance and a dynamically generated Pydantic model.</p> <p>Handles: 1. Generating a Pydantic model class definition from a Django model class. 2. Converting a Django model instance to an instance of the generated Pydantic model. 3. Converting a Pydantic model instance back to a saved Django model instance.</p> Source code in <code>src/pydantic2django/django/conversion.py</code> <pre><code>class DjangoPydanticConverter(Generic[DjangoModelT]):\n    \"\"\"\n    Manages the conversion lifecycle between a Django model instance\n    and a dynamically generated Pydantic model.\n\n    Handles:\n    1. Generating a Pydantic model class definition from a Django model class.\n    2. Converting a Django model instance to an instance of the generated Pydantic model.\n    3. Converting a Pydantic model instance back to a saved Django model instance.\n    \"\"\"\n\n    def __init__(\n        self,\n        django_model_or_instance: Union[type[DjangoModelT], DjangoModelT],\n        *,\n        max_depth: int = 3,\n        exclude: Optional[set[str]] = None,\n        pydantic_base: Optional[type[BaseModel]] = None,\n        model_name: Optional[str] = None,\n    ):\n        \"\"\"\n        Initializes the converter.\n\n        Args:\n            django_model_or_instance: Either the Django model class or an instance.\n            max_depth: Maximum recursion depth for generating/converting related models.\n            exclude: Field names to exclude during conversion *to* Pydantic.\n                     Note: Exclusion during generation is not yet implemented here,\n                     but `generate_pydantic_class` could be adapted if needed.\n            pydantic_base: Optional base for generated Pydantic model.\n            model_name: Optional name for generated Pydantic model.\n        \"\"\"\n        if isinstance(django_model_or_instance, models.Model):\n            self.django_model_cls: type[DjangoModelT] = django_model_or_instance.__class__\n            self.initial_django_instance: Optional[DjangoModelT] = django_model_or_instance\n        elif issubclass(django_model_or_instance, models.Model):\n            # Ensure the type checker understands self.django_model_cls is Type[DjangoModelT]\n            # where DjangoModelT is the specific type bound to the class instance.\n            self.django_model_cls: type[DjangoModelT] = django_model_or_instance  # Type should be consistent now\n            self.initial_django_instance = None\n        else:\n            raise TypeError(\"Input must be a Django Model class or instance.\")\n\n        self.max_depth = max_depth\n        self.exclude = exclude or set()\n        self.pydantic_base = pydantic_base or BaseModel\n        self.model_name = model_name  # Store optional name\n        # Initialize dependencies\n        self.relationship_accessor = RelationshipConversionAccessor()\n        self.mapper = BidirectionalTypeMapper(self.relationship_accessor)\n        # Register the initial model with the accessor (if needed for self-refs immediately)\n        # self.relationship_accessor.map_relationship(source_model=???, django_model=self.django_model_cls) # Need Pydantic type?\n\n        # Use the correctly defined cache type (name -&gt; model/ref)\n        self._generation_cache: GeneratedModelCache = {}\n        # _django_metadata is no longer needed\n\n        # Generate the Pydantic class definition immediately\n        generated_cls_or_ref = self._generate_pydantic_class()\n\n        # --- Resolve ForwardRef if necessary --- #\n        resolved_cls: type[BaseModel]\n        if isinstance(generated_cls_or_ref, ForwardRef):\n            logger.warning(\n                f\"Initial generation for {self.django_model_cls.__name__} resulted in a ForwardRef.\"\n                f\" Attempting resolution via model_rebuild on potentially dependent models in cache.\"\n            )\n            # Try resolving by rebuilding all models in the cache that are actual classes\n            # This assumes dependencies were generated and cached correctly.\n            # Rebuild cached models to attempt resolving ForwardRefs\n            for name, model_or_ref in self._generation_cache.items():\n                if isinstance(model_or_ref, type) and issubclass(model_or_ref, BaseModel):\n                    try:\n                        model_or_ref.model_rebuild(force=True)\n                        # logger.debug(f\"Rebuilt cached model {name} to potentially resolve ForwardRefs\")\n                        # Successfully rebuilt a cached model; continue rebuilding others\n                    except Exception as e:\n                        logger.warning(f\"Failed to rebuild cached model {name} during ForwardRef resolution: {e}\")\n\n            # After attempting rebuilds, try accessing the resolved type from the cache again\n            final_cached_item = self._generation_cache.get(generated_cls_or_ref.__forward_arg__)\n            if isinstance(final_cached_item, type) and issubclass(final_cached_item, BaseModel):\n                logger.info(f\"Successfully resolved ForwardRef for {generated_cls_or_ref.__forward_arg__}\")\n                resolved_cls = final_cached_item\n            else:\n                # If still not resolved, raise an error as we cannot proceed\n                raise TypeError(\n                    f\"Failed to resolve ForwardRef '{generated_cls_or_ref.__forward_arg__}' for the main model {self.django_model_cls.__name__} after generation and rebuild attempts.\"\n                )\n        elif isinstance(generated_cls_or_ref, type) and issubclass(generated_cls_or_ref, BaseModel):\n            # Already a valid class\n            resolved_cls = generated_cls_or_ref\n        else:\n            # Should not happen if _generate_pydantic_class works correctly\n            raise TypeError(\n                f\"_generate_pydantic_class returned an unexpected type: {type(generated_cls_or_ref)} for {self.django_model_cls.__name__}\"\n            )\n\n        # Assign the resolved class\n        self.pydantic_model_cls: type[BaseModel] = resolved_cls\n\n        #          logger.debug(f\"Skipping update_forward_refs for ForwardRef in cache: {item_name}\")\n\n        # Rebuild the main model AFTER resolving refs in the cache.\n        # No need for _types_namespace if update_forward_refs worked.\n        # Now it should be safe to call rebuild on the resolved class\n        try:\n            self.pydantic_model_cls.model_rebuild(force=True)\n            logger.info(f\"Rebuilt main model {self.pydantic_model_cls.__name__} after generation.\")\n        except Exception as e:\n            # Log warning if final rebuild still fails\n            logger.warning(\n                f\"Failed to rebuild main model {self.pydantic_model_cls.__name__} after resolution: {e}\",\n                exc_info=True,\n            )\n\n    # Helper map for TypeAdapter in to_django\n    _INTERNAL_TYPE_TO_PYTHON_TYPE = {\n        \"AutoField\": int,\n        \"BigAutoField\": int,\n        \"IntegerField\": int,\n        \"UUIDField\": UUID,\n        \"CharField\": str,\n        \"TextField\": str,\n        # Add other PK types as needed\n    }\n\n    def _generate_pydantic_class(self) -&gt; Union[type[BaseModel], ForwardRef]:\n        \"\"\"Internal helper to generate or retrieve the Pydantic model class.\"\"\"\n        # Pass the stored exclude set during generation\n        generated_cls: Union[type[BaseModel], ForwardRef] = generate_pydantic_class(\n            django_model_cls=self.django_model_cls,\n            mapper=self.mapper,\n            model_name=self.model_name,\n            cache=self._generation_cache,\n            depth=0,\n            max_depth=self.max_depth,\n            pydantic_base=self.pydantic_base,\n            exclude=self.exclude,\n        )\n\n        # Allow ForwardRef through, otherwise check for BaseModel subclass\n        # This check is good, but the type hint of the function needs to be updated\n        # if not isinstance(generated_cls, ForwardRef):\n        #     if not isinstance(generated_cls, type) or not issubclass(generated_cls, BaseModel):\n        #         raise TypeError(f\"Generated type is not a valid {self.pydantic_base.__name__} or ForwardRef.\")\n\n        # The return type must accommodate ForwardRef\n        # Cast here is less ideal than fixing the return type hint of _generate_pydantic_class\n        # However, casting here allows the rest of the __init__ to proceed assuming BaseModel\n        # We need to ensure the call site handles the ForwardRef possibility properly later.\n        # For now, we assume generation is successful for the main class.\n        # if isinstance(generated_cls, ForwardRef):\n        #     # This case should ideally not happen for the top-level model unless max_depth=0\n        #     # Or if there's an immediate self-reference issue not caught.\n        #     # Log a warning, but proceed. The rebuild step might resolve it.\n        #     logger.warning(f\"_generate_pydantic_class returned a ForwardRef for the main model {self.django_model_cls.__name__}. Rebuild might be necessary.\")\n        # We need a type[BaseModel] for the rest of the init. This is problematic.\n        # Let's rely on the rebuild step to fix this. If it fails, it will raise later.\n        # Return the ForwardRef for now, but the attribute type hint needs fixing.\n        # return generated_cls # TYPE HINT MISMATCH HERE\n\n        return generated_cls\n\n    def to_pydantic(self, db_obj: Optional[DjangoModelT] = None) -&gt; PydanticModelT:\n        \"\"\"\n        Converts a Django model instance to an instance of the generated Pydantic model.\n\n        Args:\n            db_obj: The Django model instance to convert. If None, attempts to use\n                    the instance provided during initialization.\n\n        Returns:\n            An instance of the generated Pydantic model (subclass of BaseModel).\n\n        Raises:\n            ValueError: If no Django instance is available or conversion fails.\n        \"\"\"\n        target_db_obj = db_obj or self.initial_django_instance\n        if target_db_obj is None:\n            raise ValueError(\"A Django model instance must be provided for conversion.\")\n\n        if not isinstance(target_db_obj, self.django_model_cls):\n            raise TypeError(f\"Provided instance is not of type {self.django_model_cls.__name__}\")\n\n        logger.info(\n            f\"Converting {self.django_model_cls.__name__} instance (PK: {target_db_obj.pk}) to {self.pydantic_model_cls.__name__}\"\n        )\n\n        # We know self.pydantic_model_cls is Type[BaseModel] from _generate_pydantic_class\n        # Pass the pre-extracted metadata\n        result = django_to_pydantic(\n            target_db_obj,\n            self.pydantic_model_cls,  # Now guaranteed to be type[BaseModel] by __init__\n            exclude=self.exclude,\n            max_depth=self.max_depth,\n        )\n        return cast(PydanticModelT, result)\n\n    def _determine_target_django_instance(\n        self, pydantic_instance: BaseModel, update_instance: Optional[DjangoModelT]\n    ) -&gt; DjangoModelT:\n        \"\"\"Determines the target Django instance (update existing or create new).\"\"\"\n        if update_instance:\n            if not isinstance(update_instance, self.django_model_cls):\n                raise TypeError(f\"update_instance is not of type {self.django_model_cls.__name__}\")\n            logger.debug(f\"Updating provided Django instance (PK: {update_instance.pk})\")\n            return update_instance\n        elif self.initial_django_instance:\n            logger.debug(f\"Updating initial Django instance (PK: {self.initial_django_instance.pk})\")\n            # Re-fetch to ensure we have the latest state? Maybe not necessary if we overwrite all fields.\n            return self.initial_django_instance\n        else:\n            # Check if Pydantic instance has a PK to determine if it represents an existing object\n            pk_field = self.django_model_cls._meta.pk\n            if pk_field is None:\n                raise ValueError(f\"Model {self.django_model_cls.__name__} does not have a primary key.\")\n            assert pk_field is not None  # Help type checker\n            pk_field_name = pk_field.name\n\n            pk_value = getattr(pydantic_instance, pk_field_name, None)\n            if pk_value is not None:\n                try:\n                    target_django_instance = cast(DjangoModelT, self.django_model_cls.objects.get(pk=pk_value))\n                    logger.debug(f\"Found existing Django instance by PK ({pk_value}) from Pydantic data.\")\n                    return target_django_instance\n                except self.django_model_cls.DoesNotExist:\n                    logger.warning(\n                        f\"PK ({pk_value}) found in Pydantic data, but no matching Django instance exists. Creating new.\"\n                    )\n                    return cast(DjangoModelT, self.django_model_cls())\n            else:\n                logger.debug(\"Creating new Django instance.\")\n                return cast(DjangoModelT, self.django_model_cls())\n\n    def _assign_fk_o2o_field(\n        self, target_django_instance: DjangoModelT, django_field: RelatedField, pydantic_value: Any\n    ):\n        \"\"\"Assigns a value to a ForeignKey or OneToOneField.\n        NOTE: This method still uses the passed django_field object directly,\n        as it already contains the necessary info like related_model and attname.\n        It doesn't need to re-fetch metadata for the field itself, but uses it for PK type info.\n        \"\"\"\n        related_model_cls_ref = django_field.related_model\n        related_model_cls: type[models.Model]\n        if related_model_cls_ref == \"self\":\n            related_model_cls = self.django_model_cls\n        elif isinstance(related_model_cls_ref, type) and issubclass(related_model_cls_ref, models.Model):\n            related_model_cls = related_model_cls_ref\n        else:\n            raise TypeError(\n                f\"Unexpected related_model type for field '{django_field.name}': {type(related_model_cls_ref)}\"\n            )\n\n        related_pk_field = related_model_cls._meta.pk\n        if related_pk_field is None:\n            raise ValueError(\n                f\"Related model {related_model_cls.__name__} for field '{django_field.name}' has no primary key.\"\n            )\n        related_pk_name = related_pk_field.name\n\n        if pydantic_value is None:\n            # Check if field is nullable before setting None\n            if not django_field.null and not isinstance(django_field, OneToOneField):\n                logger.warning(f\"Attempting to set non-nullable FK/O2O '{django_field.name}' to None. Skipping.\")\n                return\n            setattr(target_django_instance, django_field.name, None)\n        elif isinstance(pydantic_value, BaseModel):\n            # Assume nested Pydantic model has PK, fetch related Django obj\n            related_pk = getattr(pydantic_value, related_pk_name, None)\n            if related_pk is not None:\n                try:\n                    related_obj = related_model_cls.objects.get(pk=related_pk)\n                    setattr(target_django_instance, django_field.name, related_obj)\n                except related_model_cls.DoesNotExist:\n                    logger.error(f\"Related object for '{django_field.name}' with PK {related_pk} not found.\")\n                    if django_field.null:\n                        setattr(target_django_instance, django_field.name, None)\n                    else:\n                        raise ValueError(\n                            f\"Cannot save '{django_field.name}': Related object with PK {related_pk} not found and field is not nullable.\"\n                        ) from None\n            else:\n                logger.error(\n                    f\"Cannot set FK '{django_field.name}': Nested Pydantic model missing PK '{related_pk_name}'.\"\n                )\n                if django_field.null:\n                    setattr(target_django_instance, django_field.name, None)\n                else:\n                    raise ValueError(\n                        f\"Cannot save non-nullable FK '{django_field.name}': Nested Pydantic model missing PK.\"\n                    ) from None\n\n        elif isinstance(pydantic_value, dict):\n            # Handle dictionary input - extract PK\n            related_pk = pydantic_value.get(related_pk_name)\n            if related_pk is not None:\n                try:\n                    # Adapt and validate the extracted PK\n                    internal_type_name = django_field.target_field.get_internal_type()\n                    python_type = self._INTERNAL_TYPE_TO_PYTHON_TYPE.get(internal_type_name)\n                    adapted_pk: Any = None\n                    if python_type:\n                        pk_adapter = TypeAdapter(python_type)\n                        adapted_pk = pk_adapter.validate_python(related_pk)\n                    else:\n                        logger.warning(\n                            f\"Could not determine specific Python type for PK internal type '{internal_type_name}' on field '{django_field.name}'. Assigning raw PK value from dict.\"\n                        )\n                        adapted_pk = related_pk\n\n                    # Set the FK using the ID field (attname)\n                    fk_field_name = django_field.attname\n                    setattr(target_django_instance, fk_field_name, adapted_pk)\n                except Exception as e:\n                    logger.error(\n                        f\"Failed to adapt PK '{related_pk}' extracted from dict for FK field '{django_field.name}': {e}\"\n                    )\n                    if django_field.null or isinstance(django_field, OneToOneField):\n                        setattr(target_django_instance, django_field.attname, None)\n                        setattr(target_django_instance, django_field.name, None)  # Clear object too\n                    else:\n                        raise ValueError(\n                            f\"Invalid PK value '{related_pk}' in dict for non-nullable FK field '{django_field.name}'.\"\n                        ) from e\n            else:\n                logger.error(\n                    f\"Cannot set FK '{django_field.name}': Dictionary input missing PK '{related_pk_name}'. Dict: {pydantic_value}\"\n                )\n                if django_field.null or isinstance(django_field, OneToOneField):\n                    setattr(target_django_instance, django_field.attname, None)\n                    setattr(target_django_instance, django_field.name, None)  # Clear object too\n                else:\n                    raise ValueError(f\"Cannot save non-nullable FK '{django_field.name}': Dictionary input missing PK.\")\n\n        else:  # Assume pydantic_value is the PK itself\n            try:\n                # Use TypeAdapter for robust PK conversion\n                target_type = getattr(django_field.target_field, \"target_field\", django_field.target_field)\n                # Get the internal type for adaptation (e.g., UUID, int, str)\n                # Note: Using get_internal_type() might be less reliable than direct type check for adaptation\n                # Consider a map or checking target_field class directly\n                internal_type_name = target_type.get_internal_type()\n                python_type = self._INTERNAL_TYPE_TO_PYTHON_TYPE.get(internal_type_name)\n\n                if python_type:\n                    pk_adapter = TypeAdapter(python_type)\n                    adapted_pk = pk_adapter.validate_python(pydantic_value)\n                else:\n                    logger.warning(\n                        f\"Could not determine specific Python type for PK internal type '{internal_type_name}' on field '{django_field.name}'. Assigning raw value.\"\n                    )\n                    adapted_pk = pydantic_value\n\n            except Exception as e:\n                logger.error(f\"Failed to adapt PK value '{pydantic_value}' for FK field '{django_field.name}': {e}\")\n                if django_field.null or isinstance(django_field, OneToOneField):\n                    adapted_pk = None\n                    setattr(target_django_instance, django_field.name, None)  # Clear the object too\n                else:\n                    raise ValueError(f\"Invalid PK value type for non-nullable FK field '{django_field.name}'.\") from e\n\n            fk_field_name = django_field.attname  # Use attname to set the ID directly\n            setattr(target_django_instance, fk_field_name, adapted_pk)\n        logger.debug(f\"Assigned FK/O2O '{django_field.name}'\")\n\n    def _assign_datetime_field(\n        self, target_django_instance: DjangoModelT, django_field: models.DateTimeField, pydantic_value: Any\n    ):\n        \"\"\"Assigns a value to a DateTimeField, handling timezone awareness.\"\"\"\n        # TODO: Add more robust timezone handling based on Django settings (USE_TZ)\n        is_field_aware = getattr(django_field, \"is_aware\", False)  # Approximation\n\n        if isinstance(pydantic_value, dt.datetime):\n            current_value_aware = is_aware(pydantic_value)\n            # Assume field needs aware if Django's USE_TZ is True (needs better check)\n            if not current_value_aware and is_field_aware:\n                try:\n                    default_tz = get_default_timezone()\n                    pydantic_value = make_aware(pydantic_value, default_tz)\n                    logger.debug(f\"Made naive datetime timezone-aware for field '{django_field.name}'\")\n                except Exception as e:\n                    logger.warning(\n                        f\"Failed to make datetime timezone-aware for field '{django_field.name}'. Value: {pydantic_value}, Error: {e}\"\n                    )\n                    # Decide if we should proceed with naive datetime or raise error/skip\n\n        setattr(target_django_instance, django_field.name, pydantic_value)\n        logger.debug(f\"Assigned DateTimeField '{django_field.name}'\")\n\n    def _assign_file_field(\n        self, target_django_instance: DjangoModelT, django_field: models.FileField, pydantic_value: Any\n    ):\n        \"\"\"Handles assignment for FileField/ImageField (currently limited).\"\"\"\n        if pydantic_value is None:\n            setattr(target_django_instance, django_field.name, None)\n            logger.debug(f\"Set FileField/ImageField '{django_field.name}' to None.\")\n        elif isinstance(pydantic_value, str):\n            # Avoid overwriting if the string likely represents the existing file\n            current_file = getattr(target_django_instance, django_field.name, None)\n            if current_file:\n                matches = False\n                if hasattr(current_file, \"url\") and current_file.url == pydantic_value:\n                    matches = True\n                elif hasattr(current_file, \"name\") and current_file.name == pydantic_value:\n                    matches = True\n\n                if matches:\n                    logger.debug(\n                        f\"Skipping assignment for FileField/ImageField '{django_field.name}': value matches existing file.\"\n                    )\n                    return  # Skip assignment\n\n            logger.warning(\n                f\"Skipping assignment for FileField/ImageField '{django_field.name}' from string value '{pydantic_value}'. Direct assignment/update from URL/string not supported.\"\n            )\n        else:\n            # Allow assignment if it's not None or string (e.g., UploadedFile object)\n            setattr(target_django_instance, django_field.name, pydantic_value)\n            logger.debug(f\"Assigned non-string value to FileField/ImageField '{django_field.name}'\")\n\n    def _process_pydantic_fields(\n        self, target_django_instance: DjangoModelT, pydantic_instance: BaseModel\n    ) -&gt; dict[str, Any]:\n        \"\"\"Iterates through Pydantic fields and assigns values to the Django instance.\"\"\"\n        m2m_data = {}\n        # Use model_dump(mode='python') which might return rich types\n        # Convert specific rich types to string *before* assigning\n        pydantic_data = pydantic_instance.model_dump(mode=\"python\")\n\n        for field_name, pydantic_value in pydantic_data.items():\n            # Explicitly convert potentially problematic types here\n            value_to_process = pydantic_value\n            # Check type names instead of isinstance for robustness\n            type_name = type(pydantic_value).__name__\n            if type_name in (\"HttpUrl\", \"EmailStr\", \"IPvAnyAddress\", \"IPv4Address\", \"IPv6Address\"):\n                logger.debug(f\"Explicitly converting Pydantic type {type_name} to string for field '{field_name}'\")\n                value_to_process = str(pydantic_value)\n            # Add other necessary pre-conversions here (e.g., Enums?)\n\n            # Now call assign_field_value with the potentially converted value\n            m2m_result = self._assign_field_value(target_django_instance, field_name, value_to_process)\n            if m2m_result:\n                m2m_data[m2m_result[0]] = m2m_result[1]\n\n        return m2m_data\n\n    def _assign_field_value(\n        self, target_django_instance: DjangoModelT, field_name: str, pydantic_value: Any\n    ) -&gt; Optional[tuple[str, Any]]:\n        \"\"\"\n        Assigns a single field value from Pydantic to the Django instance.\n        Uses the Django model's _meta API to get field info.\n        Returns M2M data to be processed later, or None.\n        \"\"\"\n        # --- Refactored: Get Django field directly using _meta --- #\n        try:\n            django_field = target_django_instance._meta.get_field(field_name)\n        except models.FieldDoesNotExist:\n            # Field exists on Pydantic model but not on Django model\n            logger.warning(\n                f\"Field '{field_name}' exists on Pydantic model but not found on Django model {self.django_model_cls.__name__}. Skipping assignment.\"\n            )\n            return None  # Skip assignment for this Pydantic-only field\n\n        # Check if the field is concrete (e.g., not a reverse relation managed elsewhere)\n        if not getattr(django_field, \"concrete\", False):\n            logger.debug(f\"Skipping non-concrete field '{field_name}'.\")\n            return None\n\n        # --- Ensure it's a concrete Field before accessing Field-specific attributes ---\n        if not isinstance(django_field, models.Field):\n            logger.warning(\n                f\"Field '{field_name}' is not an instance of models.Field (type: {type(django_field).__name__}). Skipping.\"\n            )\n            return None\n\n        try:\n            # --- Skip non-editable fields based on field properties --- #\n            is_pk = django_field.primary_key\n            # AutoFields are implicitly not editable after creation if instance already has PK\n            is_auto_field = isinstance(django_field, (models.AutoField, models.BigAutoField, models.SmallAutoField))\n            is_editable = getattr(django_field, \"editable\", True)  # Default to True if editable attr doesn't exist\n\n            # Skip if field is marked non-editable, or if it's an AutoField on an existing instance\n            if not is_editable or (is_auto_field and target_django_instance.pk is not None):\n                logger.debug(f\"Skipping non-editable or auto-field '{field_name}'.\")\n                return None\n            # Explicitly skip PK update on existing instances (should be covered above, but safe check)\n            if is_pk and target_django_instance.pk is not None:\n                logger.debug(f\"Skipping primary key field '{field_name}' during update.\")\n                return None\n\n            # --- Handle field types based on isinstance checks --- #\n\n            # M2M handled separately after save - return data for later processing\n            if isinstance(django_field, models.ManyToManyField):\n                logger.debug(f\"Deferring M2M assignment for '{field_name}'\")\n                return (field_name, pydantic_value)\n\n            # FK/O2O handled by helper\n            elif isinstance(django_field, (models.ForeignKey, models.OneToOneField)):\n                # Cast to RelatedField for the helper function type hint\n                self._assign_fk_o2o_field(target_django_instance, cast(RelatedField, django_field), pydantic_value)\n\n            # JSONField\n            elif isinstance(django_field, models.JSONField):\n                # Django's JSONField usually handles dict/list directly\n                setattr(target_django_instance, field_name, pydantic_value)\n                logger.debug(f\"Assigned JSONField '{field_name}'\")\n\n            # FileField/ImageField\n            elif isinstance(django_field, models.FileField):\n                # Use helper, casting for type hint clarity\n                self._assign_file_field(target_django_instance, cast(models.FileField, django_field), pydantic_value)\n\n            # Datetime fields might need timezone handling (use helper)\n            elif isinstance(django_field, models.DateTimeField):\n                self._assign_datetime_field(\n                    target_django_instance, cast(models.DateTimeField, django_field), pydantic_value\n                )\n\n            else:  # Other simple fields (CharField, IntegerField, etc.)\n                # The value should have been pre-processed in the calling loop\n                value_to_assign = pydantic_value\n                # Log the value being assigned to be sure\n                logger.debug(\n                    f\"Assigning simple field '{field_name}': Type={type(value_to_assign).__name__}, Value={value_to_assign!r}\"\n                )\n                setattr(target_django_instance, field_name, value_to_assign)\n                # logger.debug(f\"Assigned simple field '{field_name}'\") # Slightly redundant log removed\n\n        except Exception as e:\n            logger.error(f\"Error assigning field '{field_name}': {e}\", exc_info=True)\n            # Re-raise as a ValueError to be caught by the main `to_django` method\n            raise ValueError(f\"Failed to process field '{field_name}' for saving.\") from e\n\n        return None  # Indicate no M2M data for this field (it was handled or is simple)\n\n    def _assign_m2m_fields(self, target_django_instance: DjangoModelT, m2m_data: dict[str, Any]):\n        \"\"\"Assigns ManyToMany field values using stored metadata.\"\"\"\n        if not m2m_data:\n            return\n\n        logger.debug(\"Assigning M2M relationships...\")\n        for field_name, pydantic_m2m_list in m2m_data.items():\n            if pydantic_m2m_list is None:  # Allow clearing M2M\n                pydantic_m2m_list = []\n\n            if not isinstance(pydantic_m2m_list, list):\n                raise ValueError(f\"M2M field '{field_name}' expects a list, got {type(pydantic_m2m_list)}\")\n\n            # Get metadata for the M2M field\n            # meta = self._django_metadata.get(field_name)\n            # if not meta or not meta.is_m2m or not meta.related_model:\n            #     logger.error(f\"Could not find valid M2M metadata for field '{field_name}'. Skipping assignment.\")\n            #     continue\n\n            # --- Refactored: Get Django field and related model directly using _meta --- #\n            try:\n                django_field = target_django_instance._meta.get_field(field_name)\n                # Ensure it is actually an M2M field\n                if not isinstance(django_field, models.ManyToManyField):\n                    logger.error(\n                        f\"Field '{field_name}' found but is not a ManyToManyField (type: {type(django_field).__name__}). Skipping M2M assignment.\"\n                    )\n                    continue\n\n                # Resolve related model ('self' or actual model class)\n                related_model_cls_ref = django_field.related_model\n                related_model_cls: type[models.Model]\n                if related_model_cls_ref == \"self\":\n                    related_model_cls = self.django_model_cls\n                elif isinstance(related_model_cls_ref, type) and issubclass(related_model_cls_ref, models.Model):\n                    related_model_cls = related_model_cls_ref\n                else:\n                    # This path indicates an issue with the Django model definition itself\n                    raise TypeError(\n                        f\"Unexpected related_model type for M2M field '{django_field.name}': {type(related_model_cls_ref)}\"\n                    )\n\n            except models.FieldDoesNotExist:\n                logger.error(\n                    f\"Could not find M2M field '{field_name}' on Django model {target_django_instance.__class__.__name__}. Skipping assignment.\"\n                )\n                continue\n            except TypeError as e:\n                # Catch potential error from issubclass if related_model_cls_ref is unexpected type\n                logger.error(f\"Error resolving related model for M2M field '{field_name}': {e}\", exc_info=True)\n                continue\n\n            try:\n                # Get the manager instance from the target Django object\n                manager = getattr(target_django_instance, field_name)\n\n                # Get PK name from the resolved related model\n                related_pk_field = related_model_cls._meta.pk\n                if related_pk_field is None:\n                    # Should not happen for valid Django models\n                    raise ValueError(\n                        f\"Related model {related_model_cls.__name__} for M2M field '{field_name}' has no primary key defined in its _meta.\"\n                    )\n                related_pk_name = related_pk_field.name\n\n                # --- Process the list of Pydantic items (models, dicts, or PKs) --- #\n\n                # Update type hint to include QuerySet\n                related_objs_or_pks: Union[list[models.Model], list[Any], QuerySet[models.Model]] = []\n                if not pydantic_m2m_list:\n                    pass  # Handled by manager.set([])\n                elif all(isinstance(item, BaseModel) for item in pydantic_m2m_list):\n                    # List of Pydantic models, extract PKs\n                    related_pks = [getattr(item, related_pk_name, None) for item in pydantic_m2m_list]\n                    valid_pks = [pk for pk in related_pks if pk is not None]\n                    if len(valid_pks) != len(pydantic_m2m_list):\n                        logger.warning(\n                            f\"Some related Pydantic models for M2M field '{field_name}' were missing PKs or had None PK.\"\n                        )\n                    # Query for existing Django objects using valid PKs\n                    related_objs_or_pks = list(related_model_cls.objects.filter(pk__in=valid_pks))\n                    if len(related_objs_or_pks) != len(valid_pks):\n                        logger.warning(\n                            f\"Could not find all related Django objects for M2M field '{field_name}' based on Pydantic model PKs. Found {len(related_objs_or_pks)} out of {len(valid_pks)}.\"\n                        )\n                elif all(isinstance(item, dict) for item in pydantic_m2m_list):\n                    # List of dictionaries, extract PKs based on the related model's PK name\n                    try:\n                        related_instances_pks = []\n                        for related_instance_dict in pydantic_m2m_list:\n                            pk = related_instance_dict.get(related_pk_name)\n                            if pk is not None:\n                                related_instances_pks.append(pk)\n                            else:\n                                logger.warning(\n                                    f\"Could not find PK '{related_pk_name}' in related instance data: {related_instance_dict}\"\n                                )\n                        target_queryset = related_model_cls.objects.filter(pk__in=related_instances_pks)\n                        # Assign the queryset to be used by the final set() call\n                        related_objs_or_pks = target_queryset\n                    except Exception as e:\n                        logger.error(f\"Failed to extract PKs from dictionary list for M2M field '{field_name}': {e}\")\n                        # Optionally re-raise or handle error appropriately\n                        # If we don't assign, related_objs_or_pks remains empty, clearing the relation\n\n                elif all(not isinstance(item, (BaseModel, dict)) for item in pydantic_m2m_list):\n                    # Assume list of PKs if not BaseModels or dicts, use TypeAdapter for conversion\n                    try:\n                        internal_type_str = related_pk_field.get_internal_type()\n                        python_pk_type = self._INTERNAL_TYPE_TO_PYTHON_TYPE.get(internal_type_str)\n                        if python_pk_type:\n                            pk_adapter = TypeAdapter(list[python_pk_type])\n                            adapted_pks = pk_adapter.validate_python(pydantic_m2m_list)\n                            related_objs_or_pks = adapted_pks\n                        else:\n                            logger.warning(\n                                f\"Unsupported PK internal type '{internal_type_str}' for M2M field '{field_name}'. Passing raw list to manager.set().\"\n                            )\n                            related_objs_or_pks = pydantic_m2m_list\n                    except Exception as e:\n                        logger.error(f\"Failed to adapt PK list for M2M field '{field_name}': {e}\")\n                        raise ValueError(f\"Invalid PK list type for M2M field '{field_name}'.\") from e\n                else:\n                    # Mixed list of Pydantic models and PKs? Handle error or try to process?\n                    raise ValueError(\n                        f\"M2M field '{field_name}' received a mixed list of items (models and non-models). This is not supported.\"\n                    )\n\n                manager.set(related_objs_or_pks)  # .set() handles list of objects or PKs\n                logger.debug(f\"Set M2M field '{field_name}'\")\n            except Exception as e:\n                logger.error(f\"Error setting M2M field '{field_name}': {e}\", exc_info=True)\n                raise ValueError(f\"Failed to set M2M field '{field_name}' on {target_django_instance}: {e}\") from e\n\n    def to_django(self, pydantic_instance: BaseModel, update_instance: Optional[DjangoModelT] = None) -&gt; DjangoModelT:\n        \"\"\"\n        Converts a Pydantic model instance back to a Django model instance,\n        updating an existing one or creating a new one.\n\n        Args:\n            pydantic_instance: The Pydantic model instance containing the data.\n            update_instance: An optional existing Django instance to update.\n                             If None, attempts to update the initial instance (if provided),\n                             otherwise creates a new Django instance.\n\n        Returns:\n            The saved Django model instance.\n\n        Raises:\n            TypeError: If the pydantic_instance is not of the expected type or related models are incorrect.\n            ValueError: If saving fails, PKs are invalid, or required fields are missing.\n        \"\"\"\n        if not isinstance(pydantic_instance, self.pydantic_model_cls):\n            raise TypeError(f\"Input must be an instance of {self.pydantic_model_cls.__name__}\")\n\n        logger.info(\n            f\"Attempting to save data from {self.pydantic_model_cls.__name__} instance back to Django model {self.django_model_cls.__name__}\"\n        )\n\n        # 1. Determine the target Django instance\n        target_django_instance = self._determine_target_django_instance(pydantic_instance, update_instance)\n\n        # 2. Process Pydantic fields and assign to Django instance (defer M2M)\n        try:\n            m2m_data = self._process_pydantic_fields(target_django_instance, pydantic_instance)\n        except ValueError as e:\n            # Catch errors during field assignment\n            logger.error(f\"Error processing Pydantic fields for {self.django_model_cls.__name__}: {e}\", exc_info=True)\n            raise  # Re-raise the ValueError\n\n        # 3. Save the main instance\n        try:\n            # Consider moving full_clean() call after M2M assignment if it causes issues here.\n            # target_django_instance.full_clean(exclude=[...]) # Option to exclude fields causing early validation issues\n            target_django_instance.save()\n            logger.info(f\"Saved basic fields for Django instance (PK: {target_django_instance.pk})\")\n\n        except Exception as e:  # Catch save errors (e.g., database constraints)\n            logger.exception(\n                f\"Failed initial save for Django instance (PK might be {target_django_instance.pk}) of model {self.django_model_cls.__name__}\"\n            )\n            raise ValueError(f\"Django save operation failed for {target_django_instance}: {e}\") from e\n\n        # 4. Handle M2M Assignment (After Save)\n        try:\n            self._assign_m2m_fields(target_django_instance, m2m_data)\n        except ValueError as e:\n            # Catch errors during M2M assignment\n            logger.error(\n                f\"Error assigning M2M fields for {self.django_model_cls.__name__} (PK: {target_django_instance.pk}): {e}\",\n                exc_info=True,\n            )\n            # Decide if we should re-raise or just log. Re-raising seems safer.\n            raise\n\n        # 5. Run final validation\n        try:\n            target_django_instance.full_clean()\n            logger.info(\n                f\"Successfully validated and saved Pydantic data to Django instance (PK: {target_django_instance.pk})\"\n            )\n        except Exception as e:  # Catch validation errors\n            logger.exception(\n                f\"Django validation failed after saving and M2M assignment for instance (PK: {target_django_instance.pk}) of model {self.django_model_cls.__name__}\"\n            )\n            # It's already saved, but invalid state. Raise the validation error.\n            raise ValueError(f\"Django validation failed for {target_django_instance}: {e}\") from e\n\n        return target_django_instance\n\n    # Add helper properties/methods?\n    @property\n    def generated_pydantic_model(self) -&gt; type[BaseModel]:\n        \"\"\"Returns the generated Pydantic model class.\n\n        This is guaranteed to be a resolved model class after successful initialization.\n        \"\"\"\n        # This property should return the potentially ForwardRef attribute\n        return self.pydantic_model_cls  # Return the actual attribute\n</code></pre>"},{"location":"reference/pydantic2django/django/conversion/#pydantic2django.django.conversion.DjangoPydanticConverter.generated_pydantic_model","title":"<code>generated_pydantic_model</code>  <code>property</code>","text":"<p>Returns the generated Pydantic model class.</p> <p>This is guaranteed to be a resolved model class after successful initialization.</p>"},{"location":"reference/pydantic2django/django/conversion/#pydantic2django.django.conversion.DjangoPydanticConverter.__init__","title":"<code>__init__(django_model_or_instance, *, max_depth=3, exclude=None, pydantic_base=None, model_name=None)</code>","text":"<p>Initializes the converter.</p> <p>Parameters:</p> Name Type Description Default <code>django_model_or_instance</code> <code>Union[type[DjangoModelT], DjangoModelT]</code> <p>Either the Django model class or an instance.</p> required <code>max_depth</code> <code>int</code> <p>Maximum recursion depth for generating/converting related models.</p> <code>3</code> <code>exclude</code> <code>Optional[set[str]]</code> <p>Field names to exclude during conversion to Pydantic.      Note: Exclusion during generation is not yet implemented here,      but <code>generate_pydantic_class</code> could be adapted if needed.</p> <code>None</code> <code>pydantic_base</code> <code>Optional[type[BaseModel]]</code> <p>Optional base for generated Pydantic model.</p> <code>None</code> <code>model_name</code> <code>Optional[str]</code> <p>Optional name for generated Pydantic model.</p> <code>None</code> Source code in <code>src/pydantic2django/django/conversion.py</code> <pre><code>def __init__(\n    self,\n    django_model_or_instance: Union[type[DjangoModelT], DjangoModelT],\n    *,\n    max_depth: int = 3,\n    exclude: Optional[set[str]] = None,\n    pydantic_base: Optional[type[BaseModel]] = None,\n    model_name: Optional[str] = None,\n):\n    \"\"\"\n    Initializes the converter.\n\n    Args:\n        django_model_or_instance: Either the Django model class or an instance.\n        max_depth: Maximum recursion depth for generating/converting related models.\n        exclude: Field names to exclude during conversion *to* Pydantic.\n                 Note: Exclusion during generation is not yet implemented here,\n                 but `generate_pydantic_class` could be adapted if needed.\n        pydantic_base: Optional base for generated Pydantic model.\n        model_name: Optional name for generated Pydantic model.\n    \"\"\"\n    if isinstance(django_model_or_instance, models.Model):\n        self.django_model_cls: type[DjangoModelT] = django_model_or_instance.__class__\n        self.initial_django_instance: Optional[DjangoModelT] = django_model_or_instance\n    elif issubclass(django_model_or_instance, models.Model):\n        # Ensure the type checker understands self.django_model_cls is Type[DjangoModelT]\n        # where DjangoModelT is the specific type bound to the class instance.\n        self.django_model_cls: type[DjangoModelT] = django_model_or_instance  # Type should be consistent now\n        self.initial_django_instance = None\n    else:\n        raise TypeError(\"Input must be a Django Model class or instance.\")\n\n    self.max_depth = max_depth\n    self.exclude = exclude or set()\n    self.pydantic_base = pydantic_base or BaseModel\n    self.model_name = model_name  # Store optional name\n    # Initialize dependencies\n    self.relationship_accessor = RelationshipConversionAccessor()\n    self.mapper = BidirectionalTypeMapper(self.relationship_accessor)\n    # Register the initial model with the accessor (if needed for self-refs immediately)\n    # self.relationship_accessor.map_relationship(source_model=???, django_model=self.django_model_cls) # Need Pydantic type?\n\n    # Use the correctly defined cache type (name -&gt; model/ref)\n    self._generation_cache: GeneratedModelCache = {}\n    # _django_metadata is no longer needed\n\n    # Generate the Pydantic class definition immediately\n    generated_cls_or_ref = self._generate_pydantic_class()\n\n    # --- Resolve ForwardRef if necessary --- #\n    resolved_cls: type[BaseModel]\n    if isinstance(generated_cls_or_ref, ForwardRef):\n        logger.warning(\n            f\"Initial generation for {self.django_model_cls.__name__} resulted in a ForwardRef.\"\n            f\" Attempting resolution via model_rebuild on potentially dependent models in cache.\"\n        )\n        # Try resolving by rebuilding all models in the cache that are actual classes\n        # This assumes dependencies were generated and cached correctly.\n        # Rebuild cached models to attempt resolving ForwardRefs\n        for name, model_or_ref in self._generation_cache.items():\n            if isinstance(model_or_ref, type) and issubclass(model_or_ref, BaseModel):\n                try:\n                    model_or_ref.model_rebuild(force=True)\n                    # logger.debug(f\"Rebuilt cached model {name} to potentially resolve ForwardRefs\")\n                    # Successfully rebuilt a cached model; continue rebuilding others\n                except Exception as e:\n                    logger.warning(f\"Failed to rebuild cached model {name} during ForwardRef resolution: {e}\")\n\n        # After attempting rebuilds, try accessing the resolved type from the cache again\n        final_cached_item = self._generation_cache.get(generated_cls_or_ref.__forward_arg__)\n        if isinstance(final_cached_item, type) and issubclass(final_cached_item, BaseModel):\n            logger.info(f\"Successfully resolved ForwardRef for {generated_cls_or_ref.__forward_arg__}\")\n            resolved_cls = final_cached_item\n        else:\n            # If still not resolved, raise an error as we cannot proceed\n            raise TypeError(\n                f\"Failed to resolve ForwardRef '{generated_cls_or_ref.__forward_arg__}' for the main model {self.django_model_cls.__name__} after generation and rebuild attempts.\"\n            )\n    elif isinstance(generated_cls_or_ref, type) and issubclass(generated_cls_or_ref, BaseModel):\n        # Already a valid class\n        resolved_cls = generated_cls_or_ref\n    else:\n        # Should not happen if _generate_pydantic_class works correctly\n        raise TypeError(\n            f\"_generate_pydantic_class returned an unexpected type: {type(generated_cls_or_ref)} for {self.django_model_cls.__name__}\"\n        )\n\n    # Assign the resolved class\n    self.pydantic_model_cls: type[BaseModel] = resolved_cls\n\n    #          logger.debug(f\"Skipping update_forward_refs for ForwardRef in cache: {item_name}\")\n\n    # Rebuild the main model AFTER resolving refs in the cache.\n    # No need for _types_namespace if update_forward_refs worked.\n    # Now it should be safe to call rebuild on the resolved class\n    try:\n        self.pydantic_model_cls.model_rebuild(force=True)\n        logger.info(f\"Rebuilt main model {self.pydantic_model_cls.__name__} after generation.\")\n    except Exception as e:\n        # Log warning if final rebuild still fails\n        logger.warning(\n            f\"Failed to rebuild main model {self.pydantic_model_cls.__name__} after resolution: {e}\",\n            exc_info=True,\n        )\n</code></pre>"},{"location":"reference/pydantic2django/django/conversion/#pydantic2django.django.conversion.DjangoPydanticConverter.to_django","title":"<code>to_django(pydantic_instance, update_instance=None)</code>","text":"<p>Converts a Pydantic model instance back to a Django model instance, updating an existing one or creating a new one.</p> <p>Parameters:</p> Name Type Description Default <code>pydantic_instance</code> <code>BaseModel</code> <p>The Pydantic model instance containing the data.</p> required <code>update_instance</code> <code>Optional[DjangoModelT]</code> <p>An optional existing Django instance to update.              If None, attempts to update the initial instance (if provided),              otherwise creates a new Django instance.</p> <code>None</code> <p>Returns:</p> Type Description <code>DjangoModelT</code> <p>The saved Django model instance.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the pydantic_instance is not of the expected type or related models are incorrect.</p> <code>ValueError</code> <p>If saving fails, PKs are invalid, or required fields are missing.</p> Source code in <code>src/pydantic2django/django/conversion.py</code> <pre><code>def to_django(self, pydantic_instance: BaseModel, update_instance: Optional[DjangoModelT] = None) -&gt; DjangoModelT:\n    \"\"\"\n    Converts a Pydantic model instance back to a Django model instance,\n    updating an existing one or creating a new one.\n\n    Args:\n        pydantic_instance: The Pydantic model instance containing the data.\n        update_instance: An optional existing Django instance to update.\n                         If None, attempts to update the initial instance (if provided),\n                         otherwise creates a new Django instance.\n\n    Returns:\n        The saved Django model instance.\n\n    Raises:\n        TypeError: If the pydantic_instance is not of the expected type or related models are incorrect.\n        ValueError: If saving fails, PKs are invalid, or required fields are missing.\n    \"\"\"\n    if not isinstance(pydantic_instance, self.pydantic_model_cls):\n        raise TypeError(f\"Input must be an instance of {self.pydantic_model_cls.__name__}\")\n\n    logger.info(\n        f\"Attempting to save data from {self.pydantic_model_cls.__name__} instance back to Django model {self.django_model_cls.__name__}\"\n    )\n\n    # 1. Determine the target Django instance\n    target_django_instance = self._determine_target_django_instance(pydantic_instance, update_instance)\n\n    # 2. Process Pydantic fields and assign to Django instance (defer M2M)\n    try:\n        m2m_data = self._process_pydantic_fields(target_django_instance, pydantic_instance)\n    except ValueError as e:\n        # Catch errors during field assignment\n        logger.error(f\"Error processing Pydantic fields for {self.django_model_cls.__name__}: {e}\", exc_info=True)\n        raise  # Re-raise the ValueError\n\n    # 3. Save the main instance\n    try:\n        # Consider moving full_clean() call after M2M assignment if it causes issues here.\n        # target_django_instance.full_clean(exclude=[...]) # Option to exclude fields causing early validation issues\n        target_django_instance.save()\n        logger.info(f\"Saved basic fields for Django instance (PK: {target_django_instance.pk})\")\n\n    except Exception as e:  # Catch save errors (e.g., database constraints)\n        logger.exception(\n            f\"Failed initial save for Django instance (PK might be {target_django_instance.pk}) of model {self.django_model_cls.__name__}\"\n        )\n        raise ValueError(f\"Django save operation failed for {target_django_instance}: {e}\") from e\n\n    # 4. Handle M2M Assignment (After Save)\n    try:\n        self._assign_m2m_fields(target_django_instance, m2m_data)\n    except ValueError as e:\n        # Catch errors during M2M assignment\n        logger.error(\n            f\"Error assigning M2M fields for {self.django_model_cls.__name__} (PK: {target_django_instance.pk}): {e}\",\n            exc_info=True,\n        )\n        # Decide if we should re-raise or just log. Re-raising seems safer.\n        raise\n\n    # 5. Run final validation\n    try:\n        target_django_instance.full_clean()\n        logger.info(\n            f\"Successfully validated and saved Pydantic data to Django instance (PK: {target_django_instance.pk})\"\n        )\n    except Exception as e:  # Catch validation errors\n        logger.exception(\n            f\"Django validation failed after saving and M2M assignment for instance (PK: {target_django_instance.pk}) of model {self.django_model_cls.__name__}\"\n        )\n        # It's already saved, but invalid state. Raise the validation error.\n        raise ValueError(f\"Django validation failed for {target_django_instance}: {e}\") from e\n\n    return target_django_instance\n</code></pre>"},{"location":"reference/pydantic2django/django/conversion/#pydantic2django.django.conversion.DjangoPydanticConverter.to_pydantic","title":"<code>to_pydantic(db_obj=None)</code>","text":"<p>Converts a Django model instance to an instance of the generated Pydantic model.</p> <p>Parameters:</p> Name Type Description Default <code>db_obj</code> <code>Optional[DjangoModelT]</code> <p>The Django model instance to convert. If None, attempts to use     the instance provided during initialization.</p> <code>None</code> <p>Returns:</p> Type Description <code>PydanticModelT</code> <p>An instance of the generated Pydantic model (subclass of BaseModel).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no Django instance is available or conversion fails.</p> Source code in <code>src/pydantic2django/django/conversion.py</code> <pre><code>def to_pydantic(self, db_obj: Optional[DjangoModelT] = None) -&gt; PydanticModelT:\n    \"\"\"\n    Converts a Django model instance to an instance of the generated Pydantic model.\n\n    Args:\n        db_obj: The Django model instance to convert. If None, attempts to use\n                the instance provided during initialization.\n\n    Returns:\n        An instance of the generated Pydantic model (subclass of BaseModel).\n\n    Raises:\n        ValueError: If no Django instance is available or conversion fails.\n    \"\"\"\n    target_db_obj = db_obj or self.initial_django_instance\n    if target_db_obj is None:\n        raise ValueError(\"A Django model instance must be provided for conversion.\")\n\n    if not isinstance(target_db_obj, self.django_model_cls):\n        raise TypeError(f\"Provided instance is not of type {self.django_model_cls.__name__}\")\n\n    logger.info(\n        f\"Converting {self.django_model_cls.__name__} instance (PK: {target_db_obj.pk}) to {self.pydantic_model_cls.__name__}\"\n    )\n\n    # We know self.pydantic_model_cls is Type[BaseModel] from _generate_pydantic_class\n    # Pass the pre-extracted metadata\n    result = django_to_pydantic(\n        target_db_obj,\n        self.pydantic_model_cls,  # Now guaranteed to be type[BaseModel] by __init__\n        exclude=self.exclude,\n        max_depth=self.max_depth,\n    )\n    return cast(PydanticModelT, result)\n</code></pre>"},{"location":"reference/pydantic2django/django/conversion/#pydantic2django.django.conversion.django_to_pydantic","title":"<code>django_to_pydantic(db_obj, pydantic_model, *, exclude=None, depth=0, max_depth=3)</code>","text":"<p>Converts a Django model instance to a Pydantic model instance.</p> <p>Parameters:</p> Name Type Description Default <code>db_obj</code> <code>DjangoModelT</code> <p>The Django model instance to convert.</p> required <code>pydantic_model</code> <code>type[PydanticModelT]</code> <p>The target Pydantic model class.</p> required <code>exclude</code> <code>set[str] | None</code> <p>A set of field names to exclude from the conversion.</p> <code>None</code> <code>depth</code> <code>int</code> <p>Current recursion depth (internal use).</p> <code>0</code> <code>max_depth</code> <code>int</code> <p>Maximum recursion depth for related models.</p> <code>3</code> <p>Returns:</p> Type Description <code>PydanticModelT</code> <p>An instance of the target Pydantic model populated with data</p> <code>PydanticModelT</code> <p>from the Django model instance.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If conversion fails or recursion depth is exceeded.</p> <code>AttributeError</code> <p>If a field expected by Pydantic doesn't exist on the Django model.</p> Source code in <code>src/pydantic2django/django/conversion.py</code> <pre><code>def django_to_pydantic(\n    db_obj: DjangoModelT,\n    pydantic_model: type[PydanticModelT],\n    *,\n    exclude: set[str] | None = None,\n    depth: int = 0,  # Add depth to prevent infinite recursion\n    max_depth: int = 3,  # Set a default max depth\n) -&gt; PydanticModelT:\n    \"\"\"\n    Converts a Django model instance to a Pydantic model instance.\n\n    Args:\n        db_obj: The Django model instance to convert.\n        pydantic_model: The target Pydantic model class.\n        exclude: A set of field names to exclude from the conversion.\n        depth: Current recursion depth (internal use).\n        max_depth: Maximum recursion depth for related models.\n\n    Returns:\n        An instance of the target Pydantic model populated with data\n        from the Django model instance.\n\n    Raises:\n        ValueError: If conversion fails or recursion depth is exceeded.\n        AttributeError: If a field expected by Pydantic doesn't exist on the Django model.\n    \"\"\"\n    if depth &gt; max_depth:\n        logger.warning(\n            f\"Maximum recursion depth ({max_depth}) exceeded for {pydantic_model.__name__} from {db_obj.__class__.__name__}\"\n        )\n        # Decide how to handle this: raise error or return None/partial data?\n        # For now, let's raise an error to be explicit.\n        raise ValueError(f\"Maximum recursion depth ({max_depth}) exceeded.\")\n\n    # Extract metadata if not provided\n    # if django_metadata is None: # Removed reference to django_metadata\n    #     django_metadata = _extract_django_model_metadata(db_obj.__class__)\n\n    exclude = exclude or set()\n    data: dict[str, Any] = {}\n    # Use pre-extracted metadata if available - This logic needs adjustment as _extract_django_model_metadata is not defined\n    # metadata_source = _extract_django_model_metadata(db_obj.__class__) # Directly call extraction\n\n    pydantic_fields = pydantic_model.model_fields\n\n    for field_name, pydantic_field in pydantic_fields.items():\n        if field_name in exclude:\n            logger.debug(f\"Skipping excluded field '{field_name}'\")\n            continue\n\n        logger.debug(\n            f\"Processing Pydantic field: {field_name} (Depth: {depth}) for Django model {db_obj.__class__.__name__}\"\n        )\n\n        # Check if field exists on Django model instance\n        if not hasattr(db_obj, field_name):\n            logger.warning(f\"Field '{field_name}' not found on Django model {db_obj.__class__.__name__}. Skipping.\")\n            continue\n\n        django_value = getattr(db_obj, field_name)\n        pydantic_annotation = pydantic_field.annotation\n        origin = get_origin(pydantic_annotation)\n        args = get_args(pydantic_annotation)\n        # meta = metadata_source.get(field_name) # Removed metadata_source\n\n        # Get Django field object for type checking\n        try:\n            django_field = db_obj._meta.get_field(field_name)\n        except models.FieldDoesNotExist:\n            django_field = None\n            logger.debug(f\"Could not find Django field for Pydantic field '{field_name}'. Handling as simple field.\")\n\n        # --- 1. Handle Relations ---\n        # if meta and meta.is_relation: # Replaced meta check\n        if django_field and isinstance(django_field, RelatedField):\n            if depth &gt;= max_depth:\n                logger.warning(\n                    f\"Max depth ({max_depth}) reached for relation '{field_name}' at depth {depth}. Assigning PK(s) or None/[] based on relation type.\"\n                )\n                # Replace meta.is_m2m check\n                if isinstance(django_field, ManyToManyField):\n                    data[field_name] = []  # Default for max depth M2M is empty list\n                else:  # FK or O2O\n                    data[field_name] = None  # Default for max depth FK/O2O is None\n                continue  # Relation handled (depth limit), move to next field\n\n            # --- Attempt Recursive Conversion ---\\n            target_related_pydantic_model: Optional[type[BaseModel]] = None\n\n            # Determine target Pydantic model for FK/O2O\n            # Replace meta.is_fk or meta.is_o2o check\n            if isinstance(django_field, (ForeignKey, OneToOneField)):\n                # Simplified logic: get potential model from annotation\n                potential_model_type = next((t for t in args if isinstance(t, type) and issubclass(t, BaseModel)), None)\n                if (\n                    not potential_model_type\n                    and isinstance(pydantic_annotation, type)\n                    and issubclass(pydantic_annotation, BaseModel)\n                ):\n                    potential_model_type = pydantic_annotation\n                # Add ForwardRef resolution attempt if needed (simplified)\n                if isinstance(next((t for t in args if t is not type(None)), pydantic_annotation), ForwardRef):\n                    # Ensure we use the *resolved* annotation if the original was ForwardRef\n                    resolved_annotation = pydantic_model.model_fields[field_name].annotation\n                    potential_model_type = next(\n                        (\n                            t\n                            for t in get_args(resolved_annotation)  # Check args of resolved Optional[Model] etc.\n                            if isinstance(t, type) and issubclass(t, BaseModel)\n                        ),\n                        # Fallback check if resolved annotation is directly the model (not Optional[Model])\n                        resolved_annotation\n                        if isinstance(resolved_annotation, type) and issubclass(resolved_annotation, BaseModel)\n                        else None,\n                    )\n\n                target_related_pydantic_model = potential_model_type\n\n                if target_related_pydantic_model:\n                    logger.debug(\n                        f\"Handling FK/O2O relationship for '{field_name}' -&gt; {target_related_pydantic_model.__name__}\"\n                    )\n                    related_obj = django_value\n                    if related_obj:\n                        try:\n                            data[field_name] = django_to_pydantic(\n                                related_obj,\n                                target_related_pydantic_model,\n                                exclude=exclude,\n                                depth=depth + 1,\n                                max_depth=max_depth,\n                            )\n                        except ValueError as e:\n                            logger.error(f\"Failed converting related object in FK/O2O '{field_name}': {e}\")\n                            data[field_name] = None  # Assign None on conversion failure (e.g. nested depth limit)\n                    else:\n                        data[field_name] = None  # Django FK/O2O was None\n                    continue  # Relation handled (recursion success/failure/was None), move to next field\n                else:\n                    # Could not determine target Pydantic model, assign PK\n                    logger.warning(\n                        f\"Could not determine target Pydantic model for FK/O2O '{field_name}'. Assigning PK.\"\n                    )\n                    # Extract PK using field_name_id convention\n                    data[field_name] = getattr(db_obj, f\"{field_name}_id\", None)\n                    continue  # Relation handled (assigned PK), move to next field\n\n            # Determine target Pydantic model for M2M\n            # Replace meta.is_m2m check\n            elif isinstance(django_field, ManyToManyField):\n                # Simplified logic: get potential model from List[Model] or List[ForwardRef]\n                target_related_pydantic_model = None\n                if origin is list and args:\n                    potential_model_type = args[0]\n                    if isinstance(potential_model_type, type) and issubclass(potential_model_type, BaseModel):\n                        target_related_pydantic_model = potential_model_type\n                    elif isinstance(potential_model_type, ForwardRef):\n                        # Ensure we use the *resolved* annotation if the original was ForwardRef\n                        resolved_annotation = pydantic_model.model_fields[field_name].annotation\n                        resolved_args = get_args(resolved_annotation)\n                        if (\n                            resolved_args\n                            and isinstance(resolved_args[0], type)\n                            and issubclass(resolved_args[0], BaseModel)\n                        ):\n                            target_related_pydantic_model = resolved_args[0]\n\n                if target_related_pydantic_model:\n                    logger.debug(\n                        f\"Handling M2M relationship for '{field_name}' -&gt; List[{target_related_pydantic_model.__name__}]\"\n                    )\n                    related_manager = django_value\n                    converted_related = []\n                    if hasattr(related_manager, \"all\"):\n                        try:\n                            for related_obj in related_manager.all():\n                                try:\n                                    converted_related.append(\n                                        django_to_pydantic(\n                                            related_obj,\n                                            target_related_pydantic_model,\n                                            exclude=exclude,\n                                            depth=depth + 1,\n                                            max_depth=max_depth,\n                                        )\n                                    )\n                                except ValueError as e:\n                                    logger.error(f\"Failed converting related object in M2M '{field_name}': {e}\")\n                                    continue  # Skip item on depth error\n                        except Exception as e:\n                            logger.error(f\"Error accessing M2M manager for '{field_name}': {e}\", exc_info=True)\n                    else:\n                        logger.warning(\n                            f\"Expected a manager for M2M field '{field_name}' but got {type(django_value)}. Assigning empty list.\"\n                        )\n                    data[field_name] = converted_related\n                    continue  # Relation handled (recursion), move to next field\n                else:\n                    # Could not determine target Pydantic model, assign list of PKs\n                    logger.warning(\n                        f\"Could not determine target Pydantic model for M2M '{field_name}'. Assigning list of PKs.\"\n                    )\n                    related_manager = django_value\n                    pk_list = []\n                    if hasattr(related_manager, \"all\"):\n                        try:\n                            pk_list = list(related_manager.values_list(\"pk\", flat=True))\n                        except Exception as e:\n                            logger.error(f\"Error getting PKs for M2M field '{field_name}': {e}\", exc_info=True)\n                    else:\n                        logger.warning(\n                            f\"Expected a manager for M2M field '{field_name}' but got {type(django_value)}. Assigning empty PK list.\"\n                        )\n                    data[field_name] = pk_list\n                    continue  # Relation handled (assigned PK list), move to next field\n\n        # --- 2. Handle FileField/ImageField ---\n        # if meta and issubclass(meta.django_field_type, models.FileField): # Replaced meta check\n        if django_field and isinstance(django_field, models.FileField):\n            field_value = getattr(db_obj, field_name)  # Use original django_value which is the FieldFile\n            # Check if Pydantic field is required\n            is_pydantic_required = pydantic_field.is_required()\n            # Get URL if field has a value, otherwise None\n            url_value = field_value.url if field_value else None\n\n            if url_value is None and is_pydantic_required:\n                # If Django value is None/empty but Pydantic requires a string, assign empty string\n                data[field_name] = \"\"\n                logger.debug(\n                    f\"Handling FileField/ImageField '{field_name}' -&gt; Assigned empty string for required field.\"\n                )\n            else:\n                # Otherwise, assign the URL (which might be None if Pydantic field is Optional)\n                data[field_name] = url_value\n                logger.debug(f\"Handling FileField/ImageField '{field_name}' -&gt; URL: {data.get(field_name)}\")\n            continue  # FileField handled, move to next field\n\n        # --- 3. Handle JSONField ---\n        # if meta and meta.django_field_type == models.JSONField: # Replaced meta check\n        if django_field and isinstance(django_field, models.JSONField):\n            logger.debug(f\"Field '{field_name}' is a Django JSONField. Attempting serialization.\")\n            logger.debug(f\"Field '{field_name}': Django value type: {type(django_value)}, value: {django_value!r}\")\n            value_to_assign = None\n            if django_value is not None:\n                try:\n                    # Always serialize if the source is JSONField\n                    value_to_assign = json.dumps(django_value)\n                    logger.debug(f\"Field '{field_name}': Serialized value: {value_to_assign!r}\")\n                except TypeError as e:\n                    logger.error(f\"Failed to serialize JSON for field '{field_name}': {e}\", exc_info=True)\n                    # Fallback: assign raw value if serialization fails? Or None?\n                    # Assigning raw value might still lead to validation error later.\n                    # Let's assign None for now if serialization fails.\n                    value_to_assign = None\n            data[field_name] = value_to_assign\n            logger.debug(f\"Field '{field_name}': Assigning serialized/fallback value: {data.get(field_name)!r}\")\n            continue  # JSONField handled\n\n        # --- Check Pydantic Annotation (secondary check for non-JSONField Django types) ---\n        # This path is now less likely to be hit if source is JSONField\n        is_target_pydantic_json = _is_pydantic_json_annotation(pydantic_annotation)\n        if is_target_pydantic_json and not (\n            django_field and isinstance(django_field, models.JSONField)\n        ):  # Avoid re-processing JSONField\n            logger.debug(\n                f\"Field '{field_name}' (Django type: {type(django_field).__name__ if django_field else 'Unknown'}) has pydantic.Json annotation. Attempting serialization.\"\n            )\n            # Same serialization logic as above\n            value_to_assign = None\n            if django_value is not None:\n                try:\n                    value_to_assign = json.dumps(django_value)\n                except TypeError as e:\n                    logger.error(\n                        f\"Failed to serialize non-JSONField '{field_name}' for pydantic.Json target: {e}\", exc_info=True\n                    )\n                    value_to_assign = None\n            data[field_name] = value_to_assign\n            continue\n\n        # --- 4. Handle Simple Fields (Default) ---\n        logger.debug(f\"Handling simple/property/other field '{field_name}' with value: {django_value!r}\")\n        data[field_name] = django_value\n\n    # Instantiate the Pydantic model with the collected data\n    try:\n        instance = pydantic_model(**data)\n        logger.info(\n            f\"Successfully converted {db_obj.__class__.__name__} instance (PK: {db_obj.pk}) to {pydantic_model.__name__}\"\n        )\n        return cast(PydanticModelT, instance)\n    except Exception as e:\n        logger.error(f\"Failed to instantiate Pydantic model {pydantic_model.__name__} with data {data}\", exc_info=True)\n        raise ValueError(\n            f\"Failed to create Pydantic model {pydantic_model.__name__} from Django instance {db_obj}: {e}\"\n        ) from e\n</code></pre>"},{"location":"reference/pydantic2django/django/conversion/#pydantic2django.django.conversion.generate_pydantic_class","title":"<code>generate_pydantic_class(django_model_cls, mapper, *, model_name=None, cache=None, depth=0, max_depth=3, pydantic_base=None, exclude=None)</code>","text":"<p>Dynamically generates a Pydantic model class from a Django model class, using pre-extracted metadata if provided.</p> <p>Parameters:</p> Name Type Description Default <code>django_model_cls</code> <code>type[Model]</code> <p>The Django model class to convert.</p> required <code>mapper</code> <code>BidirectionalTypeMapper</code> <p>The BidirectionalTypeMapper instance for mapping between Django and Pydantic types.</p> required <code>model_name</code> <code>Optional[str]</code> <p>Optional explicit name for the generated Pydantic model.         Defaults to f\"{django_model_cls.name}Pydantic\".</p> <code>None</code> <code>cache</code> <code>Optional[GeneratedModelCache]</code> <p>A dictionary to cache generated models and prevent recursion errors.    Must be provided for recursive generation. Keys are model names.</p> <code>None</code> <code>depth</code> <code>int</code> <p>Current recursion depth.</p> <code>0</code> <code>max_depth</code> <code>int</code> <p>Maximum recursion depth for related models.</p> <code>3</code> <code>pydantic_base</code> <code>Optional[type[BaseModel]]</code> <p>Optional base for generated Pydantic model.</p> <code>None</code> <code>exclude</code> <code>Optional[set[str]]</code> <p>Field names to exclude during generation.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[type[BaseModel], ForwardRef]</code> <p>A dynamically created Pydantic model class or a ForwardRef if max_depth is hit.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If maximum recursion depth is exceeded or generation fails.</p> <code>TypeError</code> <p>If a field type cannot be mapped.</p> Source code in <code>src/pydantic2django/django/conversion.py</code> <pre><code>def generate_pydantic_class(\n    django_model_cls: type[models.Model],\n    mapper: BidirectionalTypeMapper,\n    *,\n    model_name: Optional[str] = None,\n    cache: Optional[GeneratedModelCache] = None,\n    depth: int = 0,\n    max_depth: int = 3,\n    pydantic_base: Optional[type[BaseModel]] = None,\n    exclude: Optional[set[str]] = None,\n) -&gt; \"Union[type[BaseModel], ForwardRef]\":\n    \"\"\"\n    Dynamically generates a Pydantic model class from a Django model class,\n    using pre-extracted metadata if provided.\n\n    Args:\n        django_model_cls: The Django model class to convert.\n        mapper: The BidirectionalTypeMapper instance for mapping between Django and Pydantic types.\n        model_name: Optional explicit name for the generated Pydantic model.\n                    Defaults to f\"{django_model_cls.__name__}Pydantic\".\n        cache: A dictionary to cache generated models and prevent recursion errors.\n               Must be provided for recursive generation. Keys are model names.\n        depth: Current recursion depth.\n        max_depth: Maximum recursion depth for related models.\n        pydantic_base: Optional base for generated Pydantic model.\n        exclude: Field names to exclude during generation.\n\n    Returns:\n        A dynamically created Pydantic model class or a ForwardRef if max_depth is hit.\n\n    Raises:\n        ValueError: If maximum recursion depth is exceeded or generation fails.\n        TypeError: If a field type cannot be mapped.\n    \"\"\"\n    logger.info(f\"*** Running generate_pydantic_class [v_lazy_fix] for {django_model_cls.__name__} ***\")\n\n    if cache is None:\n        cache = {}\n\n    # Ensure exclude is a set\n    exclude_set = exclude or set()\n\n    pydantic_model_name = model_name or f\"{django_model_cls.__name__}Pydantic\"\n    logger.debug(\n        f\"Generating Pydantic model '{pydantic_model_name}' for Django model '{django_model_cls.__name__}' (Depth: {depth})\"\n    )\n\n    # --- Cache Check --- (Check if ACTUAL model is already cached)\n    if pydantic_model_name in cache:\n        cached_item = cache[pydantic_model_name]\n        if isinstance(cached_item, type) and issubclass(cached_item, BaseModel):\n            logger.debug(f\"Cache hit (actual class) for name '{pydantic_model_name}' (Depth: {depth})\")\n            return cached_item\n        elif isinstance(cached_item, ForwardRef):\n            # If a ForwardRef is in the cache, it means we hit max_depth earlier or are in a loop\n            logger.debug(f\"Cache hit (ForwardRef) for name '{pydantic_model_name}' (Depth: {depth})\")\n            return cached_item\n\n    # --- Max Depth Check --- (Return ForwardRef, but DON'T cache it here)\n    if depth &gt; max_depth:\n        logger.warning(\n            f\"Max recursion depth ({max_depth}) reached for {django_model_cls.__name__}. Returning ForwardRef.\"\n        )\n        # Store the ForwardRef in the cache ONLY if max depth is reached\n        # This prevents premature caching before dependencies are potentially resolved.\n        forward_ref = ForwardRef(pydantic_model_name)\n        cache[pydantic_model_name] = forward_ref  # Cache the ForwardRef when hitting max depth\n        return forward_ref\n\n    # --- Process Fields (Recursively generate dependencies FIRST) ---\n    # Metadata extraction is now handled within the loop using the mapper\n\n    field_definitions: dict[str, tuple[Any, Any]] = {}\n    # Use set[Any] for dependencies to avoid complex Union typing with ForwardRef\n    model_dependencies: set[Any] = set()\n\n    # TEMPORARILY place a ForwardRef to handle self-references during field processing\n    # This will be replaced by the actual model later.\n    temp_forward_ref = ForwardRef(pydantic_model_name)\n    cache[pydantic_model_name] = temp_forward_ref\n    logger.debug(f\"Placed TEMPORARY ForwardRef '{pydantic_model_name}' in cache for self-ref handling (Depth: {depth})\")\n\n    # Iterate through Django model fields directly\n    for dj_field in django_model_cls._meta.get_fields(include_hidden=False):\n        # Skip reverse relations and non-concrete fields immediately\n        if isinstance(\n            dj_field, (ForeignObjectRel, OneToOneRel, ManyToOneRel, ManyToManyRel, GenericForeignKey)\n        ) or not getattr(dj_field, \"concrete\", False):\n            logger.debug(f\"Skipping non-concrete/reverse field '{dj_field.name}' of type {type(dj_field).__name__}\")\n            continue\n\n        field_name = dj_field.name\n        # --- Check Exclusion BEFORE processing ---\n        if field_name in exclude_set:\n            logger.debug(f\"  Skipping excluded field: {field_name}\")\n            continue\n\n        logger.debug(f\"  Processing field: {field_name} ({type(dj_field).__name__}) using mapper...\")\n\n        # --- Use Mapper to get Pydantic Type and FieldInfo --- #\n        try:\n            initial_type, field_info_kwargs = mapper.get_pydantic_mapping(dj_field)\n            # Initialize final_type here to avoid unbound error later\n            final_type = initial_type\n        except Exception as e:\n            logger.error(f\"  Error getting initial Pydantic mapping for field '{field_name}': {e}\", exc_info=True)\n            continue\n\n        # --- Handle Relationships (Recursive Generation) --- #\n        is_relation_field = isinstance(dj_field, RelatedField)\n        related_model_cls = getattr(dj_field, \"related_model\", None)\n        is_self_ref = related_model_cls == \"self\" or related_model_cls == django_model_cls\n\n        if is_relation_field and related_model_cls:\n            # Check depth *before* recursive call\n            if depth &gt;= max_depth:\n                # Max depth reached - Use initial type (likely PK representation from mapper)\n                logger.warning(\n                    f\"Max recursion depth ({max_depth}) reached for {related_model_cls.__name__} relation '{field_name}'. Using initial type/PK representation.\"\n                )\n                # final_type is already initial_type from above\n                is_relation_field = False  # Treat as non-relation for dependency tracking\n            else:\n                # Depth not exceeded, proceed with recursive generation logic\n                try:  # Outer try for recursive generation logic\n                    resolved_related_model_cls: Optional[type[models.Model]] = None\n                    if related_model_cls == \"self\":\n                        resolved_related_model_cls = django_model_cls\n                    elif isinstance(related_model_cls, type) and issubclass(related_model_cls, models.Model):\n                        resolved_related_model_cls = related_model_cls\n\n                    needs_recursive_call = False\n                    if resolved_related_model_cls:\n                        # Check if it's a Django model to determine if recursion makes sense\n                        if isinstance(resolved_related_model_cls, type) and issubclass(\n                            resolved_related_model_cls, models.Model\n                        ):\n                            target_pydantic_name = f\"{resolved_related_model_cls.__name__}Pydantic\"\n                            # Prevent infinite loop on self-reference if already processing this model\n                            if not (\n                                is_self_ref\n                                and target_pydantic_name in cache\n                                and cache[target_pydantic_name] == temp_forward_ref\n                            ):\n                                needs_recursive_call = True\n\n                    if needs_recursive_call and resolved_related_model_cls:\n                        logger.debug(\n                            f\"    Recursively generating/fetching for related model: {resolved_related_model_cls.__name__}\"\n                        )\n                        related_pydantic_model_ref = generate_pydantic_class(\n                            resolved_related_model_cls,\n                            mapper,\n                            model_name=None,  # Generate default name\n                            cache=cache,\n                            depth=depth + 1,\n                            max_depth=max_depth,\n                            pydantic_base=pydantic_base,\n                            exclude=exclude,\n                        )\n\n                        # Check if recursion returned a valid type and try re-mapping\n                        if (\n                            isinstance(related_pydantic_model_ref, type)\n                            or type(related_pydantic_model_ref) is ForwardRef\n                        ):\n                            model_dependencies.add(related_pydantic_model_ref)  # type: ignore[arg-type]\n                            # --- Re-evaluate the type AFTER recursive call ---\n                            try:  # Inner try for re-mapping\n                                updated_type, updated_kwargs = mapper.get_pydantic_mapping(dj_field)\n                                final_type = updated_type  # Update final_type ONLY if re-map succeeds\n                                field_info_kwargs = updated_kwargs  # Update kwargs too\n                                logger.debug(f\"    Re-evaluated type for '{field_name}' after recursion: {final_type}\")\n                            except Exception as re_map_error:\n                                logger.warning(\n                                    f\"    Failed to re-evaluate mapping for '{field_name}' after recursion, using initial type. Error: {re_map_error}\"\n                                )\n                                # Keep final_type = initial_type (already set) on re-map failure\n                        else:\n                            # Recursion failed to return a valid type\n                            logger.warning(\n                                f\"Recursive call for relation '{field_name}' did not return Model or ForwardRef. Type: {type(related_pydantic_model_ref)}. Using initial type.\"\n                            )\n                            # Keep final_type = initial_type (already set)\n\n                    elif is_relation_field:  # Recursion skipped (e.g., self-ref or other condition)\n                        # Use the initial type determined by the mapper earlier\n                        logger.debug(\n                            f\"    Skipping recursive call for relation '{field_name}' (e.g. self-ref or other condition). Using initial type.\"\n                        )\n                        ref_to_add = get_args(initial_type)[0] if get_origin(initial_type) is list else initial_type\n                        model_dependencies.add(ref_to_add)  # type: ignore[arg-type]\n                        # Keep final_type = initial_type (already set)\n\n                except Exception as e:  # Except for outer try (recursive generation logic)\n                    logger.error(\n                        f\"Error during recursive generation logic for field '{field_name}': {e}\", exc_info=True\n                    )\n                    # Keep final_type = initial_type and continue to next field definition\n                    continue  # Skip field definition steps on this error\n        # else: # Not a relation field\n        # final_type is already initial_type\n\n        # --- Final Type Adjustment and Field Definition ---\n        # 'final_type' should now always be defined (initialized to initial_type)\n\n        # The mapper should have handled Optional wrapping based on dj_field.null\n        # The mapper's field_info_kwargs should contain defaults etc.\n        field_instance: Any = ...  # Default to required\n\n        # Determine Field definition based on potentially updated kwargs\n        if field_info_kwargs:\n            try:\n                # --- Explicitly resolve lazy proxies in choices within json_schema_extra --- #\n                if \"json_schema_extra\" in field_info_kwargs and isinstance(\n                    field_info_kwargs[\"json_schema_extra\"], dict\n                ):\n                    choices_data = field_info_kwargs[\"json_schema_extra\"].get(\"choices\")\n                    if isinstance(choices_data, list):\n                        resolved_choices = []\n                        for choice_item in choices_data:\n                            if isinstance(choice_item, (list, tuple)) and len(choice_item) == 2:\n                                value, label = choice_item\n                                # Force label to string if it's a lazy proxy or potentially other types\n                                resolved_choices.append((value, str(label)))\n                            else:\n                                resolved_choices.append(choice_item)  # Keep invalid item as is for now\n                        field_info_kwargs[\"json_schema_extra\"][\"choices\"] = resolved_choices\n                        logger.debug(f\"    Resolved lazy proxies in choices for field '{field_name}'\")\n\n                # Ensure optional Django fields are not required in Pydantic if no explicit default provided\n                if dj_field.null and \"default\" not in field_info_kwargs:\n                    field_info_kwargs[\"default\"] = None\n                field_instance = Field(**field_info_kwargs)\n            except TypeError as field_exc:\n                logger.error(f\"Invalid FieldInfo kwargs for '{field_name}': {field_info_kwargs}. Error: {field_exc}\")\n                # Fallback: Check Django field nullability\n                field_instance = None if dj_field.null else ...\n        else:\n            # No field info kwargs: Check Django field nullability\n            field_instance = None if dj_field.null else ...\n\n        field_definitions[field_name] = (final_type, field_instance)\n        logger.debug(f\"  Defined field '{field_name}': Type={final_type}, Definition={field_instance!r}\")\n\n    # --- Create the Pydantic Model Class ---\n    model_base = pydantic_base or BaseModel\n    try:\n        model_cls = create_model(\n            pydantic_model_name,\n            __base__=model_base,\n            **field_definitions,  # type: ignore\n        )\n        logger.info(f\"Successfully created Pydantic model class '{pydantic_model_name}'\")\n\n        # --- IMPORTANT: Update cache with the *actual* class, replacing the temp ForwardRef ---\n        cache[pydantic_model_name] = model_cls\n        logger.debug(f\"Updated cache for '{pydantic_model_name}' with actual class object.\")\n\n        # --- ALSO IMPORTANT: Register the mapping in the accessor ---\n        if mapper and mapper.relationship_accessor:\n            # Assume generate_pydantic_class only deals with Pydantic for now\n            mapper.relationship_accessor.map_relationship(source_model=model_cls, django_model=django_model_cls)\n            logger.debug(f\"Registered mapping for {model_cls.__name__} &lt;-&gt; {django_model_cls.__name__} in accessor.\")\n\n        return model_cls\n\n    except Exception as e:\n        logger.error(f\"Failed to create Pydantic model '{pydantic_model_name}' using create_model: {e}\", exc_info=True)\n        # If creation fails, remove the temporary ForwardRef if it exists\n        if cache.get(pydantic_model_name) is temp_forward_ref:\n            del cache[pydantic_model_name]\n        raise ValueError(f\"Failed to create Pydantic model '{pydantic_model_name}'\") from e\n</code></pre>"},{"location":"reference/pydantic2django/django/models/","title":"pydantic2django.django.models","text":"<p>Base Django model class with Pydantic conversion capabilities.</p>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.CommonBaseModel","title":"<code>CommonBaseModel</code>","text":"<p>               Bases: <code>Model</code></p> <p>Abstract base class for storing serializable Python objects in the database.</p> <p>Provides common functionality for storing Pydantic models or dataclasses.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>class CommonBaseModel(models.Model):\n    \"\"\"\n    Abstract base class for storing serializable Python objects in the database.\n\n    Provides common functionality for storing Pydantic models or dataclasses.\n    \"\"\"\n\n    id = models.UUIDField(primary_key=True, default=uuid.uuid4, editable=False)\n    name = models.CharField(max_length=255)\n    # Rename object_type to class_path for clarity\n    class_path = models.CharField(\n        max_length=255,\n        help_text=\"Fully qualified Python path of the stored object's class (e.g., my_module.MyClass)\",\n        db_index=True,  # Add index for potential lookups\n    )\n    created_at = models.DateTimeField(auto_now_add=True)\n    updated_at = models.DateTimeField(auto_now=True)\n    # Removed object_type_field as it's redundant with class_path\n\n    # Class-level cache for imported classes\n    _class_cache: ClassVar[dict[str, type]] = {}\n\n    class Meta:\n        abstract = True\n        # Add default ordering?\n        # ordering = ['name']\n\n    @classmethod\n    def _get_class_info(cls, obj: Any) -&gt; tuple[type, str, str, str]:\n        \"\"\"\n        Get information about the object's class.\n\n        Args:\n            obj: The Pydantic model or dataclass instance.\n\n        Returns:\n            Tuple of (obj_class, class_name, module_name, fully_qualified_name)\n        \"\"\"\n        obj_class = obj.__class__\n        class_name = obj_class.__name__\n        module_name = obj_class.__module__\n        fully_qualified_name = f\"{module_name}.{class_name}\"\n        return obj_class, class_name, module_name, fully_qualified_name\n\n    @classmethod\n    def _derive_name(cls, obj: Any, name: str | None, class_name: str) -&gt; str:\n        \"\"\"\n        Derive a name for the Django model instance.\n\n        Args:\n            obj: The Pydantic model or dataclass instance.\n            name: Optional name provided by the caller.\n            class_name: The name of the object's class.\n\n        Returns:\n            The derived name.\n        \"\"\"\n        if name is not None:\n            return name\n\n        # Try to get the name from the object if it has a 'name' attribute\n        obj_name = getattr(obj, \"name\", None)\n        if isinstance(obj_name, str) and obj_name:\n            return obj_name\n\n        # Fallback to the class name\n        return class_name\n\n    def _get_class(self) -&gt; type:\n        \"\"\"\n        Get the stored object's class (Pydantic model or dataclass) from its path.\n\n        Returns:\n            The class type.\n\n        Raises:\n            ValueError: If the class cannot be found or imported.\n        \"\"\"\n        if not self.class_path:\n            raise ValueError(\"Cannot load class: 'class_path' field is empty.\")\n\n        try:\n            # Check cache first\n            if self.class_path in self.__class__._class_cache:\n                return self.__class__._class_cache[self.class_path]\n\n            # Parse module and class name\n            module_path, class_name = self.class_path.rsplit(\".\", 1)\n            # Import the module and get the class\n            module = importlib.import_module(module_path)\n            loaded_class = getattr(module, class_name)\n\n            # Store in cache\n            self.__class__._class_cache[self.class_path] = loaded_class\n            return loaded_class\n        except (ImportError, AttributeError, ValueError) as e:\n            raise ValueError(f\"Could not find or import class '{self.class_path}': {e}\") from e\n\n    def _verify_object_type_match(self, obj: Any) -&gt; str:\n        \"\"\"\n        Verify that the provided object's type matches the stored class path.\n\n        Args:\n            obj: The Pydantic model or dataclass instance to check.\n\n        Returns:\n            The fully qualified name of the object's class.\n\n        Raises:\n            TypeError: If the object's type doesn't match the stored class path.\n        \"\"\"\n        obj_class = obj.__class__\n        class_name = obj_class.__name__\n        module_name = obj_class.__module__\n        fully_qualified_name = f\"{module_name}.{class_name}\"\n\n        # Direct comparison of the fully qualified path\n        if self.class_path != fully_qualified_name:\n            raise TypeError(\n                f\"Object type mismatch: Expected instance of '{self.class_path}', \"\n                f\"but got instance of '{fully_qualified_name}'.\"\n            )\n\n        return fully_qualified_name\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.Dataclass2DjangoBase","title":"<code>Dataclass2DjangoBase</code>","text":"<p>               Bases: <code>CommonBaseModel</code></p> <p>Abstract base class for storing Python Dataclass objects in the database. Inherits common fields and methods from CommonBaseModel.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>class Dataclass2DjangoBase(CommonBaseModel):\n    \"\"\"\n    Abstract base class for storing Python Dataclass objects in the database.\n    Inherits common fields and methods from CommonBaseModel.\n    \"\"\"\n\n    # Add specific attributes or methods for dataclasses if needed later\n    _expected_dataclass_type: ClassVar[type | None] = None\n\n    class Meta(CommonBaseModel.Meta):\n        abstract = True\n\n    @classmethod\n    def _check_expected_type(cls, dc_obj: Any, class_name: str) -&gt; None:\n        \"\"\"\n        Check if the object is a dataclass and matches the expected type (if set).\n\n        Args:\n            dc_obj: The object to check.\n            class_name: The name of the object's class (for error messages).\n\n        Raises:\n            TypeError: If the object is not a dataclass or doesn't match the expected type.\n        \"\"\"\n        if not dataclasses.is_dataclass(dc_obj):\n            raise TypeError(f\"Object provided is not a dataclass: type={type(dc_obj)}\")\n\n        expected_type = getattr(cls, \"_expected_dataclass_type\", None)\n        if expected_type is not None:\n            # Ensure expected_type is also a dataclass type for comparison\n            if not dataclasses.is_dataclass(expected_type):\n                # This indicates a configuration error in the subclass\n                raise TypeError(\n                    f\"Internal configuration error: _expected_dataclass_type '{expected_type}' is not a dataclass.\"\n                )\n            if not isinstance(dc_obj, expected_type):\n                expected_name = getattr(expected_type, \"__name__\", str(expected_type))\n                raise TypeError(f\"Expected dataclass object of type {expected_name}, but got {class_name}\")\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.Dataclass2DjangoBaseClass","title":"<code>Dataclass2DjangoBaseClass</code>","text":"<p>               Bases: <code>Dataclass2DjangoBase</code>, <code>Generic[DataclassT]</code></p> <p>Base class for mapping Python Dataclass fields to Django model fields.</p> <p>Inherits from Dataclass2DjangoBase and provides methods to convert between the Dataclass instance and the Django model instance by matching field names.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>class Dataclass2DjangoBaseClass(Dataclass2DjangoBase, Generic[DataclassT]):\n    \"\"\"\n    Base class for mapping Python Dataclass fields to Django model fields.\n\n    Inherits from Dataclass2DjangoBase and provides methods to convert\n    between the Dataclass instance and the Django model instance by matching field names.\n    \"\"\"\n\n    class Meta(Dataclass2DjangoBase.Meta):\n        abstract = True\n        verbose_name = \"Mapped Dataclass\"\n        verbose_name_plural = \"Mapped Dataclasses\"\n\n    # __getattr__ is less likely needed/useful for standard dataclasses compared to Pydantic models\n    # which might have complex methods. Skip for now.\n\n    @classmethod\n    def from_dataclass(cls, dc_obj: DataclassT, name: str | None = None) -&gt; \"Dataclass2DjangoBaseClass[DataclassT]\":\n        \"\"\"\n        Create a Django model instance from a Dataclass object, mapping fields.\n\n        Args:\n            dc_obj: The Dataclass object to convert.\n            name: Optional name for the Django model instance.\n\n        Returns:\n            A new instance of this Django model subclass.\n\n        Raises:\n            TypeError: If the object is not a dataclass or not of the expected type.\n        \"\"\"\n        # Get class info and check type\n        (\n            dc_class,\n            class_name,\n            module_name,\n            fully_qualified_name,\n        ) = cls._get_class_info(dc_obj)\n        cls._check_expected_type(dc_obj, class_name)  # Verifies it's a dataclass\n\n        # Derive name\n        derived_name = cls._derive_name(dc_obj, name, class_name)\n\n        # Create instance with basic fields\n        instance = cls(\n            name=derived_name,\n            class_path=fully_qualified_name,\n        )\n\n        # Update mapped fields\n        instance.update_fields_from_dataclass(dc_obj)\n\n        return instance\n\n    def update_fields_from_dataclass(self, dc_obj: DataclassT) -&gt; None:\n        \"\"\"\n        Update this Django model's fields from a Dataclass object's fields.\n\n        Args:\n            dc_obj: The Dataclass object containing source values.\n\n        Raises:\n            TypeError: If conversion to dict fails.\n        \"\"\"\n        if (\n            not dataclasses.is_dataclass(dc_obj)\n            or dc_obj.__class__.__module__ != self._get_class().__module__\n            or dc_obj.__class__.__name__ != self._get_class().__name__\n        ):\n            # Check type consistency before proceeding\n            raise TypeError(\n                f\"Provided object type {type(dc_obj)} does not match expected type {self.class_path} for update.\"\n            )\n\n        try:\n            dc_data = dataclasses.asdict(dc_obj)\n        except TypeError as e:\n            raise TypeError(f\"Could not convert dataclass '{dc_obj.__class__.__name__}' to dict for update: {e}\") from e\n\n        # Get Django model fields excluding common/meta ones\n        model_field_names = {\n            field.name\n            for field in self._meta.fields\n            if field.name not in (\"id\", \"name\", \"class_path\", \"created_at\", \"updated_at\")\n        }\n\n        for field_name in model_field_names:\n            if field_name in dc_data:\n                value = dc_data[field_name]\n                # Apply serialization (important for complex types like datetime, UUID, etc.)\n                serialized_value = serialize_value(value)\n                setattr(self, field_name, serialized_value)\n            # Else: Field exists on Django model but not on dataclass, leave it unchanged.\n\n    def to_dataclass(self) -&gt; DataclassT:\n        \"\"\"\n        Convert this Django model instance back to a Dataclass object.\n\n        Returns:\n            The reconstructed Dataclass object.\n\n        Raises:\n            ValueError: If the class cannot be loaded or instantiation fails.\n        \"\"\"\n        dataclass_type = self._get_class()\n        if not dataclasses.is_dataclass(dataclass_type):\n            raise ValueError(f\"Stored class path '{self.class_path}' does not point to a dataclass.\")\n\n        # Get data from Django fields corresponding to dataclass fields\n        data_for_dc = self._get_data_for_dataclass(dataclass_type)\n\n        # Instantiate the dataclass\n        try:\n            # TODO: Add deserialization logic if needed\n            instance = dataclass_type(**data_for_dc)\n            # Cast to the generic type variable for type hinting\n            return cast(DataclassT, instance)\n        except TypeError as e:\n            raise ValueError(\n                f\"Failed to instantiate dataclass '{dataclass_type.__name__}' from Django model fields. \"\n                f\"Ensure required fields exist and types are compatible. Error: {e}\"\n            ) from e\n        except Exception as e:\n            logger.error(f\"An unexpected error occurred during dataclass reconstruction: {e}\", exc_info=True)\n            raise ValueError(f\"An unexpected error occurred during dataclass reconstruction: {e}\") from e\n\n    def _get_data_for_dataclass(self, dataclass_type: type) -&gt; dict[str, Any]:\n        \"\"\"Get data from Django fields that correspond to the target dataclass fields.\"\"\"\n        data = {}\n        try:\n            dc_field_names = {f.name for f in dataclasses.fields(dataclass_type)}\n        except TypeError as err:\n            # Should not happen if is_dataclass check passed, but handle defensively\n            raise ValueError(f\"Could not get fields for non-dataclass type '{dataclass_type.__name__}'\") from err\n\n        # Add DB fields that are part of the dataclass\n        for field in self._meta.fields:\n            if field.name in dc_field_names:\n                # TODO: Add potential deserialization based on target dataclass field type?\n                data[field.name] = getattr(self, field.name)\n\n        # Context handling is usually Pydantic-specific, skip for dataclasses unless needed\n        return data\n\n    def update_from_dataclass(self, dc_obj: DataclassT) -&gt; None:\n        \"\"\"\n        Update this Django model with new data from a Dataclass object and save.\n\n        Args:\n            dc_obj: The Dataclass object with updated data.\n        \"\"\"\n        # Verify the object type matches first (includes check if it's a dataclass)\n        fully_qualified_name = self._verify_object_type_match(dc_obj)\n\n        # Update the class_path if somehow inconsistent\n        if self.class_path != fully_qualified_name:\n            self.class_path = fully_qualified_name\n\n        self.update_fields_from_dataclass(dc_obj)\n        self.save()\n\n    def save_as_dataclass(self) -&gt; DataclassT:\n        \"\"\"\n        Save the Django model and return the corresponding Dataclass object.\n\n        Returns:\n            The corresponding Dataclass object.\n        \"\"\"\n        self.save()\n        return self.to_dataclass()\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.Dataclass2DjangoBaseClass.from_dataclass","title":"<code>from_dataclass(dc_obj, name=None)</code>  <code>classmethod</code>","text":"<p>Create a Django model instance from a Dataclass object, mapping fields.</p> <p>Parameters:</p> Name Type Description Default <code>dc_obj</code> <code>DataclassT</code> <p>The Dataclass object to convert.</p> required <code>name</code> <code>str | None</code> <p>Optional name for the Django model instance.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dataclass2DjangoBaseClass[DataclassT]</code> <p>A new instance of this Django model subclass.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the object is not a dataclass or not of the expected type.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>@classmethod\ndef from_dataclass(cls, dc_obj: DataclassT, name: str | None = None) -&gt; \"Dataclass2DjangoBaseClass[DataclassT]\":\n    \"\"\"\n    Create a Django model instance from a Dataclass object, mapping fields.\n\n    Args:\n        dc_obj: The Dataclass object to convert.\n        name: Optional name for the Django model instance.\n\n    Returns:\n        A new instance of this Django model subclass.\n\n    Raises:\n        TypeError: If the object is not a dataclass or not of the expected type.\n    \"\"\"\n    # Get class info and check type\n    (\n        dc_class,\n        class_name,\n        module_name,\n        fully_qualified_name,\n    ) = cls._get_class_info(dc_obj)\n    cls._check_expected_type(dc_obj, class_name)  # Verifies it's a dataclass\n\n    # Derive name\n    derived_name = cls._derive_name(dc_obj, name, class_name)\n\n    # Create instance with basic fields\n    instance = cls(\n        name=derived_name,\n        class_path=fully_qualified_name,\n    )\n\n    # Update mapped fields\n    instance.update_fields_from_dataclass(dc_obj)\n\n    return instance\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.Dataclass2DjangoBaseClass.save_as_dataclass","title":"<code>save_as_dataclass()</code>","text":"<p>Save the Django model and return the corresponding Dataclass object.</p> <p>Returns:</p> Type Description <code>DataclassT</code> <p>The corresponding Dataclass object.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>def save_as_dataclass(self) -&gt; DataclassT:\n    \"\"\"\n    Save the Django model and return the corresponding Dataclass object.\n\n    Returns:\n        The corresponding Dataclass object.\n    \"\"\"\n    self.save()\n    return self.to_dataclass()\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.Dataclass2DjangoBaseClass.to_dataclass","title":"<code>to_dataclass()</code>","text":"<p>Convert this Django model instance back to a Dataclass object.</p> <p>Returns:</p> Type Description <code>DataclassT</code> <p>The reconstructed Dataclass object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the class cannot be loaded or instantiation fails.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>def to_dataclass(self) -&gt; DataclassT:\n    \"\"\"\n    Convert this Django model instance back to a Dataclass object.\n\n    Returns:\n        The reconstructed Dataclass object.\n\n    Raises:\n        ValueError: If the class cannot be loaded or instantiation fails.\n    \"\"\"\n    dataclass_type = self._get_class()\n    if not dataclasses.is_dataclass(dataclass_type):\n        raise ValueError(f\"Stored class path '{self.class_path}' does not point to a dataclass.\")\n\n    # Get data from Django fields corresponding to dataclass fields\n    data_for_dc = self._get_data_for_dataclass(dataclass_type)\n\n    # Instantiate the dataclass\n    try:\n        # TODO: Add deserialization logic if needed\n        instance = dataclass_type(**data_for_dc)\n        # Cast to the generic type variable for type hinting\n        return cast(DataclassT, instance)\n    except TypeError as e:\n        raise ValueError(\n            f\"Failed to instantiate dataclass '{dataclass_type.__name__}' from Django model fields. \"\n            f\"Ensure required fields exist and types are compatible. Error: {e}\"\n        ) from e\n    except Exception as e:\n        logger.error(f\"An unexpected error occurred during dataclass reconstruction: {e}\", exc_info=True)\n        raise ValueError(f\"An unexpected error occurred during dataclass reconstruction: {e}\") from e\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.Dataclass2DjangoBaseClass.update_fields_from_dataclass","title":"<code>update_fields_from_dataclass(dc_obj)</code>","text":"<p>Update this Django model's fields from a Dataclass object's fields.</p> <p>Parameters:</p> Name Type Description Default <code>dc_obj</code> <code>DataclassT</code> <p>The Dataclass object containing source values.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If conversion to dict fails.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>def update_fields_from_dataclass(self, dc_obj: DataclassT) -&gt; None:\n    \"\"\"\n    Update this Django model's fields from a Dataclass object's fields.\n\n    Args:\n        dc_obj: The Dataclass object containing source values.\n\n    Raises:\n        TypeError: If conversion to dict fails.\n    \"\"\"\n    if (\n        not dataclasses.is_dataclass(dc_obj)\n        or dc_obj.__class__.__module__ != self._get_class().__module__\n        or dc_obj.__class__.__name__ != self._get_class().__name__\n    ):\n        # Check type consistency before proceeding\n        raise TypeError(\n            f\"Provided object type {type(dc_obj)} does not match expected type {self.class_path} for update.\"\n        )\n\n    try:\n        dc_data = dataclasses.asdict(dc_obj)\n    except TypeError as e:\n        raise TypeError(f\"Could not convert dataclass '{dc_obj.__class__.__name__}' to dict for update: {e}\") from e\n\n    # Get Django model fields excluding common/meta ones\n    model_field_names = {\n        field.name\n        for field in self._meta.fields\n        if field.name not in (\"id\", \"name\", \"class_path\", \"created_at\", \"updated_at\")\n    }\n\n    for field_name in model_field_names:\n        if field_name in dc_data:\n            value = dc_data[field_name]\n            # Apply serialization (important for complex types like datetime, UUID, etc.)\n            serialized_value = serialize_value(value)\n            setattr(self, field_name, serialized_value)\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.Dataclass2DjangoBaseClass.update_from_dataclass","title":"<code>update_from_dataclass(dc_obj)</code>","text":"<p>Update this Django model with new data from a Dataclass object and save.</p> <p>Parameters:</p> Name Type Description Default <code>dc_obj</code> <code>DataclassT</code> <p>The Dataclass object with updated data.</p> required Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>def update_from_dataclass(self, dc_obj: DataclassT) -&gt; None:\n    \"\"\"\n    Update this Django model with new data from a Dataclass object and save.\n\n    Args:\n        dc_obj: The Dataclass object with updated data.\n    \"\"\"\n    # Verify the object type matches first (includes check if it's a dataclass)\n    fully_qualified_name = self._verify_object_type_match(dc_obj)\n\n    # Update the class_path if somehow inconsistent\n    if self.class_path != fully_qualified_name:\n        self.class_path = fully_qualified_name\n\n    self.update_fields_from_dataclass(dc_obj)\n    self.save()\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.Dataclass2DjangoStoreDataclassObject","title":"<code>Dataclass2DjangoStoreDataclassObject</code>","text":"<p>               Bases: <code>Dataclass2DjangoBase</code></p> <p>Class to store a Python Dataclass object in the database as JSON.</p> <p>All data is stored in the 'data' field. Database fields matching dataclass fields can be optionally synced.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>class Dataclass2DjangoStoreDataclassObject(Dataclass2DjangoBase):\n    \"\"\"\n    Class to store a Python Dataclass object in the database as JSON.\n\n    All data is stored in the 'data' field. Database fields matching dataclass\n    fields can be optionally synced.\n    \"\"\"\n\n    data = models.JSONField(help_text=\"JSON representation of the dataclass object.\")\n\n    class Meta(Dataclass2DjangoBase.Meta):\n        abstract = True\n        verbose_name = \"Stored Dataclass Object\"\n        verbose_name_plural = \"Stored Dataclass Objects\"\n\n    @classmethod\n    def from_dataclass(cls, dc_obj: Any, name: str | None = None) -&gt; \"Dataclass2DjangoStoreDataclassObject\":\n        \"\"\"\n        Create a Django model instance from a Dataclass object.\n\n        Args:\n            dc_obj: The Dataclass object to store.\n            name: Optional name for the Django model instance.\n\n        Returns:\n            A new instance of this Django model.\n\n        Raises:\n            TypeError: If the provided object is not a dataclass or not of the expected type.\n        \"\"\"\n        # Get class info and check type\n        (\n            dc_class,\n            class_name,\n            module_name,\n            fully_qualified_name,\n        ) = cls._get_class_info(dc_obj)\n        cls._check_expected_type(dc_obj, class_name)  # Verifies it's a dataclass\n\n        # Get data and serialize values\n        try:\n            data_dict = dataclasses.asdict(dc_obj)\n        except TypeError as e:\n            raise TypeError(f\"Could not convert dataclass '{class_name}' to dict: {e}\") from e\n\n        serialized_data = {key: serialize_value(value) for key, value in data_dict.items()}\n\n        # Derive name\n        derived_name = cls._derive_name(dc_obj, name, class_name)\n\n        instance = cls(\n            name=derived_name,\n            class_path=fully_qualified_name,\n            data=serialized_data,\n        )\n        # Optionally sync fields immediately after creation if desired\n        # instance.sync_db_fields_from_data(save=False)\n        return instance\n\n    def to_dataclass(self) -&gt; Any:\n        \"\"\"\n        Convert the stored JSON data back to a Dataclass object.\n\n        Returns:\n            The reconstructed Dataclass object.\n\n        Raises:\n            ValueError: If the class cannot be loaded or instantiation fails.\n        \"\"\"\n        dataclass_type = self._get_class()\n        if not dataclasses.is_dataclass(dataclass_type):\n            raise ValueError(f\"Stored class path '{self.class_path}' does not point to a dataclass.\")\n\n        # Use the stored JSON data\n        stored_data = self.data\n\n        # Basic reconstruction (does not handle complex types or context yet)\n        try:\n            # TODO: Add deserialization logic if serialize_value performs complex transformations\n            # For now, assume stored data is directly usable.\n            instance = dataclass_type(**stored_data)\n        except TypeError as e:\n            raise ValueError(\n                f\"Failed to instantiate dataclass '{dataclass_type.__name__}' from stored data. \"\n                f\"Ensure stored data keys match dataclass fields. Error: {e}\"\n            ) from e\n        except Exception as e:\n            logger.error(f\"An unexpected error occurred during dataclass reconstruction: {e}\", exc_info=True)\n            raise ValueError(f\"An unexpected error occurred during dataclass reconstruction: {e}\") from e\n\n        return instance\n\n    def update_from_dataclass(self, dc_obj: Any) -&gt; None:\n        \"\"\"\n        Update this Django model instance with new data from a Dataclass object.\n\n        Args:\n            dc_obj: The Dataclass object with updated data.\n\n        Raises:\n            TypeError: If the object type doesn't match or conversion fails.\n        \"\"\"\n        # Verify the object type matches the stored path\n        fully_qualified_name = self._verify_object_type_match(dc_obj)\n        # Check if it's actually a dataclass (redundant if verify works, but safe)\n        if not dataclasses.is_dataclass(dc_obj):\n            raise TypeError(\"Provided object for update is not a dataclass.\")\n\n        # Update class_path if somehow inconsistent (shouldn't happen if verify passed)\n        if self.class_path != fully_qualified_name:\n            self.class_path = fully_qualified_name  # Correctness check\n\n        # Get new data and serialize\n        try:\n            data_dict = dataclasses.asdict(dc_obj)\n        except TypeError as e:\n            raise TypeError(f\"Could not convert dataclass '{dc_obj.__class__.__name__}' to dict for update: {e}\") from e\n\n        self.data = {key: serialize_value(value) for key, value in data_dict.items()}\n        # Optionally sync fields before saving\n        # self.sync_db_fields_from_data(save=False)\n        self.save()\n\n    def sync_db_fields_from_data(self, save: bool = True) -&gt; None:\n        \"\"\"\n        Synchronize database fields from the JSON 'data' field.\n\n        Updates Django model fields (excluding common/meta fields) with values\n        from the JSON data if the field names match.\n\n        Args:\n            save: If True (default), saves the instance after updating fields.\n        \"\"\"\n        if not isinstance(self.data, dict):\n            # Log or handle cases where data is not a dict\n            return\n\n        # Get model fields, excluding common ones and the data field itself\n        model_field_names = {\n            field.name\n            for field in self._meta.fields\n            if field.name not in (\"id\", \"name\", \"class_path\", \"data\", \"created_at\", \"updated_at\")\n        }\n\n        updated_fields = []\n        for field_name in model_field_names:\n            if field_name in self.data:\n                current_value = getattr(self, field_name)\n                new_value = self.data[field_name]\n                # Basic check to avoid unnecessary updates\n                # TODO: Consider type coercion or more robust comparison if needed\n                if current_value != new_value:\n                    setattr(self, field_name, new_value)\n                    updated_fields.append(field_name)\n\n        if updated_fields and save:\n            self.save(update_fields=updated_fields)\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.Dataclass2DjangoStoreDataclassObject.from_dataclass","title":"<code>from_dataclass(dc_obj, name=None)</code>  <code>classmethod</code>","text":"<p>Create a Django model instance from a Dataclass object.</p> <p>Parameters:</p> Name Type Description Default <code>dc_obj</code> <code>Any</code> <p>The Dataclass object to store.</p> required <code>name</code> <code>str | None</code> <p>Optional name for the Django model instance.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dataclass2DjangoStoreDataclassObject</code> <p>A new instance of this Django model.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the provided object is not a dataclass or not of the expected type.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>@classmethod\ndef from_dataclass(cls, dc_obj: Any, name: str | None = None) -&gt; \"Dataclass2DjangoStoreDataclassObject\":\n    \"\"\"\n    Create a Django model instance from a Dataclass object.\n\n    Args:\n        dc_obj: The Dataclass object to store.\n        name: Optional name for the Django model instance.\n\n    Returns:\n        A new instance of this Django model.\n\n    Raises:\n        TypeError: If the provided object is not a dataclass or not of the expected type.\n    \"\"\"\n    # Get class info and check type\n    (\n        dc_class,\n        class_name,\n        module_name,\n        fully_qualified_name,\n    ) = cls._get_class_info(dc_obj)\n    cls._check_expected_type(dc_obj, class_name)  # Verifies it's a dataclass\n\n    # Get data and serialize values\n    try:\n        data_dict = dataclasses.asdict(dc_obj)\n    except TypeError as e:\n        raise TypeError(f\"Could not convert dataclass '{class_name}' to dict: {e}\") from e\n\n    serialized_data = {key: serialize_value(value) for key, value in data_dict.items()}\n\n    # Derive name\n    derived_name = cls._derive_name(dc_obj, name, class_name)\n\n    instance = cls(\n        name=derived_name,\n        class_path=fully_qualified_name,\n        data=serialized_data,\n    )\n    # Optionally sync fields immediately after creation if desired\n    # instance.sync_db_fields_from_data(save=False)\n    return instance\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.Dataclass2DjangoStoreDataclassObject.sync_db_fields_from_data","title":"<code>sync_db_fields_from_data(save=True)</code>","text":"<p>Synchronize database fields from the JSON 'data' field.</p> <p>Updates Django model fields (excluding common/meta fields) with values from the JSON data if the field names match.</p> <p>Parameters:</p> Name Type Description Default <code>save</code> <code>bool</code> <p>If True (default), saves the instance after updating fields.</p> <code>True</code> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>def sync_db_fields_from_data(self, save: bool = True) -&gt; None:\n    \"\"\"\n    Synchronize database fields from the JSON 'data' field.\n\n    Updates Django model fields (excluding common/meta fields) with values\n    from the JSON data if the field names match.\n\n    Args:\n        save: If True (default), saves the instance after updating fields.\n    \"\"\"\n    if not isinstance(self.data, dict):\n        # Log or handle cases where data is not a dict\n        return\n\n    # Get model fields, excluding common ones and the data field itself\n    model_field_names = {\n        field.name\n        for field in self._meta.fields\n        if field.name not in (\"id\", \"name\", \"class_path\", \"data\", \"created_at\", \"updated_at\")\n    }\n\n    updated_fields = []\n    for field_name in model_field_names:\n        if field_name in self.data:\n            current_value = getattr(self, field_name)\n            new_value = self.data[field_name]\n            # Basic check to avoid unnecessary updates\n            # TODO: Consider type coercion or more robust comparison if needed\n            if current_value != new_value:\n                setattr(self, field_name, new_value)\n                updated_fields.append(field_name)\n\n    if updated_fields and save:\n        self.save(update_fields=updated_fields)\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.Dataclass2DjangoStoreDataclassObject.to_dataclass","title":"<code>to_dataclass()</code>","text":"<p>Convert the stored JSON data back to a Dataclass object.</p> <p>Returns:</p> Type Description <code>Any</code> <p>The reconstructed Dataclass object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the class cannot be loaded or instantiation fails.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>def to_dataclass(self) -&gt; Any:\n    \"\"\"\n    Convert the stored JSON data back to a Dataclass object.\n\n    Returns:\n        The reconstructed Dataclass object.\n\n    Raises:\n        ValueError: If the class cannot be loaded or instantiation fails.\n    \"\"\"\n    dataclass_type = self._get_class()\n    if not dataclasses.is_dataclass(dataclass_type):\n        raise ValueError(f\"Stored class path '{self.class_path}' does not point to a dataclass.\")\n\n    # Use the stored JSON data\n    stored_data = self.data\n\n    # Basic reconstruction (does not handle complex types or context yet)\n    try:\n        # TODO: Add deserialization logic if serialize_value performs complex transformations\n        # For now, assume stored data is directly usable.\n        instance = dataclass_type(**stored_data)\n    except TypeError as e:\n        raise ValueError(\n            f\"Failed to instantiate dataclass '{dataclass_type.__name__}' from stored data. \"\n            f\"Ensure stored data keys match dataclass fields. Error: {e}\"\n        ) from e\n    except Exception as e:\n        logger.error(f\"An unexpected error occurred during dataclass reconstruction: {e}\", exc_info=True)\n        raise ValueError(f\"An unexpected error occurred during dataclass reconstruction: {e}\") from e\n\n    return instance\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.Dataclass2DjangoStoreDataclassObject.update_from_dataclass","title":"<code>update_from_dataclass(dc_obj)</code>","text":"<p>Update this Django model instance with new data from a Dataclass object.</p> <p>Parameters:</p> Name Type Description Default <code>dc_obj</code> <code>Any</code> <p>The Dataclass object with updated data.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If the object type doesn't match or conversion fails.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>def update_from_dataclass(self, dc_obj: Any) -&gt; None:\n    \"\"\"\n    Update this Django model instance with new data from a Dataclass object.\n\n    Args:\n        dc_obj: The Dataclass object with updated data.\n\n    Raises:\n        TypeError: If the object type doesn't match or conversion fails.\n    \"\"\"\n    # Verify the object type matches the stored path\n    fully_qualified_name = self._verify_object_type_match(dc_obj)\n    # Check if it's actually a dataclass (redundant if verify works, but safe)\n    if not dataclasses.is_dataclass(dc_obj):\n        raise TypeError(\"Provided object for update is not a dataclass.\")\n\n    # Update class_path if somehow inconsistent (shouldn't happen if verify passed)\n    if self.class_path != fully_qualified_name:\n        self.class_path = fully_qualified_name  # Correctness check\n\n    # Get new data and serialize\n    try:\n        data_dict = dataclasses.asdict(dc_obj)\n    except TypeError as e:\n        raise TypeError(f\"Could not convert dataclass '{dc_obj.__class__.__name__}' to dict for update: {e}\") from e\n\n    self.data = {key: serialize_value(value) for key, value in data_dict.items()}\n    # Optionally sync fields before saving\n    # self.sync_db_fields_from_data(save=False)\n    self.save()\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.Pydantic2DjangoBase","title":"<code>Pydantic2DjangoBase</code>","text":"<p>               Bases: <code>CommonBaseModel</code></p> <p>Abstract base class for storing Pydantic objects in the database. Inherits common fields and methods from CommonBaseModel.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>class Pydantic2DjangoBase(CommonBaseModel):\n    \"\"\"\n    Abstract base class for storing Pydantic objects in the database.\n    Inherits common fields and methods from CommonBaseModel.\n    \"\"\"\n\n    _expected_pydantic_type: ClassVar[type[BaseModel] | None] = None\n\n    class Meta(CommonBaseModel.Meta):\n        abstract = True\n\n    @classmethod\n    def _check_expected_type(cls, pydantic_obj: Any, class_name: str) -&gt; None:\n        \"\"\"\n        Check if the Pydantic object is of the expected type.\n\n        Args:\n            pydantic_obj: The Pydantic object to check.\n            class_name: The name of the Pydantic class (for error messages).\n\n        Raises:\n            TypeError: If the Pydantic object is not a Pydantic BaseModel or not of the expected type.\n        \"\"\"\n        if not isinstance(pydantic_obj, BaseModel):\n            raise TypeError(f\"Object provided is not a Pydantic BaseModel: type={type(pydantic_obj)}\")\n\n        expected_type = getattr(cls, \"_expected_pydantic_type\", None)\n        if expected_type is not None:\n            # Ensure expected_type is a BaseModel subclass for comparison\n            if not issubclass(expected_type, BaseModel):\n                # This indicates a configuration error in the subclass\n                raise TypeError(\n                    f\"Internal configuration error: _expected_pydantic_type '{expected_type}' is not a Pydantic BaseModel.\"\n                )\n            if not isinstance(pydantic_obj, expected_type):\n                expected_name = getattr(expected_type, \"__name__\", str(expected_type))\n                raise TypeError(f\"Expected Pydantic object of type {expected_name}, but got {class_name}\")\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.Pydantic2DjangoBaseClass","title":"<code>Pydantic2DjangoBaseClass</code>","text":"<p>               Bases: <code>Pydantic2DjangoBase</code>, <code>Generic[PydanticT]</code></p> <p>Base class for mapping Pydantic model fields to Django model fields.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>class Pydantic2DjangoBaseClass(Pydantic2DjangoBase, Generic[PydanticT]):\n    \"\"\"\n    Base class for mapping Pydantic model fields to Django model fields.\n    \"\"\"\n\n    class Meta(Pydantic2DjangoBase.Meta):\n        abstract = True\n        verbose_name = \"Mapped Pydantic Object\"\n        verbose_name_plural = \"Mapped Pydantic Objects\"\n\n    def __new__(cls, *args, **kwargs):\n        \"\"\"\n        Override __new__ to ensure proper type checking.\n        Needed because Django's model metaclass doesn't preserve Generic type parameters well.\n        \"\"\"\n        # The check itself might be complex. This placeholder ensures __new__ is considered.\n        # Proper generic handling might require metaclass adjustments beyond this scope.\n        return super().__new__(cls)\n\n    def __getattr__(self, name: str) -&gt; Any:\n        \"\"\"\n        Forward method calls to the Pydantic model implementation.\n        Enables type checking and execution of methods defined on the Pydantic model.\n        \"\"\"\n        # Get the Pydantic model class\n        try:\n            pydantic_cls = self._get_class()  # Use common method\n            if not issubclass(pydantic_cls, BaseModel):\n                # This path shouldn't be hit if used correctly, but safeguard\n                raise AttributeError(f\"Stored class '{self.class_path}' is not a Pydantic BaseModel.\")\n        except ValueError as e:\n            raise AttributeError(f\"Cannot forward attribute '{name}': {e}\") from e\n\n        # Check if the attribute exists in the Pydantic model\n        if hasattr(pydantic_cls, name):\n            attr = getattr(pydantic_cls, name)\n\n            # If it's a callable method (and not the type itself), wrap it\n            if callable(attr) and not isinstance(attr, type):\n\n                def wrapped_method(*args, **kwargs):\n                    # Convert self (Django model) to Pydantic instance first\n                    try:\n                        pydantic_instance = self.to_pydantic()  # Assuming no context needed here\n                    except ValueError as e:\n                        # Handle potential errors during conversion (e.g., context missing)\n                        raise RuntimeError(\n                            f\"Failed to convert Django model to Pydantic before calling '{name}': {e}\"\n                        ) from e\n\n                    # Call the method on the Pydantic instance\n                    result = getattr(pydantic_instance, name)(*args, **kwargs)\n                    # TODO: Handle potential need to update self from result? Unlikely for most methods.\n                    return result\n\n                return wrapped_method\n            else:\n                # For non-method attributes (like class vars), return directly\n                # This might need refinement depending on desired behavior for class vs instance attrs\n                return attr\n\n        # If attribute doesn't exist on Pydantic model, raise standard AttributeError\n        raise AttributeError(\n            f\"'{self.__class__.__name__}' object has no attribute '{name}', \"\n            f\"and Pydantic model '{pydantic_cls.__name__}' has no attribute '{name}'.\"\n        )\n\n    @classmethod\n    def from_pydantic(cls, pydantic_obj: PydanticT, name: str | None = None) -&gt; \"Pydantic2DjangoBaseClass[PydanticT]\":\n        \"\"\"\n        Create a Django model instance from a Pydantic object, mapping fields.\n\n        Args:\n            pydantic_obj: The Pydantic object to convert.\n            name: Optional name for the Django model instance.\n\n        Returns:\n            A new instance of this Django model subclass.\n\n        Raises:\n            TypeError: If the object is not a Pydantic model or not of the expected type.\n        \"\"\"\n        # Get class info and check type\n        (\n            pydantic_class,\n            class_name,\n            module_name,\n            fully_qualified_name,\n        ) = cls._get_class_info(pydantic_obj)\n        cls._check_expected_type(pydantic_obj, class_name)  # Verifies it's a Pydantic model\n\n        # Derive name\n        derived_name = cls._derive_name(pydantic_obj, name, class_name)\n\n        # Create instance with basic fields\n        instance = cls(\n            name=derived_name,\n            class_path=fully_qualified_name,  # Use renamed field\n        )\n\n        # Update mapped fields\n        instance.update_fields_from_pydantic(pydantic_obj)\n\n        return instance\n\n    def update_fields_from_pydantic(self, pydantic_obj: PydanticT) -&gt; None:\n        \"\"\"\n        Update this Django model's fields from a Pydantic object's fields.\n\n        Args:\n            pydantic_obj: The Pydantic object containing source values.\n        \"\"\"\n        if (\n            not isinstance(pydantic_obj, BaseModel)\n            or pydantic_obj.__class__.__module__ != self._get_class().__module__\n            or pydantic_obj.__class__.__name__ != self._get_class().__name__\n        ):\n            # Check type consistency before proceeding\n            raise TypeError(\n                f\"Provided object type {type(pydantic_obj)} does not match expected type {self.class_path} for update.\"\n            )\n\n        # Get data from the Pydantic object\n        try:\n            pydantic_data = pydantic_obj.model_dump()\n        except AttributeError as err:\n            raise TypeError(\n                \"Failed to dump Pydantic model for update. Ensure you are using Pydantic v2+ with model_dump().\"\n            ) from err\n\n        # Get Django model fields excluding common/meta ones\n        model_field_names = {\n            field.name\n            for field in self._meta.fields\n            if field.name not in (\"id\", \"name\", \"class_path\", \"created_at\", \"updated_at\")\n        }\n\n        # Update each Django field if it matches a field in the Pydantic data\n        for field_name in model_field_names:\n            if field_name in pydantic_data:\n                value = pydantic_data[field_name]\n                # Apply serialization (important for complex types)\n                serialized_value = serialize_value(value)\n                setattr(self, field_name, serialized_value)\n            # Else: Field exists on Django model but not on Pydantic model, leave it unchanged.\n\n    def to_pydantic(self, context: ModelContext | None = None) -&gt; PydanticT:\n        \"\"\"\n        Convert this Django model instance back to a Pydantic object.\n\n        Args:\n            context: Optional ModelContext instance containing values for non-serializable fields.\n\n        Returns:\n            The corresponding Pydantic object.\n\n        Raises:\n            ValueError: If context is required but not provided, or if class load/instantiation fails.\n        \"\"\"\n        pydantic_class = self._get_class()  # Use common method\n        if not issubclass(pydantic_class, BaseModel):\n            raise ValueError(f\"Stored class path '{self.class_path}' does not point to a Pydantic BaseModel.\")\n\n        # Get data from Django fields corresponding to Pydantic fields\n        data = self._get_data_for_pydantic(pydantic_class)\n\n        # Handle context if required and provided\n        required_context_keys = self._get_required_context_fields()  # Check if context is needed\n        if required_context_keys:\n            if not context:\n                raise ValueError(\n                    f\"Conversion to Pydantic model '{pydantic_class.__name__}' requires context \"\n                    f\"for fields: {', '.join(required_context_keys)}. Please provide a ModelContext instance.\"\n                )\n            # Validate and merge context data\n            context_dict = context.to_conversion_dict()\n            context.validate_context(context_dict)  # Validate required keys are present\n            data.update(context_dict)  # Merge context, potentially overwriting DB values if keys overlap\n\n        # Reconstruct the Pydantic object\n        try:\n            # TODO: Add potential deserialization logic here if needed before validation\n            instance = pydantic_class.model_validate(data)\n            # Cast to the generic type variable\n            return cast(PydanticT, instance)\n        except TypeError:\n            raise  # Re-raise TypeError as it's a specific, meaningful exception here\n        except Exception as e:\n            # Catch any other unexpected error during Pydantic object creation\n            logger.error(\n                f\"An unexpected error occurred creating Pydantic model for '{pydantic_class.__name__}': {e}\",\n                exc_info=True,\n            )\n            raise ValueError(f\"Unexpected error creating Pydantic model for '{pydantic_class.__name__}'\") from e\n\n    def _get_data_for_pydantic(self, pydantic_class: type[BaseModel]) -&gt; dict[str, Any]:\n        \"\"\"Get data from Django fields that correspond to the target Pydantic model fields.\"\"\"\n        data = {}\n        try:\n            pydantic_field_names = set(pydantic_class.model_fields.keys())\n        except AttributeError as err:\n            # Should not happen if issubclass(BaseModel) check passed\n            raise ValueError(f\"Could not get fields for non-Pydantic type '{pydantic_class.__name__}'\") from err\n\n        # Add DB fields that are part of the Pydantic model\n        for field in self._meta.fields:\n            if field.name in pydantic_field_names:\n                # TODO: Add potential deserialization based on target Pydantic field type?\n                data[field.name] = getattr(self, field.name)\n\n        # Context values are merged in the calling `to_pydantic` method\n        return data\n\n    def _get_required_context_fields(self) -&gt; set[str]:\n        \"\"\"\n        Get the set of field names that require context when converting to Pydantic.\n        (Placeholder implementation - needs refinement based on how context is defined).\n        \"\"\"\n        # This requires a mechanism to identify which Django fields represent\n        # non-serializable data that must come from context.\n        # For now, assume no context is required by default for the base class.\n        # Subclasses might override this or a more sophisticated mechanism could be added.\n        # Example: Check for a custom field attribute like `is_context_field=True`\n        required_fields = set()\n        # pydantic_class = self._get_class()\n        # pydantic_field_names = set(pydantic_class.model_fields.keys())\n        # for field in self._meta.fields:\n        #     if field.name in pydantic_field_names and getattr(field, 'is_context_field', False):\n        #         required_fields.add(field.name)\n        return required_fields  # Return empty set for now\n\n    def update_from_pydantic(self, pydantic_obj: PydanticT) -&gt; None:\n        \"\"\"\n        Update this Django model with new data from a Pydantic object and save.\n\n        Args:\n            pydantic_obj: The Pydantic object with updated data.\n        \"\"\"\n        # Verify the object type matches first (includes check if it's a BaseModel)\n        fully_qualified_name = self._verify_object_type_match(pydantic_obj)\n\n        # Update the class_path if somehow inconsistent\n        if self.class_path != fully_qualified_name:\n            self.class_path = fully_qualified_name\n\n        self.update_fields_from_pydantic(pydantic_obj)\n        self.save()\n\n    def save_as_pydantic(self) -&gt; PydanticT:\n        \"\"\"\n        Save the Django model and return the corresponding Pydantic object.\n\n        Returns:\n            The corresponding Pydantic object.\n        \"\"\"\n        self.save()\n        return self.to_pydantic(context=None)\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.Pydantic2DjangoBaseClass.__getattr__","title":"<code>__getattr__(name)</code>","text":"<p>Forward method calls to the Pydantic model implementation. Enables type checking and execution of methods defined on the Pydantic model.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>def __getattr__(self, name: str) -&gt; Any:\n    \"\"\"\n    Forward method calls to the Pydantic model implementation.\n    Enables type checking and execution of methods defined on the Pydantic model.\n    \"\"\"\n    # Get the Pydantic model class\n    try:\n        pydantic_cls = self._get_class()  # Use common method\n        if not issubclass(pydantic_cls, BaseModel):\n            # This path shouldn't be hit if used correctly, but safeguard\n            raise AttributeError(f\"Stored class '{self.class_path}' is not a Pydantic BaseModel.\")\n    except ValueError as e:\n        raise AttributeError(f\"Cannot forward attribute '{name}': {e}\") from e\n\n    # Check if the attribute exists in the Pydantic model\n    if hasattr(pydantic_cls, name):\n        attr = getattr(pydantic_cls, name)\n\n        # If it's a callable method (and not the type itself), wrap it\n        if callable(attr) and not isinstance(attr, type):\n\n            def wrapped_method(*args, **kwargs):\n                # Convert self (Django model) to Pydantic instance first\n                try:\n                    pydantic_instance = self.to_pydantic()  # Assuming no context needed here\n                except ValueError as e:\n                    # Handle potential errors during conversion (e.g., context missing)\n                    raise RuntimeError(\n                        f\"Failed to convert Django model to Pydantic before calling '{name}': {e}\"\n                    ) from e\n\n                # Call the method on the Pydantic instance\n                result = getattr(pydantic_instance, name)(*args, **kwargs)\n                # TODO: Handle potential need to update self from result? Unlikely for most methods.\n                return result\n\n            return wrapped_method\n        else:\n            # For non-method attributes (like class vars), return directly\n            # This might need refinement depending on desired behavior for class vs instance attrs\n            return attr\n\n    # If attribute doesn't exist on Pydantic model, raise standard AttributeError\n    raise AttributeError(\n        f\"'{self.__class__.__name__}' object has no attribute '{name}', \"\n        f\"and Pydantic model '{pydantic_cls.__name__}' has no attribute '{name}'.\"\n    )\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.Pydantic2DjangoBaseClass.__new__","title":"<code>__new__(*args, **kwargs)</code>","text":"<p>Override new to ensure proper type checking. Needed because Django's model metaclass doesn't preserve Generic type parameters well.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>def __new__(cls, *args, **kwargs):\n    \"\"\"\n    Override __new__ to ensure proper type checking.\n    Needed because Django's model metaclass doesn't preserve Generic type parameters well.\n    \"\"\"\n    # The check itself might be complex. This placeholder ensures __new__ is considered.\n    # Proper generic handling might require metaclass adjustments beyond this scope.\n    return super().__new__(cls)\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.Pydantic2DjangoBaseClass.from_pydantic","title":"<code>from_pydantic(pydantic_obj, name=None)</code>  <code>classmethod</code>","text":"<p>Create a Django model instance from a Pydantic object, mapping fields.</p> <p>Parameters:</p> Name Type Description Default <code>pydantic_obj</code> <code>PydanticT</code> <p>The Pydantic object to convert.</p> required <code>name</code> <code>str | None</code> <p>Optional name for the Django model instance.</p> <code>None</code> <p>Returns:</p> Type Description <code>Pydantic2DjangoBaseClass[PydanticT]</code> <p>A new instance of this Django model subclass.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the object is not a Pydantic model or not of the expected type.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>@classmethod\ndef from_pydantic(cls, pydantic_obj: PydanticT, name: str | None = None) -&gt; \"Pydantic2DjangoBaseClass[PydanticT]\":\n    \"\"\"\n    Create a Django model instance from a Pydantic object, mapping fields.\n\n    Args:\n        pydantic_obj: The Pydantic object to convert.\n        name: Optional name for the Django model instance.\n\n    Returns:\n        A new instance of this Django model subclass.\n\n    Raises:\n        TypeError: If the object is not a Pydantic model or not of the expected type.\n    \"\"\"\n    # Get class info and check type\n    (\n        pydantic_class,\n        class_name,\n        module_name,\n        fully_qualified_name,\n    ) = cls._get_class_info(pydantic_obj)\n    cls._check_expected_type(pydantic_obj, class_name)  # Verifies it's a Pydantic model\n\n    # Derive name\n    derived_name = cls._derive_name(pydantic_obj, name, class_name)\n\n    # Create instance with basic fields\n    instance = cls(\n        name=derived_name,\n        class_path=fully_qualified_name,  # Use renamed field\n    )\n\n    # Update mapped fields\n    instance.update_fields_from_pydantic(pydantic_obj)\n\n    return instance\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.Pydantic2DjangoBaseClass.save_as_pydantic","title":"<code>save_as_pydantic()</code>","text":"<p>Save the Django model and return the corresponding Pydantic object.</p> <p>Returns:</p> Type Description <code>PydanticT</code> <p>The corresponding Pydantic object.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>def save_as_pydantic(self) -&gt; PydanticT:\n    \"\"\"\n    Save the Django model and return the corresponding Pydantic object.\n\n    Returns:\n        The corresponding Pydantic object.\n    \"\"\"\n    self.save()\n    return self.to_pydantic(context=None)\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.Pydantic2DjangoBaseClass.to_pydantic","title":"<code>to_pydantic(context=None)</code>","text":"<p>Convert this Django model instance back to a Pydantic object.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>ModelContext | None</code> <p>Optional ModelContext instance containing values for non-serializable fields.</p> <code>None</code> <p>Returns:</p> Type Description <code>PydanticT</code> <p>The corresponding Pydantic object.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If context is required but not provided, or if class load/instantiation fails.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>def to_pydantic(self, context: ModelContext | None = None) -&gt; PydanticT:\n    \"\"\"\n    Convert this Django model instance back to a Pydantic object.\n\n    Args:\n        context: Optional ModelContext instance containing values for non-serializable fields.\n\n    Returns:\n        The corresponding Pydantic object.\n\n    Raises:\n        ValueError: If context is required but not provided, or if class load/instantiation fails.\n    \"\"\"\n    pydantic_class = self._get_class()  # Use common method\n    if not issubclass(pydantic_class, BaseModel):\n        raise ValueError(f\"Stored class path '{self.class_path}' does not point to a Pydantic BaseModel.\")\n\n    # Get data from Django fields corresponding to Pydantic fields\n    data = self._get_data_for_pydantic(pydantic_class)\n\n    # Handle context if required and provided\n    required_context_keys = self._get_required_context_fields()  # Check if context is needed\n    if required_context_keys:\n        if not context:\n            raise ValueError(\n                f\"Conversion to Pydantic model '{pydantic_class.__name__}' requires context \"\n                f\"for fields: {', '.join(required_context_keys)}. Please provide a ModelContext instance.\"\n            )\n        # Validate and merge context data\n        context_dict = context.to_conversion_dict()\n        context.validate_context(context_dict)  # Validate required keys are present\n        data.update(context_dict)  # Merge context, potentially overwriting DB values if keys overlap\n\n    # Reconstruct the Pydantic object\n    try:\n        # TODO: Add potential deserialization logic here if needed before validation\n        instance = pydantic_class.model_validate(data)\n        # Cast to the generic type variable\n        return cast(PydanticT, instance)\n    except TypeError:\n        raise  # Re-raise TypeError as it's a specific, meaningful exception here\n    except Exception as e:\n        # Catch any other unexpected error during Pydantic object creation\n        logger.error(\n            f\"An unexpected error occurred creating Pydantic model for '{pydantic_class.__name__}': {e}\",\n            exc_info=True,\n        )\n        raise ValueError(f\"Unexpected error creating Pydantic model for '{pydantic_class.__name__}'\") from e\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.Pydantic2DjangoBaseClass.update_fields_from_pydantic","title":"<code>update_fields_from_pydantic(pydantic_obj)</code>","text":"<p>Update this Django model's fields from a Pydantic object's fields.</p> <p>Parameters:</p> Name Type Description Default <code>pydantic_obj</code> <code>PydanticT</code> <p>The Pydantic object containing source values.</p> required Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>def update_fields_from_pydantic(self, pydantic_obj: PydanticT) -&gt; None:\n    \"\"\"\n    Update this Django model's fields from a Pydantic object's fields.\n\n    Args:\n        pydantic_obj: The Pydantic object containing source values.\n    \"\"\"\n    if (\n        not isinstance(pydantic_obj, BaseModel)\n        or pydantic_obj.__class__.__module__ != self._get_class().__module__\n        or pydantic_obj.__class__.__name__ != self._get_class().__name__\n    ):\n        # Check type consistency before proceeding\n        raise TypeError(\n            f\"Provided object type {type(pydantic_obj)} does not match expected type {self.class_path} for update.\"\n        )\n\n    # Get data from the Pydantic object\n    try:\n        pydantic_data = pydantic_obj.model_dump()\n    except AttributeError as err:\n        raise TypeError(\n            \"Failed to dump Pydantic model for update. Ensure you are using Pydantic v2+ with model_dump().\"\n        ) from err\n\n    # Get Django model fields excluding common/meta ones\n    model_field_names = {\n        field.name\n        for field in self._meta.fields\n        if field.name not in (\"id\", \"name\", \"class_path\", \"created_at\", \"updated_at\")\n    }\n\n    # Update each Django field if it matches a field in the Pydantic data\n    for field_name in model_field_names:\n        if field_name in pydantic_data:\n            value = pydantic_data[field_name]\n            # Apply serialization (important for complex types)\n            serialized_value = serialize_value(value)\n            setattr(self, field_name, serialized_value)\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.Pydantic2DjangoBaseClass.update_from_pydantic","title":"<code>update_from_pydantic(pydantic_obj)</code>","text":"<p>Update this Django model with new data from a Pydantic object and save.</p> <p>Parameters:</p> Name Type Description Default <code>pydantic_obj</code> <code>PydanticT</code> <p>The Pydantic object with updated data.</p> required Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>def update_from_pydantic(self, pydantic_obj: PydanticT) -&gt; None:\n    \"\"\"\n    Update this Django model with new data from a Pydantic object and save.\n\n    Args:\n        pydantic_obj: The Pydantic object with updated data.\n    \"\"\"\n    # Verify the object type matches first (includes check if it's a BaseModel)\n    fully_qualified_name = self._verify_object_type_match(pydantic_obj)\n\n    # Update the class_path if somehow inconsistent\n    if self.class_path != fully_qualified_name:\n        self.class_path = fully_qualified_name\n\n    self.update_fields_from_pydantic(pydantic_obj)\n    self.save()\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.Pydantic2DjangoStorePydanticObject","title":"<code>Pydantic2DjangoStorePydanticObject</code>","text":"<p>               Bases: <code>Pydantic2DjangoBase</code></p> <p>Class to store a Pydantic object in the database as JSON.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>class Pydantic2DjangoStorePydanticObject(Pydantic2DjangoBase):\n    \"\"\"\n    Class to store a Pydantic object in the database as JSON.\n    \"\"\"\n\n    data = models.JSONField(help_text=\"JSON representation of the Pydantic object.\")\n\n    class Meta(Pydantic2DjangoBase.Meta):\n        abstract = True\n        verbose_name = \"Stored Pydantic Object\"\n        verbose_name_plural = \"Stored Pydantic Objects\"\n\n    @classmethod\n    def from_pydantic(cls, pydantic_obj: Any, name: str | None = None) -&gt; \"Pydantic2DjangoStorePydanticObject\":\n        \"\"\"\n        Create a Django model instance from a Pydantic object.\n\n        Args:\n            pydantic_obj: The Pydantic object to store\n            name: Optional name for the object (defaults to class name if available)\n\n        Returns:\n            A new instance of the appropriate Pydantic2DjangoBaseClass subclass\n\n        Raises:\n            TypeError: If the Pydantic object is not of the correct type for this Django model\n        \"\"\"\n        try:\n            obj_class, class_name, module_name, fqn = cls._get_class_info(pydantic_obj)\n            cls._check_expected_type(pydantic_obj, class_name)\n\n            data_dict = pydantic_obj.model_dump(mode=\"json\")\n            derived_name = cls._derive_name(pydantic_obj, name, class_name)\n\n            instance = cls(name=derived_name, class_path=fqn, data=data_dict)\n            instance.sync_db_fields_from_data(save=False)\n            return instance\n\n        except TypeError:\n            raise  # Re-raise TypeError as it's a specific, meaningful exception here\n        except Exception as e:\n            # Catch any other unexpected error during Pydantic object creation\n            logger.error(f\"An unexpected error occurred creating Django model for '{class_name}': {e}\", exc_info=True)\n            raise ValueError(f\"Unexpected error creating Django model for '{class_name}'\") from e\n\n    def to_pydantic(self, context: dict[str, Any] | None = None) -&gt; Any:\n        \"\"\"\n        Convert the stored data back to a Pydantic object.\n\n        Args:\n            context: Optional dictionary containing context values for non-serializable fields\n\n        Returns:\n            The reconstructed Pydantic object\n\n        Raises:\n            ValueError: If required context is missing for non-serializable fields or class load/instantiation fails.\n        \"\"\"\n        pydantic_class = self._get_class()  # Use common method\n        if not issubclass(pydantic_class, BaseModel):\n            raise ValueError(f\"Stored class path '{self.class_path}' does not point to a Pydantic BaseModel.\")\n\n        # TODO: Integrate ModelContext logic properly if needed for this storage type\n        # This likely belongs more in the field-mapping approach.\n        # model_context = None\n        # if not model_context:\n        #     raise NotImplementedError(\"You should fix this\")\n        # # If we have context fields, validate the provided context\n        # if model_context and model_context.required_context_keys:\n        #     if not context:\n        #         raise ValueError(\n        #             f\"This model has non-serializable fields that require context: \"\n        #             f\"{', '.join(model_context.required_context_keys)}. \"\n        #             \"Please provide the context dictionary when calling to_pydantic().\"\n        #         )\n        #     model_context.validate_context(context)\n\n        # Use stored data directly\n        stored_data = self.data\n\n        # If context is provided, overlay it (simple merge, context takes precedence)\n        final_data = stored_data.copy()\n        if context:\n            final_data.update(context)\n\n        # Reconstruct the object using model_validate for Pydantic v2+\n        try:\n            # TODO: Add deserialization logic if serialize_value performs complex transformations\n            instance = pydantic_class.model_validate(final_data)\n        except TypeError:\n            raise  # Re-raise TypeError as it's a specific, meaningful exception here\n        except Exception as e:\n            # Catch any other unexpected error during Pydantic object creation\n            logger.error(\n                f\"An unexpected error occurred creating Pydantic model for '{pydantic_class.__name__}': {e}\",\n                exc_info=True,\n            )\n            raise ValueError(f\"Unexpected error creating Pydantic model for '{pydantic_class.__name__}'\") from e\n\n        return instance\n\n    def _get_data_with_db_overrides(self, pydantic_class: type[BaseModel]) -&gt; dict:\n        \"\"\"\n        Get model data with any database field overrides applied.\n        (Primarily relevant for field-mapping, less so here, but kept for potential sync logic).\n        \"\"\"\n        # This method seems less relevant when the primary source is `self.data`.\n        # Keeping it simple: return the stored data.\n        # If sync logic were more complex, this might need adjustment.\n        return self.data if isinstance(self.data, dict) else {}\n\n    def update_from_pydantic(self, pydantic_obj: Any) -&gt; None:\n        \"\"\"\n        Update this object with new data from a Pydantic object.\n\n        Args:\n            pydantic_obj: The Pydantic object with updated data\n        \"\"\"\n        # Verify the object type matches\n        fully_qualified_name = self._verify_object_type_match(pydantic_obj)\n        # Check if it's a Pydantic model (redundant if verify works, but safe)\n        if not isinstance(pydantic_obj, BaseModel):\n            raise TypeError(\"Provided object for update is not a Pydantic BaseModel.\")\n\n        # Update the class_path if somehow inconsistent\n        if self.class_path != fully_qualified_name:\n            self.class_path = fully_qualified_name\n\n        # Use model_dump for Pydantic v2+\n        try:\n            data = pydantic_obj.model_dump()\n        except AttributeError as err:\n            raise TypeError(\n                \"Failed to dump Pydantic model for update. Ensure you are using Pydantic v2+ with model_dump().\"\n            ) from err\n\n        self.data = {key: serialize_value(value) for key, value in data.items()}\n        # Optionally sync fields\n        # self.sync_db_fields_from_data(save=False)\n        self.save()\n\n    def sync_db_fields_from_data(self, save: bool = True) -&gt; None:\n        \"\"\"\n        Synchronize database fields from the JSON 'data' field.\n\n        Updates Django model fields (excluding common/meta fields) with values\n        from the JSON data if the field names match.\n\n        Args:\n            save: If True (default), saves the instance after updating fields.\n        \"\"\"\n        if not isinstance(self.data, dict):\n            return\n\n        # Get model fields, excluding common ones and the data field itself\n        model_field_names = {\n            field.name\n            for field in self._meta.fields\n            if field.name not in (\"id\", \"name\", \"class_path\", \"data\", \"created_at\", \"updated_at\")\n        }\n\n        updated_fields = []\n        for field_name in model_field_names:\n            if field_name in self.data:\n                current_value = getattr(self, field_name)\n                new_value = self.data[field_name]\n                # Basic check to avoid unnecessary updates\n                if current_value != new_value:\n                    setattr(self, field_name, new_value)\n                    updated_fields.append(field_name)\n\n        if updated_fields and save:\n            self.save(update_fields=updated_fields)\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.Pydantic2DjangoStorePydanticObject.from_pydantic","title":"<code>from_pydantic(pydantic_obj, name=None)</code>  <code>classmethod</code>","text":"<p>Create a Django model instance from a Pydantic object.</p> <p>Parameters:</p> Name Type Description Default <code>pydantic_obj</code> <code>Any</code> <p>The Pydantic object to store</p> required <code>name</code> <code>str | None</code> <p>Optional name for the object (defaults to class name if available)</p> <code>None</code> <p>Returns:</p> Type Description <code>Pydantic2DjangoStorePydanticObject</code> <p>A new instance of the appropriate Pydantic2DjangoBaseClass subclass</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the Pydantic object is not of the correct type for this Django model</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>@classmethod\ndef from_pydantic(cls, pydantic_obj: Any, name: str | None = None) -&gt; \"Pydantic2DjangoStorePydanticObject\":\n    \"\"\"\n    Create a Django model instance from a Pydantic object.\n\n    Args:\n        pydantic_obj: The Pydantic object to store\n        name: Optional name for the object (defaults to class name if available)\n\n    Returns:\n        A new instance of the appropriate Pydantic2DjangoBaseClass subclass\n\n    Raises:\n        TypeError: If the Pydantic object is not of the correct type for this Django model\n    \"\"\"\n    try:\n        obj_class, class_name, module_name, fqn = cls._get_class_info(pydantic_obj)\n        cls._check_expected_type(pydantic_obj, class_name)\n\n        data_dict = pydantic_obj.model_dump(mode=\"json\")\n        derived_name = cls._derive_name(pydantic_obj, name, class_name)\n\n        instance = cls(name=derived_name, class_path=fqn, data=data_dict)\n        instance.sync_db_fields_from_data(save=False)\n        return instance\n\n    except TypeError:\n        raise  # Re-raise TypeError as it's a specific, meaningful exception here\n    except Exception as e:\n        # Catch any other unexpected error during Pydantic object creation\n        logger.error(f\"An unexpected error occurred creating Django model for '{class_name}': {e}\", exc_info=True)\n        raise ValueError(f\"Unexpected error creating Django model for '{class_name}'\") from e\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.Pydantic2DjangoStorePydanticObject.sync_db_fields_from_data","title":"<code>sync_db_fields_from_data(save=True)</code>","text":"<p>Synchronize database fields from the JSON 'data' field.</p> <p>Updates Django model fields (excluding common/meta fields) with values from the JSON data if the field names match.</p> <p>Parameters:</p> Name Type Description Default <code>save</code> <code>bool</code> <p>If True (default), saves the instance after updating fields.</p> <code>True</code> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>def sync_db_fields_from_data(self, save: bool = True) -&gt; None:\n    \"\"\"\n    Synchronize database fields from the JSON 'data' field.\n\n    Updates Django model fields (excluding common/meta fields) with values\n    from the JSON data if the field names match.\n\n    Args:\n        save: If True (default), saves the instance after updating fields.\n    \"\"\"\n    if not isinstance(self.data, dict):\n        return\n\n    # Get model fields, excluding common ones and the data field itself\n    model_field_names = {\n        field.name\n        for field in self._meta.fields\n        if field.name not in (\"id\", \"name\", \"class_path\", \"data\", \"created_at\", \"updated_at\")\n    }\n\n    updated_fields = []\n    for field_name in model_field_names:\n        if field_name in self.data:\n            current_value = getattr(self, field_name)\n            new_value = self.data[field_name]\n            # Basic check to avoid unnecessary updates\n            if current_value != new_value:\n                setattr(self, field_name, new_value)\n                updated_fields.append(field_name)\n\n    if updated_fields and save:\n        self.save(update_fields=updated_fields)\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.Pydantic2DjangoStorePydanticObject.to_pydantic","title":"<code>to_pydantic(context=None)</code>","text":"<p>Convert the stored data back to a Pydantic object.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>dict[str, Any] | None</code> <p>Optional dictionary containing context values for non-serializable fields</p> <code>None</code> <p>Returns:</p> Type Description <code>Any</code> <p>The reconstructed Pydantic object</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required context is missing for non-serializable fields or class load/instantiation fails.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>def to_pydantic(self, context: dict[str, Any] | None = None) -&gt; Any:\n    \"\"\"\n    Convert the stored data back to a Pydantic object.\n\n    Args:\n        context: Optional dictionary containing context values for non-serializable fields\n\n    Returns:\n        The reconstructed Pydantic object\n\n    Raises:\n        ValueError: If required context is missing for non-serializable fields or class load/instantiation fails.\n    \"\"\"\n    pydantic_class = self._get_class()  # Use common method\n    if not issubclass(pydantic_class, BaseModel):\n        raise ValueError(f\"Stored class path '{self.class_path}' does not point to a Pydantic BaseModel.\")\n\n    # TODO: Integrate ModelContext logic properly if needed for this storage type\n    # This likely belongs more in the field-mapping approach.\n    # model_context = None\n    # if not model_context:\n    #     raise NotImplementedError(\"You should fix this\")\n    # # If we have context fields, validate the provided context\n    # if model_context and model_context.required_context_keys:\n    #     if not context:\n    #         raise ValueError(\n    #             f\"This model has non-serializable fields that require context: \"\n    #             f\"{', '.join(model_context.required_context_keys)}. \"\n    #             \"Please provide the context dictionary when calling to_pydantic().\"\n    #         )\n    #     model_context.validate_context(context)\n\n    # Use stored data directly\n    stored_data = self.data\n\n    # If context is provided, overlay it (simple merge, context takes precedence)\n    final_data = stored_data.copy()\n    if context:\n        final_data.update(context)\n\n    # Reconstruct the object using model_validate for Pydantic v2+\n    try:\n        # TODO: Add deserialization logic if serialize_value performs complex transformations\n        instance = pydantic_class.model_validate(final_data)\n    except TypeError:\n        raise  # Re-raise TypeError as it's a specific, meaningful exception here\n    except Exception as e:\n        # Catch any other unexpected error during Pydantic object creation\n        logger.error(\n            f\"An unexpected error occurred creating Pydantic model for '{pydantic_class.__name__}': {e}\",\n            exc_info=True,\n        )\n        raise ValueError(f\"Unexpected error creating Pydantic model for '{pydantic_class.__name__}'\") from e\n\n    return instance\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.Pydantic2DjangoStorePydanticObject.update_from_pydantic","title":"<code>update_from_pydantic(pydantic_obj)</code>","text":"<p>Update this object with new data from a Pydantic object.</p> <p>Parameters:</p> Name Type Description Default <code>pydantic_obj</code> <code>Any</code> <p>The Pydantic object with updated data</p> required Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>def update_from_pydantic(self, pydantic_obj: Any) -&gt; None:\n    \"\"\"\n    Update this object with new data from a Pydantic object.\n\n    Args:\n        pydantic_obj: The Pydantic object with updated data\n    \"\"\"\n    # Verify the object type matches\n    fully_qualified_name = self._verify_object_type_match(pydantic_obj)\n    # Check if it's a Pydantic model (redundant if verify works, but safe)\n    if not isinstance(pydantic_obj, BaseModel):\n        raise TypeError(\"Provided object for update is not a Pydantic BaseModel.\")\n\n    # Update the class_path if somehow inconsistent\n    if self.class_path != fully_qualified_name:\n        self.class_path = fully_qualified_name\n\n    # Use model_dump for Pydantic v2+\n    try:\n        data = pydantic_obj.model_dump()\n    except AttributeError as err:\n        raise TypeError(\n            \"Failed to dump Pydantic model for update. Ensure you are using Pydantic v2+ with model_dump().\"\n        ) from err\n\n    self.data = {key: serialize_value(value) for key, value in data.items()}\n    # Optionally sync fields\n    # self.sync_db_fields_from_data(save=False)\n    self.save()\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.TypedClass2DjangoBase","title":"<code>TypedClass2DjangoBase</code>","text":"<p>               Bases: <code>CommonBaseModel</code></p> <p>Abstract base class for storing generic Python class objects in the database. Inherits common fields and methods from CommonBaseModel.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>class TypedClass2DjangoBase(CommonBaseModel):\n    \"\"\"\n    Abstract base class for storing generic Python class objects in the database.\n    Inherits common fields and methods from CommonBaseModel.\n    \"\"\"\n\n    _expected_typedclass_type: ClassVar[type | None] = None  # For type checking specific generic classes\n\n    class Meta(CommonBaseModel.Meta):\n        abstract = True\n\n    @classmethod\n    def _check_expected_type(cls, typed_obj: Any, class_name: str) -&gt; None:\n        \"\"\"\n        Check if the object matches the expected type (if set).\n        For generic classes, this might be a simple type check or more involved\n        if we introduce markers or specific base classes for discoverable typed classes.\n        \"\"\"\n        # Basic check: is it an instance of the class itself?\n        # More advanced checks could be added, e.g., if it inherits from a specific\n        # user-defined base for conversion, or has a special attribute.\n\n        expected_type = getattr(cls, \"_expected_typedclass_type\", None)\n        if expected_type is not None:\n            if not isinstance(typed_obj, expected_type):\n                expected_name = getattr(expected_type, \"__name__\", str(expected_type))\n                raise TypeError(f\"Expected object of type {expected_name}, but got {class_name}\")\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.TypedClass2DjangoBaseClass","title":"<code>TypedClass2DjangoBaseClass</code>","text":"<p>               Bases: <code>TypedClass2DjangoBase</code>, <code>Generic[TypedClassT]</code></p> <p>Base class for mapping generic Python class fields to Django model fields.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>class TypedClass2DjangoBaseClass(TypedClass2DjangoBase, Generic[TypedClassT]):\n    \"\"\"\n    Base class for mapping generic Python class fields to Django model fields.\n    \"\"\"\n\n    class Meta(TypedClass2DjangoBase.Meta):\n        abstract = True\n        verbose_name = \"Mapped Typed Class\"\n        verbose_name_plural = \"Mapped Typed Classes\"\n\n    @classmethod\n    def from_typedclass(\n        cls, typed_obj: TypedClassT, name: str | None = None\n    ) -&gt; \"TypedClass2DjangoBaseClass[TypedClassT]\":\n        \"\"\"\n        Create a Django model instance from a generic class object, mapping fields.\n        \"\"\"\n        obj_class, class_name, module_name, fqn = cls._get_class_info(typed_obj)\n        cls._check_expected_type(typed_obj, class_name)\n        derived_name = cls._derive_name(typed_obj, name, class_name)\n\n        instance = cls(\n            name=derived_name,\n            class_path=fqn,\n        )\n        instance.update_fields_from_typedclass(typed_obj)\n        return instance\n\n    def update_fields_from_typedclass(self, typed_obj: TypedClassT) -&gt; None:\n        \"\"\"\n        Update this Django model's fields from a generic class object's fields.\n        \"\"\"\n        if (\n            typed_obj.__class__.__module__ != self._get_class().__module__\n            or typed_obj.__class__.__name__ != self._get_class().__name__\n        ):\n            raise TypeError(\n                f\"Provided object type {type(typed_obj)} does not match expected type {self.class_path} for update.\"\n            )\n\n        # Extract data. For mapped fields, we prefer __init__ params or direct attrs.\n        # Using __dict__ might be too broad here if not all dict items are mapped fields.\n        # Let's assume attributes corresponding to Django fields are directly accessible.\n\n        model_field_names = {\n            field.name\n            for field in self._meta.fields\n            if field.name not in (\"id\", \"name\", \"class_path\", \"created_at\", \"updated_at\")\n        }\n\n        for field_name in model_field_names:\n            if hasattr(typed_obj, field_name):\n                value = getattr(typed_obj, field_name)\n                setattr(self, field_name, serialize_value(value))\n            # Else: Field on Django model, not on typed_obj. Leave as is or handle.\n\n    def to_typedclass(self) -&gt; TypedClassT:\n        \"\"\"\n        Convert this Django model instance back to a generic class object.\n        \"\"\"\n        target_class = self._get_class()\n        data_for_typedclass = self._get_data_for_typedclass(target_class)\n\n        try:\n            # This assumes target_class can be instantiated with these parameters.\n            # Deserialization from serialize_value needs to be considered for complex types.\n            # TODO: Implement robust deserialization\n            deserialized_data = dict(data_for_typedclass.items())  # Placeholder\n\n            init_sig = inspect.signature(target_class.__init__)\n            valid_params = {k: v for k, v in deserialized_data.items() if k in init_sig.parameters}\n\n            instance = target_class(**valid_params)\n\n            # For attributes not in __init__ but set on Django model, try to setattr\n            for key, value in deserialized_data.items():\n                if key not in valid_params and hasattr(instance, key) and not key.startswith(\"_\"):\n                    try:\n                        setattr(instance, key, value)  # value is already deserialized_data[key]\n                    except AttributeError:\n                        logger.debug(\n                            f\"Could not setattr {key} on {target_class.__name__} during to_typedclass reconstruction.\"\n                        )\n\n            return cast(TypedClassT, instance)\n        except TypeError as e:\n            raise ValueError(\n                f\"Failed to instantiate typed class '{target_class.__name__}' from Django fields. Error: {e}\"\n            ) from e\n        except Exception as e:\n            logger.error(f\"An unexpected error occurred during typed class reconstruction: {e}\", exc_info=True)\n            raise ValueError(f\"An unexpected error occurred during typed class reconstruction: {e}\") from e\n\n    def _get_data_for_typedclass(self, target_class: type) -&gt; dict[str, Any]:\n        \"\"\"Get data from Django fields that correspond to the target class's likely attributes.\"\"\"\n        data = {}\n        # Heuristic: get attributes from __init__ signature and class annotations\n        # This is a simplified version. A robust solution might need to align with TypedClassFieldFactory's discovery.\n\n        # Potential attributes: from __init__\n        potential_attrs = set()\n        try:\n            init_sig = inspect.signature(target_class.__init__)\n            potential_attrs.update(p for p in init_sig.parameters if p != \"self\")\n        except (TypeError, ValueError):\n            pass\n\n        # Potential attributes: from class annotations (less reliable for instance data if not in __init__)\n        # try:\n        #     annotations = inspect.get_annotations(target_class)\n        #     potential_attrs.update(annotations.keys())\n        # except Exception:\n        #     pass\n\n        for field in self._meta.fields:\n            if field.name in potential_attrs or hasattr(target_class, field.name):  # Check if it's a likely attribute\n                # TODO: Add deserialization logic based on target_class's type hint for field.name\n                data[field.name] = getattr(self, field.name)\n        return data\n\n    def update_from_typedclass(self, typed_obj: TypedClassT) -&gt; None:\n        \"\"\"\n        Update this Django model with new data from a generic class object and save.\n        \"\"\"\n        fqn = self._verify_object_type_match(typed_obj)\n        if self.class_path != fqn:\n            self.class_path = fqn\n\n        self.update_fields_from_typedclass(typed_obj)\n        self.save()\n\n    def save_as_typedclass(self) -&gt; TypedClassT:\n        \"\"\"\n        Save the Django model and return the corresponding generic class object.\n        \"\"\"\n        self.save()\n        return self.to_typedclass()\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.TypedClass2DjangoBaseClass.from_typedclass","title":"<code>from_typedclass(typed_obj, name=None)</code>  <code>classmethod</code>","text":"<p>Create a Django model instance from a generic class object, mapping fields.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>@classmethod\ndef from_typedclass(\n    cls, typed_obj: TypedClassT, name: str | None = None\n) -&gt; \"TypedClass2DjangoBaseClass[TypedClassT]\":\n    \"\"\"\n    Create a Django model instance from a generic class object, mapping fields.\n    \"\"\"\n    obj_class, class_name, module_name, fqn = cls._get_class_info(typed_obj)\n    cls._check_expected_type(typed_obj, class_name)\n    derived_name = cls._derive_name(typed_obj, name, class_name)\n\n    instance = cls(\n        name=derived_name,\n        class_path=fqn,\n    )\n    instance.update_fields_from_typedclass(typed_obj)\n    return instance\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.TypedClass2DjangoBaseClass.save_as_typedclass","title":"<code>save_as_typedclass()</code>","text":"<p>Save the Django model and return the corresponding generic class object.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>def save_as_typedclass(self) -&gt; TypedClassT:\n    \"\"\"\n    Save the Django model and return the corresponding generic class object.\n    \"\"\"\n    self.save()\n    return self.to_typedclass()\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.TypedClass2DjangoBaseClass.to_typedclass","title":"<code>to_typedclass()</code>","text":"<p>Convert this Django model instance back to a generic class object.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>def to_typedclass(self) -&gt; TypedClassT:\n    \"\"\"\n    Convert this Django model instance back to a generic class object.\n    \"\"\"\n    target_class = self._get_class()\n    data_for_typedclass = self._get_data_for_typedclass(target_class)\n\n    try:\n        # This assumes target_class can be instantiated with these parameters.\n        # Deserialization from serialize_value needs to be considered for complex types.\n        # TODO: Implement robust deserialization\n        deserialized_data = dict(data_for_typedclass.items())  # Placeholder\n\n        init_sig = inspect.signature(target_class.__init__)\n        valid_params = {k: v for k, v in deserialized_data.items() if k in init_sig.parameters}\n\n        instance = target_class(**valid_params)\n\n        # For attributes not in __init__ but set on Django model, try to setattr\n        for key, value in deserialized_data.items():\n            if key not in valid_params and hasattr(instance, key) and not key.startswith(\"_\"):\n                try:\n                    setattr(instance, key, value)  # value is already deserialized_data[key]\n                except AttributeError:\n                    logger.debug(\n                        f\"Could not setattr {key} on {target_class.__name__} during to_typedclass reconstruction.\"\n                    )\n\n        return cast(TypedClassT, instance)\n    except TypeError as e:\n        raise ValueError(\n            f\"Failed to instantiate typed class '{target_class.__name__}' from Django fields. Error: {e}\"\n        ) from e\n    except Exception as e:\n        logger.error(f\"An unexpected error occurred during typed class reconstruction: {e}\", exc_info=True)\n        raise ValueError(f\"An unexpected error occurred during typed class reconstruction: {e}\") from e\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.TypedClass2DjangoBaseClass.update_fields_from_typedclass","title":"<code>update_fields_from_typedclass(typed_obj)</code>","text":"<p>Update this Django model's fields from a generic class object's fields.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>def update_fields_from_typedclass(self, typed_obj: TypedClassT) -&gt; None:\n    \"\"\"\n    Update this Django model's fields from a generic class object's fields.\n    \"\"\"\n    if (\n        typed_obj.__class__.__module__ != self._get_class().__module__\n        or typed_obj.__class__.__name__ != self._get_class().__name__\n    ):\n        raise TypeError(\n            f\"Provided object type {type(typed_obj)} does not match expected type {self.class_path} for update.\"\n        )\n\n    # Extract data. For mapped fields, we prefer __init__ params or direct attrs.\n    # Using __dict__ might be too broad here if not all dict items are mapped fields.\n    # Let's assume attributes corresponding to Django fields are directly accessible.\n\n    model_field_names = {\n        field.name\n        for field in self._meta.fields\n        if field.name not in (\"id\", \"name\", \"class_path\", \"created_at\", \"updated_at\")\n    }\n\n    for field_name in model_field_names:\n        if hasattr(typed_obj, field_name):\n            value = getattr(typed_obj, field_name)\n            setattr(self, field_name, serialize_value(value))\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.TypedClass2DjangoBaseClass.update_from_typedclass","title":"<code>update_from_typedclass(typed_obj)</code>","text":"<p>Update this Django model with new data from a generic class object and save.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>def update_from_typedclass(self, typed_obj: TypedClassT) -&gt; None:\n    \"\"\"\n    Update this Django model with new data from a generic class object and save.\n    \"\"\"\n    fqn = self._verify_object_type_match(typed_obj)\n    if self.class_path != fqn:\n        self.class_path = fqn\n\n    self.update_fields_from_typedclass(typed_obj)\n    self.save()\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.TypedClass2DjangoStoreTypedClassObject","title":"<code>TypedClass2DjangoStoreTypedClassObject</code>","text":"<p>               Bases: <code>TypedClass2DjangoBase</code>, <code>Generic[TypedClassT]</code></p> <p>Class to store a generic Python class object in the database as JSON. All data is stored in the 'data' field.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>class TypedClass2DjangoStoreTypedClassObject(TypedClass2DjangoBase, Generic[TypedClassT]):\n    \"\"\"\n    Class to store a generic Python class object in the database as JSON.\n    All data is stored in the 'data' field.\n    \"\"\"\n\n    data = models.JSONField(help_text=\"JSON representation of the typed class object.\")\n\n    class Meta(TypedClass2DjangoBase.Meta):\n        abstract = True\n        verbose_name = \"Stored Typed Class Object\"\n        verbose_name_plural = \"Stored Typed Class Objects\"\n\n    @classmethod\n    def _extract_data_from_typedclass(cls, typed_obj: TypedClassT) -&gt; dict[str, Any]:\n        \"\"\"\n        Helper to extract data from a generic class instance for serialization.\n        This is a key challenge and might need to be configurable.\n        Initial approach: inspect __init__ params and class vars with type hints.\n        More advanced: __dict__, __slots__, or a user-defined method.\n        \"\"\"\n        data_dict = {}\n        # Attempt 1: Inspect __init__ parameters if they correspond to attributes\n        try:\n            sig = inspect.signature(typed_obj.__class__.__init__)\n            for param_name in sig.parameters:\n                if param_name == \"self\":\n                    continue\n                if hasattr(typed_obj, param_name):\n                    data_dict[param_name] = getattr(typed_obj, param_name)\n        except (TypeError, ValueError):  # Non-inspectable __init__\n            pass  # Fallback to __dict__ or other methods\n\n        # Attempt 2: Use __dict__ if available and not already covered (simplistic)\n        if hasattr(typed_obj, \"__dict__\") and not data_dict:  # Prioritize __init__ if it yielded something\n            data_dict.update(typed_obj.__dict__)\n\n        # Attempt 3: Iterate over class annotations if available (for class vars not in __init__ / __dict__)\n        # This part requires careful handling to avoid unintended data or methods.\n        # For now, a basic __dict__ or __init__ based extraction is safer.\n        # Consider a __slots__ check as well if present.\n\n        if not data_dict:\n            logger.warning(\n                f\"Could not automatically extract data from {typed_obj.__class__.__name__}. \"\n                f\"Consider implementing a 'to_dict()' method or using __slots__.\"\n            )\n\n        return {key: serialize_value(value) for key, value in data_dict.items() if not key.startswith(\"_\")}\n\n    @classmethod\n    def from_typedclass(\n        cls, typed_obj: TypedClassT, name: str | None = None\n    ) -&gt; \"TypedClass2DjangoStoreTypedClassObject[TypedClassT]\":\n        \"\"\"\n        Create a Django model instance from a generic class object.\n        \"\"\"\n        obj_class, class_name, module_name, fqn = cls._get_class_info(typed_obj)\n        cls._check_expected_type(typed_obj, class_name)\n\n        data_dict = cls._extract_data_from_typedclass(typed_obj)\n        derived_name = cls._derive_name(typed_obj, name, class_name)\n\n        instance = cls(\n            name=derived_name,\n            class_path=fqn,\n            data=data_dict,\n        )\n        return instance\n\n    def to_typedclass(self) -&gt; TypedClassT:\n        \"\"\"\n        Convert the stored JSON data back to a generic class object.\n        This is highly challenging and relies on the class having a suitable __init__.\n        \"\"\"\n        target_class = self._get_class()\n        stored_data = self.data if isinstance(self.data, dict) else {}\n\n        # Attempt to instantiate using stored_data.\n        # This assumes keys in stored_data match __init__ parameters.\n        # Deserialization of complex nested values from serialize_value needs to be handled.\n        # For now, assume stored data is directly usable or simple types.\n        # TODO: Implement robust deserialization matching serialize_value\n        deserialized_data = {}\n        for key, value in stored_data.items():\n            # Placeholder: a proper deserialization function is needed here.\n            # For example, if serialize_value converted datetimes to ISO strings,\n            # this would need to parse them back.\n            deserialized_data[key] = value\n\n        try:\n            # Filter data to only include parameters expected by __init__\n            init_sig = inspect.signature(target_class.__init__)\n            valid_params = {k: v for k, v in deserialized_data.items() if k in init_sig.parameters}\n\n            instance = target_class(**valid_params)\n            # For attributes not in __init__ but in stored_data (e.g. from __dict__),\n            # attempt to setattr them if they are not private.\n            for key, value in deserialized_data.items():\n                if key not in valid_params and hasattr(instance, key) and not key.startswith(\"_\"):\n                    try:\n                        setattr(instance, key, value)\n                    except AttributeError:  # Read-only property, etc.\n                        logger.debug(\n                            f\"Could not setattr {key} on {target_class.__name__} during to_typedclass reconstruction.\"\n                        )\n            return cast(TypedClassT, instance)\n\n        except TypeError as e:\n            raise ValueError(\n                f\"Failed to instantiate typed class '{target_class.__name__}' from stored data. \"\n                f\"Ensure stored data keys match __init__ parameters or class attributes. Error: {e}\"\n            ) from e\n        except Exception as e:\n            logger.error(f\"An unexpected error occurred during typed class reconstruction: {e}\", exc_info=True)\n            raise ValueError(f\"An unexpected error occurred during typed class reconstruction: {e}\") from e\n\n    def update_from_typedclass(self, typed_obj: TypedClassT) -&gt; None:\n        \"\"\"\n        Update this Django model instance with new data from a generic class object.\n        \"\"\"\n        fqn = self._verify_object_type_match(typed_obj)\n        if self.class_path != fqn:\n            self.class_path = fqn\n\n        self.data = self._extract_data_from_typedclass(typed_obj)\n        self.save()\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.TypedClass2DjangoStoreTypedClassObject.from_typedclass","title":"<code>from_typedclass(typed_obj, name=None)</code>  <code>classmethod</code>","text":"<p>Create a Django model instance from a generic class object.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>@classmethod\ndef from_typedclass(\n    cls, typed_obj: TypedClassT, name: str | None = None\n) -&gt; \"TypedClass2DjangoStoreTypedClassObject[TypedClassT]\":\n    \"\"\"\n    Create a Django model instance from a generic class object.\n    \"\"\"\n    obj_class, class_name, module_name, fqn = cls._get_class_info(typed_obj)\n    cls._check_expected_type(typed_obj, class_name)\n\n    data_dict = cls._extract_data_from_typedclass(typed_obj)\n    derived_name = cls._derive_name(typed_obj, name, class_name)\n\n    instance = cls(\n        name=derived_name,\n        class_path=fqn,\n        data=data_dict,\n    )\n    return instance\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.TypedClass2DjangoStoreTypedClassObject.to_typedclass","title":"<code>to_typedclass()</code>","text":"<p>Convert the stored JSON data back to a generic class object. This is highly challenging and relies on the class having a suitable init.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>def to_typedclass(self) -&gt; TypedClassT:\n    \"\"\"\n    Convert the stored JSON data back to a generic class object.\n    This is highly challenging and relies on the class having a suitable __init__.\n    \"\"\"\n    target_class = self._get_class()\n    stored_data = self.data if isinstance(self.data, dict) else {}\n\n    # Attempt to instantiate using stored_data.\n    # This assumes keys in stored_data match __init__ parameters.\n    # Deserialization of complex nested values from serialize_value needs to be handled.\n    # For now, assume stored data is directly usable or simple types.\n    # TODO: Implement robust deserialization matching serialize_value\n    deserialized_data = {}\n    for key, value in stored_data.items():\n        # Placeholder: a proper deserialization function is needed here.\n        # For example, if serialize_value converted datetimes to ISO strings,\n        # this would need to parse them back.\n        deserialized_data[key] = value\n\n    try:\n        # Filter data to only include parameters expected by __init__\n        init_sig = inspect.signature(target_class.__init__)\n        valid_params = {k: v for k, v in deserialized_data.items() if k in init_sig.parameters}\n\n        instance = target_class(**valid_params)\n        # For attributes not in __init__ but in stored_data (e.g. from __dict__),\n        # attempt to setattr them if they are not private.\n        for key, value in deserialized_data.items():\n            if key not in valid_params and hasattr(instance, key) and not key.startswith(\"_\"):\n                try:\n                    setattr(instance, key, value)\n                except AttributeError:  # Read-only property, etc.\n                    logger.debug(\n                        f\"Could not setattr {key} on {target_class.__name__} during to_typedclass reconstruction.\"\n                    )\n        return cast(TypedClassT, instance)\n\n    except TypeError as e:\n        raise ValueError(\n            f\"Failed to instantiate typed class '{target_class.__name__}' from stored data. \"\n            f\"Ensure stored data keys match __init__ parameters or class attributes. Error: {e}\"\n        ) from e\n    except Exception as e:\n        logger.error(f\"An unexpected error occurred during typed class reconstruction: {e}\", exc_info=True)\n        raise ValueError(f\"An unexpected error occurred during typed class reconstruction: {e}\") from e\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.TypedClass2DjangoStoreTypedClassObject.update_from_typedclass","title":"<code>update_from_typedclass(typed_obj)</code>","text":"<p>Update this Django model instance with new data from a generic class object.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>def update_from_typedclass(self, typed_obj: TypedClassT) -&gt; None:\n    \"\"\"\n    Update this Django model instance with new data from a generic class object.\n    \"\"\"\n    fqn = self._verify_object_type_match(typed_obj)\n    if self.class_path != fqn:\n        self.class_path = fqn\n\n    self.data = self._extract_data_from_typedclass(typed_obj)\n    self.save()\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.Xml2DjangoBaseClass","title":"<code>Xml2DjangoBaseClass</code>","text":"<p>               Bases: <code>Model</code></p> <p>Base class for Django models generated from XML Schema definitions. Provides XML-specific conversion methods and metadata handling.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>class Xml2DjangoBaseClass(models.Model):\n    \"\"\"\n    Base class for Django models generated from XML Schema definitions.\n    Provides XML-specific conversion methods and metadata handling.\n    \"\"\"\n\n    # Schema linking fields\n    conversion_session = models.UUIDField(null=True, blank=True, help_text=\"Links to the conversion session\")\n    schema_source = models.CharField(max_length=255, null=True, blank=True, help_text=\"Source XSD file\")\n\n    # XML Schema metadata (can be overridden in generated models)\n    _xml_namespace: ClassVar[str | None] = None\n    _xml_schema_location: ClassVar[str | None] = None\n    _xml_type_name: ClassVar[str | None] = None\n\n    class Meta:\n        abstract = True\n\n    @classmethod\n    def from_xml_dict(cls, xml_data: dict[str, Any], **kwargs) -&gt; \"Xml2DjangoBaseClass\":\n        \"\"\"\n        Create Django model instance from XML data dictionary.\n\n        Args:\n            xml_data: Dictionary representation of XML data\n            **kwargs: Additional field values to override\n\n        Returns:\n            Django model instance\n        \"\"\"\n        # Convert XML data (field names might need conversion from XML naming)\n        data = cls._convert_xml_data(xml_data)\n\n        # If this model is Timescale-enabled (has 'time' field), ensure timestamp mapping\n        try:\n            model_field_names = {f.name for f in cls._meta.fields}\n        except Exception:\n            model_field_names = set()\n        if has_timescale_time_field(model_field_names) and \"time\" not in data:\n            map_time_alias_into_time(data, aliases=TIMESERIES_TIME_ALIASES)\n            if \"time\" not in data:\n                raise TimeseriesTimestampMissingError(cls.__name__, attempted_aliases=list(TIMESERIES_TIME_ALIASES))\n\n        # Override with any provided kwargs\n        data.update(kwargs)\n\n        # Create Django instance\n        return cls(**data)\n\n    @classmethod\n    def _convert_xml_data(cls, xml_data: dict[str, Any]) -&gt; dict[str, Any]:\n        \"\"\"\n        Convert XML data dictionary to Django model field format.\n        Can be overridden to handle XML-to-Django field name mapping.\n        \"\"\"\n        # Basic implementation - can be enhanced to handle:\n        # - camelCase to snake_case conversion\n        # - XML attribute vs element handling\n        # - Namespace handling\n        converted = {}\n        for key, value in xml_data.items():\n            # Skip XML metadata\n            if key.startswith(\"_\") or key.startswith(\"@\"):\n                continue\n\n            # Normalize XML name into a Django-safe field identifier\n            django_field_name = cls._xml_name_to_django_field(key)\n            converted[django_field_name] = value\n\n        return converted\n\n    @classmethod\n    def _xml_name_to_django_field(cls, xml_name: str) -&gt; str:\n        \"\"\"Convert XML element/attribute name to Django field name.\"\"\"\n        return sanitize_field_identifier(xml_name)\n\n    def to_xml_dict(self, include_metadata: bool = False) -&gt; dict[str, Any]:\n        \"\"\"\n        Convert Django model instance to XML-compatible dictionary.\n\n        Args:\n            include_metadata: Whether to include XML namespace and schema information\n\n        Returns:\n            Dictionary suitable for XML serialization\n        \"\"\"\n        data = {}\n\n        # Get field values\n        for field in self._meta.fields:\n            value = getattr(self, field.name)\n            if value is not None:\n                # Convert Django field name back to XML name\n                xml_name = self._django_field_to_xml_name(field.name)\n                data[xml_name] = self._serialize_field_value(value, field)\n\n        # Include metadata if requested\n        if include_metadata:\n            if self._xml_namespace:\n                data[\"_namespace\"] = self._xml_namespace\n            if self._xml_schema_location:\n                data[\"_schema_location\"] = self._xml_schema_location\n            if self._xml_type_name:\n                data[\"_type_name\"] = self._xml_type_name\n\n        return data\n\n    def _django_field_to_xml_name(self, field_name: str) -&gt; str:\n        \"\"\"Convert Django field name back to XML element/attribute name.\"\"\"\n        # Simple snake_case to camelCase conversion\n        components = field_name.split(\"_\")\n        return components[0] + \"\".join(word.capitalize() for word in components[1:])\n\n    def _serialize_field_value(self, value: Any, field: models.Field) -&gt; Any:\n        \"\"\"Serialize field value for XML output.\"\"\"\n        # Handle different field types\n        if isinstance(field, (models.DateTimeField, models.DateField, models.TimeField)):\n            return value.isoformat() if value else None\n        elif isinstance(field, models.UUIDField):\n            return str(value) if value else None\n        elif isinstance(field, (models.ForeignKey, models.OneToOneField)):\n            # For relationships, return the related object's primary key\n            return value.pk if value else None\n        elif isinstance(field, models.ManyToManyField):\n            # For M2M, return list of primary keys\n            return [obj.pk for obj in value.all()] if value else []\n        else:\n            return value\n\n    def to_xml_string(self, root_element_name: str | None = None, include_declaration: bool = True) -&gt; str:\n        \"\"\"\n        Convert Django model instance to XML string.\n\n        Args:\n            root_element_name: Name for the root XML element (defaults to model class name)\n            include_declaration: Whether to include XML declaration\n\n        Returns:\n            XML string representation\n        \"\"\"\n        try:\n            import xml.etree.ElementTree as ET\n        except ImportError as err:\n            raise ImportError(\"XML support requires xml.etree.ElementTree\") from err\n\n        root_name = root_element_name or self._xml_type_name or self.__class__.__name__\n        root = ET.Element(root_name)\n\n        # Add namespace if available\n        if self._xml_namespace:\n            root.set(\"xmlns\", self._xml_namespace)\n\n        # Convert to dict and build XML\n        data = self.to_xml_dict()\n        for key, value in data.items():\n            if key.startswith(\"_\"):  # Skip metadata\n                continue\n            if value is not None:\n                if isinstance(value, list):\n                    # Handle lists (from maxOccurs=\"unbounded\")\n                    for item in value:\n                        elem = ET.SubElement(root, key)\n                        elem.text = str(item)\n                else:\n                    elem = ET.SubElement(root, key)\n                    elem.text = str(value)\n\n        # Convert to string\n        xml_str = ET.tostring(root, encoding=\"unicode\")\n\n        if include_declaration:\n            xml_str = '&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n' + xml_str\n\n        return xml_str\n\n    @classmethod\n    def from_xml_string(cls, xml_string: str) -&gt; \"Xml2DjangoBaseClass\":\n        \"\"\"\n        Create Django model instance from XML string.\n\n        Args:\n            xml_string: XML string to parse\n\n        Returns:\n            Django model instance\n        \"\"\"\n        try:\n            import xml.etree.ElementTree as ET\n        except ImportError as err:\n            raise ImportError(\"XML support requires xml.etree.ElementTree\") from err\n\n        root = ET.fromstring(xml_string)\n        data = {}\n\n        # Extract elements\n        for elem in root:\n            if elem.tag in data:\n                # Handle multiple elements with same name (lists)\n                if not isinstance(data[elem.tag], list):\n                    data[elem.tag] = [data[elem.tag]]\n                data[elem.tag].append(elem.text)\n            else:\n                data[elem.tag] = elem.text\n\n        # Extract attributes (prefixed with @)\n        for attr_name, attr_value in root.attrib.items():\n            if not attr_name.startswith(\"{\"):  # Skip namespace declarations\n                data[f\"@{attr_name}\"] = attr_value\n\n        return cls.from_xml_dict(data)\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.Xml2DjangoBaseClass.from_xml_dict","title":"<code>from_xml_dict(xml_data, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create Django model instance from XML data dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>xml_data</code> <code>dict[str, Any]</code> <p>Dictionary representation of XML data</p> required <code>**kwargs</code> <p>Additional field values to override</p> <code>{}</code> <p>Returns:</p> Type Description <code>Xml2DjangoBaseClass</code> <p>Django model instance</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>@classmethod\ndef from_xml_dict(cls, xml_data: dict[str, Any], **kwargs) -&gt; \"Xml2DjangoBaseClass\":\n    \"\"\"\n    Create Django model instance from XML data dictionary.\n\n    Args:\n        xml_data: Dictionary representation of XML data\n        **kwargs: Additional field values to override\n\n    Returns:\n        Django model instance\n    \"\"\"\n    # Convert XML data (field names might need conversion from XML naming)\n    data = cls._convert_xml_data(xml_data)\n\n    # If this model is Timescale-enabled (has 'time' field), ensure timestamp mapping\n    try:\n        model_field_names = {f.name for f in cls._meta.fields}\n    except Exception:\n        model_field_names = set()\n    if has_timescale_time_field(model_field_names) and \"time\" not in data:\n        map_time_alias_into_time(data, aliases=TIMESERIES_TIME_ALIASES)\n        if \"time\" not in data:\n            raise TimeseriesTimestampMissingError(cls.__name__, attempted_aliases=list(TIMESERIES_TIME_ALIASES))\n\n    # Override with any provided kwargs\n    data.update(kwargs)\n\n    # Create Django instance\n    return cls(**data)\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.Xml2DjangoBaseClass.from_xml_string","title":"<code>from_xml_string(xml_string)</code>  <code>classmethod</code>","text":"<p>Create Django model instance from XML string.</p> <p>Parameters:</p> Name Type Description Default <code>xml_string</code> <code>str</code> <p>XML string to parse</p> required <p>Returns:</p> Type Description <code>Xml2DjangoBaseClass</code> <p>Django model instance</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>@classmethod\ndef from_xml_string(cls, xml_string: str) -&gt; \"Xml2DjangoBaseClass\":\n    \"\"\"\n    Create Django model instance from XML string.\n\n    Args:\n        xml_string: XML string to parse\n\n    Returns:\n        Django model instance\n    \"\"\"\n    try:\n        import xml.etree.ElementTree as ET\n    except ImportError as err:\n        raise ImportError(\"XML support requires xml.etree.ElementTree\") from err\n\n    root = ET.fromstring(xml_string)\n    data = {}\n\n    # Extract elements\n    for elem in root:\n        if elem.tag in data:\n            # Handle multiple elements with same name (lists)\n            if not isinstance(data[elem.tag], list):\n                data[elem.tag] = [data[elem.tag]]\n            data[elem.tag].append(elem.text)\n        else:\n            data[elem.tag] = elem.text\n\n    # Extract attributes (prefixed with @)\n    for attr_name, attr_value in root.attrib.items():\n        if not attr_name.startswith(\"{\"):  # Skip namespace declarations\n            data[f\"@{attr_name}\"] = attr_value\n\n    return cls.from_xml_dict(data)\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.Xml2DjangoBaseClass.to_xml_dict","title":"<code>to_xml_dict(include_metadata=False)</code>","text":"<p>Convert Django model instance to XML-compatible dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>include_metadata</code> <code>bool</code> <p>Whether to include XML namespace and schema information</p> <code>False</code> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary suitable for XML serialization</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>def to_xml_dict(self, include_metadata: bool = False) -&gt; dict[str, Any]:\n    \"\"\"\n    Convert Django model instance to XML-compatible dictionary.\n\n    Args:\n        include_metadata: Whether to include XML namespace and schema information\n\n    Returns:\n        Dictionary suitable for XML serialization\n    \"\"\"\n    data = {}\n\n    # Get field values\n    for field in self._meta.fields:\n        value = getattr(self, field.name)\n        if value is not None:\n            # Convert Django field name back to XML name\n            xml_name = self._django_field_to_xml_name(field.name)\n            data[xml_name] = self._serialize_field_value(value, field)\n\n    # Include metadata if requested\n    if include_metadata:\n        if self._xml_namespace:\n            data[\"_namespace\"] = self._xml_namespace\n        if self._xml_schema_location:\n            data[\"_schema_location\"] = self._xml_schema_location\n        if self._xml_type_name:\n            data[\"_type_name\"] = self._xml_type_name\n\n    return data\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.Xml2DjangoBaseClass.to_xml_string","title":"<code>to_xml_string(root_element_name=None, include_declaration=True)</code>","text":"<p>Convert Django model instance to XML string.</p> <p>Parameters:</p> Name Type Description Default <code>root_element_name</code> <code>str | None</code> <p>Name for the root XML element (defaults to model class name)</p> <code>None</code> <code>include_declaration</code> <code>bool</code> <p>Whether to include XML declaration</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>XML string representation</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>def to_xml_string(self, root_element_name: str | None = None, include_declaration: bool = True) -&gt; str:\n    \"\"\"\n    Convert Django model instance to XML string.\n\n    Args:\n        root_element_name: Name for the root XML element (defaults to model class name)\n        include_declaration: Whether to include XML declaration\n\n    Returns:\n        XML string representation\n    \"\"\"\n    try:\n        import xml.etree.ElementTree as ET\n    except ImportError as err:\n        raise ImportError(\"XML support requires xml.etree.ElementTree\") from err\n\n    root_name = root_element_name or self._xml_type_name or self.__class__.__name__\n    root = ET.Element(root_name)\n\n    # Add namespace if available\n    if self._xml_namespace:\n        root.set(\"xmlns\", self._xml_namespace)\n\n    # Convert to dict and build XML\n    data = self.to_xml_dict()\n    for key, value in data.items():\n        if key.startswith(\"_\"):  # Skip metadata\n            continue\n        if value is not None:\n            if isinstance(value, list):\n                # Handle lists (from maxOccurs=\"unbounded\")\n                for item in value:\n                    elem = ET.SubElement(root, key)\n                    elem.text = str(item)\n            else:\n                elem = ET.SubElement(root, key)\n                elem.text = str(value)\n\n    # Convert to string\n    xml_str = ET.tostring(root, encoding=\"unicode\")\n\n    if include_declaration:\n        xml_str = '&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\\n' + xml_str\n\n    return xml_str\n</code></pre>"},{"location":"reference/pydantic2django/django/models/#pydantic2django.django.models.XmlTimescaleBase","title":"<code>XmlTimescaleBase</code>","text":"<p>               Bases: <code>Xml2DjangoBaseClass</code>, <code>TimescaleModel</code></p> <p>Abstract base class combining XML helpers with TimescaleDB features.</p> <p>Inherits from <code>Xml2DjangoBaseClass</code> and <code>timescale.db.models.models.TimescaleModel</code>. When the TimescaleDB package is not installed, <code>TimescaleModel</code> is a no-op mixin.</p> Source code in <code>src/pydantic2django/django/models.py</code> <pre><code>class XmlTimescaleBase(Xml2DjangoBaseClass, TimescaleModel):\n    \"\"\"Abstract base class combining XML helpers with TimescaleDB features.\n\n    Inherits from `Xml2DjangoBaseClass` and `timescale.db.models.models.TimescaleModel`.\n    When the TimescaleDB package is not installed, `TimescaleModel` is a no-op mixin.\n    \"\"\"\n\n    class Meta:\n        abstract = True\n</code></pre>"},{"location":"reference/pydantic2django/django/timescale/","title":"pydantic2django.django.timescale","text":"<p>TimescaleDB integration helpers and bases.</p> <p>This namespace centralizes Timescale-specific support so it can be reused by XML, Pydantic, Dataclass, and other generators without duplicating logic.</p>"},{"location":"reference/pydantic2django/django/timescale/#pydantic2django.django.timescale.classify_xml_complex_types","title":"<code>classify_xml_complex_types(models, overrides=None, *, config=None)</code>","text":"<p>Classify XML complex types into hypertable vs dimension.</p> <p>Scoring (defaults shown; adjust with :class:<code>TimescaleConfig</code>): - +2 for time-like fields - +2 for observation/event-like names - +1 for unbounded/list growth - \u22122 for definition/metadata-like names</p> <p>Any type with total score \u2265 <code>threshold</code> (default 3) is a hypertable; others are dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>models</code> <code>Iterable</code> <p>Iterable of complex types (objects providing <code>name</code>, <code>elements</code>, and <code>attributes</code>).</p> required <code>overrides</code> <code>dict[str, TimescaleRole] | None</code> <p>Optional mapping of type name to explicit :class:<code>TimescaleRole</code> to force classification.</p> <code>None</code> <code>config</code> <code>TimescaleConfig | None</code> <p>Optional configuration (<code>threshold</code>, soft ref defaults).</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, TimescaleRole]</code> <p>Mapping <code>{type_name: TimescaleRole}</code>.</p> Source code in <code>src/pydantic2django/django/timescale/heuristics.py</code> <pre><code>def classify_xml_complex_types(\n    models: Iterable,\n    overrides: dict[str, TimescaleRole] | None = None,\n    *,\n    config: TimescaleConfig | None = None,\n) -&gt; dict[str, TimescaleRole]:\n    \"\"\"Classify XML complex types into hypertable vs dimension.\n\n    Scoring (defaults shown; adjust with :class:`TimescaleConfig`):\n    - +2 for time-like fields\n    - +2 for observation/event-like names\n    - +1 for unbounded/list growth\n    - \u22122 for definition/metadata-like names\n\n    Any type with total score \u2265 ``threshold`` (default 3) is a hypertable; others are dimensions.\n\n    Args:\n        models: Iterable of complex types (objects providing ``name``, ``elements``, and ``attributes``).\n        overrides: Optional mapping of type name to explicit :class:`TimescaleRole` to force classification.\n        config: Optional configuration (``threshold``, soft ref defaults).\n\n    Returns:\n        Mapping ``{type_name: TimescaleRole}``.\n    \"\"\"\n    cfg = config or TimescaleConfig()\n    result: dict[str, TimescaleRole] = {}\n    overrides = overrides or {}\n\n    # Build a quick lookup of complex types by name for descendant analysis\n    name_to_model: dict[str, Any] = {}\n    for m in models:\n        try:\n            n = getattr(m, \"name\", None) or getattr(m, \"__name__\", str(m))\n            name_to_model[n] = m\n        except Exception:\n            continue\n\n    def _child_complex_types(xct: Any) -&gt; list[Any]:\n        children: list[Any] = []\n        try:\n            for el in getattr(xct, \"elements\", []) or []:\n                # Prefer an inline complex_type, otherwise resolve by type_name\n                child = getattr(el, \"complex_type\", None)\n                if child is None:\n                    tname = getattr(el, \"type_name\", None)\n                    if tname and tname in name_to_model:\n                        child = name_to_model[tname]\n                if child is not None:\n                    children.append(child)\n        except Exception:\n            pass\n        return children\n\n    # First pass: base scoring\n    for m in models:\n        name = getattr(m, \"name\", None) or getattr(m, \"__name__\", str(m))\n        if name in overrides:\n            result[name] = overrides[name]\n            continue\n\n        score = _score_name(name) + _score_xml_features(m)\n        role = TimescaleRole.HYPERTABLE if score &gt;= cfg.threshold else TimescaleRole.DIMENSION\n        result[name] = role\n\n    # Second pass: demote types without direct time fields and promote leaf children with time fields\n    for m in models:\n        name = getattr(m, \"name\", None) or getattr(m, \"__name__\", str(m))\n        try:\n            # Never override explicit user overrides\n            if name in overrides:\n                continue\n            # Skip if this type already has direct time features\n            if has_direct_time_feature(m):\n                continue\n\n            # Find descendant complex types that have direct time features\n            first_children = _child_complex_types(m)\n            stack = list(first_children)\n            leaf_time_types: set[str] = set()\n            seen: set[int] = set()\n            while stack:\n                node = stack.pop()\n                if id(node) in seen:\n                    continue\n                seen.add(id(node))\n                if has_direct_time_feature(node):\n                    leaf_time_types.add(getattr(node, \"name\", getattr(node, \"__name__\", str(node))))\n                else:\n                    stack.extend(_child_complex_types(node))\n\n            if leaf_time_types:\n                # If this container was initially classified hypertable, demote it\n                if result.get(name) == TimescaleRole.HYPERTABLE:\n                    result[name] = TimescaleRole.DIMENSION\n                # Ensure all leaf time-bearing types are hypertables\n                for tname in leaf_time_types:\n                    if tname not in overrides:\n                        result[tname] = TimescaleRole.HYPERTABLE\n            else:\n                # No leaf time types found. If it lacks direct time, do not allow hypertable classification\n                if result.get(name) == TimescaleRole.HYPERTABLE:\n                    result[name] = TimescaleRole.DIMENSION\n        except Exception:\n            # Be defensive: never fail classification due to structure issues\n            continue\n\n    return result\n</code></pre>"},{"location":"reference/pydantic2django/django/timescale/#pydantic2django.django.timescale.is_hypertable","title":"<code>is_hypertable(model_name, roles)</code>","text":"<p>Return <code>True</code> if <code>model_name</code> is classified as a hypertable in <code>roles</code>.</p> Source code in <code>src/pydantic2django/django/timescale/heuristics.py</code> <pre><code>def is_hypertable(model_name: str, roles: dict[str, TimescaleRole]) -&gt; bool:\n    \"\"\"Return ``True`` if ``model_name`` is classified as a hypertable in ``roles``.\"\"\"\n    return roles.get(model_name) == TimescaleRole.HYPERTABLE\n</code></pre>"},{"location":"reference/pydantic2django/django/timescale/#pydantic2django.django.timescale.should_soft_reference","title":"<code>should_soft_reference(source_model_name, target_model_name, roles)</code>","text":"<p>Return <code>True</code> if an FK from <code>source</code> to <code>target</code> would be hypertable\u2192hypertable.</p> <p>TimescaleDB disallows hypertable\u2192hypertable FKs. Callers should emit an indexed scalar field (e.g., <code>UUIDField(db_index=True, null=True, blank=True)</code>) instead of a <code>ForeignKey</code>.</p> Source code in <code>src/pydantic2django/django/timescale/heuristics.py</code> <pre><code>def should_soft_reference(source_model_name: str, target_model_name: str, roles: dict[str, TimescaleRole]) -&gt; bool:\n    \"\"\"Return ``True`` if an FK from ``source`` to ``target`` would be hypertable\u2192hypertable.\n\n    TimescaleDB disallows hypertable\u2192hypertable FKs. Callers should emit an indexed scalar field\n    (e.g., ``UUIDField(db_index=True, null=True, blank=True)``) instead of a ``ForeignKey``.\n    \"\"\"\n    return is_hypertable(source_model_name, roles) and is_hypertable(target_model_name, roles)\n</code></pre>"},{"location":"reference/pydantic2django/django/timescale/#pydantic2django.django.timescale.should_use_timescale_base","title":"<code>should_use_timescale_base(model_name, roles)</code>","text":"<p>Return <code>True</code> if the model should inherit from a Timescale-enabled base.</p> <p>Used by generators to pick an appropriate base class (e.g., <code>XmlTimescaleBase</code>) for hypertables.</p> Source code in <code>src/pydantic2django/django/timescale/heuristics.py</code> <pre><code>def should_use_timescale_base(model_name: str, roles: dict[str, TimescaleRole]) -&gt; bool:\n    \"\"\"Return ``True`` if the model should inherit from a Timescale-enabled base.\n\n    Used by generators to pick an appropriate base class (e.g., ``XmlTimescaleBase``) for hypertables.\n    \"\"\"\n    return is_hypertable(model_name, roles)\n</code></pre>"},{"location":"reference/pydantic2django/django/timescale/bases/","title":"pydantic2django.django.timescale.bases","text":"<p>Common abstract base classes for TimescaleDB-enabled models.</p> <p>These combine existing base classes with the TimescaleModel mixin when available.</p>"},{"location":"reference/pydantic2django/django/timescale/bases/#pydantic2django.django.timescale.bases.TimescaleBaseMixin","title":"<code>TimescaleBaseMixin</code>","text":"<p>               Bases: <code>TimescaleModel</code></p> Source code in <code>src/pydantic2django/django/timescale/bases.py</code> <pre><code>class TimescaleBaseMixin(TimescaleModel):\n    class Meta:\n        abstract = True\n\n    def __init_subclass__(cls, **kwargs):  # type: ignore[override]\n        \"\"\"Avoid emitting redundant constraints; rely on the PK semantics for ``id``.\n\n        Because ``django-timescaledb`` expects to drop the default primary-key constraint\n        (``&lt;table&gt;_pkey``) during hypertable creation, adding an additional named unique\n        constraint on ``id`` in migrations can cause conflicts. We therefore do not add\n        any extra unique constraint on ``id`` here.\n        \"\"\"\n        super().__init_subclass__(**kwargs)\n</code></pre>"},{"location":"reference/pydantic2django/django/timescale/bases/#pydantic2django.django.timescale.bases.TimescaleBaseMixin.__init_subclass__","title":"<code>__init_subclass__(**kwargs)</code>","text":"<p>Avoid emitting redundant constraints; rely on the PK semantics for <code>id</code>.</p> <p>Because <code>django-timescaledb</code> expects to drop the default primary-key constraint (<code>&lt;table&gt;_pkey</code>) during hypertable creation, adding an additional named unique constraint on <code>id</code> in migrations can cause conflicts. We therefore do not add any extra unique constraint on <code>id</code> here.</p> Source code in <code>src/pydantic2django/django/timescale/bases.py</code> <pre><code>def __init_subclass__(cls, **kwargs):  # type: ignore[override]\n    \"\"\"Avoid emitting redundant constraints; rely on the PK semantics for ``id``.\n\n    Because ``django-timescaledb`` expects to drop the default primary-key constraint\n    (``&lt;table&gt;_pkey``) during hypertable creation, adding an additional named unique\n    constraint on ``id`` in migrations can cause conflicts. We therefore do not add\n    any extra unique constraint on ``id`` here.\n    \"\"\"\n    super().__init_subclass__(**kwargs)\n</code></pre>"},{"location":"reference/pydantic2django/django/timescale/heuristics/","title":"pydantic2django.django.timescale.heuristics","text":"<p>Heuristics and helpers to classify models for TimescaleDB usage.</p> <p>Primary goals: - Decide whether a model should be a hypertable (Timescale-enabled) or a regular table - Enforce safe FK behavior: hypertable \u2192 hypertable FKs are not allowed (soft references instead)</p> <p>Scoring overview for XML complex types (default threshold = 3): - +2 if a time-like element/attribute exists (e.g., <code>time</code>, <code>timestamp</code>, <code>sequence</code>, <code>effectiveTime</code>, <code>sampleRate</code>) - +2 if the type name matches observation/event patterns (e.g., <code>Samples</code>, <code>Events</code>, <code>Condition</code>, <code>*Changed</code>, <code>*Removed</code>, <code>*Added</code>, <code>Streams</code>) - +1 if the schema suggests unbounded/high-cardinality growth (e.g., an element with <code>is_list=True</code>) - \u22122 if the name matches definition/metadata categories (e.g., <code>*Definition*</code>, <code>*Definitions*</code>, <code>*Parameters*</code>, <code>Header</code>, <code>Counts</code>, <code>Configuration</code>, <code>Description</code>, <code>Location</code>, <code>Limits</code>, <code>Reference</code>, <code>Relationships</code>)</p> <p>Types with score \u2265 threshold are treated as hypertables; otherwise they are dimensions. Soft references are realized as indexed scalar fields (e.g., <code>UUIDField(db_index=True, null=True, blank=True)</code>) in place of illegal hypertable\u2192hypertable FKs.</p>"},{"location":"reference/pydantic2django/django/timescale/heuristics/#pydantic2django.django.timescale.heuristics.TimescaleConfig","title":"<code>TimescaleConfig</code>  <code>dataclass</code>","text":"<p>Configuration knobs for the heuristics.</p> <p>Attributes:</p> Name Type Description <code>threshold</code> <code>int</code> <p>Total score required to classify as a hypertable (default: 3).</p> <code>default_soft_ref_field</code> <code>str</code> <p>Name of the Django field type to use for soft references. Defaults to <code>UUIDField</code>; you can layer PK-specific soft refs if needed.</p> Source code in <code>src/pydantic2django/django/timescale/heuristics.py</code> <pre><code>@dataclass(frozen=True)\nclass TimescaleConfig:\n    \"\"\"Configuration knobs for the heuristics.\n\n    Attributes:\n        threshold: Total score required to classify as a hypertable (default: 3).\n        default_soft_ref_field: Name of the Django field type to use for soft references.\n            Defaults to ``UUIDField``; you can layer PK-specific soft refs if needed.\n    \"\"\"\n\n    threshold: int = 3\n    default_soft_ref_field: str = \"UUIDField\"\n</code></pre>"},{"location":"reference/pydantic2django/django/timescale/heuristics/#pydantic2django.django.timescale.heuristics.classify_dataclass_types","title":"<code>classify_dataclass_types(models, overrides=None, *, config=None)</code>","text":"<p>Classify Python dataclasses using name/field heuristics.</p> Source code in <code>src/pydantic2django/django/timescale/heuristics.py</code> <pre><code>def classify_dataclass_types(\n    models: Iterable[Any], overrides: dict[str, TimescaleRole] | None = None, *, config: TimescaleConfig | None = None\n) -&gt; dict[str, TimescaleRole]:\n    \"\"\"Classify Python dataclasses using name/field heuristics.\"\"\"\n    import dataclasses as _dc\n\n    cfg = config or TimescaleConfig()\n    result: dict[str, TimescaleRole] = {}\n    overrides = overrides or {}\n\n    for cls in models:\n        name = getattr(cls, \"__name__\", str(cls))\n        if name in overrides:\n            result[name] = overrides[name]\n            continue\n\n        score = _score_name(name)\n        # time-like fields by name\n        try:\n            if _dc.is_dataclass(cls):\n                score += _score_time_fields_in_mapping(f.name for f in _dc.fields(cls))\n                # list/collection fields via annotations\n                for f in getattr(cls, \"__annotations__\", {}).values():\n                    if _has_list_annotation(f):\n                        score += 1\n                        break\n        except Exception:\n            pass\n\n        role = TimescaleRole.HYPERTABLE if score &gt;= cfg.threshold else TimescaleRole.DIMENSION\n        result[name] = role\n\n    return result\n</code></pre>"},{"location":"reference/pydantic2django/django/timescale/heuristics/#pydantic2django.django.timescale.heuristics.classify_pydantic_models","title":"<code>classify_pydantic_models(models, overrides=None, *, config=None)</code>","text":"<p>Classify Pydantic <code>BaseModel</code> types using name/field heuristics.</p> <p>Signals: - +2 if any field name is time-like - +2 if class name matches observation/event patterns - +1 if any field annotation is a list/collection (append-only growth signal) - \u22122 for definition/metadata-like class names</p> Source code in <code>src/pydantic2django/django/timescale/heuristics.py</code> <pre><code>def classify_pydantic_models(\n    models: Iterable[Any], overrides: dict[str, TimescaleRole] | None = None, *, config: TimescaleConfig | None = None\n) -&gt; dict[str, TimescaleRole]:\n    \"\"\"Classify Pydantic ``BaseModel`` types using name/field heuristics.\n\n    Signals:\n    - +2 if any field name is time-like\n    - +2 if class name matches observation/event patterns\n    - +1 if any field annotation is a list/collection (append-only growth signal)\n    - \u22122 for definition/metadata-like class names\n    \"\"\"\n    cfg = config or TimescaleConfig()\n    result: dict[str, TimescaleRole] = {}\n    overrides = overrides or {}\n\n    for cls in models:\n        name = getattr(cls, \"__name__\", str(cls))\n        if name in overrides:\n            result[name] = overrides[name]\n            continue\n\n        score = _score_name(name)\n        # time-like fields by name\n        try:\n            model_fields = getattr(cls, \"model_fields\", {}) or {}\n            score += _score_time_fields_in_mapping(model_fields.keys())\n            # list/collection fields\n            for f in getattr(cls, \"__annotations__\", {}).values():\n                if _has_list_annotation(f):\n                    score += 1\n                    break\n        except Exception:\n            pass\n\n        role = TimescaleRole.HYPERTABLE if score &gt;= cfg.threshold else TimescaleRole.DIMENSION\n        result[name] = role\n\n    return result\n</code></pre>"},{"location":"reference/pydantic2django/django/timescale/heuristics/#pydantic2django.django.timescale.heuristics.classify_xml_complex_types","title":"<code>classify_xml_complex_types(models, overrides=None, *, config=None)</code>","text":"<p>Classify XML complex types into hypertable vs dimension.</p> <p>Scoring (defaults shown; adjust with :class:<code>TimescaleConfig</code>): - +2 for time-like fields - +2 for observation/event-like names - +1 for unbounded/list growth - \u22122 for definition/metadata-like names</p> <p>Any type with total score \u2265 <code>threshold</code> (default 3) is a hypertable; others are dimensions.</p> <p>Parameters:</p> Name Type Description Default <code>models</code> <code>Iterable</code> <p>Iterable of complex types (objects providing <code>name</code>, <code>elements</code>, and <code>attributes</code>).</p> required <code>overrides</code> <code>dict[str, TimescaleRole] | None</code> <p>Optional mapping of type name to explicit :class:<code>TimescaleRole</code> to force classification.</p> <code>None</code> <code>config</code> <code>TimescaleConfig | None</code> <p>Optional configuration (<code>threshold</code>, soft ref defaults).</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, TimescaleRole]</code> <p>Mapping <code>{type_name: TimescaleRole}</code>.</p> Source code in <code>src/pydantic2django/django/timescale/heuristics.py</code> <pre><code>def classify_xml_complex_types(\n    models: Iterable,\n    overrides: dict[str, TimescaleRole] | None = None,\n    *,\n    config: TimescaleConfig | None = None,\n) -&gt; dict[str, TimescaleRole]:\n    \"\"\"Classify XML complex types into hypertable vs dimension.\n\n    Scoring (defaults shown; adjust with :class:`TimescaleConfig`):\n    - +2 for time-like fields\n    - +2 for observation/event-like names\n    - +1 for unbounded/list growth\n    - \u22122 for definition/metadata-like names\n\n    Any type with total score \u2265 ``threshold`` (default 3) is a hypertable; others are dimensions.\n\n    Args:\n        models: Iterable of complex types (objects providing ``name``, ``elements``, and ``attributes``).\n        overrides: Optional mapping of type name to explicit :class:`TimescaleRole` to force classification.\n        config: Optional configuration (``threshold``, soft ref defaults).\n\n    Returns:\n        Mapping ``{type_name: TimescaleRole}``.\n    \"\"\"\n    cfg = config or TimescaleConfig()\n    result: dict[str, TimescaleRole] = {}\n    overrides = overrides or {}\n\n    # Build a quick lookup of complex types by name for descendant analysis\n    name_to_model: dict[str, Any] = {}\n    for m in models:\n        try:\n            n = getattr(m, \"name\", None) or getattr(m, \"__name__\", str(m))\n            name_to_model[n] = m\n        except Exception:\n            continue\n\n    def _child_complex_types(xct: Any) -&gt; list[Any]:\n        children: list[Any] = []\n        try:\n            for el in getattr(xct, \"elements\", []) or []:\n                # Prefer an inline complex_type, otherwise resolve by type_name\n                child = getattr(el, \"complex_type\", None)\n                if child is None:\n                    tname = getattr(el, \"type_name\", None)\n                    if tname and tname in name_to_model:\n                        child = name_to_model[tname]\n                if child is not None:\n                    children.append(child)\n        except Exception:\n            pass\n        return children\n\n    # First pass: base scoring\n    for m in models:\n        name = getattr(m, \"name\", None) or getattr(m, \"__name__\", str(m))\n        if name in overrides:\n            result[name] = overrides[name]\n            continue\n\n        score = _score_name(name) + _score_xml_features(m)\n        role = TimescaleRole.HYPERTABLE if score &gt;= cfg.threshold else TimescaleRole.DIMENSION\n        result[name] = role\n\n    # Second pass: demote types without direct time fields and promote leaf children with time fields\n    for m in models:\n        name = getattr(m, \"name\", None) or getattr(m, \"__name__\", str(m))\n        try:\n            # Never override explicit user overrides\n            if name in overrides:\n                continue\n            # Skip if this type already has direct time features\n            if has_direct_time_feature(m):\n                continue\n\n            # Find descendant complex types that have direct time features\n            first_children = _child_complex_types(m)\n            stack = list(first_children)\n            leaf_time_types: set[str] = set()\n            seen: set[int] = set()\n            while stack:\n                node = stack.pop()\n                if id(node) in seen:\n                    continue\n                seen.add(id(node))\n                if has_direct_time_feature(node):\n                    leaf_time_types.add(getattr(node, \"name\", getattr(node, \"__name__\", str(node))))\n                else:\n                    stack.extend(_child_complex_types(node))\n\n            if leaf_time_types:\n                # If this container was initially classified hypertable, demote it\n                if result.get(name) == TimescaleRole.HYPERTABLE:\n                    result[name] = TimescaleRole.DIMENSION\n                # Ensure all leaf time-bearing types are hypertables\n                for tname in leaf_time_types:\n                    if tname not in overrides:\n                        result[tname] = TimescaleRole.HYPERTABLE\n            else:\n                # No leaf time types found. If it lacks direct time, do not allow hypertable classification\n                if result.get(name) == TimescaleRole.HYPERTABLE:\n                    result[name] = TimescaleRole.DIMENSION\n        except Exception:\n            # Be defensive: never fail classification due to structure issues\n            continue\n\n    return result\n</code></pre>"},{"location":"reference/pydantic2django/django/timescale/heuristics/#pydantic2django.django.timescale.heuristics.has_direct_time_feature","title":"<code>has_direct_time_feature(xct)</code>","text":"<p>Return True if the XML complex type exposes a direct time-like attribute/element.</p> <p>Checks both attributes and elements for common time-related names.</p> Source code in <code>src/pydantic2django/django/timescale/heuristics.py</code> <pre><code>def has_direct_time_feature(xct: Any) -&gt; bool:\n    \"\"\"Return True if the XML complex type exposes a direct time-like attribute/element.\n\n    Checks both attributes and elements for common time-related names.\n    \"\"\"\n    try:\n        time_keys = {\"time\", \"timestamp\", \"sequence\", \"effectiveTime\", \"sampleRate\", \"date\", \"datetime\"}\n        for attr_name in getattr(xct, \"attributes\", {}).keys():\n            if any(k.lower() in attr_name.lower() for k in time_keys):\n                return True\n        for el in getattr(xct, \"elements\", []) or []:\n            if any(k.lower() in getattr(el, \"name\", \"\").lower() for k in time_keys):\n                return True\n        return False\n    except Exception:\n        return False\n</code></pre>"},{"location":"reference/pydantic2django/django/timescale/heuristics/#pydantic2django.django.timescale.heuristics.is_hypertable","title":"<code>is_hypertable(model_name, roles)</code>","text":"<p>Return <code>True</code> if <code>model_name</code> is classified as a hypertable in <code>roles</code>.</p> Source code in <code>src/pydantic2django/django/timescale/heuristics.py</code> <pre><code>def is_hypertable(model_name: str, roles: dict[str, TimescaleRole]) -&gt; bool:\n    \"\"\"Return ``True`` if ``model_name`` is classified as a hypertable in ``roles``.\"\"\"\n    return roles.get(model_name) == TimescaleRole.HYPERTABLE\n</code></pre>"},{"location":"reference/pydantic2django/django/timescale/heuristics/#pydantic2django.django.timescale.heuristics.should_invert_fk","title":"<code>should_invert_fk(source_model_name, target_model_name, roles)</code>","text":"<p>Return True if FK should be inverted to point from hypertable -&gt; dimension.</p> <p>Policy: - If source is a dimension (not hypertable) and target is a hypertable, invert. - This avoids illegal/fragile FKs to hypertables (PK dropped on hypertable creation),   while preserving joinability by referencing the dimension from the hypertable instead. - Hypertable -&gt; hypertable remains disallowed (handled by should_soft_reference).</p> Source code in <code>src/pydantic2django/django/timescale/heuristics.py</code> <pre><code>def should_invert_fk(source_model_name: str, target_model_name: str, roles: dict[str, TimescaleRole]) -&gt; bool:\n    \"\"\"Return True if FK should be inverted to point from hypertable -&gt; dimension.\n\n    Policy:\n    - If source is a dimension (not hypertable) and target is a hypertable, invert.\n    - This avoids illegal/fragile FKs to hypertables (PK dropped on hypertable creation),\n      while preserving joinability by referencing the dimension from the hypertable instead.\n    - Hypertable -&gt; hypertable remains disallowed (handled by should_soft_reference).\n    \"\"\"\n    return (not is_hypertable(source_model_name, roles)) and is_hypertable(target_model_name, roles)\n</code></pre>"},{"location":"reference/pydantic2django/django/timescale/heuristics/#pydantic2django.django.timescale.heuristics.should_soft_reference","title":"<code>should_soft_reference(source_model_name, target_model_name, roles)</code>","text":"<p>Return <code>True</code> if an FK from <code>source</code> to <code>target</code> would be hypertable\u2192hypertable.</p> <p>TimescaleDB disallows hypertable\u2192hypertable FKs. Callers should emit an indexed scalar field (e.g., <code>UUIDField(db_index=True, null=True, blank=True)</code>) instead of a <code>ForeignKey</code>.</p> Source code in <code>src/pydantic2django/django/timescale/heuristics.py</code> <pre><code>def should_soft_reference(source_model_name: str, target_model_name: str, roles: dict[str, TimescaleRole]) -&gt; bool:\n    \"\"\"Return ``True`` if an FK from ``source`` to ``target`` would be hypertable\u2192hypertable.\n\n    TimescaleDB disallows hypertable\u2192hypertable FKs. Callers should emit an indexed scalar field\n    (e.g., ``UUIDField(db_index=True, null=True, blank=True)``) instead of a ``ForeignKey``.\n    \"\"\"\n    return is_hypertable(source_model_name, roles) and is_hypertable(target_model_name, roles)\n</code></pre>"},{"location":"reference/pydantic2django/django/timescale/heuristics/#pydantic2django.django.timescale.heuristics.should_use_timescale_base","title":"<code>should_use_timescale_base(model_name, roles)</code>","text":"<p>Return <code>True</code> if the model should inherit from a Timescale-enabled base.</p> <p>Used by generators to pick an appropriate base class (e.g., <code>XmlTimescaleBase</code>) for hypertables.</p> Source code in <code>src/pydantic2django/django/timescale/heuristics.py</code> <pre><code>def should_use_timescale_base(model_name: str, roles: dict[str, TimescaleRole]) -&gt; bool:\n    \"\"\"Return ``True`` if the model should inherit from a Timescale-enabled base.\n\n    Used by generators to pick an appropriate base class (e.g., ``XmlTimescaleBase``) for hypertables.\n    \"\"\"\n    return is_hypertable(model_name, roles)\n</code></pre>"},{"location":"reference/pydantic2django/django/typing/","title":"pydantic2django.django.typing","text":""},{"location":"reference/pydantic2django/django/typing/#pydantic2django.django.typing.clean_field_for_template","title":"<code>clean_field_for_template(field_type)</code>","text":"<p>Cleans a field type string for use within Django templates. Removes problematic characters or constructs.</p> <p>Parameters:</p> Name Type Description Default <code>field_type</code> <code>str</code> <p>The field type string.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The cleaned field type string suitable for templates.</p> Source code in <code>src/pydantic2django/django/typing.py</code> <pre><code>def clean_field_for_template(field_type: str) -&gt; str:\n    \"\"\"\n    Cleans a field type string for use within Django templates.\n    Removes problematic characters or constructs.\n\n    Args:\n        field_type: The field type string.\n\n    Returns:\n        The cleaned field type string suitable for templates.\n    \"\"\"\n    # Example cleaning: remove Optional[] wrapper, handle complex generics simply\n    cleaned = re.sub(r\"Optional\\[(.*?)\\]\", r\"\\1\", field_type)\n    # Add more specific cleaning rules as needed\n    cleaned = cleaned.replace(\"list[\", \"List[\")  # Example normalization\n    # ... further cleaning ...\n    return cleaned\n</code></pre>"},{"location":"reference/pydantic2django/django/typing/#pydantic2django.django.typing.clean_field_type_for_template","title":"<code>clean_field_type_for_template(field)</code>","text":"<p>Cleans the 'field_type' within a field dictionary for template usage.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>dict</code> <p>The field dictionary.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The cleaned field type string.</p> Source code in <code>src/pydantic2django/django/typing.py</code> <pre><code>def clean_field_type_for_template(field: dict) -&gt; str:\n    \"\"\"\n    Cleans the 'field_type' within a field dictionary for template usage.\n\n    Args:\n        field: The field dictionary.\n\n    Returns:\n        The cleaned field type string.\n    \"\"\"\n    field_type = field.get(\"field_type\", \"Any\")\n    return clean_field_for_template(field_type)\n</code></pre>"},{"location":"reference/pydantic2django/django/utils/","title":"pydantic2django.django.utils","text":""},{"location":"reference/pydantic2django/django/utils/naming/","title":"pydantic2django.django.utils.naming","text":""},{"location":"reference/pydantic2django/django/utils/naming/#pydantic2django.django.utils.naming.enum_class_name_from_field","title":"<code>enum_class_name_from_field(name)</code>","text":"<p>Proxy to core helper for deriving enum class names from field names.</p> Source code in <code>src/pydantic2django/django/utils/naming.py</code> <pre><code>def enum_class_name_from_field(name: str) -&gt; str:\n    \"\"\"Proxy to core helper for deriving enum class names from field names.\"\"\"\n    return _core_enum_class_name_from_field(name)\n</code></pre>"},{"location":"reference/pydantic2django/django/utils/naming/#pydantic2django.django.utils.naming.get_related_model_name","title":"<code>get_related_model_name(field)</code>","text":"<p>Safely get the related model name from a Django relationship field.</p> <p>Handles potential errors and different ways the related model might be specified.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>Field</code> <p>The Django relationship field (ForeignKey, ManyToManyField, OneToOneField).</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>The name of the related model (e.g., 'AppName.ModelName' or 'ModelName')</p> <code>Optional[str]</code> <p>or None if it cannot be determined.</p> Source code in <code>src/pydantic2django/django/utils/naming.py</code> <pre><code>def get_related_model_name(field: models.Field) -&gt; Optional[str]:\n    \"\"\"\n    Safely get the related model name from a Django relationship field.\n\n    Handles potential errors and different ways the related model might be specified.\n\n    Args:\n        field: The Django relationship field (ForeignKey, ManyToManyField, OneToOneField).\n\n    Returns:\n        The name of the related model (e.g., 'AppName.ModelName' or 'ModelName')\n        or None if it cannot be determined.\n    \"\"\"\n    related_model = None\n    try:\n        if hasattr(field, \"remote_field\") and field.remote_field:\n            related_model = field.remote_field.model\n        elif hasattr(field, \"related_model\"):  # Older Django versions might use this\n            related_model = field.related_model\n\n        if related_model:\n            if isinstance(related_model, str):\n                # If it's already a string 'app.Model' or 'Model', use it\n                return related_model\n            elif hasattr(related_model, \"_meta\"):\n                # Standard case: access _meta.label or construct from app_label and model_name\n                if hasattr(related_model._meta, \"label\"):\n                    return related_model._meta.label  # 'app_label.ModelName'\n                else:\n                    app_label = getattr(related_model._meta, \"app_label\", None)\n                    model_name = getattr(related_model._meta, \"model_name\", None)\n                    if app_label and model_name:\n                        return f\"{app_label}.{model_name}\"\n                    elif model_name:  # Fallback if only model name is available\n                        return model_name\n            elif hasattr(related_model, \"__name__\"):\n                # Fallback for simpler cases or potential custom models without _meta\n                return related_model.__name__\n\n    except Exception:\n        # logger.warning(f\"Could not determine related model name for field {getattr(field, 'name', '?')}: {e}\")\n        pass\n\n    # logger.warning(f\"Failed to determine related model name for field {getattr(field, 'name', '?')}\")\n    return None\n</code></pre>"},{"location":"reference/pydantic2django/django/utils/naming/#pydantic2django.django.utils.naming.sanitize_model_field_name","title":"<code>sanitize_model_field_name(name)</code>","text":"<p>Convert an arbitrary XML name to a valid, snake_cased Django/Python field name.</p> <ul> <li>Replace namespace separators and punctuation (\":\", \".\", \"-\", spaces) with \"_\"</li> <li>Convert CamelCase to snake_case</li> <li>Remove invalid characters (keep [a-zA-Z0-9_])</li> <li>Ensure name starts with a letter or underscore; if not, prefix with \"_\"</li> <li>Lowercase the result</li> <li>Avoid Python keywords/builtins by suffixing with \"_value\" if necessary</li> </ul> Source code in <code>src/pydantic2django/django/utils/naming.py</code> <pre><code>def sanitize_model_field_name(name: str) -&gt; str:\n    \"\"\"\n    Convert an arbitrary XML name to a valid, snake_cased Django/Python field name.\n\n    - Replace namespace separators and punctuation (\":\", \".\", \"-\", spaces) with \"_\"\n    - Convert CamelCase to snake_case\n    - Remove invalid characters (keep [a-zA-Z0-9_])\n    - Ensure name starts with a letter or underscore; if not, prefix with \"_\"\n    - Lowercase the result\n    - Avoid Python keywords/builtins by suffixing with \"_value\" if necessary\n    \"\"\"\n    return _core_sanitize_field_identifier(name)\n</code></pre>"},{"location":"reference/pydantic2django/django/utils/naming/#pydantic2django.django.utils.naming.sanitize_related_name","title":"<code>sanitize_related_name(name, model_name='', field_name='')</code>","text":"<p>Sanitize a related name for Django models.</p> <p>Ensures the name is a valid Python identifier, replacing invalid characters and providing fallbacks if the name is empty.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name to sanitize</p> required <code>model_name</code> <code>str</code> <p>Optional model name for context in fallback generation</p> <code>''</code> <code>field_name</code> <code>str</code> <p>Optional field name for context in fallback generation</p> <code>''</code> <p>Returns:</p> Type Description <code>str</code> <p>A sanitized related name suitable for Django's related_name attribute.</p> Source code in <code>src/pydantic2django/django/utils/naming.py</code> <pre><code>def sanitize_related_name(name: str, model_name: str = \"\", field_name: str = \"\") -&gt; str:\n    \"\"\"\n    Sanitize a related name for Django models.\n\n    Ensures the name is a valid Python identifier, replacing invalid characters\n    and providing fallbacks if the name is empty.\n\n    Args:\n        name: The name to sanitize\n        model_name: Optional model name for context in fallback generation\n        field_name: Optional field name for context in fallback generation\n\n    Returns:\n        A sanitized related name suitable for Django's related_name attribute.\n    \"\"\"\n    # Replace spaces and special characters with underscores\n    sanitized = re.sub(r\"[^\\w]\", \"_\", str(name))  # Ensure input is string\n\n    # Ensure it starts with a letter or underscore\n    if sanitized and not sanitized[0].isalpha() and sanitized[0] != \"_\":\n        sanitized = f\"_{sanitized}\"\n\n    # If it's empty after sanitization, generate a fallback name\n    if not sanitized:\n        if model_name and field_name:\n            sanitized = f\"{model_name.lower()}_{field_name.lower()}_set\"\n        elif model_name:\n            sanitized = f\"{model_name.lower()}_set\"\n        elif field_name:\n            sanitized = f\"{field_name.lower()}_set\"\n        else:\n            sanitized = \"related_items\"  # Generic fallback\n\n    # Ensure it's not a Python keyword (add underscore if it is)\n    # Requires importing keyword module\n    # import keyword\n    # if keyword.iskeyword(sanitized):\n    #     sanitized += \"_\"\n\n    # Ensure it doesn't end with '+' (invalid related_name suffix)\n    if sanitized.endswith(\"+\"):\n        sanitized = sanitized.rstrip(\"+\") + \"_\"\n\n    # Django convention: lower case\n    return sanitized.lower()\n</code></pre>"},{"location":"reference/pydantic2django/django/utils/serialization/","title":"pydantic2django.django.utils.serialization","text":""},{"location":"reference/pydantic2django/django/utils/serialization/#pydantic2django.django.utils.serialization.FieldSerializer","title":"<code>FieldSerializer</code>","text":"<p>Handles extraction and processing of field attributes into string form from Django model fields for code generation.</p> Source code in <code>src/pydantic2django/django/utils/serialization.py</code> <pre><code>class FieldSerializer:\n    \"\"\"\n    Handles extraction and processing of field attributes into string form\n    from Django model fields for code generation.\n    \"\"\"\n\n    @staticmethod\n    def serialize_field_attributes(field: models.Field) -&gt; list[str]:\n        \"\"\"\n        Serialize common Django model field attributes to a list of parameter strings.\n\n        Args:\n            field: The Django model field instance.\n\n        Returns:\n            List of parameter strings (e.g., \"null=True\", \"max_length=255\").\n        \"\"\"\n        params = []\n        field_name = getattr(field, \"name\", \"?\")  # For logging\n\n        # Common field parameters\n        try:\n            if hasattr(field, \"verbose_name\") and field.verbose_name:\n                # Check if verbose_name is different from the auto-generated one\n                auto_verbose_name = field.name.replace(\"_\", \" \").capitalize()\n                if str(field.verbose_name) != auto_verbose_name:\n                    params.append(f\"verbose_name='{sanitize_string(field.verbose_name)}'\")\n\n            if hasattr(field, \"help_text\") and field.help_text:\n                params.append(f\"help_text='{sanitize_string(field.help_text)}'\")\n\n            # Explicitly include null/blank only if they differ from the default for the field type\n            # (Most fields default to null=False, blank=False)\n            if hasattr(field, \"null\") and field.null:\n                params.append(f\"null={field.null}\")\n\n            if hasattr(field, \"blank\") and field.blank:\n                # Only add blank=True if null=True is also set, common Django pattern\n                if hasattr(field, \"null\") and field.null:\n                    params.append(f\"blank={field.blank}\")\n                else:\n                    logger.debug(\n                        f\"Field '{field_name}' has blank=True but null=False. Omitting blank=True from serialization.\"\n                    )\n\n            # Handle choices\n            if hasattr(field, \"choices\") and field.choices:\n                # TODO: Need a robust way to serialize choices, might be complex tuples/enums\n                try:\n                    choices_repr = repr(field.choices)  # Basic repr, might need improvement\n                    params.append(f\"choices={choices_repr}\")\n                except Exception as e:\n                    logger.warning(f\"Could not serialize choices for field '{field_name}': {e}\")\n\n            # Handle default value\n            if hasattr(field, \"default\") and field.default is not models.NOT_PROVIDED:\n                # Skip default for AutoFields as it's implicit\n                if not isinstance(field, (models.AutoField, models.BigAutoField)):\n                    try:\n                        default_repr = repr(field.default)  # Use repr for safety\n                        # Avoid adding default=None if null=True is already implied\n                        if not (field.null and field.default is None):\n                            params.append(f\"default={default_repr}\")\n                    except Exception as e:\n                        logger.warning(f\"Could not serialize default value for field '{field_name}': {e}\")\n\n            # Field-specific parameters\n            if isinstance(field, (models.CharField, models.SlugField, models.FilePathField)):\n                # Only add max_length if it's not the default (which varies)\n                # We need a way to know the default for each field type if we want to omit defaults.\n                # For now, always include it if present.\n                if hasattr(field, \"max_length\") and field.max_length is not None:\n                    params.append(f\"max_length={field.max_length}\")\n\n            if isinstance(field, models.DecimalField):\n                # Defaults are max_digits=None, decimal_places=None\n                if hasattr(field, \"max_digits\") and field.max_digits is not None:\n                    params.append(f\"max_digits={field.max_digits}\")\n                if hasattr(field, \"decimal_places\") and field.decimal_places is not None:\n                    params.append(f\"decimal_places={field.decimal_places}\")\n\n        except Exception as e:\n            logger.error(f\"Error serializing attributes for field '{field_name}': {e}\", exc_info=True)\n\n        return params\n\n    @staticmethod\n    def serialize_field(field: models.Field) -&gt; str:\n        \"\"\"\n        Serialize a Django model field to its string representation for code generation.\n\n        Args:\n            field: The Django model field instance.\n\n        Returns:\n            String representation (e.g., \"models.CharField(max_length=255, null=True)\").\n        \"\"\"\n        field_name = getattr(field, \"name\", \"?\")\n        logger.debug(f\"Serializing field: {field_name} (Type: {type(field).__name__})\")\n        try:\n            field_type_name = type(field).__name__\n            params = FieldSerializer.serialize_field_attributes(field)\n\n            # Handle relationship fields specifically\n            if isinstance(field, (models.ForeignKey, models.OneToOneField, models.ManyToManyField)):\n                related_model_name = get_related_model_name(field)\n                if not related_model_name:\n                    # Attempt to get from field.related_model as another fallback\n                    related_model_obj = getattr(field, \"related_model\", None)\n                    if related_model_obj:\n                        # Avoid accessing __name__ directly on Literal['self']\n                        if isinstance(related_model_obj, str) and related_model_obj == \"self\":\n                            related_model_name = \"self\"\n                        else:\n                            related_model_name = getattr(related_model_obj, \"__name__\", str(related_model_obj))\n\n                if related_model_name:\n                    # Quote the model name (might be 'app.Model' or 'Model')\n                    params.append(f\"to='{related_model_name}'\")\n                else:\n                    # This should ideally not happen if discovery worked correctly\n                    logger.error(\n                        f\"CRITICAL: Could not determine related model for relationship field '{field_name}'. Defaulting to 'self'.\"\n                    )\n                    params.append(\"to='self'\")\n\n                # on_delete for ForeignKey/OneToOneField\n                if isinstance(field, (models.ForeignKey, models.OneToOneField)):\n                    try:\n                        # Use remote_field preferentially\n                        remote_field = getattr(field, \"remote_field\", None)\n                        if remote_field and hasattr(remote_field, \"on_delete\"):\n                            on_delete_func = remote_field.on_delete\n                            on_delete_name = getattr(on_delete_func, \"__name__\", None)\n                            if on_delete_name:\n                                params.append(f\"on_delete=models.{on_delete_name}\")\n                            else:\n                                logger.warning(\n                                    f\"Could not determine on_delete name for field '{field_name}'. Defaulting to CASCADE.\"\n                                )\n                                params.append(\"on_delete=models.CASCADE\")  # Sensible default\n                        else:\n                            logger.warning(\n                                f\"Could not find remote_field or on_delete for field '{field_name}'. Defaulting to CASCADE.\"\n                            )\n                            params.append(\"on_delete=models.CASCADE\")\n                    except AttributeError as e:\n                        logger.warning(\n                            f\"AttributeError determining on_delete for '{field_name}': {e}. Defaulting to CASCADE.\"\n                        )\n                        params.append(\"on_delete=models.CASCADE\")\n\n                # related_name (common to all relationships)\n                # Use remote_field preferentially\n                remote_field_obj = getattr(field, \"remote_field\", None)\n                related_name = getattr(remote_field_obj, \"related_name\", None)\n                if related_name:\n                    # Sanitize related_name before adding\n                    params.append(f\"related_name='{sanitize_related_name(related_name, field_name=field_name)}'\")\n\n                # ManyToMany specific attributes\n                if isinstance(field, models.ManyToManyField):\n                    remote_m2m_field = getattr(field, \"remote_field\", None)\n                    through_model = getattr(remote_m2m_field, \"through\", None) if remote_m2m_field else None\n                    # Check auto_created on the through_model itself if it exists\n                    auto_created_meta = False\n                    if through_model and not isinstance(through_model, str) and hasattr(through_model, \"_meta\"):\n                        auto_created_meta = getattr(through_model._meta, \"auto_created\", False)\n\n                    if through_model and not isinstance(through_model, str) and not auto_created_meta:\n                        through_name = getattr(through_model, \"__name__\", str(through_model))\n                        params.append(f\"through={through_name}\")\n                    elif isinstance(through_model, str):\n                        params.append(f\"through='{through_model}'\")  # Use string directly\n\n            # Construct final definition string\n            param_str = \", \".join(params)\n            final_def = f\"models.{field_type_name}({param_str})\"\n            logger.debug(f\"Serialized definition for '{field_name}': {final_def}\")\n            return final_def\n\n        except Exception as e:\n            logger.error(f\"Failed to serialize field '{field_name}' (Type: {type(field).__name__}): {e}\", exc_info=True)\n            # Fallback to a simple TextField to avoid crashing generation\n            return f\"models.TextField(help_text='Serialization failed for field {field_name}: {e}')\"\n</code></pre>"},{"location":"reference/pydantic2django/django/utils/serialization/#pydantic2django.django.utils.serialization.FieldSerializer.serialize_field","title":"<code>serialize_field(field)</code>  <code>staticmethod</code>","text":"<p>Serialize a Django model field to its string representation for code generation.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>Field</code> <p>The Django model field instance.</p> required <p>Returns:</p> Type Description <code>str</code> <p>String representation (e.g., \"models.CharField(max_length=255, null=True)\").</p> Source code in <code>src/pydantic2django/django/utils/serialization.py</code> <pre><code>@staticmethod\ndef serialize_field(field: models.Field) -&gt; str:\n    \"\"\"\n    Serialize a Django model field to its string representation for code generation.\n\n    Args:\n        field: The Django model field instance.\n\n    Returns:\n        String representation (e.g., \"models.CharField(max_length=255, null=True)\").\n    \"\"\"\n    field_name = getattr(field, \"name\", \"?\")\n    logger.debug(f\"Serializing field: {field_name} (Type: {type(field).__name__})\")\n    try:\n        field_type_name = type(field).__name__\n        params = FieldSerializer.serialize_field_attributes(field)\n\n        # Handle relationship fields specifically\n        if isinstance(field, (models.ForeignKey, models.OneToOneField, models.ManyToManyField)):\n            related_model_name = get_related_model_name(field)\n            if not related_model_name:\n                # Attempt to get from field.related_model as another fallback\n                related_model_obj = getattr(field, \"related_model\", None)\n                if related_model_obj:\n                    # Avoid accessing __name__ directly on Literal['self']\n                    if isinstance(related_model_obj, str) and related_model_obj == \"self\":\n                        related_model_name = \"self\"\n                    else:\n                        related_model_name = getattr(related_model_obj, \"__name__\", str(related_model_obj))\n\n            if related_model_name:\n                # Quote the model name (might be 'app.Model' or 'Model')\n                params.append(f\"to='{related_model_name}'\")\n            else:\n                # This should ideally not happen if discovery worked correctly\n                logger.error(\n                    f\"CRITICAL: Could not determine related model for relationship field '{field_name}'. Defaulting to 'self'.\"\n                )\n                params.append(\"to='self'\")\n\n            # on_delete for ForeignKey/OneToOneField\n            if isinstance(field, (models.ForeignKey, models.OneToOneField)):\n                try:\n                    # Use remote_field preferentially\n                    remote_field = getattr(field, \"remote_field\", None)\n                    if remote_field and hasattr(remote_field, \"on_delete\"):\n                        on_delete_func = remote_field.on_delete\n                        on_delete_name = getattr(on_delete_func, \"__name__\", None)\n                        if on_delete_name:\n                            params.append(f\"on_delete=models.{on_delete_name}\")\n                        else:\n                            logger.warning(\n                                f\"Could not determine on_delete name for field '{field_name}'. Defaulting to CASCADE.\"\n                            )\n                            params.append(\"on_delete=models.CASCADE\")  # Sensible default\n                    else:\n                        logger.warning(\n                            f\"Could not find remote_field or on_delete for field '{field_name}'. Defaulting to CASCADE.\"\n                        )\n                        params.append(\"on_delete=models.CASCADE\")\n                except AttributeError as e:\n                    logger.warning(\n                        f\"AttributeError determining on_delete for '{field_name}': {e}. Defaulting to CASCADE.\"\n                    )\n                    params.append(\"on_delete=models.CASCADE\")\n\n            # related_name (common to all relationships)\n            # Use remote_field preferentially\n            remote_field_obj = getattr(field, \"remote_field\", None)\n            related_name = getattr(remote_field_obj, \"related_name\", None)\n            if related_name:\n                # Sanitize related_name before adding\n                params.append(f\"related_name='{sanitize_related_name(related_name, field_name=field_name)}'\")\n\n            # ManyToMany specific attributes\n            if isinstance(field, models.ManyToManyField):\n                remote_m2m_field = getattr(field, \"remote_field\", None)\n                through_model = getattr(remote_m2m_field, \"through\", None) if remote_m2m_field else None\n                # Check auto_created on the through_model itself if it exists\n                auto_created_meta = False\n                if through_model and not isinstance(through_model, str) and hasattr(through_model, \"_meta\"):\n                    auto_created_meta = getattr(through_model._meta, \"auto_created\", False)\n\n                if through_model and not isinstance(through_model, str) and not auto_created_meta:\n                    through_name = getattr(through_model, \"__name__\", str(through_model))\n                    params.append(f\"through={through_name}\")\n                elif isinstance(through_model, str):\n                    params.append(f\"through='{through_model}'\")  # Use string directly\n\n        # Construct final definition string\n        param_str = \", \".join(params)\n        final_def = f\"models.{field_type_name}({param_str})\"\n        logger.debug(f\"Serialized definition for '{field_name}': {final_def}\")\n        return final_def\n\n    except Exception as e:\n        logger.error(f\"Failed to serialize field '{field_name}' (Type: {type(field).__name__}): {e}\", exc_info=True)\n        # Fallback to a simple TextField to avoid crashing generation\n        return f\"models.TextField(help_text='Serialization failed for field {field_name}: {e}')\"\n</code></pre>"},{"location":"reference/pydantic2django/django/utils/serialization/#pydantic2django.django.utils.serialization.FieldSerializer.serialize_field_attributes","title":"<code>serialize_field_attributes(field)</code>  <code>staticmethod</code>","text":"<p>Serialize common Django model field attributes to a list of parameter strings.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>Field</code> <p>The Django model field instance.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of parameter strings (e.g., \"null=True\", \"max_length=255\").</p> Source code in <code>src/pydantic2django/django/utils/serialization.py</code> <pre><code>@staticmethod\ndef serialize_field_attributes(field: models.Field) -&gt; list[str]:\n    \"\"\"\n    Serialize common Django model field attributes to a list of parameter strings.\n\n    Args:\n        field: The Django model field instance.\n\n    Returns:\n        List of parameter strings (e.g., \"null=True\", \"max_length=255\").\n    \"\"\"\n    params = []\n    field_name = getattr(field, \"name\", \"?\")  # For logging\n\n    # Common field parameters\n    try:\n        if hasattr(field, \"verbose_name\") and field.verbose_name:\n            # Check if verbose_name is different from the auto-generated one\n            auto_verbose_name = field.name.replace(\"_\", \" \").capitalize()\n            if str(field.verbose_name) != auto_verbose_name:\n                params.append(f\"verbose_name='{sanitize_string(field.verbose_name)}'\")\n\n        if hasattr(field, \"help_text\") and field.help_text:\n            params.append(f\"help_text='{sanitize_string(field.help_text)}'\")\n\n        # Explicitly include null/blank only if they differ from the default for the field type\n        # (Most fields default to null=False, blank=False)\n        if hasattr(field, \"null\") and field.null:\n            params.append(f\"null={field.null}\")\n\n        if hasattr(field, \"blank\") and field.blank:\n            # Only add blank=True if null=True is also set, common Django pattern\n            if hasattr(field, \"null\") and field.null:\n                params.append(f\"blank={field.blank}\")\n            else:\n                logger.debug(\n                    f\"Field '{field_name}' has blank=True but null=False. Omitting blank=True from serialization.\"\n                )\n\n        # Handle choices\n        if hasattr(field, \"choices\") and field.choices:\n            # TODO: Need a robust way to serialize choices, might be complex tuples/enums\n            try:\n                choices_repr = repr(field.choices)  # Basic repr, might need improvement\n                params.append(f\"choices={choices_repr}\")\n            except Exception as e:\n                logger.warning(f\"Could not serialize choices for field '{field_name}': {e}\")\n\n        # Handle default value\n        if hasattr(field, \"default\") and field.default is not models.NOT_PROVIDED:\n            # Skip default for AutoFields as it's implicit\n            if not isinstance(field, (models.AutoField, models.BigAutoField)):\n                try:\n                    default_repr = repr(field.default)  # Use repr for safety\n                    # Avoid adding default=None if null=True is already implied\n                    if not (field.null and field.default is None):\n                        params.append(f\"default={default_repr}\")\n                except Exception as e:\n                    logger.warning(f\"Could not serialize default value for field '{field_name}': {e}\")\n\n        # Field-specific parameters\n        if isinstance(field, (models.CharField, models.SlugField, models.FilePathField)):\n            # Only add max_length if it's not the default (which varies)\n            # We need a way to know the default for each field type if we want to omit defaults.\n            # For now, always include it if present.\n            if hasattr(field, \"max_length\") and field.max_length is not None:\n                params.append(f\"max_length={field.max_length}\")\n\n        if isinstance(field, models.DecimalField):\n            # Defaults are max_digits=None, decimal_places=None\n            if hasattr(field, \"max_digits\") and field.max_digits is not None:\n                params.append(f\"max_digits={field.max_digits}\")\n            if hasattr(field, \"decimal_places\") and field.decimal_places is not None:\n                params.append(f\"decimal_places={field.decimal_places}\")\n\n    except Exception as e:\n        logger.error(f\"Error serializing attributes for field '{field_name}': {e}\", exc_info=True)\n\n    return params\n</code></pre>"},{"location":"reference/pydantic2django/django/utils/serialization/#pydantic2django.django.utils.serialization.RawCode","title":"<code>RawCode</code>","text":"<p>A wrapper to inject raw code strings into the generated output.</p> Source code in <code>src/pydantic2django/django/utils/serialization.py</code> <pre><code>class RawCode:\n    \"\"\"A wrapper to inject raw code strings into the generated output.\"\"\"\n\n    def __init__(self, code: str):\n        self.code = code\n\n    def __repr__(self):\n        return self.code\n</code></pre>"},{"location":"reference/pydantic2django/django/utils/serialization/#pydantic2django.django.utils.serialization.generate_field_definition_string","title":"<code>generate_field_definition_string(field_class, field_kwargs, app_label)</code>","text":"<p>Generates the string representation of a Django field definition from its class and kwargs.</p> <p>Parameters:</p> Name Type Description Default <code>field_class</code> <code>type[Field]</code> <p>The Django field class (e.g., models.CharField).</p> required <code>field_kwargs</code> <code>dict[str, Any]</code> <p>A dictionary of keyword arguments for the field.</p> required <code>app_label</code> <code>str</code> <p>The app label of the model the field belongs to.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The field definition string (e.g., \"models.CharField(max_length=255)\").</p> Source code in <code>src/pydantic2django/django/utils/serialization.py</code> <pre><code>def generate_field_definition_string(\n    field_class: type[models.Field],\n    field_kwargs: dict[str, Any],\n    app_label: str,  # Needed for resolving 'self' in relationships\n) -&gt; str:\n    \"\"\"\n    Generates the string representation of a Django field definition from its class and kwargs.\n\n    Args:\n        field_class: The Django field class (e.g., models.CharField).\n        field_kwargs: A dictionary of keyword arguments for the field.\n        app_label: The app label of the model the field belongs to.\n\n    Returns:\n        The field definition string (e.g., \"models.CharField(max_length=255)\").\n    \"\"\"\n    param_parts = []\n    # Need to import sanitize_string\n    from ...core.utils.strings import sanitize_string\n\n    sorted_kwargs = sorted(field_kwargs.items())\n\n    for key, value in sorted_kwargs:\n        # Special handling for values that should be raw code\n        if isinstance(value, RawCode):\n            param_parts.append(f\"{key}={value.code}\")\n            continue\n\n        # Special handling for relationship 'to' and 'through' fields\n        if key == \"to\" or key == \"through\":\n            model_name_str = None\n            if isinstance(value, str):\n                # Use string directly (e.g., 'self', 'app_label.ModelName')\n                model_name_str = value\n            elif isinstance(value, type) and issubclass(value, models.Model):\n                # Get name from model type\n                try:\n                    # Prefer app_label.ModelName format if possible\n                    meta = getattr(value, \"_meta\", None)\n                    if meta:\n                        app_label_val = getattr(meta, \"app_label\", None)\n                        object_name_val = getattr(meta, \"object_name\", None)  # Use object_name for class name\n                        if app_label_val and object_name_val:\n                            model_name_str = f\"{app_label_val}.{object_name_val}\"\n                        elif object_name_val:\n                            model_name_str = object_name_val  # Fallback to just name\n                        else:\n                            model_name_str = value.__name__  # Final fallback\n                    else:\n                        model_name_str = value.__name__  # Fallback if no _meta\n                except AttributeError:\n                    model_name_str = value.__name__  # Fallback on error\n            else:\n                # Fallback for unexpected types\n                logger.warning(f\"Unexpected type for relationship '{key}' argument: {type(value)}. Using repr().\")\n                model_name_str = repr(value)\n\n            if model_name_str:\n                # Ensure self is quoted correctly\n                if model_name_str == \"self\":\n                    param_parts.append(f\"{key}='self'\")\n                else:\n                    param_parts.append(f\"{key}='{model_name_str}'\")\n            else:\n                logger.error(\n                    f\"Could not determine model name for relationship '{key}' argument: {value}. Omitting from definition.\"\n                )\n\n        # Special handling for on_delete\n        elif key == \"on_delete\":\n            # Allow RawCode override for on_delete\n            if isinstance(value, RawCode):\n                param_parts.append(f\"on_delete={value.code}\")\n            elif callable(value) and hasattr(value, \"__name__\"):\n                param_parts.append(f\"on_delete=models.{value.__name__}\")\n            else:\n                # Default to CASCADE if unresolvable\n                param_parts.append(\"on_delete=models.CASCADE\")\n                logger.warning(f\"Could not serialize on_delete value: {value}. Defaulting to CASCADE.\")\n        # General kwarg serialization\n        else:\n            try:\n                # Use sanitize_string for string values to handle quotes/escapes\n                if isinstance(value, str):\n                    repr_value = f\"'{sanitize_string(value)}'\"\n                # Use repr for others (bool, int, float, complex types like choices)\n                else:\n                    repr_value = repr(value)\n                param_parts.append(f\"{key}={repr_value}\")\n            except Exception as e:\n                logger.warning(f\"Could not serialize kwarg '{key}={value}' for field class {field_class.__name__}: {e}\")\n                param_parts.append(f\"{key}=None  # Serialization failed\")\n\n    param_str = \", \".join(param_parts)\n    return f\"models.{field_class.__name__}({param_str})\"\n</code></pre>"},{"location":"reference/pydantic2django/entrypoint/","title":"pydantic2django.entrypoint","text":""},{"location":"reference/pydantic2django/entrypoint/#pydantic2django.entrypoint.ModuleConverter","title":"<code>ModuleConverter</code>  <code>dataclass</code>","text":"<p>Convert a module to a set of Django models.</p> Source code in <code>src/pydantic2django/entrypoint.py</code> <pre><code>@dataclass\nclass ModuleConverter:\n    \"\"\"\n    Convert a module to a set of Django models.\n    \"\"\"\n\n    module: Any\n    output_path: str\n\n    def discover(self):\n        \"\"\"\n        Discover the models in the module.\n        \"\"\"\n        self.dataclass_discovery = DataclassDiscovery()\n\n    def generate(self):\n        \"\"\"Generate Django models for the discovered items (stub).\"\"\"\n        pass\n</code></pre>"},{"location":"reference/pydantic2django/entrypoint/#pydantic2django.entrypoint.ModuleConverter.discover","title":"<code>discover()</code>","text":"<p>Discover the models in the module.</p> Source code in <code>src/pydantic2django/entrypoint.py</code> <pre><code>def discover(self):\n    \"\"\"\n    Discover the models in the module.\n    \"\"\"\n    self.dataclass_discovery = DataclassDiscovery()\n</code></pre>"},{"location":"reference/pydantic2django/entrypoint/#pydantic2django.entrypoint.ModuleConverter.generate","title":"<code>generate()</code>","text":"<p>Generate Django models for the discovered items (stub).</p> Source code in <code>src/pydantic2django/entrypoint.py</code> <pre><code>def generate(self):\n    \"\"\"Generate Django models for the discovered items (stub).\"\"\"\n    pass\n</code></pre>"},{"location":"reference/pydantic2django/pydantic/","title":"pydantic2django.pydantic","text":""},{"location":"reference/pydantic2django/pydantic/discovery/","title":"pydantic2django.pydantic.discovery","text":""},{"location":"reference/pydantic2django/pydantic/discovery/#pydantic2django.pydantic.discovery.PydanticDiscovery","title":"<code>PydanticDiscovery</code>","text":"<p>               Bases: <code>BaseDiscovery[type[BaseModel]]</code></p> <p>Discovers Pydantic models within specified packages.</p> Source code in <code>src/pydantic2django/pydantic/discovery.py</code> <pre><code>class PydanticDiscovery(BaseDiscovery[type[BaseModel]]):\n    \"\"\"Discovers Pydantic models within specified packages.\"\"\"\n\n    # __init__ is inherited and sufficient\n\n    def _is_target_model(self, obj: Any) -&gt; bool:\n        \"\"\"Check if an object is a Pydantic BaseModel, excluding the base itself.\"\"\"\n        return inspect.isclass(obj) and issubclass(obj, BaseModel) and obj is not BaseModel\n\n    def _default_eligibility_filter(self, model: type[BaseModel]) -&gt; bool:\n        \"\"\"Check default eligibility: not abstract and not inheriting directly from ABC.\"\"\"\n        # Skip models that directly inherit from ABC\n        if abc.ABC in model.__bases__:\n            logger.debug(f\"Filtering out {model.__name__} (inherits directly from ABC)\")\n            return False\n\n        # Skip models that are marked as abstract\n        if getattr(model, \"__abstract__\", False):\n            logger.debug(f\"Filtering out {model.__name__} (marked as __abstract__)\")\n            return False\n\n        # Example for potentially filtering Pydantic internal models (uncomment if needed)\n        # if model.__module__.startswith('pydantic._internal'):\n        #     logger.debug(f\"Filtering out internal Pydantic model: {model.__name__}\")\n        #     return False\n\n        return True  # Eligible by default\n\n    def discover_models(\n        self,\n        packages: list[str],\n        app_label: str,\n        user_filters: Optional[\n            Union[Callable[[type[BaseModel]], bool], list[Callable[[type[BaseModel]], bool]]]\n        ] = None,\n    ):\n        \"\"\"Discover Pydantic models in the specified packages, applying filters.\"\"\"\n        # Pass user_filters directly to the base class method\n        super().discover_models(packages, app_label, user_filters=user_filters)\n\n    # --- analyze_dependencies and get_models_in_registration_order remain ---\n\n    def analyze_dependencies(self) -&gt; None:\n        \"\"\"Build the dependency graph for the filtered Pydantic models.\"\"\"\n        logger.info(\"Analyzing dependencies between filtered Pydantic models...\")\n        self.dependencies: dict[type[BaseModel], set[type[BaseModel]]] = {}\n\n        filtered_model_qualnames = set(self.filtered_models.keys())\n\n        def _find_and_add_dependency(model_type: type[BaseModel], potential_dep_type: Any):\n            if not self._is_target_model(potential_dep_type):\n                return\n\n            dep_qualname = f\"{potential_dep_type.__module__}.{potential_dep_type.__name__}\"\n\n            if dep_qualname in filtered_model_qualnames and potential_dep_type is not model_type:\n                dep_model_obj = self.filtered_models.get(dep_qualname)\n                if dep_model_obj:\n                    if model_type in self.dependencies:\n                        self.dependencies[model_type].add(dep_model_obj)\n                    else:\n                        # Initialize if missing (shouldn't happen often with new base discover_models)\n                        logger.warning(\n                            f\"Model {model_type.__name__} wasn't pre-initialized in dependencies dict during analysis. Initializing now.\"\n                        )\n                        self.dependencies[model_type] = {dep_model_obj}\n                else:\n                    logger.warning(\n                        f\"Inconsistency: Dependency '{dep_qualname}' for model '{model_type.__name__}' found by name but not object in filtered set.\"\n                    )\n\n        # Initialize keys based on filtered models (important step)\n        for model_type in self.filtered_models.values():\n            self.dependencies[model_type] = set()\n\n        # Analyze fields\n        for model_type in self.filtered_models.values():\n            for field in model_type.model_fields.values():\n                annotation = field.annotation\n                if annotation is None:\n                    continue\n\n                origin = get_origin(annotation)\n                args = get_args(annotation)\n\n                if origin is Union and type(None) in args and len(args) == 2:\n                    annotation = next(arg for arg in args if arg is not type(None))\n                    origin = get_origin(annotation)\n                    args = get_args(annotation)\n\n                _find_and_add_dependency(model_type, annotation)\n\n                if origin in (list, dict, set, tuple):\n                    for arg in args:\n                        arg_origin = get_origin(arg)\n                        arg_args = get_args(arg)\n\n                        if arg_origin is Union and type(None) in arg_args and len(arg_args) == 2:\n                            nested_model_type = next(t for t in arg_args if t is not type(None))\n                            _find_and_add_dependency(model_type, nested_model_type)\n                        else:\n                            _find_and_add_dependency(model_type, arg)\n\n        logger.info(\"Dependency analysis complete.\")\n        # Debug logging moved inside BaseDiscovery\n\n    def get_models_in_registration_order(self) -&gt; list[type[BaseModel]]:\n        \"\"\"\n        Return models sorted topologically based on dependencies.\n        Models with no dependencies come first.\n        \"\"\"\n        if not self.dependencies:\n            logger.warning(\"No dependencies found or analyzed, returning Pydantic models in arbitrary order.\")\n            return list(self.filtered_models.values())\n\n        sorted_models = []\n        visited: set[type[BaseModel]] = set()\n        visiting: set[type[BaseModel]] = set()\n        filtered_model_objects = set(self.filtered_models.values())\n\n        def visit(model: type[BaseModel]):\n            if model in visited:\n                return\n            if model in visiting:\n                logger.error(f\"Circular dependency detected involving Pydantic model {model.__name__}\")\n                # Option: raise TypeError(...)\n                return  # Break cycle\n\n            visiting.add(model)\n\n            if model in self.dependencies:\n                # Use .get for safety, ensure deps are also in filtered set\n                for dep in self.dependencies.get(model, set()):\n                    if dep in filtered_model_objects:\n                        visit(dep)\n\n            visiting.remove(model)\n            visited.add(model)\n            sorted_models.append(model)\n\n        all_target_models = list(self.filtered_models.values())\n        for model in all_target_models:\n            if model not in visited:\n                visit(model)\n\n        logger.info(f\"Pydantic models sorted for registration: {[m.__name__ for m in sorted_models]}\")\n        return sorted_models\n</code></pre>"},{"location":"reference/pydantic2django/pydantic/discovery/#pydantic2django.pydantic.discovery.PydanticDiscovery.analyze_dependencies","title":"<code>analyze_dependencies()</code>","text":"<p>Build the dependency graph for the filtered Pydantic models.</p> Source code in <code>src/pydantic2django/pydantic/discovery.py</code> <pre><code>def analyze_dependencies(self) -&gt; None:\n    \"\"\"Build the dependency graph for the filtered Pydantic models.\"\"\"\n    logger.info(\"Analyzing dependencies between filtered Pydantic models...\")\n    self.dependencies: dict[type[BaseModel], set[type[BaseModel]]] = {}\n\n    filtered_model_qualnames = set(self.filtered_models.keys())\n\n    def _find_and_add_dependency(model_type: type[BaseModel], potential_dep_type: Any):\n        if not self._is_target_model(potential_dep_type):\n            return\n\n        dep_qualname = f\"{potential_dep_type.__module__}.{potential_dep_type.__name__}\"\n\n        if dep_qualname in filtered_model_qualnames and potential_dep_type is not model_type:\n            dep_model_obj = self.filtered_models.get(dep_qualname)\n            if dep_model_obj:\n                if model_type in self.dependencies:\n                    self.dependencies[model_type].add(dep_model_obj)\n                else:\n                    # Initialize if missing (shouldn't happen often with new base discover_models)\n                    logger.warning(\n                        f\"Model {model_type.__name__} wasn't pre-initialized in dependencies dict during analysis. Initializing now.\"\n                    )\n                    self.dependencies[model_type] = {dep_model_obj}\n            else:\n                logger.warning(\n                    f\"Inconsistency: Dependency '{dep_qualname}' for model '{model_type.__name__}' found by name but not object in filtered set.\"\n                )\n\n    # Initialize keys based on filtered models (important step)\n    for model_type in self.filtered_models.values():\n        self.dependencies[model_type] = set()\n\n    # Analyze fields\n    for model_type in self.filtered_models.values():\n        for field in model_type.model_fields.values():\n            annotation = field.annotation\n            if annotation is None:\n                continue\n\n            origin = get_origin(annotation)\n            args = get_args(annotation)\n\n            if origin is Union and type(None) in args and len(args) == 2:\n                annotation = next(arg for arg in args if arg is not type(None))\n                origin = get_origin(annotation)\n                args = get_args(annotation)\n\n            _find_and_add_dependency(model_type, annotation)\n\n            if origin in (list, dict, set, tuple):\n                for arg in args:\n                    arg_origin = get_origin(arg)\n                    arg_args = get_args(arg)\n\n                    if arg_origin is Union and type(None) in arg_args and len(arg_args) == 2:\n                        nested_model_type = next(t for t in arg_args if t is not type(None))\n                        _find_and_add_dependency(model_type, nested_model_type)\n                    else:\n                        _find_and_add_dependency(model_type, arg)\n\n    logger.info(\"Dependency analysis complete.\")\n</code></pre>"},{"location":"reference/pydantic2django/pydantic/discovery/#pydantic2django.pydantic.discovery.PydanticDiscovery.discover_models","title":"<code>discover_models(packages, app_label, user_filters=None)</code>","text":"<p>Discover Pydantic models in the specified packages, applying filters.</p> Source code in <code>src/pydantic2django/pydantic/discovery.py</code> <pre><code>def discover_models(\n    self,\n    packages: list[str],\n    app_label: str,\n    user_filters: Optional[\n        Union[Callable[[type[BaseModel]], bool], list[Callable[[type[BaseModel]], bool]]]\n    ] = None,\n):\n    \"\"\"Discover Pydantic models in the specified packages, applying filters.\"\"\"\n    # Pass user_filters directly to the base class method\n    super().discover_models(packages, app_label, user_filters=user_filters)\n</code></pre>"},{"location":"reference/pydantic2django/pydantic/discovery/#pydantic2django.pydantic.discovery.PydanticDiscovery.get_models_in_registration_order","title":"<code>get_models_in_registration_order()</code>","text":"<p>Return models sorted topologically based on dependencies. Models with no dependencies come first.</p> Source code in <code>src/pydantic2django/pydantic/discovery.py</code> <pre><code>def get_models_in_registration_order(self) -&gt; list[type[BaseModel]]:\n    \"\"\"\n    Return models sorted topologically based on dependencies.\n    Models with no dependencies come first.\n    \"\"\"\n    if not self.dependencies:\n        logger.warning(\"No dependencies found or analyzed, returning Pydantic models in arbitrary order.\")\n        return list(self.filtered_models.values())\n\n    sorted_models = []\n    visited: set[type[BaseModel]] = set()\n    visiting: set[type[BaseModel]] = set()\n    filtered_model_objects = set(self.filtered_models.values())\n\n    def visit(model: type[BaseModel]):\n        if model in visited:\n            return\n        if model in visiting:\n            logger.error(f\"Circular dependency detected involving Pydantic model {model.__name__}\")\n            # Option: raise TypeError(...)\n            return  # Break cycle\n\n        visiting.add(model)\n\n        if model in self.dependencies:\n            # Use .get for safety, ensure deps are also in filtered set\n            for dep in self.dependencies.get(model, set()):\n                if dep in filtered_model_objects:\n                    visit(dep)\n\n        visiting.remove(model)\n        visited.add(model)\n        sorted_models.append(model)\n\n    all_target_models = list(self.filtered_models.values())\n    for model in all_target_models:\n        if model not in visited:\n            visit(model)\n\n    logger.info(f\"Pydantic models sorted for registration: {[m.__name__ for m in sorted_models]}\")\n    return sorted_models\n</code></pre>"},{"location":"reference/pydantic2django/pydantic/factory/","title":"pydantic2django.pydantic.factory","text":""},{"location":"reference/pydantic2django/pydantic/factory/#pydantic2django.pydantic.factory.PydanticFieldFactory","title":"<code>PydanticFieldFactory</code>","text":"<p>               Bases: <code>BaseFieldFactory[FieldInfo]</code></p> <p>Creates Django fields from Pydantic fields (FieldInfo).</p> Source code in <code>src/pydantic2django/pydantic/factory.py</code> <pre><code>class PydanticFieldFactory(BaseFieldFactory[FieldInfo]):\n    \"\"\"Creates Django fields from Pydantic fields (FieldInfo).\"\"\"\n\n    # Dependencies injected\n    relationship_accessor: RelationshipConversionAccessor\n    bidirectional_mapper: BidirectionalTypeMapper\n\n    def __init__(\n        self, relationship_accessor: RelationshipConversionAccessor, bidirectional_mapper: BidirectionalTypeMapper\n    ):\n        \"\"\"Initializes with dependencies.\"\"\"\n        self.relationship_accessor = relationship_accessor\n        self.bidirectional_mapper = bidirectional_mapper\n        # No super().__init__() needed\n\n    def create_field(\n        self, field_info: FieldInfo, model_name: str, carrier: ConversionCarrier[type[BaseModel]]\n    ) -&gt; FieldConversionResult:\n        \"\"\"\n        Convert a Pydantic FieldInfo to a Django field instance.\n        Implements the abstract method from BaseFieldFactory.\n        Uses BidirectionalTypeMapper and local instantiation.\n        \"\"\"\n        # Use alias first, then the actual key from model_fields as name\n        field_name = field_info.alias or next(\n            (k for k, v in carrier.source_model.model_fields.items() if v is field_info), \"&lt;unknown&gt;\"\n        )\n\n        # Initialize result with the source field info and determined name\n        result = FieldConversionResult(field_info=field_info, field_name=field_name)\n\n        try:\n            # Handle potential 'id' field conflict\n            if id_field := self._handle_id_field(field_name, field_info):\n                result.django_field = id_field\n                # Need to capture kwargs for serialization if possible\n                # For now, assume default kwargs for ID fields\n                # TODO: Extract actual kwargs used in _handle_id_field\n                result.field_kwargs = {\"primary_key\": True}\n                if isinstance(id_field, models.CharField):\n                    result.field_kwargs[\"max_length\"] = getattr(id_field, \"max_length\", 255)\n                elif isinstance(id_field, models.UUIDField):\n                    pass  # No extra kwargs needed typically\n                else:  # AutoField\n                    pass  # No extra kwargs needed typically\n\n                result.field_definition_str = self._generate_field_def_string(result, carrier.meta_app_label)\n                return result  # ID field handled, return early\n\n            # Get field type from annotation\n            field_type = field_info.annotation\n            if field_type is None:\n                logger.warning(f\"Field '{model_name}.{field_name}' has no annotation, treating as context field.\")\n                result.context_field = field_info\n                return result\n\n            # --- Use BidirectionalTypeMapper --- #\n            try:\n                django_field_class, constructor_kwargs = self.bidirectional_mapper.get_django_mapping(\n                    python_type=field_type, field_info=field_info\n                )\n            except MappingError as e:\n                # Handle errors specifically from the mapper (e.g., missing relationship)\n                logger.error(f\"Mapping error for '{model_name}.{field_name}' (type: {field_type}): {e}\")\n                result.error_str = str(e)\n                result.context_field = field_info  # Treat as context on mapping error\n                return result\n            except Exception as e:\n                # Handle unexpected errors during mapping lookup\n                logger.error(\n                    f\"Unexpected error getting Django mapping for '{model_name}.{field_name}': {e}\", exc_info=True\n                )\n                result.error_str = f\"Unexpected mapping error: {e}\"\n                result.context_field = field_info\n                return result\n\n            # Store raw kwargs before modifications/checks\n            result.raw_mapper_kwargs = constructor_kwargs.copy()\n\n            # --- Check for Multi-FK Union Signal --- #\n            union_details = constructor_kwargs.pop(\"_union_details\", None)\n            if union_details and isinstance(union_details, dict):\n                # If GFK mode is enabled and policy says to use it, record as pending GFK child\n                if getattr(carrier, \"enable_gfk\", False) and self._should_route_to_gfk(union_details, carrier):\n                    logger.info(\n                        f\"[GFK] Routing union field '{field_name}' on '{model_name}' to GenericEntry (policy={carrier.gfk_policy}).\"\n                    )\n                    carrier.pending_gfk_children.append(\n                        {\n                            \"field_name\": field_name,\n                            \"union_details\": union_details,\n                            \"model_name\": model_name,\n                        }\n                    )\n                    # Do not generate a concrete field for this union\n                    return result\n                # Otherwise, fall back to existing multi-FK behavior\n                logger.info(f\"Detected multi-FK union signal for '{field_name}'. Deferring field generation.\")\n                # Store the original field name and the details for the generator\n                carrier.pending_multi_fk_unions.append((field_name, union_details))\n                return result  # Return early, deferring generation\n\n            # --- Check for GFK placeholder signal from mapper --- #\n            gfk_details = constructor_kwargs.pop(\"_gfk_details\", None)\n            if gfk_details and isinstance(gfk_details, dict):\n                if getattr(carrier, \"enable_gfk\", False):\n                    logger.info(\n                        f\"[GFK] Mapper signaled GFK for '{field_name}' on '{model_name}'. Recording as pending GFK child.\"\n                    )\n                    carrier.pending_gfk_children.append(\n                        {\n                            \"field_name\": field_name,\n                            \"gfk_details\": gfk_details,\n                            \"model_name\": model_name,\n                        }\n                    )\n                    # Do not generate a concrete field\n                    return result\n                else:\n                    logger.warning(\n                        f\"Received _gfk_details for '{field_name}' but enable_gfk is False. Falling back to JSON field.\"\n                    )\n\n            # --- Handle Relationships Specifically (Adjust Kwargs) --- #\n            # Check if it's a relationship type *after* getting mapping AND checking for union signal\n            is_relationship = issubclass(\n                django_field_class, (models.ForeignKey, models.OneToOneField, models.ManyToManyField)\n            )\n\n            if is_relationship:\n                # Apply specific relationship logic (like related_name uniqueness)\n                # The mapper should have set 'to' and basic 'on_delete'\n                if \"to\" not in constructor_kwargs:\n                    # This indicates an issue in the mapper or relationship accessor setup\n                    result.error_str = f\"Mapper failed to determine 'to' for relationship field '{field_name}'.\"\n                    logger.error(result.error_str)\n                    result.context_field = field_info\n                    return result\n\n                # Sanitize and ensure unique related_name\n                # Check Pydantic Field(..., json_schema_extra={\"related_name\": ...})\n                user_related_name = (\n                    field_info.json_schema_extra.get(\"related_name\")\n                    if isinstance(field_info.json_schema_extra, dict)\n                    else None\n                )\n                target_django_model_str = constructor_kwargs[\"to\"]  # Mapper returns string like app_label.ModelName\n\n                # Try to get the actual target model class to pass to sanitize_related_name if possible\n                # This relies on the target model being importable/available\n                target_model_cls = None\n                target_model_cls_name_only = target_django_model_str  # Default fallback\n                try:\n                    app_label, model_cls_name = target_django_model_str.split(\".\")\n                    target_model_cls = apps.get_model(app_label, model_cls_name)  # Use apps.get_model\n                    target_model_cls_name_only = model_cls_name  # Use name from split\n                except Exception:\n                    logger.warning(\n                        f\"Could not get target model class for '{target_django_model_str}' when generating related_name for '{field_name}'. Using model name string.\"\n                    )\n                    # Fallback: try splitting by dot just for name, otherwise use whole string\n                    target_model_cls_name_only = target_django_model_str.split(\".\")[-1]\n\n                related_name_base = (\n                    user_related_name\n                    if user_related_name\n                    else f\"{carrier.source_model.__name__.lower()}_{field_name}_set\"\n                )\n                final_related_name_base = sanitize_related_name(\n                    str(related_name_base),\n                    target_model_cls.__name__ if target_model_cls else target_model_cls_name_only,\n                    field_name,\n                )\n\n                # Ensure uniqueness using carrier's tracker\n                target_model_key_for_tracker = (\n                    target_model_cls.__name__ if target_model_cls else target_django_model_str\n                )\n                target_related_names = carrier.used_related_names_per_target.setdefault(\n                    target_model_key_for_tracker, set()\n                )\n                unique_related_name = final_related_name_base\n                counter = 1\n                while unique_related_name in target_related_names:\n                    unique_related_name = f\"{final_related_name_base}_{counter}\"\n                    counter += 1\n                target_related_names.add(unique_related_name)\n                constructor_kwargs[\"related_name\"] = unique_related_name\n                logger.debug(f\"[REL] Field '{field_name}': Assigning related_name='{unique_related_name}'\")\n\n                # Re-confirm on_delete (mapper should set default based on Optional)\n                if (\n                    django_field_class in (models.ForeignKey, models.OneToOneField)\n                    and \"on_delete\" not in constructor_kwargs\n                ):\n                    is_optional = is_pydantic_model_field_optional(field_type)\n                    constructor_kwargs[\"on_delete\"] = models.SET_NULL if is_optional else models.CASCADE\n                elif django_field_class == models.ManyToManyField:\n                    constructor_kwargs.pop(\"on_delete\", None)\n                    # M2M doesn't use null=True, mapper handles this\n                    constructor_kwargs.pop(\"null\", None)\n                    constructor_kwargs[\"blank\"] = constructor_kwargs.get(\"blank\", True)  # M2M usually blank=True\n\n            # --- Perform Instantiation Locally --- #\n            try:\n                logger.debug(\n                    f\"Instantiating {django_field_class.__name__} for '{field_name}' with kwargs: {constructor_kwargs}\"\n                )\n                result.django_field = django_field_class(**constructor_kwargs)\n                result.field_kwargs = constructor_kwargs  # Store final kwargs\n            except Exception as e:\n                error_msg = f\"Failed to instantiate Django field '{field_name}' (type: {django_field_class.__name__}) with kwargs {constructor_kwargs}: {e}\"\n                logger.error(error_msg, exc_info=True)\n                result.error_str = error_msg\n                result.context_field = field_info  # Fallback to context\n                return result\n\n            # --- Generate Field Definition String --- #\n            result.field_definition_str = self._generate_field_def_string(result, carrier.meta_app_label)\n\n            return result  # Success\n\n        except Exception as e:\n            # Catch-all for unexpected errors during conversion\n            error_msg = f\"Unexpected error converting field '{model_name}.{field_name}': {e}\"\n            logger.error(error_msg, exc_info=True)\n            result.error_str = error_msg\n            result.context_field = field_info  # Fallback to context\n            return result\n\n    def _should_route_to_gfk(self, union_details: dict, carrier: ConversionCarrier[type[BaseModel]]) -&gt; bool:\n        \"\"\"Return True if this union field should be handled via GFK based on carrier policy.\n\n        For now, support simple policies:\n        - \"all_nested\": always route\n        - \"threshold_by_children\": route when number of union models &gt;= gfk_threshold_children\n        Otherwise: False.\n        \"\"\"\n        try:\n            policy = (carrier.gfk_policy or \"\").strip()\n            if policy == \"all_nested\":\n                return True\n            if policy == \"threshold_by_children\":\n                threshold = carrier.gfk_threshold_children or 0\n                models_in_union = len(union_details.get(\"models\", []) or [])\n                return models_in_union &gt;= threshold if threshold &gt; 0 else False\n        except Exception:\n            pass\n        return False\n\n    def _generate_field_def_string(self, result: FieldConversionResult, app_label: str) -&gt; str:\n        \"\"\"Generates the field definition string safely.\"\"\"\n        if not result.django_field:\n            return \"# Field generation failed\"\n        try:\n            if result.field_kwargs:\n                return generate_field_definition_string(type(result.django_field), result.field_kwargs, app_label)\n            else:\n                logger.warning(\n                    f\"Could not generate definition string for '{result.field_name}': final kwargs not found in result. Using basic serialization.\"\n                )\n                return FieldSerializer.serialize_field(result.django_field)\n        except Exception as e:\n            logger.error(\n                f\"Failed to generate field definition string for '{result.field_name}': {e}\",\n                exc_info=True,\n            )\n            return f\"# Error generating definition: {e}\"\n\n    def _handle_id_field(self, field_name: str, field_info: FieldInfo) -&gt; Optional[models.Field]:\n        \"\"\"Handle potential ID field naming conflicts (logic moved from original factory).\"\"\"\n        if field_name.lower() == \"id\":\n            field_type = field_info.annotation\n            # Default to AutoField unless explicitly specified by type\n            field_class = models.AutoField\n            field_kwargs = {\"primary_key\": True, \"verbose_name\": \"ID\"}\n\n            # Use mapper to find appropriate Django PK field if type is specified\n            # But only override AutoField if it's clearly not a standard int sequence\n            pk_field_class_override = None\n            if field_type is UUID:\n                pk_field_class_override = models.UUIDField\n                field_kwargs.pop(\"verbose_name\")  # UUIDField doesn't need verbose_name='ID'\n            elif field_type is str:\n                # Default Pydantic str ID to CharField PK\n                pk_field_class_override = models.CharField\n                field_kwargs[\"max_length\"] = 255  # Default length\n            elif field_type is int:\n                pass  # Default AutoField is fine\n            elif field_type:\n                # Check if mapper finds a specific non-auto int field (e.g., BigIntegerField)\n                try:\n                    mapped_cls, mapped_kwargs = self.bidirectional_mapper.get_django_mapping(field_type, field_info)\n                    if issubclass(mapped_cls, models.IntegerField) and not issubclass(mapped_cls, models.AutoField):\n                        pk_field_class_override = mapped_cls\n                        field_kwargs.update(mapped_kwargs)\n                        # Ensure primary_key=True is set\n                        field_kwargs[\"primary_key\"] = True\n                    elif not issubclass(mapped_cls, models.AutoField):\n                        logger.warning(\n                            f\"Field 'id' has type {field_type} mapping to non-integer {mapped_cls.__name__}. Using AutoField PK.\"\n                        )\n                except MappingError:\n                    logger.warning(f\"Field 'id' has unmappable type {field_type}. Using AutoField PK.\")\n\n            if pk_field_class_override:\n                field_class = pk_field_class_override\n            else:\n                # Stick with AutoField, apply title if present\n                if field_info.title:\n                    field_kwargs[\"verbose_name\"] = field_info.title\n\n            logger.debug(f\"Handling field '{field_name}' as primary key using {field_class.__name__}\")\n            # Instantiate the ID field\n            try:\n                return field_class(**field_kwargs)\n            except Exception as e:\n                logger.error(\n                    f\"Failed to instantiate ID field {field_class.__name__} with kwargs {field_kwargs}: {e}\",\n                    exc_info=True,\n                )\n                # Fallback to basic AutoField? Or let error propagate?\n                # Let's return None and let the main create_field handle error reporting\n                return None\n        return None\n</code></pre>"},{"location":"reference/pydantic2django/pydantic/factory/#pydantic2django.pydantic.factory.PydanticFieldFactory.__init__","title":"<code>__init__(relationship_accessor, bidirectional_mapper)</code>","text":"<p>Initializes with dependencies.</p> Source code in <code>src/pydantic2django/pydantic/factory.py</code> <pre><code>def __init__(\n    self, relationship_accessor: RelationshipConversionAccessor, bidirectional_mapper: BidirectionalTypeMapper\n):\n    \"\"\"Initializes with dependencies.\"\"\"\n    self.relationship_accessor = relationship_accessor\n    self.bidirectional_mapper = bidirectional_mapper\n</code></pre>"},{"location":"reference/pydantic2django/pydantic/factory/#pydantic2django.pydantic.factory.PydanticFieldFactory.create_field","title":"<code>create_field(field_info, model_name, carrier)</code>","text":"<p>Convert a Pydantic FieldInfo to a Django field instance. Implements the abstract method from BaseFieldFactory. Uses BidirectionalTypeMapper and local instantiation.</p> Source code in <code>src/pydantic2django/pydantic/factory.py</code> <pre><code>def create_field(\n    self, field_info: FieldInfo, model_name: str, carrier: ConversionCarrier[type[BaseModel]]\n) -&gt; FieldConversionResult:\n    \"\"\"\n    Convert a Pydantic FieldInfo to a Django field instance.\n    Implements the abstract method from BaseFieldFactory.\n    Uses BidirectionalTypeMapper and local instantiation.\n    \"\"\"\n    # Use alias first, then the actual key from model_fields as name\n    field_name = field_info.alias or next(\n        (k for k, v in carrier.source_model.model_fields.items() if v is field_info), \"&lt;unknown&gt;\"\n    )\n\n    # Initialize result with the source field info and determined name\n    result = FieldConversionResult(field_info=field_info, field_name=field_name)\n\n    try:\n        # Handle potential 'id' field conflict\n        if id_field := self._handle_id_field(field_name, field_info):\n            result.django_field = id_field\n            # Need to capture kwargs for serialization if possible\n            # For now, assume default kwargs for ID fields\n            # TODO: Extract actual kwargs used in _handle_id_field\n            result.field_kwargs = {\"primary_key\": True}\n            if isinstance(id_field, models.CharField):\n                result.field_kwargs[\"max_length\"] = getattr(id_field, \"max_length\", 255)\n            elif isinstance(id_field, models.UUIDField):\n                pass  # No extra kwargs needed typically\n            else:  # AutoField\n                pass  # No extra kwargs needed typically\n\n            result.field_definition_str = self._generate_field_def_string(result, carrier.meta_app_label)\n            return result  # ID field handled, return early\n\n        # Get field type from annotation\n        field_type = field_info.annotation\n        if field_type is None:\n            logger.warning(f\"Field '{model_name}.{field_name}' has no annotation, treating as context field.\")\n            result.context_field = field_info\n            return result\n\n        # --- Use BidirectionalTypeMapper --- #\n        try:\n            django_field_class, constructor_kwargs = self.bidirectional_mapper.get_django_mapping(\n                python_type=field_type, field_info=field_info\n            )\n        except MappingError as e:\n            # Handle errors specifically from the mapper (e.g., missing relationship)\n            logger.error(f\"Mapping error for '{model_name}.{field_name}' (type: {field_type}): {e}\")\n            result.error_str = str(e)\n            result.context_field = field_info  # Treat as context on mapping error\n            return result\n        except Exception as e:\n            # Handle unexpected errors during mapping lookup\n            logger.error(\n                f\"Unexpected error getting Django mapping for '{model_name}.{field_name}': {e}\", exc_info=True\n            )\n            result.error_str = f\"Unexpected mapping error: {e}\"\n            result.context_field = field_info\n            return result\n\n        # Store raw kwargs before modifications/checks\n        result.raw_mapper_kwargs = constructor_kwargs.copy()\n\n        # --- Check for Multi-FK Union Signal --- #\n        union_details = constructor_kwargs.pop(\"_union_details\", None)\n        if union_details and isinstance(union_details, dict):\n            # If GFK mode is enabled and policy says to use it, record as pending GFK child\n            if getattr(carrier, \"enable_gfk\", False) and self._should_route_to_gfk(union_details, carrier):\n                logger.info(\n                    f\"[GFK] Routing union field '{field_name}' on '{model_name}' to GenericEntry (policy={carrier.gfk_policy}).\"\n                )\n                carrier.pending_gfk_children.append(\n                    {\n                        \"field_name\": field_name,\n                        \"union_details\": union_details,\n                        \"model_name\": model_name,\n                    }\n                )\n                # Do not generate a concrete field for this union\n                return result\n            # Otherwise, fall back to existing multi-FK behavior\n            logger.info(f\"Detected multi-FK union signal for '{field_name}'. Deferring field generation.\")\n            # Store the original field name and the details for the generator\n            carrier.pending_multi_fk_unions.append((field_name, union_details))\n            return result  # Return early, deferring generation\n\n        # --- Check for GFK placeholder signal from mapper --- #\n        gfk_details = constructor_kwargs.pop(\"_gfk_details\", None)\n        if gfk_details and isinstance(gfk_details, dict):\n            if getattr(carrier, \"enable_gfk\", False):\n                logger.info(\n                    f\"[GFK] Mapper signaled GFK for '{field_name}' on '{model_name}'. Recording as pending GFK child.\"\n                )\n                carrier.pending_gfk_children.append(\n                    {\n                        \"field_name\": field_name,\n                        \"gfk_details\": gfk_details,\n                        \"model_name\": model_name,\n                    }\n                )\n                # Do not generate a concrete field\n                return result\n            else:\n                logger.warning(\n                    f\"Received _gfk_details for '{field_name}' but enable_gfk is False. Falling back to JSON field.\"\n                )\n\n        # --- Handle Relationships Specifically (Adjust Kwargs) --- #\n        # Check if it's a relationship type *after* getting mapping AND checking for union signal\n        is_relationship = issubclass(\n            django_field_class, (models.ForeignKey, models.OneToOneField, models.ManyToManyField)\n        )\n\n        if is_relationship:\n            # Apply specific relationship logic (like related_name uniqueness)\n            # The mapper should have set 'to' and basic 'on_delete'\n            if \"to\" not in constructor_kwargs:\n                # This indicates an issue in the mapper or relationship accessor setup\n                result.error_str = f\"Mapper failed to determine 'to' for relationship field '{field_name}'.\"\n                logger.error(result.error_str)\n                result.context_field = field_info\n                return result\n\n            # Sanitize and ensure unique related_name\n            # Check Pydantic Field(..., json_schema_extra={\"related_name\": ...})\n            user_related_name = (\n                field_info.json_schema_extra.get(\"related_name\")\n                if isinstance(field_info.json_schema_extra, dict)\n                else None\n            )\n            target_django_model_str = constructor_kwargs[\"to\"]  # Mapper returns string like app_label.ModelName\n\n            # Try to get the actual target model class to pass to sanitize_related_name if possible\n            # This relies on the target model being importable/available\n            target_model_cls = None\n            target_model_cls_name_only = target_django_model_str  # Default fallback\n            try:\n                app_label, model_cls_name = target_django_model_str.split(\".\")\n                target_model_cls = apps.get_model(app_label, model_cls_name)  # Use apps.get_model\n                target_model_cls_name_only = model_cls_name  # Use name from split\n            except Exception:\n                logger.warning(\n                    f\"Could not get target model class for '{target_django_model_str}' when generating related_name for '{field_name}'. Using model name string.\"\n                )\n                # Fallback: try splitting by dot just for name, otherwise use whole string\n                target_model_cls_name_only = target_django_model_str.split(\".\")[-1]\n\n            related_name_base = (\n                user_related_name\n                if user_related_name\n                else f\"{carrier.source_model.__name__.lower()}_{field_name}_set\"\n            )\n            final_related_name_base = sanitize_related_name(\n                str(related_name_base),\n                target_model_cls.__name__ if target_model_cls else target_model_cls_name_only,\n                field_name,\n            )\n\n            # Ensure uniqueness using carrier's tracker\n            target_model_key_for_tracker = (\n                target_model_cls.__name__ if target_model_cls else target_django_model_str\n            )\n            target_related_names = carrier.used_related_names_per_target.setdefault(\n                target_model_key_for_tracker, set()\n            )\n            unique_related_name = final_related_name_base\n            counter = 1\n            while unique_related_name in target_related_names:\n                unique_related_name = f\"{final_related_name_base}_{counter}\"\n                counter += 1\n            target_related_names.add(unique_related_name)\n            constructor_kwargs[\"related_name\"] = unique_related_name\n            logger.debug(f\"[REL] Field '{field_name}': Assigning related_name='{unique_related_name}'\")\n\n            # Re-confirm on_delete (mapper should set default based on Optional)\n            if (\n                django_field_class in (models.ForeignKey, models.OneToOneField)\n                and \"on_delete\" not in constructor_kwargs\n            ):\n                is_optional = is_pydantic_model_field_optional(field_type)\n                constructor_kwargs[\"on_delete\"] = models.SET_NULL if is_optional else models.CASCADE\n            elif django_field_class == models.ManyToManyField:\n                constructor_kwargs.pop(\"on_delete\", None)\n                # M2M doesn't use null=True, mapper handles this\n                constructor_kwargs.pop(\"null\", None)\n                constructor_kwargs[\"blank\"] = constructor_kwargs.get(\"blank\", True)  # M2M usually blank=True\n\n        # --- Perform Instantiation Locally --- #\n        try:\n            logger.debug(\n                f\"Instantiating {django_field_class.__name__} for '{field_name}' with kwargs: {constructor_kwargs}\"\n            )\n            result.django_field = django_field_class(**constructor_kwargs)\n            result.field_kwargs = constructor_kwargs  # Store final kwargs\n        except Exception as e:\n            error_msg = f\"Failed to instantiate Django field '{field_name}' (type: {django_field_class.__name__}) with kwargs {constructor_kwargs}: {e}\"\n            logger.error(error_msg, exc_info=True)\n            result.error_str = error_msg\n            result.context_field = field_info  # Fallback to context\n            return result\n\n        # --- Generate Field Definition String --- #\n        result.field_definition_str = self._generate_field_def_string(result, carrier.meta_app_label)\n\n        return result  # Success\n\n    except Exception as e:\n        # Catch-all for unexpected errors during conversion\n        error_msg = f\"Unexpected error converting field '{model_name}.{field_name}': {e}\"\n        logger.error(error_msg, exc_info=True)\n        result.error_str = error_msg\n        result.context_field = field_info  # Fallback to context\n        return result\n</code></pre>"},{"location":"reference/pydantic2django/pydantic/factory/#pydantic2django.pydantic.factory.PydanticModelFactory","title":"<code>PydanticModelFactory</code>","text":"<p>               Bases: <code>BaseModelFactory[type[BaseModel], FieldInfo]</code></p> <p>Creates Django models from Pydantic models.</p> Source code in <code>src/pydantic2django/pydantic/factory.py</code> <pre><code>class PydanticModelFactory(BaseModelFactory[type[BaseModel], FieldInfo]):\n    \"\"\"Creates Django models from Pydantic models.\"\"\"\n\n    # Cache specific to Pydantic models\n    _converted_models: dict[str, ConversionCarrier[type[BaseModel]]] = {}\n\n    relationship_accessor: RelationshipConversionAccessor\n    # No need for field_factory instance here if Base class handles it\n\n    def __init__(self, field_factory: PydanticFieldFactory, relationship_accessor: RelationshipConversionAccessor):\n        \"\"\"Initialize with field factory and relationship accessor.\"\"\"\n        self.relationship_accessor = relationship_accessor\n        # Pass the field_factory up to the base class\n        super().__init__(field_factory=field_factory)\n\n    # Overrides the base method to add caching and relationship mapping\n    def make_django_model(self, carrier: ConversionCarrier[type[BaseModel]]) -&gt; None:\n        \"\"\"Creates a Django model from Pydantic, checking cache first and mapping relationships.\"\"\"\n        model_key = carrier.model_key()\n        logger.debug(f\"PydanticFactory: Attempting to create Django model for {model_key}\")\n\n        # --- Check Cache --- #\n        if model_key in self._converted_models and not carrier.existing_model:\n            logger.debug(f\"PydanticFactory: Using cached conversion result for {model_key}\")\n            cached_carrier = self._converted_models[model_key]\n            # Update the passed-in carrier with cached results\n            carrier.__dict__.update(cached_carrier.__dict__)\n            # Ensure used_related_names is properly updated (dict update might not merge sets correctly)\n            for target, names in cached_carrier.used_related_names_per_target.items():\n                carrier.used_related_names_per_target.setdefault(target, set()).update(names)\n            return\n\n        # --- Call Base Implementation for Core Logic --- #\n        # This calls _process_source_fields, _assemble_django_model_class etc.\n        super().make_django_model(carrier)\n\n        # --- Register Relationship Mapping (if successful) --- #\n        if carrier.source_model and carrier.django_model:\n            logger.debug(\n                f\"PydanticFactory: Registering mapping for {carrier.source_model.__name__} -&gt; {carrier.django_model.__name__}\"\n            )\n            self.relationship_accessor.map_relationship(\n                source_model=carrier.source_model, django_model=carrier.django_model\n            )\n\n        # --- Cache Result --- #\n        if carrier.django_model and not carrier.existing_model:\n            logger.debug(f\"PydanticFactory: Caching conversion result for {model_key}\")\n            # Store a copy to prevent modification issues? Simple assignment for now.\n            self._converted_models[model_key] = carrier\n        elif not carrier.django_model:\n            logger.error(\n                f\"PydanticFactory: Failed to create Django model for {model_key}. Invalid fields: {carrier.invalid_fields}\"\n            )\n\n    # --- Implementation of Abstract Methods --- #\n\n    def _process_source_fields(self, carrier: ConversionCarrier[type[BaseModel]]):\n        \"\"\"Iterate through Pydantic fields and convert them using the field factory.\"\"\"\n        source_model = carrier.source_model\n        model_name = source_model.__name__\n\n        for field_name_original, field_info in get_model_fields(cast(type[BaseModel], source_model)).items():\n            field_name = field_info.alias or field_name_original\n\n            # Normalize the output Django field identifier consistently with XML/dataclass\n            try:\n                from ..core.utils.naming import sanitize_field_identifier\n\n                normalized_field_name = sanitize_field_identifier(field_name)\n            except Exception:\n                normalized_field_name = field_name\n\n            # Skip 'id' field if updating an existing model definition\n            # Note: _handle_id_field in field factory handles primary key logic\n            if field_name.lower() == \"id\" and carrier.existing_model:\n                logger.debug(f\"Skipping 'id' field for existing model update: {carrier.existing_model.__name__}\")\n                continue\n\n            # Cast needed because BaseFactory uses generic TFieldInfo\n            field_factory_typed = cast(PydanticFieldFactory, self.field_factory)\n            conversion_result = field_factory_typed.create_field(\n                field_info=field_info, model_name=model_name, carrier=carrier\n            )\n\n            # Store results in the carrier\n            if conversion_result.django_field:\n                # Store definition string first\n                if conversion_result.field_definition_str:\n                    carrier.django_field_definitions[normalized_field_name] = conversion_result.field_definition_str\n                else:\n                    logger.warning(\n                        f\"Missing field definition string for successfully created field '{normalized_field_name}'\"\n                    )\n\n                # Store the field instance itself\n                if isinstance(\n                    conversion_result.django_field, (models.ForeignKey, models.ManyToManyField, models.OneToOneField)\n                ):\n                    carrier.relationship_fields[normalized_field_name] = conversion_result.django_field\n                else:\n                    carrier.django_fields[normalized_field_name] = conversion_result.django_field\n\n            elif conversion_result.context_field:\n                carrier.context_fields[normalized_field_name] = conversion_result.context_field\n            elif conversion_result.error_str:\n                carrier.invalid_fields.append((normalized_field_name, conversion_result.error_str))\n            else:\n                # Should not happen if FieldConversionResult is used correctly\n                error = f\"Field factory returned unexpected empty result for {model_name}.{field_name_original}\"\n                logger.error(error)\n                carrier.invalid_fields.append((normalized_field_name, error))\n\n    def _build_pydantic_model_context(self, carrier: ConversionCarrier[type[BaseModel]]):\n        \"\"\"Builds the ModelContext specifically for Pydantic source models.\"\"\"\n        # Renamed to match base class expectation\n        self._build_model_context(carrier)\n\n    # Actual implementation of the abstract method\n    def _build_model_context(self, carrier: ConversionCarrier[type[BaseModel]]):\n        \"\"\"Builds the ModelContext specifically for Pydantic source models.\"\"\"\n        if not carrier.source_model or not carrier.django_model:\n            logger.debug(\"Skipping context build: missing source or django model.\")\n            return\n\n        try:\n            model_context = ModelContext(  # Removed generic type hint for base class compatibility\n                django_model=carrier.django_model,\n                source_class=carrier.source_model,\n            )\n            for field_name, field_info in carrier.context_fields.items():\n                if isinstance(field_info, FieldInfo) and field_info.annotation is not None:\n                    optional = is_pydantic_model_field_optional(field_info.annotation)\n                    # Use repr() for field_type_str as expected by ModelContext.add_field\n                    field_type_str = repr(field_info.annotation)\n                    model_context.add_field(\n                        field_name=field_name,\n                        field_type_str=field_type_str,  # Pass string representation\n                        is_optional=optional,\n                        annotation=field_info.annotation,  # Keep annotation if needed elsewhere\n                    )\n                elif isinstance(field_info, FieldInfo):\n                    logger.warning(f\"Context field '{field_name}' has no annotation, cannot add to ModelContext.\")\n                else:\n                    logger.warning(\n                        f\"Context field '{field_name}' is not a FieldInfo ({type(field_info)}), cannot add to ModelContext.\"\n                    )\n            carrier.model_context = model_context\n            logger.debug(f\"Successfully built ModelContext for {carrier.model_key()}\")  # Call model_key()\n        except Exception as e:\n            logger.error(f\"Failed to build ModelContext for {carrier.model_key()}: {e}\", exc_info=True)\n            carrier.model_context = None\n</code></pre>"},{"location":"reference/pydantic2django/pydantic/factory/#pydantic2django.pydantic.factory.PydanticModelFactory.__init__","title":"<code>__init__(field_factory, relationship_accessor)</code>","text":"<p>Initialize with field factory and relationship accessor.</p> Source code in <code>src/pydantic2django/pydantic/factory.py</code> <pre><code>def __init__(self, field_factory: PydanticFieldFactory, relationship_accessor: RelationshipConversionAccessor):\n    \"\"\"Initialize with field factory and relationship accessor.\"\"\"\n    self.relationship_accessor = relationship_accessor\n    # Pass the field_factory up to the base class\n    super().__init__(field_factory=field_factory)\n</code></pre>"},{"location":"reference/pydantic2django/pydantic/factory/#pydantic2django.pydantic.factory.PydanticModelFactory.make_django_model","title":"<code>make_django_model(carrier)</code>","text":"<p>Creates a Django model from Pydantic, checking cache first and mapping relationships.</p> Source code in <code>src/pydantic2django/pydantic/factory.py</code> <pre><code>def make_django_model(self, carrier: ConversionCarrier[type[BaseModel]]) -&gt; None:\n    \"\"\"Creates a Django model from Pydantic, checking cache first and mapping relationships.\"\"\"\n    model_key = carrier.model_key()\n    logger.debug(f\"PydanticFactory: Attempting to create Django model for {model_key}\")\n\n    # --- Check Cache --- #\n    if model_key in self._converted_models and not carrier.existing_model:\n        logger.debug(f\"PydanticFactory: Using cached conversion result for {model_key}\")\n        cached_carrier = self._converted_models[model_key]\n        # Update the passed-in carrier with cached results\n        carrier.__dict__.update(cached_carrier.__dict__)\n        # Ensure used_related_names is properly updated (dict update might not merge sets correctly)\n        for target, names in cached_carrier.used_related_names_per_target.items():\n            carrier.used_related_names_per_target.setdefault(target, set()).update(names)\n        return\n\n    # --- Call Base Implementation for Core Logic --- #\n    # This calls _process_source_fields, _assemble_django_model_class etc.\n    super().make_django_model(carrier)\n\n    # --- Register Relationship Mapping (if successful) --- #\n    if carrier.source_model and carrier.django_model:\n        logger.debug(\n            f\"PydanticFactory: Registering mapping for {carrier.source_model.__name__} -&gt; {carrier.django_model.__name__}\"\n        )\n        self.relationship_accessor.map_relationship(\n            source_model=carrier.source_model, django_model=carrier.django_model\n        )\n\n    # --- Cache Result --- #\n    if carrier.django_model and not carrier.existing_model:\n        logger.debug(f\"PydanticFactory: Caching conversion result for {model_key}\")\n        # Store a copy to prevent modification issues? Simple assignment for now.\n        self._converted_models[model_key] = carrier\n    elif not carrier.django_model:\n        logger.error(\n            f\"PydanticFactory: Failed to create Django model for {model_key}. Invalid fields: {carrier.invalid_fields}\"\n        )\n</code></pre>"},{"location":"reference/pydantic2django/pydantic/factory/#pydantic2django.pydantic.factory.create_pydantic_factory","title":"<code>create_pydantic_factory(relationship_accessor, bidirectional_mapper)</code>","text":"<p>Helper to create the Pydantic factory stack with dependencies.</p> Source code in <code>src/pydantic2django/pydantic/factory.py</code> <pre><code>def create_pydantic_factory(\n    relationship_accessor: RelationshipConversionAccessor, bidirectional_mapper: BidirectionalTypeMapper\n) -&gt; PydanticModelFactory:\n    \"\"\"Helper to create the Pydantic factory stack with dependencies.\"\"\"\n    field_factory = PydanticFieldFactory(\n        relationship_accessor=relationship_accessor, bidirectional_mapper=bidirectional_mapper\n    )\n    model_factory = PydanticModelFactory(field_factory=field_factory, relationship_accessor=relationship_accessor)\n    return model_factory\n</code></pre>"},{"location":"reference/pydantic2django/pydantic/generator/","title":"pydantic2django.pydantic.generator","text":""},{"location":"reference/pydantic2django/pydantic/generator/#pydantic2django.pydantic.generator.StaticPydanticModelGenerator","title":"<code>StaticPydanticModelGenerator</code>","text":"<p>               Bases: <code>BaseStaticGenerator[type[BaseModel], FieldInfo]</code></p> <p>Generates Django models and their context classes from Pydantic models. Inherits common logic from BaseStaticGenerator.</p> Source code in <code>src/pydantic2django/pydantic/generator.py</code> <pre><code>class StaticPydanticModelGenerator(\n    BaseStaticGenerator[type[BaseModel], FieldInfo]\n):  # TModel=type[BaseModel], TFieldInfo=FieldInfo\n    \"\"\"\n    Generates Django models and their context classes from Pydantic models.\n    Inherits common logic from BaseStaticGenerator.\n    \"\"\"\n\n    def __init__(\n        self,\n        output_path: str = \"generated_models.py\",  # Keep original default\n        packages: Optional[list[str]] = None,\n        app_label: str = \"django_app\",  # Keep original default\n        filter_function: Callable[..., bool] | None = None,\n        verbose: bool = False,\n        discovery_module: Optional[PydanticDiscovery] = None,\n        module_mappings: Optional[dict[str, str]] = None,\n        # Pydantic specific factories can be passed or constructed here\n        # NOTE: Injecting factory instances is less preferred now due to mapper dependency\n        # field_factory_instance: Optional[PydanticFieldFactory] = None,\n        # model_factory_instance: Optional[PydanticModelFactory] = None,\n        # Inject mapper instead?\n        bidirectional_mapper_instance: Optional[BidirectionalTypeMapper] = None,\n        enable_timescale: bool = True,\n        # --- GFK flags ---\n        enable_gfk: bool = True,\n        gfk_policy: str | None = \"threshold_by_children\",\n        gfk_threshold_children: int | None = 8,\n        gfk_value_mode: str | None = \"typed_columns\",\n        gfk_normalize_common_attrs: bool = False,\n    ):\n        # 1. Initialize Pydantic-specific discovery\n        # Use provided instance or create a default one\n        self.pydantic_discovery_instance = discovery_module or PydanticDiscovery()\n\n        # 2. Initialize RelationshipAccessor (needed by factories and mapper)\n        self.relationship_accessor = RelationshipConversionAccessor()\n\n        # 3. Initialize BidirectionalTypeMapper (pass relationship accessor)\n        self.bidirectional_mapper = bidirectional_mapper_instance or BidirectionalTypeMapper(\n            relationship_accessor=self.relationship_accessor\n        )\n\n        # 4. Initialize Pydantic-specific factories (pass mapper and accessor)\n        # Remove dependency on passed-in factory instances, create them here\n        self.pydantic_model_factory = create_pydantic_factory(\n            relationship_accessor=self.relationship_accessor, bidirectional_mapper=self.bidirectional_mapper\n        )\n\n        # 5. Call the base class __init__ with all required arguments\n        super().__init__(\n            output_path=output_path,\n            packages=packages or [\"pydantic_models\"],  # Default Pydantic package\n            app_label=app_label,\n            filter_function=filter_function,\n            verbose=verbose,\n            discovery_instance=self.pydantic_discovery_instance,  # Pass the specific discovery instance\n            model_factory_instance=self.pydantic_model_factory,  # Pass the newly created model factory\n            module_mappings=module_mappings,\n            base_model_class=self._get_default_base_model_class(),\n            # Jinja setup is handled by base class\n            enable_timescale=enable_timescale,\n            enable_gfk=enable_gfk,\n            gfk_policy=gfk_policy,\n            gfk_threshold_children=gfk_threshold_children,\n            gfk_value_mode=gfk_value_mode,\n            gfk_normalize_common_attrs=gfk_normalize_common_attrs,\n        )\n\n        # 6. Pydantic-specific Jinja setup or context generator\n        # Context generator needs the jinja_env from the base class\n        self.context_generator = ContextClassGenerator(jinja_env=self.jinja_env)\n\n        # 7. Track context-specific info during generation (reset in generate_models_file)\n        self.context_definitions: list[str] = []\n        self.model_has_context: dict[str, bool] = {}\n        self.context_class_names: list[str] = []\n        self.seen_context_classes: set[str] = set()\n        # Timescale classification results cached per run (name -&gt; role)\n        self._timescale_roles: dict[str, TimescaleRole] = {}\n\n    # --- Implement Abstract Methods from Base ---\n\n    def _get_source_model_name(self, carrier: ConversionCarrier[type[BaseModel]]) -&gt; str:\n        \"\"\"Get the name of the original Pydantic model.\"\"\"\n        # Ensure source_model is not None before accessing __name__\n        return carrier.source_model.__name__ if carrier.source_model else \"UnknownPydanticModel\"\n\n    def _add_source_model_import(self, carrier: ConversionCarrier[type[BaseModel]]):\n        \"\"\"Add import for the original Pydantic model.\"\"\"\n        if carrier.source_model:\n            # Use the correct method from ImportHandler\n            self.import_handler.add_pydantic_model_import(carrier.source_model)\n        else:\n            logger.warning(\"Cannot add source model import: source_model is missing from carrier.\")\n\n    def _get_models_in_processing_order(self) -&gt; list[type[BaseModel]]:\n        \"\"\"Return models in Pydantic dependency order.\"\"\"\n        # Discovery must have run first (called by base generate_models_file -&gt; discover_models)\n        # Cast the discovery_instance from the base class to the specific Pydantic type\n        discovery = cast(PydanticDiscovery, self.discovery_instance)\n        if not discovery.filtered_models:\n            logger.warning(\"No models discovered or passed filter, cannot determine processing order.\")\n            return []\n        # Ensure dependencies are analyzed if not already done (base class should handle this)\n        # if not discovery.dependencies:\n        #     discovery.analyze_dependencies() # Base class analyze_dependencies called in discover_models\n        return discovery.get_models_in_registration_order()\n\n    def _prepare_template_context(self, unique_model_definitions, django_model_names, imports) -&gt; dict:\n        \"\"\"Prepare the Pydantic-specific context for the main models_file.py.j2 template.\"\"\"\n        # Base context items (model_definitions, django_model_names, imports) are passed in.\n        # Add Pydantic-specific items gathered during generate_models_file override.\n        base_context = {\n            \"model_definitions\": unique_model_definitions,\n            \"django_model_names\": django_model_names,  # For __all__\n            # --- Imports (already structured by base class import_handler) ---\n            \"django_imports\": sorted(imports.get(\"django\", [])),\n            \"pydantic_imports\": sorted(imports.get(\"pydantic\", [])),  # Check if import handler categorizes these\n            \"general_imports\": sorted(imports.get(\"general\", [])),\n            \"context_imports\": sorted(imports.get(\"context\", [])),  # Check if import handler categorizes these\n            # It might be simpler to rely on the structured imports dict directly in the template\n            \"imports\": imports,  # Pass the whole structured dict\n            # --- Pydantic Specific ---\n            \"context_definitions\": self.context_definitions,  # Populated in generate_models_file override\n            \"all_models\": [  # This seems redundant if django_model_names covers __all__\n                f\"'{name}'\"\n                for name in django_model_names  # Use Django names for __all__ consistency?\n            ],\n            \"context_class_names\": self.context_class_names,  # Populated in generate_models_file override\n            \"model_has_context\": self.model_has_context,  # Populated in generate_models_file override\n            \"generation_source_type\": \"pydantic\",  # Flag for template logic\n        }\n        # Note: Common items like timestamp, base_model info, extra_type_imports\n        # are added by the base class generate_models_file method after calling this.\n        return base_context\n\n    def _get_model_definition_extra_context(self, carrier: ConversionCarrier[type[BaseModel]]) -&gt; dict:\n        \"\"\"Provide Pydantic-specific context for model_definition.py.j2.\"\"\"\n        context_fields_info = []\n        context_class_name = \"\"\n        has_context_for_this_model = False  # Track if this specific model has context\n\n        if carrier.model_context and carrier.model_context.context_fields:\n            has_context_for_this_model = True\n            django_model_name = (\n                self._clean_generic_type(carrier.django_model.__name__) if carrier.django_model else \"UnknownModel\"\n            )\n            context_class_name = f\"{django_model_name}Context\"\n\n            for field_name, field_context_info in carrier.model_context.context_fields.items():\n                field_type_attr = getattr(field_context_info, \"field_type\", None) or getattr(\n                    field_context_info, \"annotation\", None\n                )\n\n                if field_type_attr:\n                    type_name = TypeHandler.format_type_string(field_type_attr)\n                    # Add imports for context field types via import_handler\n                    # Use the correct method which handles nested types and typing imports\n                    self.import_handler.add_context_field_type_import(field_type_attr)\n\n                    # Remove explicit add_extra_import calls, handled by add_context_field_type_import\n                    # if getattr(field_context_info, 'is_optional', False):\n                    #      self.import_handler.add_extra_import(\"Optional\", \"typing\")\n                    # if getattr(field_context_info, 'is_list', False):\n                    #      self.import_handler.add_extra_import(\"List\", \"typing\")\n                else:\n                    type_name = \"Any\"  # Fallback\n                    logger.warning(\n                        f\"Could not determine context type annotation for field '{field_name}' in {django_model_name}\"\n                    )\n\n                context_fields_info.append((field_name, type_name))\n\n        return {\n            \"context_class_name\": context_class_name,\n            \"context_fields\": context_fields_info,\n            \"is_pydantic_source\": True,\n            \"is_dataclass_source\": False,\n            \"has_context\": has_context_for_this_model,\n            \"field_definitions\": carrier.django_field_definitions,\n        }\n\n    def _get_default_base_model_class(self) -&gt; type[models.Model]:\n        \"\"\"Return the default Django base model for Pydantic conversion.\n\n        Raises a clear ImportError if the base cannot be imported.\n        \"\"\"\n        if not django_apps.ready:\n            raise AppRegistryNotReady(\n                \"Django apps are not loaded. Call django.setup() or run within a configured Django context before \"\n                \"instantiating StaticPydanticModelGenerator.\"\n            )\n        try:\n            from typed2django.django.models import Pydantic2DjangoBaseClass as _Base\n\n            return _Base\n        except Exception as exc:  # pragma: no cover - defensive path\n            raise ImportError(\n                \"typed2django.django.models.Pydantic2DjangoBaseClass is required for Pydantic generation.\"\n            ) from exc\n\n    # --- Override generate_models_file to handle Pydantic context class generation ---\n\n    def generate_models_file(self) -&gt; str:\n        \"\"\"\n        Generates the complete models.py file content, including Pydantic context classes.\n        Overrides the base method to add context class handling during the generation loop.\n        \"\"\"\n        # 1. Base discovery and model ordering\n        self.discover_models()  # Calls base discovery and dependency analysis\n        models_to_process = self._get_models_in_processing_order()  # Uses overridden method\n\n        # 2. Reset state for this run (imports handled by base reset)\n        self.carriers = []\n        # Manually reset ImportHandler state instead of calling non-existent reset()\n        self.import_handler.extra_type_imports.clear()\n        self.import_handler.pydantic_imports.clear()\n        self.import_handler.context_class_imports.clear()\n        self.import_handler.imported_names.clear()\n        self.import_handler.processed_field_types.clear()\n\n        # Re-add base model import after clearing\n        # Note: add_pydantic_model_import might not be the right method here if base_model_class isn't Pydantic\n        # Need a more general import method on ImportHandler or handle it differently.\n        # For now, let's assume a general import is needed or handled by template.\n        # self.import_handler.add_import(self.base_model_class.__module__, self.base_model_class.__name__)\n        # Let's add it back using _add_type_import, although it's protected.\n        # A public add_general_import(module, name) on ImportHandler would be better.\n        try:\n            # This is a workaround - ideally ImportHandler would have a public method\n            self.import_handler._add_type_import(self.base_model_class)\n        except Exception as e:\n            logger.warning(f\"Could not add base model import via _add_type_import: {e}\")\n\n        # Reset Pydantic-specific tracking lists\n        self.context_definitions = []\n        self.model_has_context = {}  # Map of Pydantic model name -&gt; bool\n        self.context_class_names = []  # For __all__\n        self.seen_context_classes = set()  # For deduplication of definitions\n\n        # --- State tracking within the loop ---\n        model_definitions = []  # Store generated Django model definition strings\n        django_model_names = []  # Store generated Django model names for __all__\n        context_only_models = []  # Track Pydantic models yielding only context\n        gfk_used = False\n\n        # 3. Setup Django models (populates self.carriers via base method calling factory)\n        for source_model in models_to_process:\n            self.setup_django_model(source_model)  # Uses base setup_django_model\n\n        # 4. Generate definitions (Django models AND Pydantic Context classes)\n        for carrier in self.carriers:\n            model_name = self._get_source_model_name(carrier)  # Pydantic model name\n\n            try:\n                # --- GFK finalize hook: inject GenericRelation on parents ---\n                if getattr(carrier, \"enable_gfk\", False) and getattr(carrier, \"pending_gfk_children\", None):\n                    # Ensure imports for contenttypes\n                    self.import_handler.add_import(\"django.contrib.contenttypes.fields\", \"GenericRelation\")\n                    self.import_handler.add_import(\"django.contrib.contenttypes.fields\", \"GenericForeignKey\")\n                    self.import_handler.add_import(\"django.contrib.contenttypes.models\", \"ContentType\")\n\n                    # Inject a GenericRelation field into the parent model definition strings\n                    # Field name 'entries' for reverse access\n                    try:\n                        carrier.django_field_definitions[\n                            \"entries\"\n                        ] = \"GenericRelation('GenericEntry', related_query_name='entries')\"\n                    except Exception:\n                        pass\n                    gfk_used = True\n\n                django_model_def = \"\"\n                django_model_name_cleaned = \"\"\n\n                # --- A. Generate Django Model Definition (if applicable) ---\n                if carrier.django_model:\n                    # Check fields using safe getattr for many_to_many\n                    has_concrete_fields = any(not f.primary_key for f in carrier.django_model._meta.fields)\n                    # Use getattr for safety\n                    m2m_fields = getattr(carrier.django_model._meta, \"many_to_many\", [])\n                    has_m2m = bool(m2m_fields)\n                    has_fields = bool(carrier.django_model._meta.fields)\n\n                    if has_concrete_fields or has_m2m or (not has_concrete_fields and not has_m2m and has_fields):\n                        django_model_def = self.generate_model_definition(carrier)\n                        if django_model_def:\n                            model_definitions.append(django_model_def)\n                            django_model_name_cleaned = self._clean_generic_type(carrier.django_model.__name__)\n                            django_model_names.append(f\"'{django_model_name_cleaned}'\")\n                        else:\n                            logger.warning(f\"Base generate_model_definition returned empty for {model_name}, skipping.\")\n                    else:\n                        # Model exists but seems empty (no concrete fields/M2M)\n                        # Check if it *does* have context fields\n                        if carrier.model_context and carrier.model_context.context_fields:\n                            context_only_models.append(model_name)\n                            logger.info(f\"Skipping Django model definition for {model_name} - only has context fields.\")\n                        else:\n                            logger.warning(\n                                f\"Model {model_name} resulted in an empty Django model with no context fields. Skipping definition.\"\n                            )\n                            # Continue to next carrier if no Django model AND no context\n                            if not (carrier.model_context and carrier.model_context.context_fields):\n                                continue\n\n                # --- B. Generate Context Class Definition (Pydantic Specific) ---\n                has_context = False\n                if carrier.model_context and carrier.model_context.context_fields:\n                    has_context = True\n                    # Generate context class definition string using the context_generator\n                    # This also handles adding necessary imports for context fields via TypeHandler/ImportHandler calls within it\n                    context_def = self.context_generator.generate_context_class(carrier.model_context)\n\n                    # Determine context class name (needs Django model name)\n                    # Use the cleaned name if available, otherwise construct from Pydantic name?\n                    base_name_for_context = django_model_name_cleaned if django_model_name_cleaned else model_name\n                    context_class_name = f\"{base_name_for_context}Context\"\n\n                    # Add context class definition if not seen before\n                    if context_class_name not in self.seen_context_classes:\n                        self.context_definitions.append(context_def)\n                        self.context_class_names.append(f\"'{context_class_name}'\")\n                        self.seen_context_classes.add(context_class_name)\n\n                    # Add imports for context fields (should be handled by context_generator now)\n                    # self.import_handler.add_context_field_imports(carrier.model_context) # Example hypothetical method\n\n                # --- C. Update Tracking and Add Source Import ---\n                self.model_has_context[model_name] = has_context\n\n                # Add import for the original source model (Pydantic model)\n                self._add_source_model_import(carrier)\n\n            except Exception as e:\n                logger.error(f\"Error processing carrier for source model {model_name}: {e}\", exc_info=True)\n\n        # 5. Log Summary\n        if context_only_models:\n            logger.info(\n                f\"Skipped Django definitions for {len(context_only_models)} models with only context fields: {', '.join(context_only_models)}\"\n            )\n\n        # If GFK is used anywhere, emit GenericEntry model once per file\n        if gfk_used:\n            model_definitions.append(self._build_generic_entry_model_definition())\n            django_model_names.append(\"'GenericEntry'\")\n\n        # 6. Deduplicate Definitions (Django models only, context defs deduplicated by name during loop)\n        unique_model_definitions = self._deduplicate_definitions(model_definitions)  # Use base method\n\n        # 7. Get Imports (handled by base import_handler)\n        imports = self.import_handler.deduplicate_imports()\n\n        # 8. Prepare Template Context (using overridden Pydantic-specific method)\n        template_context = self._prepare_template_context(unique_model_definitions, django_model_names, imports)\n\n        # 9. Add Common Context Items (handled by base class) - Reuse base class logic\n        template_context.update(\n            {\n                \"generation_timestamp\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n                \"base_model_module\": self.base_model_class.__module__,\n                \"base_model_name\": self.base_model_class.__name__,\n                \"extra_type_imports\": sorted(self.import_handler.extra_type_imports),\n                # Ensure generation_source_type is set by _prepare_template_context\n            }\n        )\n\n        # 10. Render the main template\n        template = self.jinja_env.get_template(\"models_file.py.j2\")\n        return template.render(**template_context)\n\n    def _build_generic_entry_model_definition(self) -&gt; str:\n        \"\"\"Build the GenericEntry model definition string.\n\n        Uses contenttypes GenericForeignKey to attach to any parent model.\n        Includes common columns and optional typed value columns based on flags.\n        \"\"\"\n        # Ensure imports present\n        self.import_handler.add_import(\"django.contrib.contenttypes.fields\", \"GenericForeignKey\")\n        self.import_handler.add_import(\"django.contrib.contenttypes.fields\", \"GenericRelation\")\n        self.import_handler.add_import(\"django.contrib.contenttypes.models\", \"ContentType\")\n\n        fields: list[str] = []\n        fields.append(\"content_type = models.ForeignKey('contenttypes.ContentType', on_delete=models.CASCADE)\")\n        fields.append(\"object_id = models.PositiveIntegerField()\")\n        fields.append(\"content_object = GenericForeignKey('content_type', 'object_id')\")\n        fields.append(\"element_qname = models.CharField(max_length=255)\")\n        fields.append(\"type_qname = models.CharField(max_length=255, null=True, blank=True)\")\n        fields.append(\"attrs_json = models.JSONField(default=dict, blank=True)\")\n        # Optional typed value columns if configured\n        if getattr(self, \"gfk_value_mode\", None) == \"typed_columns\":\n            fields.append(\"text_value = models.TextField(null=True, blank=True)\")\n            fields.append(\"num_value = models.DecimalField(max_digits=20, decimal_places=6, null=True, blank=True)\")\n            fields.append(\"time_value = models.DateTimeField(null=True, blank=True)\")\n        fields.append(\"order_index = models.IntegerField(default=0)\")\n        fields.append(\"path_hint = models.CharField(max_length=255, null=True, blank=True)\")\n\n        # Build indexes list\n        indexes_lines = [\"models.Index(fields=['content_type', 'object_id'])\"]\n        if getattr(self, \"gfk_value_mode\", None) == \"typed_columns\":\n            indexes_lines.append(\"models.Index(fields=['element_qname'])\")\n            indexes_lines.append(\"models.Index(fields=['type_qname'])\")\n            indexes_lines.append(\"models.Index(fields=['time_value'])\")\n            indexes_lines.append(\"models.Index(fields=['content_type', 'object_id', '-time_value'])\")\n\n        # Compose class string\n        lines: list[str] = []\n        lines.append(f\"class GenericEntry({self.base_model_class.__name__}):\")\n        for f in fields:\n            lines.append(f\"    {f}\")\n        lines.append(\"\")\n        lines.append(\"    class Meta:\")\n        lines.append(f\"        app_label = '{self.app_label}'\")\n        lines.append(\"        abstract = False\")\n        lines.append(\"        indexes = [\")\n        for idx in indexes_lines:\n            lines.append(f\"            {idx},\")\n        lines.append(\"        ]\")\n        lines.append(\"\")\n        return \"\\n\".join(lines)\n\n    # Choose Timescale base per model (lazy roles computation)\n    def setup_django_model(self, source_model: type[BaseModel]) -&gt; ConversionCarrier | None:  # type: ignore[override]\n        try:\n            from pydantic2django.django.timescale.bases import PydanticTimescaleBase\n            from pydantic2django.django.timescale.heuristics import (\n                classify_pydantic_models,\n                should_use_timescale_base,\n            )\n        except Exception:\n            classify_pydantic_models = None  # type: ignore\n            should_use_timescale_base = None  # type: ignore\n            PydanticTimescaleBase = None  # type: ignore\n\n        # Compute roles lazily if not present\n        if self.enable_timescale and not getattr(self, \"_timescale_roles\", None):\n            roles: dict[str, TimescaleRole] = {}\n            try:\n                models_to_score = []\n                try:\n                    models_to_score = self._get_models_in_processing_order() or []\n                except Exception:\n                    pass\n                if not models_to_score:\n                    models_to_score = [source_model]\n                if classify_pydantic_models:\n                    roles = classify_pydantic_models(models_to_score)\n            except Exception:\n                roles = {}\n            self._timescale_roles = roles\n\n        # Select base class\n        base_cls: type[models.Model] = self.base_model_class\n        if self.enable_timescale:\n            try:\n                name = source_model.__name__\n                if should_use_timescale_base and PydanticTimescaleBase:\n                    if should_use_timescale_base(name, self._timescale_roles):  # type: ignore[arg-type]\n                        base_cls = PydanticTimescaleBase\n            except Exception:\n                pass\n\n        prev_base = self.base_model_class\n        self.base_model_class = base_cls\n        try:\n            carrier = super().setup_django_model(source_model)\n        finally:\n            self.base_model_class = prev_base\n\n        if carrier is not None:\n            carrier.context_data[\"_timescale_roles\"] = getattr(self, \"_timescale_roles\", {})\n        return carrier\n</code></pre>"},{"location":"reference/pydantic2django/pydantic/generator/#pydantic2django.pydantic.generator.StaticPydanticModelGenerator.generate_models_file","title":"<code>generate_models_file()</code>","text":"<p>Generates the complete models.py file content, including Pydantic context classes. Overrides the base method to add context class handling during the generation loop.</p> Source code in <code>src/pydantic2django/pydantic/generator.py</code> <pre><code>def generate_models_file(self) -&gt; str:\n    \"\"\"\n    Generates the complete models.py file content, including Pydantic context classes.\n    Overrides the base method to add context class handling during the generation loop.\n    \"\"\"\n    # 1. Base discovery and model ordering\n    self.discover_models()  # Calls base discovery and dependency analysis\n    models_to_process = self._get_models_in_processing_order()  # Uses overridden method\n\n    # 2. Reset state for this run (imports handled by base reset)\n    self.carriers = []\n    # Manually reset ImportHandler state instead of calling non-existent reset()\n    self.import_handler.extra_type_imports.clear()\n    self.import_handler.pydantic_imports.clear()\n    self.import_handler.context_class_imports.clear()\n    self.import_handler.imported_names.clear()\n    self.import_handler.processed_field_types.clear()\n\n    # Re-add base model import after clearing\n    # Note: add_pydantic_model_import might not be the right method here if base_model_class isn't Pydantic\n    # Need a more general import method on ImportHandler or handle it differently.\n    # For now, let's assume a general import is needed or handled by template.\n    # self.import_handler.add_import(self.base_model_class.__module__, self.base_model_class.__name__)\n    # Let's add it back using _add_type_import, although it's protected.\n    # A public add_general_import(module, name) on ImportHandler would be better.\n    try:\n        # This is a workaround - ideally ImportHandler would have a public method\n        self.import_handler._add_type_import(self.base_model_class)\n    except Exception as e:\n        logger.warning(f\"Could not add base model import via _add_type_import: {e}\")\n\n    # Reset Pydantic-specific tracking lists\n    self.context_definitions = []\n    self.model_has_context = {}  # Map of Pydantic model name -&gt; bool\n    self.context_class_names = []  # For __all__\n    self.seen_context_classes = set()  # For deduplication of definitions\n\n    # --- State tracking within the loop ---\n    model_definitions = []  # Store generated Django model definition strings\n    django_model_names = []  # Store generated Django model names for __all__\n    context_only_models = []  # Track Pydantic models yielding only context\n    gfk_used = False\n\n    # 3. Setup Django models (populates self.carriers via base method calling factory)\n    for source_model in models_to_process:\n        self.setup_django_model(source_model)  # Uses base setup_django_model\n\n    # 4. Generate definitions (Django models AND Pydantic Context classes)\n    for carrier in self.carriers:\n        model_name = self._get_source_model_name(carrier)  # Pydantic model name\n\n        try:\n            # --- GFK finalize hook: inject GenericRelation on parents ---\n            if getattr(carrier, \"enable_gfk\", False) and getattr(carrier, \"pending_gfk_children\", None):\n                # Ensure imports for contenttypes\n                self.import_handler.add_import(\"django.contrib.contenttypes.fields\", \"GenericRelation\")\n                self.import_handler.add_import(\"django.contrib.contenttypes.fields\", \"GenericForeignKey\")\n                self.import_handler.add_import(\"django.contrib.contenttypes.models\", \"ContentType\")\n\n                # Inject a GenericRelation field into the parent model definition strings\n                # Field name 'entries' for reverse access\n                try:\n                    carrier.django_field_definitions[\n                        \"entries\"\n                    ] = \"GenericRelation('GenericEntry', related_query_name='entries')\"\n                except Exception:\n                    pass\n                gfk_used = True\n\n            django_model_def = \"\"\n            django_model_name_cleaned = \"\"\n\n            # --- A. Generate Django Model Definition (if applicable) ---\n            if carrier.django_model:\n                # Check fields using safe getattr for many_to_many\n                has_concrete_fields = any(not f.primary_key for f in carrier.django_model._meta.fields)\n                # Use getattr for safety\n                m2m_fields = getattr(carrier.django_model._meta, \"many_to_many\", [])\n                has_m2m = bool(m2m_fields)\n                has_fields = bool(carrier.django_model._meta.fields)\n\n                if has_concrete_fields or has_m2m or (not has_concrete_fields and not has_m2m and has_fields):\n                    django_model_def = self.generate_model_definition(carrier)\n                    if django_model_def:\n                        model_definitions.append(django_model_def)\n                        django_model_name_cleaned = self._clean_generic_type(carrier.django_model.__name__)\n                        django_model_names.append(f\"'{django_model_name_cleaned}'\")\n                    else:\n                        logger.warning(f\"Base generate_model_definition returned empty for {model_name}, skipping.\")\n                else:\n                    # Model exists but seems empty (no concrete fields/M2M)\n                    # Check if it *does* have context fields\n                    if carrier.model_context and carrier.model_context.context_fields:\n                        context_only_models.append(model_name)\n                        logger.info(f\"Skipping Django model definition for {model_name} - only has context fields.\")\n                    else:\n                        logger.warning(\n                            f\"Model {model_name} resulted in an empty Django model with no context fields. Skipping definition.\"\n                        )\n                        # Continue to next carrier if no Django model AND no context\n                        if not (carrier.model_context and carrier.model_context.context_fields):\n                            continue\n\n            # --- B. Generate Context Class Definition (Pydantic Specific) ---\n            has_context = False\n            if carrier.model_context and carrier.model_context.context_fields:\n                has_context = True\n                # Generate context class definition string using the context_generator\n                # This also handles adding necessary imports for context fields via TypeHandler/ImportHandler calls within it\n                context_def = self.context_generator.generate_context_class(carrier.model_context)\n\n                # Determine context class name (needs Django model name)\n                # Use the cleaned name if available, otherwise construct from Pydantic name?\n                base_name_for_context = django_model_name_cleaned if django_model_name_cleaned else model_name\n                context_class_name = f\"{base_name_for_context}Context\"\n\n                # Add context class definition if not seen before\n                if context_class_name not in self.seen_context_classes:\n                    self.context_definitions.append(context_def)\n                    self.context_class_names.append(f\"'{context_class_name}'\")\n                    self.seen_context_classes.add(context_class_name)\n\n                # Add imports for context fields (should be handled by context_generator now)\n                # self.import_handler.add_context_field_imports(carrier.model_context) # Example hypothetical method\n\n            # --- C. Update Tracking and Add Source Import ---\n            self.model_has_context[model_name] = has_context\n\n            # Add import for the original source model (Pydantic model)\n            self._add_source_model_import(carrier)\n\n        except Exception as e:\n            logger.error(f\"Error processing carrier for source model {model_name}: {e}\", exc_info=True)\n\n    # 5. Log Summary\n    if context_only_models:\n        logger.info(\n            f\"Skipped Django definitions for {len(context_only_models)} models with only context fields: {', '.join(context_only_models)}\"\n        )\n\n    # If GFK is used anywhere, emit GenericEntry model once per file\n    if gfk_used:\n        model_definitions.append(self._build_generic_entry_model_definition())\n        django_model_names.append(\"'GenericEntry'\")\n\n    # 6. Deduplicate Definitions (Django models only, context defs deduplicated by name during loop)\n    unique_model_definitions = self._deduplicate_definitions(model_definitions)  # Use base method\n\n    # 7. Get Imports (handled by base import_handler)\n    imports = self.import_handler.deduplicate_imports()\n\n    # 8. Prepare Template Context (using overridden Pydantic-specific method)\n    template_context = self._prepare_template_context(unique_model_definitions, django_model_names, imports)\n\n    # 9. Add Common Context Items (handled by base class) - Reuse base class logic\n    template_context.update(\n        {\n            \"generation_timestamp\": datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n            \"base_model_module\": self.base_model_class.__module__,\n            \"base_model_name\": self.base_model_class.__name__,\n            \"extra_type_imports\": sorted(self.import_handler.extra_type_imports),\n            # Ensure generation_source_type is set by _prepare_template_context\n        }\n    )\n\n    # 10. Render the main template\n    template = self.jinja_env.get_template(\"models_file.py.j2\")\n    return template.render(**template_context)\n</code></pre>"},{"location":"reference/pydantic2django/pydantic/utils/","title":"pydantic2django.pydantic.utils","text":""},{"location":"reference/pydantic2django/pydantic/utils/introspection/","title":"pydantic2django.pydantic.utils.introspection","text":""},{"location":"reference/pydantic2django/pydantic/utils/introspection/#pydantic2django.pydantic.utils.introspection.get_model_fields","title":"<code>get_model_fields(model_class)</code>","text":"<p>Get the fields from a Pydantic model, handling potential errors.</p> <p>Parameters:</p> Name Type Description Default <code>model_class</code> <code>type[BaseModel]</code> <p>The Pydantic model class</p> required <p>Returns:</p> Type Description <code>dict[str, FieldInfo]</code> <p>A dictionary of field names to FieldInfo objects.</p> Source code in <code>src/pydantic2django/pydantic/utils/introspection.py</code> <pre><code>def get_model_fields(model_class: type[BaseModel]) -&gt; dict[str, FieldInfo]:\n    \"\"\"\n    Get the fields from a Pydantic model, handling potential errors.\n\n    Args:\n        model_class: The Pydantic model class\n\n    Returns:\n        A dictionary of field names to FieldInfo objects.\n    \"\"\"\n    if not inspect.isclass(model_class) or not issubclass(model_class, BaseModel):\n        # logger.warning(f\"Attempted to get fields from non-Pydantic model: {model_class}\")\n        return {}\n    try:\n        return model_class.model_fields\n    except Exception:\n        # logger.error(f\"Error retrieving fields for Pydantic model {model_class.__name__}: {e}\")\n        return {}\n</code></pre>"},{"location":"reference/pydantic2django/pydantic/utils/introspection/#pydantic2django.pydantic.utils.introspection.is_pydantic_model_field_optional","title":"<code>is_pydantic_model_field_optional(field_type)</code>","text":"<p>Check if a Pydantic field type annotation represents an Optional field.</p> Source code in <code>src/pydantic2django/pydantic/utils/introspection.py</code> <pre><code>def is_pydantic_model_field_optional(field_type: Any) -&gt; bool:\n    \"\"\"Check if a Pydantic field type annotation represents an Optional field.\"\"\"\n    origin = get_origin(field_type)\n    args = get_args(field_type)\n    # Check for Union[T, NoneType]\n    return origin is Union and type(None) in args\n</code></pre>"},{"location":"reference/pydantic2django/typedclass/","title":"pydantic2django.typedclass","text":""},{"location":"reference/pydantic2django/typedclass/discovery/","title":"pydantic2django.typedclass.discovery","text":""},{"location":"reference/pydantic2django/typedclass/discovery/#pydantic2django.typedclass.discovery.TypedClassDiscovery","title":"<code>TypedClassDiscovery</code>","text":"<p>               Bases: <code>BaseDiscovery[TypedClassType]</code></p> <p>Discovers generic Python classes within specified packages.</p> Source code in <code>src/pydantic2django/typedclass/discovery.py</code> <pre><code>class TypedClassDiscovery(BaseDiscovery[TypedClassType]):\n    \"\"\"Discovers generic Python classes within specified packages.\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        # TypedClass specific attributes, if any, can be initialized here.\n\n    def _is_pydantic_model(self, obj: Any) -&gt; bool:\n        \"\"\"Checks if an object is a Pydantic model.\"\"\"\n        return inspect.isclass(obj) and issubclass(obj, BaseModel)\n\n    def _is_target_model(self, obj: Any) -&gt; bool:\n        \"\"\"\n        Check if an object is a generic class suitable for conversion.\n        It must be a class, not an ABC, not a Pydantic model, and not a dataclass.\n        \"\"\"\n        if not inspect.isclass(obj):\n            return False\n        if inspect.isabstract(obj):\n            logger.debug(f\"Skipping abstract class {obj.__name__}\")\n            return False\n        if self._is_pydantic_model(obj):  # Check if it's a Pydantic model\n            logger.debug(f\"Skipping Pydantic model {obj.__name__}\")\n            return False\n        if dataclasses.is_dataclass(obj):\n            logger.debug(f\"Skipping dataclass {obj.__name__}\")\n            return False\n\n        # Further checks can be added here, e.g., must be in a specific list\n        # or have certain characteristics. For now, this is a basic filter.\n        logger.debug(f\"Identified potential target typed class: {obj.__name__}\")\n        return True\n\n    def _default_eligibility_filter(self, model: TypedClassType) -&gt; bool:\n        \"\"\"\n        Check default eligibility for generic classes.\n        For example, we might want to ensure it's not an ABC, though\n        _is_target_model should already catch this.\n        \"\"\"\n        # Redundant check if _is_target_model is comprehensive, but good for safety.\n        if inspect.isabstract(model):\n            logger.debug(f\"Filtering out typed class {model.__name__} (is abstract)\")\n            return False\n\n        # Add other default checks if needed.\n        # For instance, are there specific base classes (not ABCs) we want to exclude/include?\n        return True\n\n    def analyze_dependencies(self) -&gt; None:\n        \"\"\"\n        Build the dependency graph for the filtered generic classes.\n        Dependencies are determined by type hints in __init__ arguments\n        and class-level attribute annotations.\n        \"\"\"\n        logger.info(\"Analyzing dependencies between filtered typed classes...\")\n        self.dependencies: dict[TypedClassType, set[TypedClassType]] = {}\n\n        # Ensure all filtered models are keys in the dependencies dict\n        for model_qualname in self.filtered_models:\n            model_obj = self.filtered_models[model_qualname]\n            self.dependencies[model_obj] = set()\n\n        filtered_model_qualnames = set(self.filtered_models.keys())\n\n        def _find_and_add_dependency(source_model: TypedClassType, potential_dep_type: Any):\n            \"\"\"\n            Helper to check if a potential dependency type is a target model\n            and add it to the graph.\n            \"\"\"\n            # Check if potential_dep_type itself is a class and one of our targets\n            if self._is_target_model(potential_dep_type):\n                dep_qualname = f\"{potential_dep_type.__module__}.{potential_dep_type.__name__}\"\n                if dep_qualname in filtered_model_qualnames and potential_dep_type is not source_model:\n                    dep_model_obj = self.filtered_models.get(dep_qualname)\n                    if dep_model_obj:\n                        self.dependencies[source_model].add(dep_model_obj)\n                    else:\n                        logger.warning(\n                            f\"Inconsistency: Dependency '{dep_qualname}' for typed class \"\n                            f\"'{source_model.__name__}' found by name but not as object in filtered set.\"\n                        )\n            # TODO: Handle generics like list[TargetType], dict[str, TargetType], Union[TargetType, None]\n            # This would involve using get_origin and get_args from typing.\n\n        for model_type in self.filtered_models.values():\n            # 1. Analyze __init__ parameters\n            try:\n                init_signature = inspect.signature(model_type.__init__)\n                for param in init_signature.parameters.values():\n                    if param.name == \"self\" or param.annotation is inspect.Parameter.empty:\n                        continue\n                    _find_and_add_dependency(model_type, param.annotation)\n            except (ValueError, TypeError) as e:  # Some built-ins or exotic classes might not have inspectable __init__\n                logger.debug(f\"Could not inspect __init__ for {model_type.__name__}: {e}\")\n\n            # 2. Analyze class-level annotations\n            try:\n                annotations = inspect.get_annotations(model_type, eval_str=True)\n                for _, attr_type in annotations.items():\n                    _find_and_add_dependency(model_type, attr_type)\n            except Exception as e:\n                logger.debug(f\"Could not get annotations for {model_type.__name__}: {e}\")\n\n        logger.info(\"Typed class dependency analysis complete.\")\n        # Debug logging of dependencies will be handled by BaseDiscovery.log_dependencies\n\n    def get_models_in_registration_order(self) -&gt; list[TypedClassType]:\n        \"\"\"\n        Return generic classes sorted topologically based on dependencies.\n        This method can often be inherited from BaseDiscovery if the dependency\n        graph is built correctly.\n        \"\"\"\n        # For now, assume BaseDiscovery's implementation is sufficient.\n        # If specific logic for typed classes is needed, override here.\n        return super().get_models_in_registration_order()\n</code></pre>"},{"location":"reference/pydantic2django/typedclass/discovery/#pydantic2django.typedclass.discovery.TypedClassDiscovery.analyze_dependencies","title":"<code>analyze_dependencies()</code>","text":"<p>Build the dependency graph for the filtered generic classes. Dependencies are determined by type hints in init arguments and class-level attribute annotations.</p> Source code in <code>src/pydantic2django/typedclass/discovery.py</code> <pre><code>def analyze_dependencies(self) -&gt; None:\n    \"\"\"\n    Build the dependency graph for the filtered generic classes.\n    Dependencies are determined by type hints in __init__ arguments\n    and class-level attribute annotations.\n    \"\"\"\n    logger.info(\"Analyzing dependencies between filtered typed classes...\")\n    self.dependencies: dict[TypedClassType, set[TypedClassType]] = {}\n\n    # Ensure all filtered models are keys in the dependencies dict\n    for model_qualname in self.filtered_models:\n        model_obj = self.filtered_models[model_qualname]\n        self.dependencies[model_obj] = set()\n\n    filtered_model_qualnames = set(self.filtered_models.keys())\n\n    def _find_and_add_dependency(source_model: TypedClassType, potential_dep_type: Any):\n        \"\"\"\n        Helper to check if a potential dependency type is a target model\n        and add it to the graph.\n        \"\"\"\n        # Check if potential_dep_type itself is a class and one of our targets\n        if self._is_target_model(potential_dep_type):\n            dep_qualname = f\"{potential_dep_type.__module__}.{potential_dep_type.__name__}\"\n            if dep_qualname in filtered_model_qualnames and potential_dep_type is not source_model:\n                dep_model_obj = self.filtered_models.get(dep_qualname)\n                if dep_model_obj:\n                    self.dependencies[source_model].add(dep_model_obj)\n                else:\n                    logger.warning(\n                        f\"Inconsistency: Dependency '{dep_qualname}' for typed class \"\n                        f\"'{source_model.__name__}' found by name but not as object in filtered set.\"\n                    )\n        # TODO: Handle generics like list[TargetType], dict[str, TargetType], Union[TargetType, None]\n        # This would involve using get_origin and get_args from typing.\n\n    for model_type in self.filtered_models.values():\n        # 1. Analyze __init__ parameters\n        try:\n            init_signature = inspect.signature(model_type.__init__)\n            for param in init_signature.parameters.values():\n                if param.name == \"self\" or param.annotation is inspect.Parameter.empty:\n                    continue\n                _find_and_add_dependency(model_type, param.annotation)\n        except (ValueError, TypeError) as e:  # Some built-ins or exotic classes might not have inspectable __init__\n            logger.debug(f\"Could not inspect __init__ for {model_type.__name__}: {e}\")\n\n        # 2. Analyze class-level annotations\n        try:\n            annotations = inspect.get_annotations(model_type, eval_str=True)\n            for _, attr_type in annotations.items():\n                _find_and_add_dependency(model_type, attr_type)\n        except Exception as e:\n            logger.debug(f\"Could not get annotations for {model_type.__name__}: {e}\")\n\n    logger.info(\"Typed class dependency analysis complete.\")\n</code></pre>"},{"location":"reference/pydantic2django/typedclass/discovery/#pydantic2django.typedclass.discovery.TypedClassDiscovery.get_models_in_registration_order","title":"<code>get_models_in_registration_order()</code>","text":"<p>Return generic classes sorted topologically based on dependencies. This method can often be inherited from BaseDiscovery if the dependency graph is built correctly.</p> Source code in <code>src/pydantic2django/typedclass/discovery.py</code> <pre><code>def get_models_in_registration_order(self) -&gt; list[TypedClassType]:\n    \"\"\"\n    Return generic classes sorted topologically based on dependencies.\n    This method can often be inherited from BaseDiscovery if the dependency\n    graph is built correctly.\n    \"\"\"\n    # For now, assume BaseDiscovery's implementation is sufficient.\n    # If specific logic for typed classes is needed, override here.\n    return super().get_models_in_registration_order()\n</code></pre>"},{"location":"reference/pydantic2django/typedclass/factory/","title":"pydantic2django.typedclass.factory","text":""},{"location":"reference/pydantic2django/typedclass/factory/#pydantic2django.typedclass.factory.TypedClassFieldFactory","title":"<code>TypedClassFieldFactory</code>","text":"<p>               Bases: <code>BaseFieldFactory[TypedClassFieldInfo, TypedClassType]</code></p> <p>Generates Django model field definitions from TypedClassFieldInfo.</p> Source code in <code>src/pydantic2django/typedclass/factory.py</code> <pre><code>class TypedClassFieldFactory(BaseFieldFactory[TypedClassFieldInfo, TypedClassType]):\n    \"\"\"Generates Django model field definitions from TypedClassFieldInfo.\"\"\"\n\n    def __init__(self, relationship_accessor: RelationshipConversionAccessor):\n        super().__init__(\n            type_translator=TypedClassTypeTranslator(relationship_accessor),\n            relationship_accessor=relationship_accessor,\n        )\n\n    def create_field_definition(\n        self, field_info: TypedClassFieldInfo, carrier: ConversionCarrier[TypedClassType]\n    ) -&gt; str:\n        \"\"\"\n        Creates a Django field definition string.\n        Example: \"my_field = models.CharField(max_length=100, null=True)\"\n        \"\"\"\n        params = self.type_translator.translate_type(field_info.type_hint, field_info.name, carrier)\n\n        field_class = params.pop(\"field_class\", models.TextField)  # Default fallback\n        # Ensure the import for the field_class is added\n        carrier.add_django_field_import(field_class)\n\n        # Handle default values - this needs careful conversion from Python defaults to Django defaults\n        # For now, a simple string representation or skip if complex.\n        default_value_str = \"\"\n        if field_info.default_value is not inspect.Parameter.empty:\n            if isinstance(field_info.default_value, (str, int, float, bool, datetime.date, datetime.datetime)):\n                default_value_str = f\", default={repr(field_info.default_value)}\"\n            elif field_info.default_value is None:\n                default_value_str = \", default=None\"\n            # else: default for complex types is tricky, might need callable or be handled by null=True\n\n        # Constructing the field string:\n        # e.g. my_field = models.CharField(max_length=255, null=True, blank=True, default='foo')\n        param_str = []\n        for k, v in params.items():\n            if k in [\"null\", \"blank\"] and v:  # Add null=True, blank=True\n                param_str.append(f\"{k}=True\")\n            elif k == \"to\" and isinstance(v, str):  # For relationships, 'to' model name\n                param_str.append(f\"to='{v}'\")\n            elif isinstance(v, str) and k not in [\"related_name\"]:  # other string params\n                param_str.append(f\"{k}='{v}'\")\n            elif isinstance(v, type) and issubclass(v, models.Model):  # 'to' can be a model class\n                param_str.append(f\"to={v.__name__}\")  # This assumes 'to' model is in same app or imported\n            elif v is not None:  # For other types like max_length\n                param_str.append(f\"{k}={v}\")\n\n        # Add related_name for relationships if present\n        if \"related_name\" in params and params[\"related_name\"]:\n            param_str.append(f\"related_name='{params['related_name']}'\")\n\n        field_str = f\"{field_info.name} = {IMPORT_MAPPING[field_class.__module__ + '.' + field_class.__name__]}({', '.join(param_str)}{default_value_str})\"\n\n        # Placeholder for \"reckless mode\" JSONField and serialization methods\n        # if 'is_reckless' and not directly_mappable:\n        #    field_str = f\"{field_info.name} = models.JSONField(null=True, blank=True)\"\n        #    carrier.add_django_field_import(models.JSONField)\n        #    # Add placeholder for serialization/deserialization methods to carrier\n        #    carrier.add_custom_method(f\"def serialize_{field_info.name}(self): ...\")\n        #    carrier.add_custom_method(f\"@property def {field_info.name}_deserialized(self): ...\")\n\n        return field_str\n</code></pre>"},{"location":"reference/pydantic2django/typedclass/factory/#pydantic2django.typedclass.factory.TypedClassFieldFactory.create_field_definition","title":"<code>create_field_definition(field_info, carrier)</code>","text":"<p>Creates a Django field definition string. Example: \"my_field = models.CharField(max_length=100, null=True)\"</p> Source code in <code>src/pydantic2django/typedclass/factory.py</code> <pre><code>def create_field_definition(\n    self, field_info: TypedClassFieldInfo, carrier: ConversionCarrier[TypedClassType]\n) -&gt; str:\n    \"\"\"\n    Creates a Django field definition string.\n    Example: \"my_field = models.CharField(max_length=100, null=True)\"\n    \"\"\"\n    params = self.type_translator.translate_type(field_info.type_hint, field_info.name, carrier)\n\n    field_class = params.pop(\"field_class\", models.TextField)  # Default fallback\n    # Ensure the import for the field_class is added\n    carrier.add_django_field_import(field_class)\n\n    # Handle default values - this needs careful conversion from Python defaults to Django defaults\n    # For now, a simple string representation or skip if complex.\n    default_value_str = \"\"\n    if field_info.default_value is not inspect.Parameter.empty:\n        if isinstance(field_info.default_value, (str, int, float, bool, datetime.date, datetime.datetime)):\n            default_value_str = f\", default={repr(field_info.default_value)}\"\n        elif field_info.default_value is None:\n            default_value_str = \", default=None\"\n        # else: default for complex types is tricky, might need callable or be handled by null=True\n\n    # Constructing the field string:\n    # e.g. my_field = models.CharField(max_length=255, null=True, blank=True, default='foo')\n    param_str = []\n    for k, v in params.items():\n        if k in [\"null\", \"blank\"] and v:  # Add null=True, blank=True\n            param_str.append(f\"{k}=True\")\n        elif k == \"to\" and isinstance(v, str):  # For relationships, 'to' model name\n            param_str.append(f\"to='{v}'\")\n        elif isinstance(v, str) and k not in [\"related_name\"]:  # other string params\n            param_str.append(f\"{k}='{v}'\")\n        elif isinstance(v, type) and issubclass(v, models.Model):  # 'to' can be a model class\n            param_str.append(f\"to={v.__name__}\")  # This assumes 'to' model is in same app or imported\n        elif v is not None:  # For other types like max_length\n            param_str.append(f\"{k}={v}\")\n\n    # Add related_name for relationships if present\n    if \"related_name\" in params and params[\"related_name\"]:\n        param_str.append(f\"related_name='{params['related_name']}'\")\n\n    field_str = f\"{field_info.name} = {IMPORT_MAPPING[field_class.__module__ + '.' + field_class.__name__]}({', '.join(param_str)}{default_value_str})\"\n\n    # Placeholder for \"reckless mode\" JSONField and serialization methods\n    # if 'is_reckless' and not directly_mappable:\n    #    field_str = f\"{field_info.name} = models.JSONField(null=True, blank=True)\"\n    #    carrier.add_django_field_import(models.JSONField)\n    #    # Add placeholder for serialization/deserialization methods to carrier\n    #    carrier.add_custom_method(f\"def serialize_{field_info.name}(self): ...\")\n    #    carrier.add_custom_method(f\"@property def {field_info.name}_deserialized(self): ...\")\n\n    return field_str\n</code></pre>"},{"location":"reference/pydantic2django/typedclass/factory/#pydantic2django.typedclass.factory.TypedClassFieldInfo","title":"<code>TypedClassFieldInfo</code>  <code>dataclass</code>","text":"<p>Holds information about a discovered attribute in a generic class.</p> Source code in <code>src/pydantic2django/typedclass/factory.py</code> <pre><code>@dataclass\nclass TypedClassFieldInfo:\n    \"\"\"Holds information about a discovered attribute in a generic class.\"\"\"\n\n    name: str\n    type_hint: Any  # The type hint for the attribute\n    default_value: Any = inspect.Parameter.empty  # Default value from __init__ or class\n    is_from_init: bool = True  # True if from __init__, False if class var not in __init__\n</code></pre>"},{"location":"reference/pydantic2django/typedclass/factory/#pydantic2django.typedclass.factory.TypedClassModelFactory","title":"<code>TypedClassModelFactory</code>","text":"<p>               Bases: <code>BaseModelFactory[TypedClassType, TypedClassFieldInfo]</code></p> <p>Creates Django model definitions from generic Python classes.</p> Source code in <code>src/pydantic2django/typedclass/factory.py</code> <pre><code>class TypedClassModelFactory(BaseModelFactory[TypedClassType, TypedClassFieldInfo]):\n    \"\"\"Creates Django model definitions from generic Python classes.\"\"\"\n\n    def __init__(\n        self,\n        field_factory: TypedClassFieldFactory,\n        relationship_accessor: RelationshipConversionAccessor,\n        # Add 'reckless_mode: bool = False' if implementing that flag\n    ):\n        super().__init__(field_factory, relationship_accessor)\n        # self.reckless_mode = reckless_mode\n\n    def _get_model_fields_info(\n        self, model_class: TypedClassType, carrier: ConversionCarrier\n    ) -&gt; list[TypedClassFieldInfo]:\n        \"\"\"\n        Extracts attribute information from a generic class.\n        Prioritizes __init__ signature, then class-level annotations.\n        \"\"\"\n        field_infos = []\n        processed_params = set()\n\n        # 1. Inspect __init__ method\n        try:\n            init_signature = inspect.signature(model_class.__init__)\n            for name, param in init_signature.parameters.items():\n                if name == \"self\":\n                    continue\n\n                type_hint = param.annotation if param.annotation is not inspect.Parameter.empty else Any\n                default_val = param.default if param.default is not inspect.Parameter.empty else inspect.Parameter.empty\n\n                field_infos.append(\n                    TypedClassFieldInfo(name=name, type_hint=type_hint, default_value=default_val, is_from_init=True)\n                )\n                processed_params.add(name)\n        except (ValueError, TypeError) as e:\n            logger.debug(\n                f\"Could not inspect __init__ for {model_class.__name__}: {e}. Proceeding with class annotations.\"\n            )\n\n        # 2. Inspect class-level annotations (for attributes not in __init__)\n        try:\n            annotations = inspect.get_annotations(model_class, eval_str=True)\n            for name, type_hint in annotations.items():\n                if name not in processed_params and not name.startswith(\"_\"):  # Avoid private/protected by convention\n                    default_val = getattr(model_class, name, inspect.Parameter.empty)\n                    field_infos.append(\n                        TypedClassFieldInfo(\n                            name=name, type_hint=type_hint, default_value=default_val, is_from_init=False\n                        )\n                    )\n        except Exception as e:  # Broad exception as get_annotations can fail in various ways\n            logger.debug(f\"Could not get class annotations for {model_class.__name__}: {e}\")\n\n        logger.debug(f\"Discovered field infos for {model_class.__name__}: {field_infos}\")\n        return field_infos\n\n    def create_model_definition(\n        self,\n        model_class: TypedClassType,\n        app_label: str,\n        base_model_class: type[models.Model],\n        module_mappings: Optional[dict[str, str]] = None,\n    ) -&gt; ConversionCarrier[TypedClassType]:\n        \"\"\"\n        Generates a ConversionCarrier containing the Django model string and related info.\n        \"\"\"\n        model_name = model_class.__name__\n        django_model_name = f\"{model_name}DjangoModel\"  # Or some other naming convention\n\n        carrier = ConversionCarrier(\n            source_model=model_class,\n            source_model_name=model_name,\n            django_model_name=django_model_name,\n            app_label=app_label,\n            module_mappings=module_mappings or {},\n            relationship_accessor=self.relationship_accessor,\n        )\n\n        # Add import for the base model class\n        carrier.add_django_model_import(base_model_class)\n\n        field_definitions = []\n        model_fields_info = self._get_model_fields_info(model_class, carrier)\n\n        if not model_fields_info:\n            logger.warning(f\"No fields discovered for class {model_name}. Generating an empty Django model.\")\n            # Optionally, add a default placeholder field if empty models are problematic\n            # field_definitions.append(\"    # No convertible fields found\")\n\n        for field_info in model_fields_info:\n            try:\n                field_def_str = self.field_factory.create_field_definition(field_info, carrier)\n                field_definitions.append(f\"    {field_def_str}\")\n            except Exception as e:\n                logger.error(\n                    f\"Error creating field definition for {field_info.name} in {model_name}: {e}\", exc_info=True\n                )\n                # Optionally, add a placeholder or skip this field\n                field_definitions.append(f\"    # Error processing field: {field_info.name} - {e}\")\n\n        carrier.django_field_definitions = field_definitions\n\n        # Meta class\n        carrier.meta_class_string = generate_meta_class_string(\n            app_label=app_label,\n            django_model_name=django_model_name,  # Use the generated Django model name\n            verbose_name=model_name,\n        )\n\n        # __str__ method\n        # Heuristic: use 'name' or 'id' attribute if present in field_infos, else default\n        str_field = \"id\"  # Django models get 'id' by default from models.Model\n        for finfo in model_fields_info:\n            if finfo.name in [\"name\", \"title\", \"identifier\"]:  # common __str__ candidates\n                str_field = finfo.name\n                break\n\n        carrier.str_method_string = f\"    def __str__(self):\\n        return str(self.{str_field})\"\n\n        logger.info(f\"Prepared ConversionCarrier for {model_name} -&gt; {django_model_name}\")\n        return carrier\n</code></pre>"},{"location":"reference/pydantic2django/typedclass/factory/#pydantic2django.typedclass.factory.TypedClassModelFactory.create_model_definition","title":"<code>create_model_definition(model_class, app_label, base_model_class, module_mappings=None)</code>","text":"<p>Generates a ConversionCarrier containing the Django model string and related info.</p> Source code in <code>src/pydantic2django/typedclass/factory.py</code> <pre><code>def create_model_definition(\n    self,\n    model_class: TypedClassType,\n    app_label: str,\n    base_model_class: type[models.Model],\n    module_mappings: Optional[dict[str, str]] = None,\n) -&gt; ConversionCarrier[TypedClassType]:\n    \"\"\"\n    Generates a ConversionCarrier containing the Django model string and related info.\n    \"\"\"\n    model_name = model_class.__name__\n    django_model_name = f\"{model_name}DjangoModel\"  # Or some other naming convention\n\n    carrier = ConversionCarrier(\n        source_model=model_class,\n        source_model_name=model_name,\n        django_model_name=django_model_name,\n        app_label=app_label,\n        module_mappings=module_mappings or {},\n        relationship_accessor=self.relationship_accessor,\n    )\n\n    # Add import for the base model class\n    carrier.add_django_model_import(base_model_class)\n\n    field_definitions = []\n    model_fields_info = self._get_model_fields_info(model_class, carrier)\n\n    if not model_fields_info:\n        logger.warning(f\"No fields discovered for class {model_name}. Generating an empty Django model.\")\n        # Optionally, add a default placeholder field if empty models are problematic\n        # field_definitions.append(\"    # No convertible fields found\")\n\n    for field_info in model_fields_info:\n        try:\n            field_def_str = self.field_factory.create_field_definition(field_info, carrier)\n            field_definitions.append(f\"    {field_def_str}\")\n        except Exception as e:\n            logger.error(\n                f\"Error creating field definition for {field_info.name} in {model_name}: {e}\", exc_info=True\n            )\n            # Optionally, add a placeholder or skip this field\n            field_definitions.append(f\"    # Error processing field: {field_info.name} - {e}\")\n\n    carrier.django_field_definitions = field_definitions\n\n    # Meta class\n    carrier.meta_class_string = generate_meta_class_string(\n        app_label=app_label,\n        django_model_name=django_model_name,  # Use the generated Django model name\n        verbose_name=model_name,\n    )\n\n    # __str__ method\n    # Heuristic: use 'name' or 'id' attribute if present in field_infos, else default\n    str_field = \"id\"  # Django models get 'id' by default from models.Model\n    for finfo in model_fields_info:\n        if finfo.name in [\"name\", \"title\", \"identifier\"]:  # common __str__ candidates\n            str_field = finfo.name\n            break\n\n    carrier.str_method_string = f\"    def __str__(self):\\n        return str(self.{str_field})\"\n\n    logger.info(f\"Prepared ConversionCarrier for {model_name} -&gt; {django_model_name}\")\n    return carrier\n</code></pre>"},{"location":"reference/pydantic2django/typedclass/factory/#pydantic2django.typedclass.factory.TypedClassTypeTranslator","title":"<code>TypedClassTypeTranslator</code>","text":"<p>               Bases: <code>BaseTypeTranslator</code></p> <p>Translates Python types from generic classes to Django field parameters.</p> Source code in <code>src/pydantic2django/typedclass/factory.py</code> <pre><code>class TypedClassTypeTranslator(BaseTypeTranslator):\n    \"\"\"Translates Python types from generic classes to Django field parameters.\"\"\"\n\n    def __init__(self, relationship_accessor: RelationshipConversionAccessor):\n        super().__init__(relationship_accessor)\n        # Add any TypedClass specific mappings or overrides\n        self.type_mapping.update(\n            {\n                # Example: maybe some specific handling for common non-serializable types\n                # in reckless mode later.\n            }\n        )\n\n    def translate_type(self, field_type: Any, field_name: str, carrier: ConversionCarrier) -&gt; dict[str, Any]:\n        \"\"\"\n        Translates a Python type to Django field parameters.\n        Focuses on __init__ args and class vars.\n        \"\"\"\n        # For TypedClass, we primarily rely on direct type hints.\n        # This will be expanded for \"reckless\" mode.\n        origin = get_origin(field_type)\n        args = get_args(field_type)\n\n        if origin is Union and type(None) in args:  # Optional field\n            # Get the actual type from Union[T, None]\n            actual_type = next(arg for arg in args if arg is not type(None))\n            params = self._get_django_field_params(actual_type, field_name, carrier)\n            params[\"null\"] = True\n            params[\"blank\"] = True  # Often good practice for optional fields\n            return params\n\n        # Handle basic types\n        params = self._get_django_field_params(field_type, field_name, carrier)\n\n        # If no direct mapping, and we're not in \"reckless mode\" (to be added)\n        # we might return a default or raise an error/warning.\n        # For now, if _get_django_field_params returns a TextField, that's the fallback.\n        if not params.get(\"field_class\"):\n            logger.warning(\n                f\"Could not map type {field_type} for field {field_name} in {carrier.source_model_name}. Defaulting to TextField.\"\n            )\n            params[\"field_class\"] = models.TextField\n            params[\"default\"] = \"None\"  # Or some indication it's a placeholder\n            carrier.add_django_field_import(models.TextField)\n\n        # Placeholder for relationship handling (ForeignKey, OneToOne, ManyToMany)\n        # This will require checking if 'field_type' or 'actual_type' is a discovered TypedClassType\n        # and using the relationship_accessor.\n        # For now, assume simple fields or fallback to TextField/JSONField.\n\n        return params\n</code></pre>"},{"location":"reference/pydantic2django/typedclass/factory/#pydantic2django.typedclass.factory.TypedClassTypeTranslator.translate_type","title":"<code>translate_type(field_type, field_name, carrier)</code>","text":"<p>Translates a Python type to Django field parameters. Focuses on init args and class vars.</p> Source code in <code>src/pydantic2django/typedclass/factory.py</code> <pre><code>def translate_type(self, field_type: Any, field_name: str, carrier: ConversionCarrier) -&gt; dict[str, Any]:\n    \"\"\"\n    Translates a Python type to Django field parameters.\n    Focuses on __init__ args and class vars.\n    \"\"\"\n    # For TypedClass, we primarily rely on direct type hints.\n    # This will be expanded for \"reckless\" mode.\n    origin = get_origin(field_type)\n    args = get_args(field_type)\n\n    if origin is Union and type(None) in args:  # Optional field\n        # Get the actual type from Union[T, None]\n        actual_type = next(arg for arg in args if arg is not type(None))\n        params = self._get_django_field_params(actual_type, field_name, carrier)\n        params[\"null\"] = True\n        params[\"blank\"] = True  # Often good practice for optional fields\n        return params\n\n    # Handle basic types\n    params = self._get_django_field_params(field_type, field_name, carrier)\n\n    # If no direct mapping, and we're not in \"reckless mode\" (to be added)\n    # we might return a default or raise an error/warning.\n    # For now, if _get_django_field_params returns a TextField, that's the fallback.\n    if not params.get(\"field_class\"):\n        logger.warning(\n            f\"Could not map type {field_type} for field {field_name} in {carrier.source_model_name}. Defaulting to TextField.\"\n        )\n        params[\"field_class\"] = models.TextField\n        params[\"default\"] = \"None\"  # Or some indication it's a placeholder\n        carrier.add_django_field_import(models.TextField)\n\n    # Placeholder for relationship handling (ForeignKey, OneToOne, ManyToMany)\n    # This will require checking if 'field_type' or 'actual_type' is a discovered TypedClassType\n    # and using the relationship_accessor.\n    # For now, assume simple fields or fallback to TextField/JSONField.\n\n    return params\n</code></pre>"},{"location":"reference/pydantic2django/typedclass/generator/","title":"pydantic2django.typedclass.generator","text":""},{"location":"reference/pydantic2django/typedclass/generator/#pydantic2django.typedclass.generator.TypedClassDjangoModelGenerator","title":"<code>TypedClassDjangoModelGenerator</code>","text":"<p>               Bases: <code>BaseStaticGenerator[TypedClassType, TypedClassFieldInfo]</code></p> <p>Generates Django models.py file content from generic Python classes.</p> Source code in <code>src/pydantic2django/typedclass/generator.py</code> <pre><code>class TypedClassDjangoModelGenerator(\n    BaseStaticGenerator[TypedClassType, TypedClassFieldInfo]  # Use TypedClass types\n):\n    \"\"\"Generates Django models.py file content from generic Python classes.\"\"\"\n\n    def __init__(\n        self,\n        output_path: str,\n        app_label: str,\n        filter_function: Optional[Callable[[TypedClassType], bool]],\n        verbose: bool,\n        packages: list[str] | None = None,\n        discovery_instance: Optional[TypedClassDiscovery] = None,\n        model_factory_instance: Optional[TypedClassModelFactory] = None,\n        field_factory_instance: Optional[TypedClassFieldFactory] = None,\n        relationship_accessor: Optional[RelationshipConversionAccessor] = None,\n        module_mappings: Optional[dict[str, str]] = None,\n        base_model_class: type[models.Model] = TypedClass2DjangoBaseClass,\n        # Add reckless_mode: bool = False if we pass it down to factories\n    ):\n        # 1. Initialize TypedClass-specific discovery\n        self.typedclass_discovery_instance = discovery_instance or TypedClassDiscovery()\n\n        # 2. Initialize TypedClass-specific factories\n        self.relationship_accessor = relationship_accessor or RelationshipConversionAccessor()\n        # self.bidirectional_mapper = BidirectionalTypeMapper(relationship_accessor=self.relationship_accessor) # If needed\n\n        self.typedclass_field_factory = field_factory_instance or TypedClassFieldFactory(\n            relationship_accessor=self.relationship_accessor,\n            # Pass reckless_mode here if implemented\n        )\n        self.typedclass_model_factory = model_factory_instance or TypedClassModelFactory(\n            field_factory=self.typedclass_field_factory,\n            relationship_accessor=self.relationship_accessor,\n            # Pass reckless_mode here if implemented\n        )\n\n        # 3. Call the base class __init__\n        super().__init__(\n            output_path=output_path,\n            packages=packages,\n            app_label=app_label,\n            filter_function=filter_function,\n            verbose=verbose,\n            discovery_instance=self.typedclass_discovery_instance,\n            model_factory_instance=self.typedclass_model_factory,\n            module_mappings=module_mappings,\n            base_model_class=base_model_class,\n        )\n        logger.info(\"TypedClassDjangoModelGenerator initialized.\")\n\n    # --- Implement abstract methods from BaseStaticGenerator ---\n\n    def _get_source_model_name(self, carrier: ConversionCarrier[TypedClassType]) -&gt; str:\n        \"\"\"Get the name of the original generic class from the carrier.\"\"\"\n        if carrier.source_model:\n            return carrier.source_model.__name__\n        return \"UnknownTypedClass\"\n\n    def _add_source_model_import(self, carrier: ConversionCarrier[TypedClassType]):\n        \"\"\"Add the necessary import for the original generic class.\"\"\"\n        model_to_import = carrier.source_model\n        if model_to_import:\n            # For generic classes, these are just standard Python class imports\n            self.import_handler.add_general_import(model_to_import.__module__, model_to_import.__name__)\n        else:\n            logger.warning(\"Cannot add source model import: source model missing in carrier.\")\n\n    def _prepare_template_context(self, unique_model_definitions, django_model_names, imports) -&gt; dict:\n        \"\"\"Prepare the context specific to generic classes for the main models_file.py.j2 template.\"\"\"\n        base_context = {\n            \"model_definitions\": unique_model_definitions,\n            \"django_model_names\": django_model_names,\n            \"imports\": imports,\n            \"generation_source_type\": \"typedclass\",  # Flag for template logic\n            # Ensure compatibility with existing templates or adapt templates\n            \"context_definitions\": [],\n            \"context_class_names\": [],\n            \"model_has_context\": {},\n        }\n        return base_context\n\n    def _get_models_in_processing_order(self) -&gt; list[TypedClassType]:\n        \"\"\"Return generic classes in dependency order using the discovery instance.\"\"\"\n        assert isinstance(\n            self.discovery_instance, TypedClassDiscovery\n        ), \"Discovery instance must be TypedClassDiscovery for this generator\"\n        return self.discovery_instance.get_models_in_registration_order()\n\n    def _get_model_definition_extra_context(self, carrier: ConversionCarrier[TypedClassType]) -&gt; dict:\n        \"\"\"Provide extra context specific to generic classes for model_definition.py.j2.\"\"\"\n        return {\n            \"is_typedclass_source\": True,  # New flag for typedclass\n            \"is_dataclass_source\": False,\n            \"is_pydantic_source\": False,\n            \"has_context\": False,\n            \"field_definitions\": carrier.django_field_definitions,\n            # Add custom methods if reckless mode generated any serialization helpers\n            # \"custom_methods\": carrier.custom_methods\n        }\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/","title":"pydantic2django.xmlschema","text":"<p>Lightweight xmlschema package initializer.</p> <p>Exports objects lazily to avoid importing Django-dependent code unless needed.</p> <p>Also exposes a minimal in-memory registry for dynamically generated Django model classes so that utilities like the XML ingestor can resolve model classes even when the target app is not installed in Django's app registry. This is especially useful for unsaved instance workflows in tests.</p>"},{"location":"reference/pydantic2django/xmlschema/#pydantic2django.xmlschema.XmlInstanceIngestor","title":"<code>XmlInstanceIngestor</code>","text":"<p>Schema-aware ingestor for XML instance documents.</p> <p>Given XSD schema files and an app_label where models were generated, this ingestor parses an XML instance and creates the corresponding Django model instances, wiring up relationships according to generation strategy.</p> Source code in <code>src/pydantic2django/xmlschema/ingestor.py</code> <pre><code>class XmlInstanceIngestor:\n    \"\"\"\n    Schema-aware ingestor for XML instance documents.\n\n    Given XSD schema files and an app_label where models were generated,\n    this ingestor parses an XML instance and creates the corresponding Django\n    model instances, wiring up relationships according to generation strategy.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        schema_files: list[str | Path],\n        app_label: str,\n        dynamic_model_fallback: bool = False,\n        strict: bool = False,\n    ):\n        \"\"\"\n        Initialize a schema-aware XML ingestor.\n\n        Args:\n            schema_files: Paths to XSD files used to parse/resolve types.\n            app_label: Django app label expected to contain installed models.\n            dynamic_model_fallback: Controls behavior when an installed model is missing.\n                - True: fall back to dynamically generated stand-ins from the\n                  in-memory registry, allowing unsaved/ephemeral workflows.\n                - False (default): raise ModelResolutionError when a discovered complex type cannot be\n                  resolved to an installed Django model, avoiding implicit usage of stand-ins.\n\n        Behavior:\n            - Regardless of the fallback flag, the ingestor will always prefer installed Django\n              models and pre-cache them for all discovered complex types to ensure consistent\n              resolution across root and nested elements.\n        \"\"\"\n        try:  # Validate dependency early\n            import lxml.etree  # noqa: F401\n        except ImportError as exc:  # pragma: no cover - environment dependent\n            raise ImportError(\"lxml is required for XML ingestion. Install with: pip install lxml\") from exc\n\n        self.app_label = app_label\n        self._save_objects: bool = True\n        self._dynamic_model_fallback: bool = bool(dynamic_model_fallback)\n        self._strict: bool = bool(strict)\n        self.created_instances: list[Any] = []\n\n        discovery = XmlSchemaDiscovery()\n        discovery.discover_models(packages=[str(p) for p in schema_files], app_label=app_label)\n        # Keep references for mapping\n        self._schemas: list[XmlSchemaDefinition] = list(discovery.parsed_schemas)\n\n        # Pre-resolve and cache installed Django model classes for all discovered complex types.\n        # This guarantees we consistently use concrete models (with managers) when available.\n        self._model_resolution_cache: dict[str, type] = {}\n        try:\n            discovered_names: set[str] = set()\n            for schema in self._schemas:\n                for ct in schema.get_all_complex_types():\n                    discovered_names.add(ct.name)\n            for model_name in discovered_names:\n                try:\n                    model_cls = django_apps.get_model(f\"{self.app_label}.{model_name}\")\n                except Exception:\n                    model_cls = None\n                if model_cls is not None:\n                    self._model_resolution_cache[model_name] = model_cls\n        except Exception:\n            # Cache population is best-effort; fall back to on-demand lookups.\n            self._model_resolution_cache = {}\n\n    # --- Public API ---\n    def ingest_from_string(self, xml_string: str, *, save: bool = True) -&gt; Any:\n        \"\"\"\n        Ingest an XML instance from a string, returning the created root Django instance.\n        \"\"\"\n        import lxml.etree as _etree\n\n        self._save_objects = bool(save)\n        self.created_instances = []\n        root = _etree.fromstring(xml_string)\n        return self._ingest_root_element(root)\n\n    def ingest_from_file(self, xml_path: str | Path, *, save: bool = True) -&gt; Any:\n        \"\"\"\n        Ingest an XML instance from a file path, returning the created root Django instance.\n        \"\"\"\n        xml_path = Path(xml_path)\n        import lxml.etree as _etree\n\n        self._save_objects = bool(save)\n        self.created_instances = []\n        with xml_path.open(\"rb\") as f:\n            tree = _etree.parse(f)\n        root = tree.getroot()\n        return self._ingest_root_element(root)\n\n    def validate_models(self, *, strict: bool = True) -&gt; list[ContractIssue]:\n        \"\"\"\n        Validate that discovered schema types align with installed Django models.\n\n        - Ensures expected fields from attributes, simple elements, and single nested complex\n          elements exist on the corresponding Django model.\n        - Flags Timescale-based models that are missing the canonical 'time' field.\n\n        Returns a list of ContractIssue. If strict=True and issues exist, raises SchemaSyncError\n        with a concise remediation message.\n        \"\"\"\n        issues: list[ContractIssue] = []\n        for schema in self._schemas:\n            try:\n                complex_types = list(schema.get_all_complex_types())\n            except Exception:\n                continue\n            for ct in complex_types:\n                model_cls = self._get_model_for_complex_type(ct)\n                if model_cls is None:\n                    issues.append(\n                        ContractIssue(\n                            complex_type_name=ct.name,\n                            model_name=f\"{self.app_label}.{ct.name}\",\n                            missing_fields=[\"&lt;installed model not found&gt;\"],\n                            extra_fields=[],\n                            problems=[\"No installed Django model resolved for complex type\"],\n                        )\n                    )\n                    continue\n\n                try:\n                    model_field_names = {f.name for f in model_cls._meta.fields}\n                except Exception:\n                    model_field_names = set()\n\n                expected_fields: set[str] = set()\n                # Attributes\n                for attr_name in getattr(ct, \"attributes\", {}).keys():\n                    expected_fields.add(self._xml_name_to_django_field(attr_name))\n                # Elements (simple + parent links for single nested complex)\n                for el in getattr(ct, \"elements\", []):\n                    if getattr(el, \"type_name\", None) and getattr(el, \"base_type\", None) is None:\n                        if not getattr(el, \"is_list\", False):\n                            expected_fields.add(self._xml_name_to_django_field(el.name))\n                        continue\n                    expected_fields.add(self._xml_name_to_django_field(el.name))\n\n                missing = sorted([fn for fn in expected_fields if fn not in model_field_names])\n                problems: list[str] = []\n                if self._is_timescale_model(model_cls) and \"time\" not in model_field_names:\n                    problems.append(\"Timescale model missing required 'time' field\")\n\n                if missing or problems:\n                    issues.append(\n                        ContractIssue(\n                            complex_type_name=ct.name,\n                            model_name=f\"{self.app_label}.{model_cls.__name__}\",\n                            missing_fields=missing,\n                            extra_fields=[],\n                            problems=problems,\n                        )\n                    )\n\n        if strict and issues:\n            details = \"; \".join(\n                f\"[{i.model_name}] missing={i.missing_fields or '-'} problems={i.problems or '-'}\" for i in issues[:5]\n            )\n            raise SchemaSyncError(\n                f\"Schema and static model are out of sync; verify schema and/or regenerate static Django models and re-migrate. Details: {details}\"\n            )\n        return issues\n\n    # --- Core ingestion ---\n    def _ingest_root_element(self, elem: Any) -&gt; Any:\n        local_name = self._local_name(elem.tag)\n        complex_type = self._resolve_root_complex_type(local_name)\n        if complex_type is None:\n            known_types = []\n            try:\n                for s in self._schemas:\n                    known_types.extend([ct.name for ct in s.get_all_complex_types()])\n            except Exception:\n                pass\n            raise ValueError(\n                \"Could not resolve complex type for root element \"\n                f\"'{local_name}'. Known discovered types (sample): \"\n                f\"{known_types[:10]}\"\n            )\n\n        model_cls = self._get_model_for_complex_type(complex_type)\n        if model_cls is None:\n            raise ValueError(f\"Could not find Django model for complex type '{complex_type.name}'\")\n\n        instance = self._build_instance_from_element(elem, complex_type, model_cls, parent_instance=None)\n        return instance\n\n    def _build_instance_from_element(\n        self,\n        elem: Any,\n        complex_type: XmlSchemaComplexType,\n        model_cls: type,\n        parent_instance: Any | None,\n        parent_link_field: str | None = None,\n    ) -&gt; Any:\n        \"\"\"\n        Create and save a Django model instance from an XML element according to its complex type.\n        \"\"\"\n        # Prepare field values for simple elements and attributes first\n        field_values: dict[str, Any] = {}\n\n        # Attributes on complex types\n        for attr_name, _attr in complex_type.attributes.items():\n            xml_attr_value = elem.get(attr_name)\n            if xml_attr_value is not None:\n                dj_name = self._xml_name_to_django_field(attr_name)\n                field_values[dj_name] = xml_attr_value\n\n        # Child elements\n        children_by_local: dict[str, list[Any]] = {}\n        for child in elem:\n            if not isinstance(child.tag, str):\n                continue\n            lname = self._local_name(child.tag)\n            children_by_local.setdefault(lname, []).append(child)\n\n        # Strict mode: detect unexpected XML child elements and attributes not declared in the schema\n        if self._strict:\n            expected_child_names = {e.name for e in getattr(complex_type, \"elements\", [])}\n            unexpected_children = sorted([n for n in children_by_local.keys() if n not in expected_child_names])\n            if unexpected_children:\n                raise SchemaSyncError(\n                    \"Schema and static model are out of sync; verify schema and/or regenerate static Django models and re-migrate. \"\n                    f\"Unexpected XML elements for type '{complex_type.name}': {unexpected_children}\"\n                )\n            expected_attr_names = set(getattr(complex_type, \"attributes\", {}).keys())\n            xml_attr_local = {self._local_name(k) for k in getattr(elem, \"attrib\", {}).keys()}\n            unexpected_attrs = sorted([n for n in xml_attr_local if n not in expected_attr_names])\n            if unexpected_attrs:\n                raise SchemaSyncError(\n                    \"Schema and static model are out of sync; verify schema and/or regenerate static Django models and re-migrate. \"\n                    f\"Unmapped XML attributes for type '{complex_type.name}': {unexpected_attrs}\"\n                )\n\n        # Map simple fields and collect nested complex elements to process\n        nested_to_process: list[tuple[XmlSchemaElement, list[Any]]] = []\n        for el_def in complex_type.elements:\n            name = el_def.name\n            matched_children = children_by_local.get(name, [])\n            if not matched_children:\n                continue\n\n            if el_def.type_name and el_def.base_type is None:\n                # Nested complex type\n                nested_to_process.append((el_def, matched_children))\n                continue\n\n            # Simple content\n            # Multiple occurrences of a simple element -&gt; pick first; advanced list handling can be added later\n            first = matched_children[0]\n            dj_name = self._xml_name_to_django_field(name)\n            field_values[dj_name] = first.text\n\n        # If this element is a repeated child (child_fk strategy), and we know the FK field name\n        # on the child model, include it in initial field values to satisfy NOT NULL constraints.\n        if parent_instance is not None and parent_link_field:\n            try:\n                model_field_names = {f.name for f in model_cls._meta.fields}\n                if parent_link_field in model_field_names:\n                    field_values.setdefault(parent_link_field, parent_instance)\n            except Exception:\n                pass\n\n        # Instantiate without other relationships first\n        instance: Any\n        if self._save_objects:\n            # Timescale-aware timestamp remapping: if model expects a 'time' field,\n            # remap common XML timestamp attributes (e.g., creationTime \u2192 creation_time)\n            # into 'time' when not already provided.\n            try:\n                model_field_names = {f.name for f in model_cls._meta.fields}\n            except Exception:\n                model_field_names = set()\n            if has_timescale_time_field(model_field_names) and \"time\" not in field_values:\n                # Always attempt a helpful alias remap; only enforce requiredness for Timescale models\n                map_time_alias_into_time(field_values, aliases=TIMESERIES_TIME_ALIASES)\n                if self._is_timescale_model(model_cls) and \"time\" not in field_values:\n                    raise TimeseriesTimestampMissingError(\n                        model_cls.__name__, attempted_aliases=list(TIMESERIES_TIME_ALIASES)\n                    )\n\n            instance = model_cls.objects.create(**field_values)\n        else:\n            try:\n                # Mirror the same remapping for unsaved construction flows\n                try:\n                    model_field_names = {f.name for f in model_cls._meta.fields}\n                except Exception:\n                    model_field_names = set()\n                if has_timescale_time_field(model_field_names) and \"time\" not in field_values:\n                    # For unsaved flows, remap aliases but do not enforce a hard requirement\n                    map_time_alias_into_time(field_values, aliases=TIMESERIES_TIME_ALIASES)\n                instance = model_cls(**field_values)\n            except TypeError as exc:\n                # If the dynamically generated class is abstract (not installed app),\n                # construct a lightweight proxy object for unsaved workflows.\n                is_abstract = getattr(getattr(model_cls, \"_meta\", None), \"abstract\", None)\n                if is_abstract:\n                    try:\n                        proxy_cls = type(model_cls.__name__, (), {})\n                        instance = proxy_cls()\n                        for k, v in field_values.items():\n                            setattr(instance, k, v)\n                    except Exception:\n                        raise TypeError(f\"Cannot instantiate abstract model '{model_cls.__name__}'\") from exc\n                else:\n                    raise\n\n        # Track created/instantiated instance\n        try:\n            self.created_instances.append(instance)\n        except Exception:\n            pass\n\n        # Attach any remaining XML attributes as dynamic attributes if not mapped\n        try:\n            model_field_names = {f.name for f in model_cls._meta.fields}\n            unmapped_dynamic: list[str] = []\n            for attr_name, attr_val in getattr(elem, \"attrib\", {}).items():\n                dj_name = self._xml_name_to_django_field(attr_name)\n                if dj_name not in field_values and dj_name not in model_field_names:\n                    if self._strict:\n                        unmapped_dynamic.append(dj_name)\n                    else:\n                        setattr(instance, dj_name, attr_val)\n            if self._strict and unmapped_dynamic:\n                raise SchemaSyncError(\n                    \"Schema and static model are out of sync; verify schema and/or regenerate static Django models and re-migrate. \"\n                    f\"Unmapped XML attributes for type '{complex_type.name}': {unmapped_dynamic}\"\n                )\n        except Exception:\n            pass\n\n        # Handle nested complex elements\n        for el_def, elements in nested_to_process:\n            target_type_name = (el_def.type_name or \"\").split(\":\")[-1]\n            target_complex_type = self._find_complex_type(target_type_name)\n            if target_complex_type is None:\n                logger.warning(\"Unknown nested complex type '%s' for element '%s'\", target_type_name, el_def.name)\n                continue\n            target_model_cls = self._get_model_for_complex_type(target_complex_type)\n            if target_model_cls is None:\n                logger.warning(\n                    \"Missing Django model for nested type '%s' (element '%s')\", target_type_name, el_def.name\n                )\n                continue\n\n            if el_def.is_list:\n                # If the parent model exposes GenericRelation('entries'), persist as GenericEntry rows\n                try:\n                    has_entries = hasattr(instance, \"entries\")\n                except Exception:\n                    has_entries = False\n                if has_entries:\n                    try:\n                        GenericEntry = django_apps.get_model(f\"{self.app_label}.GenericEntry\")\n                    except Exception:\n                        GenericEntry = None\n                    if GenericEntry is not None:\n                        for idx, child_elem in enumerate(elements):\n                            entry_data_kwargs = self._build_generic_entry_kwargs(\n                                instance=instance,\n                                el_name=el_def.name,\n                                target_type_name=target_type_name,\n                                child_elem=child_elem,\n                                GenericEntry=GenericEntry,\n                                order_index=idx,\n                            )\n                            # Create entry row\n                            if self._save_objects:\n                                GenericEntry.objects.create(**entry_data_kwargs)\n                        # Skip concrete child instance creation when using GFK\n                        continue\n                # Default generation style 'child_fk': inject FK on child named after parent class in lowercase\n                parent_fk_field = instance.__class__.__name__.lower()\n                for child_elem in elements:\n                    child_instance: Any = self._build_instance_from_element(\n                        child_elem,\n                        target_complex_type,\n                        target_model_cls,\n                        parent_instance=instance,\n                        parent_link_field=parent_fk_field,\n                    )\n                    # Set parent FK on child; save update if field exists\n                    if hasattr(child_instance, parent_fk_field):\n                        setattr(child_instance, parent_fk_field, instance)\n                        if self._save_objects:\n                            child_instance.save(update_fields=[parent_fk_field])\n                    else:\n                        # If strategy was m2m/json, this will be a no-op; can extend later\n                        logger.debug(\n                            \"Child model %s lacks FK field '%s' to parent; skipping back-link\",\n                            child_instance.__class__.__name__,\n                            parent_fk_field,\n                        )\n                continue\n\n            # Single nested complex element: either FK on parent or GFK entry (all_nested policy)\n            parent_fk_name = self._xml_name_to_django_field(el_def.name)\n            # If parent exposes entries and does NOT define a concrete FK field for this element, persist as GenericEntry\n            use_gfk_single = False\n            try:\n                has_entries = hasattr(instance, \"entries\")\n            except Exception:\n                has_entries = False\n            if has_entries:\n                try:\n                    model_field_names = {f.name for f in model_cls._meta.fields}\n                except Exception:\n                    model_field_names = set()\n                if parent_fk_name not in model_field_names:\n                    use_gfk_single = True\n\n            if use_gfk_single:\n                try:\n                    GenericEntry = django_apps.get_model(f\"{self.app_label}.GenericEntry\")\n                except Exception:\n                    GenericEntry = None\n                if GenericEntry is not None:\n                    child_elem = elements[0]\n                    single_entry_kwargs = self._build_generic_entry_kwargs(\n                        instance=instance,\n                        el_name=el_def.name,\n                        target_type_name=target_type_name,\n                        child_elem=child_elem,\n                        GenericEntry=GenericEntry,\n                        order_index=0,\n                    )\n                    if self._save_objects:\n                        GenericEntry.objects.create(**single_entry_kwargs)\n                # Skip concrete child instance creation\n                continue\n\n            # Default: create child instance and set FK on parent\n            child_elem = elements[0]\n            child_instance = self._build_instance_from_element(\n                child_elem, target_complex_type, target_model_cls, parent_instance=instance\n            )\n\n            # For proxy instances (non-Django), the attribute may not exist yet; set unconditionally\n            try:\n                setattr(instance, parent_fk_name, child_instance)\n                if self._save_objects and hasattr(instance, \"save\"):\n                    instance.save(update_fields=[parent_fk_name])\n            except Exception:\n                logger.debug(\n                    \"Could not set nested field '%s' on parent %s\",\n                    parent_fk_name,\n                    instance.__class__.__name__,\n                )\n\n        return instance\n\n    @staticmethod\n    def _build_generic_entry_kwargs(\n        *,\n        instance: Any,\n        el_name: str,\n        target_type_name: str | None,\n        child_elem: Any,\n        GenericEntry: Any,\n        order_index: int,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Build keyword args for creating a `GenericEntry` row from an XML child element.\n\n        Behavior is aligned with GFK flag semantics:\n        - `gfk_value_mode=\"json_only\"`: element text is stored inside `attrs_json[\"value\"]`.\n        - `gfk_value_mode=\"typed_columns\"`: if the `GenericEntry` model declares typed\n          columns (`text_value`, `num_value`, `time_value`), element text is copied to\n          `text_value` and parsed into `num_value` and/or `time_value` when unambiguous.\n          Remaining attributes are preserved in `attrs_json`.\n\n        Args:\n            instance: The owning Django model instance for `content_object`.\n            el_name: Local name of the XML element.\n            target_type_name: Resolved type name for the element (simple or complex), if available.\n            child_elem: Parsed XML element (lxml Element or equivalent).\n            GenericEntry: The `GenericEntry` Django model class for this app.\n            order_index: Monotonic index of this entry under the owner, used to preserve order.\n\n        Returns:\n            A dict of keyword arguments suitable for `GenericEntry.objects.create(**kwargs)`.\n        \"\"\"\n        entry_attrs_json: dict[str, Any] = {}\n        try:\n            for k, v in getattr(child_elem, \"attrib\", {}).items():\n                entry_attrs_json[k] = v\n        except Exception:\n            pass\n\n        entry_kwargs: dict[str, Any] = {\n            \"content_object\": instance,\n            \"element_qname\": el_name,\n            \"type_qname\": target_type_name or None,\n            \"attrs_json\": entry_attrs_json,\n            \"order_index\": order_index,\n            \"path_hint\": el_name,\n        }\n\n        # Handle text value\n        try:\n            text_val = getattr(child_elem, \"text\", None)\n        except Exception:\n            text_val = None\n        if text_val and str(text_val).strip() != \"\":\n            raw = str(text_val).strip()\n            # If GenericEntry defines typed columns, attempt parse to numeric/time\n            has_text_col = False\n            try:\n                has_text_col = hasattr(GenericEntry, \"_meta\") and any(\n                    f.name == \"text_value\" for f in GenericEntry._meta.fields\n                )\n            except Exception:\n                has_text_col = False\n            if has_text_col:\n                entry_kwargs[\"text_value\"] = raw\n                try:\n                    entry_kwargs[\"num_value\"] = Decimal(raw)\n                except Exception:\n                    pass\n                try:\n                    iso = raw.replace(\"Z\", \"+00:00\")\n                    entry_kwargs[\"time_value\"] = _dt.fromisoformat(iso)\n                except Exception:\n                    pass\n            else:\n                entry_attrs_json[\"value\"] = raw\n\n        return entry_kwargs\n\n    # --- Helpers ---\n    def _resolve_root_complex_type(self, root_local_name: str) -&gt; XmlSchemaComplexType | None:\n        # Try global elements with explicit type\n        for schema in self._schemas:\n            element = schema.elements.get(root_local_name)\n            if element and element.type_name:\n                type_name = element.type_name.split(\":\")[-1]\n                ct = schema.find_complex_type(type_name, namespace=schema.target_namespace)\n                if ct:\n                    return ct\n        # Try complex type named exactly as root\n        for schema in self._schemas:\n            ct = schema.find_complex_type(root_local_name, namespace=schema.target_namespace)\n            if ct:\n                return ct\n        return None\n\n    def _find_complex_type(self, type_name: str) -&gt; XmlSchemaComplexType | None:\n        for schema in self._schemas:\n            ct = schema.find_complex_type(type_name, namespace=schema.target_namespace)\n            if ct:\n                return ct\n        return None\n\n    def _get_model_for_complex_type(self, complex_type: XmlSchemaComplexType) -&gt; type | None:\n        model_name = complex_type.name\n        # Prefer cache of installed models first for determinism across root and nested types\n        cached = getattr(self, \"_model_resolution_cache\", {}).get(model_name)\n        if cached is not None:\n            return cached\n\n        # Otherwise, attempt to resolve from app registry, then fall back to generated stand-ins\n        try:\n            model_cls = django_apps.get_model(f\"{self.app_label}.{model_name}\")\n            # Populate cache for subsequent lookups\n            try:\n                self._model_resolution_cache[model_name] = model_cls\n            except Exception:\n                pass\n            return model_cls\n        except Exception as exc:\n            # Optionally fallback to in-memory registry for dynamically generated classes\n            if not getattr(self, \"_dynamic_model_fallback\", True):\n                # Provide a detailed error to aid configuration\n                raise ModelResolutionError(\n                    app_label=self.app_label,\n                    model_name=model_name,\n                    message=(\n                        \"Installed Django model not found and dynamic model fallback is disabled. \"\n                        \"Ensure your concrete models are installed under the correct app label, or enable \"\n                        \"dynamic fallback explicitly.\"\n                    ),\n                ) from exc\n            try:\n                return get_generated_model(self.app_label, model_name)\n            except Exception:\n                return None\n\n    @staticmethod\n    def _is_timescale_model(model_cls: type) -&gt; bool:\n        try:\n            from pydantic2django.django.models import TimescaleModel as _TsModel\n        except Exception:\n            return False\n        try:\n            return issubclass(model_cls, _TsModel)\n        except Exception:\n            return False\n\n    @staticmethod\n    def _local_name(qname: str) -&gt; str:\n        if \"}\" in qname:\n            return qname.split(\"}\", 1)[1]\n        if \":\" in qname:\n            return qname.split(\":\", 1)[1]\n        return qname\n\n    @staticmethod\n    def _xml_name_to_django_field(xml_name: str) -&gt; str:\n        # Use shared normalization so all ingestion paths stay consistent\n        return sanitize_field_identifier(xml_name)\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/#pydantic2django.xmlschema.XmlInstanceIngestor.__init__","title":"<code>__init__(*, schema_files, app_label, dynamic_model_fallback=False, strict=False)</code>","text":"<p>Initialize a schema-aware XML ingestor.</p> <p>Parameters:</p> Name Type Description Default <code>schema_files</code> <code>list[str | Path]</code> <p>Paths to XSD files used to parse/resolve types.</p> required <code>app_label</code> <code>str</code> <p>Django app label expected to contain installed models.</p> required <code>dynamic_model_fallback</code> <code>bool</code> <p>Controls behavior when an installed model is missing. - True: fall back to dynamically generated stand-ins from the   in-memory registry, allowing unsaved/ephemeral workflows. - False (default): raise ModelResolutionError when a discovered complex type cannot be   resolved to an installed Django model, avoiding implicit usage of stand-ins.</p> <code>False</code> Behavior <ul> <li>Regardless of the fallback flag, the ingestor will always prefer installed Django   models and pre-cache them for all discovered complex types to ensure consistent   resolution across root and nested elements.</li> </ul> Source code in <code>src/pydantic2django/xmlschema/ingestor.py</code> <pre><code>def __init__(\n    self,\n    *,\n    schema_files: list[str | Path],\n    app_label: str,\n    dynamic_model_fallback: bool = False,\n    strict: bool = False,\n):\n    \"\"\"\n    Initialize a schema-aware XML ingestor.\n\n    Args:\n        schema_files: Paths to XSD files used to parse/resolve types.\n        app_label: Django app label expected to contain installed models.\n        dynamic_model_fallback: Controls behavior when an installed model is missing.\n            - True: fall back to dynamically generated stand-ins from the\n              in-memory registry, allowing unsaved/ephemeral workflows.\n            - False (default): raise ModelResolutionError when a discovered complex type cannot be\n              resolved to an installed Django model, avoiding implicit usage of stand-ins.\n\n    Behavior:\n        - Regardless of the fallback flag, the ingestor will always prefer installed Django\n          models and pre-cache them for all discovered complex types to ensure consistent\n          resolution across root and nested elements.\n    \"\"\"\n    try:  # Validate dependency early\n        import lxml.etree  # noqa: F401\n    except ImportError as exc:  # pragma: no cover - environment dependent\n        raise ImportError(\"lxml is required for XML ingestion. Install with: pip install lxml\") from exc\n\n    self.app_label = app_label\n    self._save_objects: bool = True\n    self._dynamic_model_fallback: bool = bool(dynamic_model_fallback)\n    self._strict: bool = bool(strict)\n    self.created_instances: list[Any] = []\n\n    discovery = XmlSchemaDiscovery()\n    discovery.discover_models(packages=[str(p) for p in schema_files], app_label=app_label)\n    # Keep references for mapping\n    self._schemas: list[XmlSchemaDefinition] = list(discovery.parsed_schemas)\n\n    # Pre-resolve and cache installed Django model classes for all discovered complex types.\n    # This guarantees we consistently use concrete models (with managers) when available.\n    self._model_resolution_cache: dict[str, type] = {}\n    try:\n        discovered_names: set[str] = set()\n        for schema in self._schemas:\n            for ct in schema.get_all_complex_types():\n                discovered_names.add(ct.name)\n        for model_name in discovered_names:\n            try:\n                model_cls = django_apps.get_model(f\"{self.app_label}.{model_name}\")\n            except Exception:\n                model_cls = None\n            if model_cls is not None:\n                self._model_resolution_cache[model_name] = model_cls\n    except Exception:\n        # Cache population is best-effort; fall back to on-demand lookups.\n        self._model_resolution_cache = {}\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/#pydantic2django.xmlschema.XmlInstanceIngestor.ingest_from_file","title":"<code>ingest_from_file(xml_path, *, save=True)</code>","text":"<p>Ingest an XML instance from a file path, returning the created root Django instance.</p> Source code in <code>src/pydantic2django/xmlschema/ingestor.py</code> <pre><code>def ingest_from_file(self, xml_path: str | Path, *, save: bool = True) -&gt; Any:\n    \"\"\"\n    Ingest an XML instance from a file path, returning the created root Django instance.\n    \"\"\"\n    xml_path = Path(xml_path)\n    import lxml.etree as _etree\n\n    self._save_objects = bool(save)\n    self.created_instances = []\n    with xml_path.open(\"rb\") as f:\n        tree = _etree.parse(f)\n    root = tree.getroot()\n    return self._ingest_root_element(root)\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/#pydantic2django.xmlschema.XmlInstanceIngestor.ingest_from_string","title":"<code>ingest_from_string(xml_string, *, save=True)</code>","text":"<p>Ingest an XML instance from a string, returning the created root Django instance.</p> Source code in <code>src/pydantic2django/xmlschema/ingestor.py</code> <pre><code>def ingest_from_string(self, xml_string: str, *, save: bool = True) -&gt; Any:\n    \"\"\"\n    Ingest an XML instance from a string, returning the created root Django instance.\n    \"\"\"\n    import lxml.etree as _etree\n\n    self._save_objects = bool(save)\n    self.created_instances = []\n    root = _etree.fromstring(xml_string)\n    return self._ingest_root_element(root)\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/#pydantic2django.xmlschema.XmlInstanceIngestor.validate_models","title":"<code>validate_models(*, strict=True)</code>","text":"<p>Validate that discovered schema types align with installed Django models.</p> <ul> <li>Ensures expected fields from attributes, simple elements, and single nested complex   elements exist on the corresponding Django model.</li> <li>Flags Timescale-based models that are missing the canonical 'time' field.</li> </ul> <p>Returns a list of ContractIssue. If strict=True and issues exist, raises SchemaSyncError with a concise remediation message.</p> Source code in <code>src/pydantic2django/xmlschema/ingestor.py</code> <pre><code>def validate_models(self, *, strict: bool = True) -&gt; list[ContractIssue]:\n    \"\"\"\n    Validate that discovered schema types align with installed Django models.\n\n    - Ensures expected fields from attributes, simple elements, and single nested complex\n      elements exist on the corresponding Django model.\n    - Flags Timescale-based models that are missing the canonical 'time' field.\n\n    Returns a list of ContractIssue. If strict=True and issues exist, raises SchemaSyncError\n    with a concise remediation message.\n    \"\"\"\n    issues: list[ContractIssue] = []\n    for schema in self._schemas:\n        try:\n            complex_types = list(schema.get_all_complex_types())\n        except Exception:\n            continue\n        for ct in complex_types:\n            model_cls = self._get_model_for_complex_type(ct)\n            if model_cls is None:\n                issues.append(\n                    ContractIssue(\n                        complex_type_name=ct.name,\n                        model_name=f\"{self.app_label}.{ct.name}\",\n                        missing_fields=[\"&lt;installed model not found&gt;\"],\n                        extra_fields=[],\n                        problems=[\"No installed Django model resolved for complex type\"],\n                    )\n                )\n                continue\n\n            try:\n                model_field_names = {f.name for f in model_cls._meta.fields}\n            except Exception:\n                model_field_names = set()\n\n            expected_fields: set[str] = set()\n            # Attributes\n            for attr_name in getattr(ct, \"attributes\", {}).keys():\n                expected_fields.add(self._xml_name_to_django_field(attr_name))\n            # Elements (simple + parent links for single nested complex)\n            for el in getattr(ct, \"elements\", []):\n                if getattr(el, \"type_name\", None) and getattr(el, \"base_type\", None) is None:\n                    if not getattr(el, \"is_list\", False):\n                        expected_fields.add(self._xml_name_to_django_field(el.name))\n                    continue\n                expected_fields.add(self._xml_name_to_django_field(el.name))\n\n            missing = sorted([fn for fn in expected_fields if fn not in model_field_names])\n            problems: list[str] = []\n            if self._is_timescale_model(model_cls) and \"time\" not in model_field_names:\n                problems.append(\"Timescale model missing required 'time' field\")\n\n            if missing or problems:\n                issues.append(\n                    ContractIssue(\n                        complex_type_name=ct.name,\n                        model_name=f\"{self.app_label}.{model_cls.__name__}\",\n                        missing_fields=missing,\n                        extra_fields=[],\n                        problems=problems,\n                    )\n                )\n\n    if strict and issues:\n        details = \"; \".join(\n            f\"[{i.model_name}] missing={i.missing_fields or '-'} problems={i.problems or '-'}\" for i in issues[:5]\n        )\n        raise SchemaSyncError(\n            f\"Schema and static model are out of sync; verify schema and/or regenerate static Django models and re-migrate. Details: {details}\"\n        )\n    return issues\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/#pydantic2django.xmlschema.XmlSchemaDjangoModelGenerator","title":"<code>XmlSchemaDjangoModelGenerator</code>","text":"<p>               Bases: <code>BaseStaticGenerator[XmlSchemaComplexType, XmlSchemaFieldInfo]</code></p> <p>Main class to orchestrate the generation of Django models from XML Schemas.</p> Source code in <code>src/pydantic2django/xmlschema/generator.py</code> <pre><code>class XmlSchemaDjangoModelGenerator(BaseStaticGenerator[XmlSchemaComplexType, XmlSchemaFieldInfo]):\n    \"\"\"\n    Main class to orchestrate the generation of Django models from XML Schemas.\n    \"\"\"\n\n    def __init__(\n        self,\n        schema_files: list[str | Path],\n        output_path: str = \"generated_models.py\",\n        app_label: str = \"xmlschema_app\",\n        filter_function: Callable[[XmlSchemaComplexType], bool] | None = None,\n        verbose: bool = False,\n        module_mappings: dict[str, str] | None = None,\n        class_name_prefix: str = \"\",\n        # Relationship handling for nested complex types\n        nested_relationship_strategy: str = \"fk\",  # one of: \"fk\", \"json\", \"auto\"\n        list_relationship_style: str = \"child_fk\",  # one of: \"child_fk\", \"m2m\", \"json\"\n        nesting_depth_threshold: int = 1,\n        # Optional override for the Django base model class\n        base_model_class: type[models.Model] | None = None,\n        # Feature flags\n        enable_timescale: bool = True,\n        # Timescale configuration\n        timescale_overrides: dict[str, TimescaleRole] | None = None,\n        timescale_config: TimescaleConfig | None = None,\n        timescale_strict: bool = False,\n        # Control behavior for missing leaf/child targets in finalize pass\n        auto_generate_missing_leaves: bool = False,\n        # --- GFK flags ---\n        enable_gfk: bool = True,\n        gfk_policy: str | None = \"threshold_by_children\",\n        gfk_threshold_children: int | None = 8,\n        gfk_value_mode: str | None = \"typed_columns\",\n        gfk_normalize_common_attrs: bool = False,\n        gfk_overrides: dict[str, bool] | None = None,\n    ):\n        \"\"\"Create a XML Schema \u2192 Django generator with optional Generic Entries mode.\n\n        GFK (Generic Entries) flags:\n        - enable_gfk: Enable Generic Entries mode. When True and policy matches, parents\n          gain `entries = GenericRelation('GenericEntry')` and concrete children are not emitted.\n        - gfk_policy: Controls which nested elements are routed to GenericEntry.\n          Values: \"substitution_only\" | \"repeating_only\" | \"all_nested\" | \"threshold_by_children\".\n        - gfk_threshold_children: Used only with \"threshold_by_children\"; minimum distinct child\n          complex types inside a wrapper-like container to activate GFK.\n        - gfk_value_mode: \"json_only\" stores text under attrs_json['value']; \"typed_columns\"\n          extracts text into text_value, num_value, time_value when unambiguous.\n        - gfk_normalize_common_attrs: Reserved (default False) for promoting frequently used\n          attributes (e.g., timestamp) into normalized columns when using typed columns.\n        - gfk_overrides: Optional per-element overrides by local element name;\n          True forces GFK, False disables it even if the policy would apply.\n        \"\"\"\n        discovery = XmlSchemaDiscovery()\n        model_factory = XmlSchemaModelFactory(\n            app_label=app_label,\n            nested_relationship_strategy=nested_relationship_strategy,\n            list_relationship_style=list_relationship_style,\n            nesting_depth_threshold=nesting_depth_threshold,\n            enable_gfk=enable_gfk,\n            gfk_policy=gfk_policy,\n            gfk_threshold_children=gfk_threshold_children,\n            gfk_overrides=gfk_overrides,\n        )\n\n        super().__init__(\n            output_path=output_path,\n            packages=[str(f) for f in schema_files],\n            app_label=app_label,\n            discovery_instance=discovery,\n            model_factory_instance=model_factory,\n            base_model_class=base_model_class or self._get_default_base_model_class(),\n            class_name_prefix=class_name_prefix,\n            module_mappings=module_mappings,\n            verbose=verbose,\n            filter_function=filter_function,\n            enable_timescale=enable_timescale,\n            enable_gfk=enable_gfk,\n            gfk_policy=gfk_policy,\n            gfk_threshold_children=gfk_threshold_children,\n            gfk_value_mode=gfk_value_mode,\n            gfk_normalize_common_attrs=gfk_normalize_common_attrs,\n        )\n        # Timescale classification results cached per run\n        self._timescale_roles: dict[str, TimescaleRole] = {}\n        self._timescale_overrides: dict[str, TimescaleRole] | None = timescale_overrides\n        self._timescale_config: TimescaleConfig | None = timescale_config\n        self._timescale_strict: bool = timescale_strict\n        self._auto_generate_missing_leaves: bool = auto_generate_missing_leaves\n\n    def _get_model_definition_extra_context(self, carrier: ConversionCarrier) -&gt; dict:\n        \"\"\"\n        Extracts additional context required for rendering the Django model,\n        including field definitions and enum classes.\n        \"\"\"\n        field_definitions = carrier.django_field_definitions\n\n        enum_classes = carrier.context_data.get(\"enums\", {})\n\n        return {\n            \"field_definitions\": field_definitions,\n            \"enum_classes\": enum_classes.values(),\n            # Extra Meta emission (e.g., indexes)\n            \"meta_indexes\": carrier.context_data.get(\"meta_indexes\", []),\n        }\n\n    # All rendering logic is now handled by the BaseStaticGenerator using the implemented abstract methods.\n    # The custom generate and _generate_file_content methods are no longer needed.\n\n    # --- Implement abstract methods from BaseStaticGenerator ---\n\n    def _get_source_model_name(self, carrier: ConversionCarrier[XmlSchemaComplexType]) -&gt; str:\n        \"\"\"Get the name of the original source model from the carrier.\"\"\"\n        return carrier.source_model.name\n\n    def _add_source_model_import(self, carrier: ConversionCarrier[XmlSchemaComplexType]):\n        \"\"\"Add the necessary import for the original source model.\"\"\"\n        # For XML Schema, the models are generated, not imported\n        pass\n\n    def _prepare_template_context(\n        self, unique_model_definitions: list[str], django_model_names: list[str], imports: dict\n    ) -&gt; dict:\n        \"\"\"Prepare the subclass-specific context for the main models_file.py.j2 template.\"\"\"\n        return {\n            \"model_definitions\": unique_model_definitions,\n            \"django_model_names\": django_model_names,\n            # Flattened import categories for templates\n            \"pydantic_imports\": sorted(imports.get(\"pydantic\", [])),\n            \"context_imports\": sorted(imports.get(\"context\", [])),\n            \"django_imports\": sorted(imports.get(\"django\", [])),\n            \"general_imports\": sorted(imports.get(\"general\", [])),\n            # Also pass the structured imports dict if templates need it\n            \"imports\": imports,\n            \"generation_timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n            \"app_label\": self.app_label,\n            \"generation_source_type\": \"xmlschema\",\n        }\n\n    def _get_default_base_model_class(self) -&gt; type[models.Model]:\n        \"\"\"Return the default Django base model for XML Schema conversion.\"\"\"\n        # Prefer local pydantic2django base; fall back to typed2django if present\n        try:\n            from pydantic2django.django.models import Xml2DjangoBaseClass as _Base\n\n            return _Base\n        except Exception as exc:\n            logger.debug(\n                \"Default base model import failed; will try typed2django fallback: %s\",\n                exc,\n                exc_info=True,\n            )\n        try:\n            from typed2django.django.models import (\n                Xml2DjangoBaseClass as _Base,  # type: ignore[import-not-found]\n            )\n\n            return _Base\n        except Exception as exc:  # pragma: no cover - defensive\n            raise ImportError(\n                \"pydantic2django.django.models.Xml2DjangoBaseClass (or typed2django equivalent) is required for XML Schema generation.\"\n            ) from exc\n\n    def _get_models_in_processing_order(self) -&gt; list[XmlSchemaComplexType]:\n        \"\"\"Return source models in the correct processing (dependency) order.\"\"\"\n        return self.discovery_instance.get_models_in_registration_order()\n\n    # --- Additional XML Schema specific methods ---\n\n    def generate_models_with_xml_metadata(self) -&gt; str:\n        \"\"\"\n        Generate Django models with additional XML metadata.\n\n        This method extends the base generate() to add XML-specific\n        comments and metadata to the generated models.\n        \"\"\"\n        content = self.generate_models_file()\n\n        # Add XML Schema file references as comments at the top\n        schema_files_comment = \"\\n\".join(\n            [f\"# Generated from XML Schema: {schema_file}\" for schema_file in self.schema_files]\n        )\n\n        # Insert after the initial comments\n        lines = content.split(\"\\n\")\n        insert_index = 0\n        for i, line in enumerate(lines):\n            if line.startswith('\"\"\"') and '\"\"\"' in line[3:]:  # Single line docstring\n                insert_index = i + 1\n                break\n            elif line.startswith('\"\"\"'):  # Multi-line docstring start\n                for j in range(i + 1, len(lines)):\n                    if '\"\"\"' in lines[j]:\n                        insert_index = j + 1\n                        break\n                break\n\n        lines.insert(insert_index, schema_files_comment)\n        lines.insert(insert_index + 1, \"\")\n\n        return \"\\n\".join(lines)\n\n    def get_schema_statistics(self) -&gt; dict:\n        \"\"\"Get statistics about the parsed schemas.\"\"\"\n        stats = {\n            \"total_schemas\": len(self.discovery_instance.parsed_schemas),\n            \"total_complex_types\": len(self.discovery_instance.all_models),\n            \"filtered_complex_types\": len(self.discovery_instance.filtered_models),\n            \"generated_models\": len(self.carriers),\n        }\n\n        # Add per-schema breakdown\n        schema_breakdown = []\n        for schema_def in self.discovery_instance.parsed_schemas:\n            schema_breakdown.append(\n                {\n                    \"schema_location\": schema_def.schema_location,\n                    \"target_namespace\": schema_def.target_namespace,\n                    \"complex_types\": len(schema_def.complex_types),\n                    \"simple_types\": len(schema_def.simple_types),\n                    \"elements\": len(schema_def.elements),\n                }\n            )\n\n        stats[\"schema_breakdown\"] = schema_breakdown\n        return stats\n\n    def validate_schemas(self) -&gt; list[str]:\n        \"\"\"\n        Validate the parsed schemas and return any warnings or errors.\n\n        Returns:\n            List of validation messages\n        \"\"\"\n        messages = []\n\n        for schema_def in self.discovery_instance.parsed_schemas:\n            # Check for common issues\n            if not schema_def.target_namespace:\n                messages.append(f\"Schema {schema_def.schema_location} has no target namespace\")\n\n            # Check for name conflicts\n            all_names = set()\n            for complex_type in schema_def.complex_types.values():\n                if complex_type.name in all_names:\n                    messages.append(f\"Duplicate type name: {complex_type.name}\")\n                all_names.add(complex_type.name)\n\n        return messages\n\n    @classmethod\n    def from_schema_files(cls, schema_files: list[str | Path], **kwargs) -&gt; \"XmlSchemaDjangoModelGenerator\":\n        \"\"\"\n        Convenience class method to create generator from schema files.\n\n        Args:\n            schema_files: List of XSD file paths\n            **kwargs: Additional arguments passed to __init__\n\n        Returns:\n            Configured XmlSchemaDjangoModelGenerator instance\n        \"\"\"\n        return cls(schema_files=schema_files, **kwargs)\n\n    def _render_choices_class(self, choices_info: dict) -&gt; str:\n        \"\"\"Render a single TextChoices class.\"\"\"\n        class_name = choices_info[\"name\"]\n        choices = choices_info[\"choices\"]\n        lines = [f\"class {class_name}(models.TextChoices):\"]\n        for value, label in choices:\n            # Attempt to create a valid Python identifier for the member name\n            member_name = re.sub(r\"[^a-zA-Z0-9_]\", \"_\", label.upper())\n            if not member_name or not member_name[0].isalpha():\n                member_name = f\"CHOICE_{member_name}\"\n            lines.append(f'    {member_name} = \"{value}\", \"{label}\"')\n        return \"\\\\n\".join(lines)\n\n    def generate(self):\n        \"\"\"\n        Main method to generate the Django models file.\n        \"\"\"\n        logger.info(f\"Starting Django model generation to {self.output_path}\")\n\n        # The base class now handles the full generation pipeline\n        super().generate()\n\n        logger.info(f\"Successfully generated Django models in {self.output_path}\")\n\n    def generate_models_file(self) -&gt; str:\n        \"\"\"\n        Override to allow relationship finalization after carriers are built\n        but before rendering templates.\n        \"\"\"\n        # Discover and create carriers first via base implementation pieces\n        self.discover_models()\n        # --- GFK Prepass: when enabled, identify wrapper owners and exclude their polymorphic children ---\n        if getattr(self, \"enable_gfk\", False) and getattr(self, \"gfk_policy\", None):\n            try:\n                excluded_child_types: set[str] = set()\n                gfk_owner_wrappers: set[str] = set()\n                policy = str(getattr(self, \"gfk_policy\", \"\"))\n                threshold_val = int(getattr(self, \"gfk_threshold_children\", 0) or 0)\n                # Walk parsed schemas to infer wrapper owners and their child types\n                for schema_def in getattr(self.discovery_instance, \"parsed_schemas\", []) or []:\n                    try:\n                        for ct in schema_def.get_all_complex_types():\n                            # Iterate elements of each complex type\n                            for el in ct.elements:\n                                # Only interested in complex targets\n                                if not el.type_name or el.base_type is not None:\n                                    continue\n                                target_type = el.type_name.split(\":\")[-1]\n                                target_ct = schema_def.complex_types.get(target_type)\n                                if not target_ct:\n                                    continue\n                                # Wrapper-like heuristic (may be used by providers; structural checks take precedence)\n                                _is_wrapper_like = bool(el.name[:1].isupper()) or str(target_type).endswith(\n                                    \"WrapperType\"\n                                )\n                                # Collect distinct child complex types under target wrapper\n                                distinct_child_types: set[str] = set()\n                                has_substitution_members = False\n                                repeating_leaf_child: str | None = None\n                                for child_el in target_ct.elements:\n                                    if getattr(child_el, \"substitution_group\", None):\n                                        has_substitution_members = True\n                                    if getattr(child_el, \"type_name\", None):\n                                        cand = child_el.type_name.split(\":\")[-1]\n                                        if cand:\n                                            distinct_child_types.add(cand)\n                                    if getattr(child_el, \"is_list\", False) and getattr(child_el, \"type_name\", None):\n                                        repeating_leaf_child = child_el.type_name.split(\":\")[-1]\n\n                                # Decide based on policy (favor generic structural checks over name heuristics)\n                                route_by_subst = policy == \"substitution_only\" and has_substitution_members\n                                route_by_all_nested = policy == \"all_nested\"\n                                route_by_threshold = policy == \"threshold_by_children\" and (\n                                    threshold_val &lt;= 0 or len(distinct_child_types) &gt;= threshold_val\n                                )\n                                route_by_repeating = policy == \"repeating_only\" and bool(el.is_list)\n\n                                if route_by_subst or route_by_all_nested or route_by_threshold or route_by_repeating:\n                                    # Mark the CURRENT complex type (container) as the owner so placeholders on it are suppressed\n                                    try:\n                                        owner_name = getattr(ct, \"name\", None) or getattr(ct, \"__name__\", str(ct))\n                                    except Exception:\n                                        owner_name = target_type\n                                    gfk_owner_wrappers.add(owner_name)\n                                    if getattr(self, \"verbose\", False):\n                                        reason = (\n                                            \"substitution_only\"\n                                            if route_by_subst\n                                            else (\n                                                \"all_nested\"\n                                                if route_by_all_nested\n                                                else (\n                                                    \"threshold_by_children\" if route_by_threshold else \"repeating_only\"\n                                                )\n                                            )\n                                        )\n                                        logger.info(\n                                            \"[GFK][prepass] owner=%s element=%s policy=%s distinct_children=%d\",\n                                            owner_name,\n                                            el.name,\n                                            reason,\n                                            len(distinct_child_types),\n                                        )\n                                    # Exclude polymorphic children under this wrapper from concrete generation\n                                    if el.is_list and repeating_leaf_child:\n                                        excluded_child_types.add(repeating_leaf_child)\n                                    # Also exclude distinct child complex types observed under the wrapper\n                                    excluded_child_types.update({t for t in distinct_child_types if t})\n                    except Exception:\n                        logger.error(\n                            \"[GFK][prepass] Failed while processing schema '%s'\",\n                            getattr(schema_def, \"schema_location\", \"?\"),\n                            exc_info=True,\n                        )\n                        continue  # best-effort per schema\n\n                # Filter discovery output to remove excluded children, keeping wrappers/parents\n                try:\n                    # filtered_models is a mapping of qualname -&gt; complex_type\n                    fm = dict(getattr(self.discovery_instance, \"filtered_models\", {}) or {})\n                    if fm and excluded_child_types:\n                        new_fm = {k: v for k, v in fm.items() if getattr(v, \"name\", None) not in excluded_child_types}\n                        self.discovery_instance.filtered_models = new_fm  # type: ignore[attr-defined]\n                        if getattr(self, \"verbose\", False):\n                            logger.info(\n                                \"[GFK][prepass] excluded_children_count=%d (examples: %s)\",\n                                len(excluded_child_types),\n                                \", \".join(sorted(excluded_child_types)[:10]),\n                            )\n                except Exception as exc:\n                    logger.error(\"[GFK][prepass] Failed to filter excluded children: %s\", exc, exc_info=True)\n\n                # Share exclusions and owner marks with the field factory for intra-model decisions\n                try:\n                    ff = getattr(self.model_factory_instance, \"field_factory\", None)\n                    if ff is not None:\n                        ff._gfk_excluded_child_types = excluded_child_types\n                        # Seed owner names to suppress JSON placeholders for wrapper children\n                        owner_names = set(getattr(ff, \"_gfk_owner_names\", set()))\n                        owner_names.update(gfk_owner_wrappers)\n                        ff._gfk_owner_names = owner_names\n                except Exception as exc:\n                    logger.error(\"[GFK][prepass] Failed to share state with field factory: %s\", exc, exc_info=True)\n            except Exception as exc:\n                # Prepass is best-effort and should never crash generation\n                logger.error(\"[GFK][prepass] Unexpected error: %s\", exc, exc_info=True)\n        # Conflict guard: if GFK owners detected but JSON relationship styles are requested, fail fast.\n        if getattr(self, \"enable_gfk\", False):\n            ff = getattr(self.model_factory_instance, \"field_factory\", None)\n            owners = set(getattr(ff, \"_gfk_owner_names\", set()) or set())\n            list_style = getattr(ff, \"list_relationship_style\", \"child_fk\")\n            nested_style = getattr(ff, \"nested_relationship_strategy\", \"fk\")\n            if owners and (list_style == \"json\" or nested_style == \"json\"):\n                raise ValueError(\n                    \"GFK mode active with owners detected, but JSON relationship strategy was requested. \"\n                    \"This configuration would produce dual emission. Adjust flags or disable JSON strategy.\"\n                )\n        # Inform the field factory which models are actually included so it can\n        # avoid generating relations to filtered-out models (fallback to JSON).\n        try:\n            # Prefer local (unqualified) complex type names to match factory lookups\n            # filtered_models keys may be qualified as \"{namespace}.{name}\", so derive plain names\n            included_local_names = {\n                getattr(m, \"name\", str(m))\n                for m in self.discovery_instance.filtered_models.values()  # type: ignore[attr-defined]\n            }\n            # Also include any already-qualified keys for maximum compatibility\n            included_qualified_keys = set(self.discovery_instance.filtered_models.keys())  # type: ignore[attr-defined]\n            included_names = included_local_names | included_qualified_keys\n            # type: ignore[attr-defined]\n            self.model_factory_instance.field_factory.included_model_names = included_names  # noqa: E501\n            # Also pass through any GFK prepass exclusions to the field factory if not already set\n            ff = self.model_factory_instance.field_factory\n            if not getattr(ff, \"_gfk_excluded_child_types\", None):\n                ff._gfk_excluded_child_types = set()\n        except Exception as exc:\n            logger.error(\n                \"Failed to inform field factory about included models or defaults: %s\",\n                exc,\n                exc_info=True,\n            )\n        models_to_process = self._get_models_in_processing_order()\n\n        # Classify models for Timescale usage (hypertable vs dimension)\n        if self.enable_timescale:\n            try:\n                self._timescale_roles = classify_xml_complex_types(\n                    models_to_process,\n                    overrides=self._timescale_overrides,\n                    config=self._timescale_config,\n                )\n            except Exception as exc:\n                logger.error(\"Timescale classification failed; proceeding without roles: %s\", exc, exc_info=True)\n                self._timescale_roles = {}\n        else:\n            self._timescale_roles = {}\n\n        # Strict validation: if any hypertable lacks a direct time-like field, fail fast\n        if self.enable_timescale and self._timescale_strict:\n            for m in models_to_process:\n                name = getattr(m, \"name\", getattr(m, \"__name__\", str(m)))\n                if self._timescale_roles.get(name) == TimescaleRole.HYPERTABLE and not has_direct_time_feature(m):\n                    raise ValueError(\n                        f\"Timescale strict mode: '{name}' classified as hypertable but has no direct time-like field. \"\n                        f\"Add a time/timestamp-like attribute/element or demote via overrides.\"\n                    )\n\n        # Reset state for this run (mirror BaseStaticGenerator)\n        self.carriers = []\n        self.import_handler.extra_type_imports.clear()\n        self.import_handler.pydantic_imports.clear()\n        self.import_handler.context_class_imports.clear()\n        self.import_handler.imported_names.clear()\n        self.import_handler.processed_field_types.clear()\n        self.import_handler._add_type_import(self.base_model_class)\n\n        for source_model in models_to_process:\n            self.setup_django_model(source_model)\n\n        # Give the factory a chance to add cross-model relationship fields (e.g., child FKs)\n        carriers_by_name = {\n            (getattr(c.source_model, \"name\", None) or getattr(c.source_model, \"__name__\", \"\")): c\n            for c in self.carriers\n            if c.django_model is not None\n        }\n        if hasattr(self.model_factory_instance, \"finalize_relationships\"):\n            try:\n                # type: ignore[attr-defined]\n                self.model_factory_instance.finalize_relationships(carriers_by_name, self.app_label)  # noqa: E501\n            except Exception as e:\n                logger.error(\"Error finalizing XML relationships: %s\", e, exc_info=True)\n\n        # Inject GenericRelation on parents when GFK is enabled and pending children exist\n        gfk_used = False\n        try:\n            for carrier in self.carriers:\n                if getattr(carrier, \"enable_gfk\", False) and getattr(carrier, \"pending_gfk_children\", None):\n                    # Ensure contenttypes imports\n                    self.import_handler.add_import(\"django.contrib.contenttypes.fields\", \"GenericRelation\")\n                    self.import_handler.add_import(\"django.contrib.contenttypes.fields\", \"GenericForeignKey\")\n                    self.import_handler.add_import(\"django.contrib.contenttypes.models\", \"ContentType\")\n                    # Add reverse relation field on parent model\n                    carrier.django_field_definitions[\n                        \"entries\"\n                    ] = \"GenericRelation('GenericEntry', related_query_name='entries')\"\n                    gfk_used = True\n        except Exception as exc:\n            logger.error(\"Failed while injecting GenericRelation/GenericEntry fields: %s\", exc, exc_info=True)\n\n        # Optional diagnostics: summarize GFK routing per parent\n        if gfk_used and getattr(self, \"verbose\", False):\n            try:\n                summary: dict[str, int] = {}\n                for carrier in self.carriers:\n                    for entry in getattr(carrier, \"pending_gfk_children\", []) or []:\n                        owner = str(entry.get(\"owner\") or getattr(carrier.source_model, \"name\", \"\"))\n                        summary[owner] = summary.get(owner, 0) + 1\n                if summary:\n                    details = \", \".join(f\"{k}:{v}\" for k, v in sorted(summary.items()))\n                    logger.info(f\"[GFK] Routed children counts by owner: {details}\")\n            except Exception as exc:\n                logger.error(\"Failed to summarize GFK routing: %s\", exc, exc_info=True)\n\n        # Register generated model classes for in-memory lookup by ingestors\n        try:\n            for carrier in self.carriers:\n                if carrier.django_model is not None:\n                    register_generated_model(self.app_label, carrier.django_model)\n                    logger.info(\n                        \"Registered generated model %s (abstract=%s) for app '%s'\",\n                        carrier.django_model.__name__,\n                        getattr(getattr(carrier.django_model, \"_meta\", None), \"abstract\", None),\n                        self.app_label,\n                    )\n                    # Prevent dynamic classes from polluting Django's global app registry\n                    try:\n                        from django.apps import (\n                            apps as django_apps,  # Local import to avoid hard dependency\n                        )\n\n                        model_lower = getattr(getattr(carrier.django_model, \"_meta\", None), \"model_name\", None)\n                        if model_lower:\n                            django_apps.all_models.get(self.app_label, {}).pop(model_lower, None)\n                    except Exception as exc:\n                        # Best-effort cleanup only, but log for visibility\n                        logger.error(\"Failed to clean up Django global app registry: %s\", exc, exc_info=True)\n        except Exception as e:\n            logger.error(\n                \"Non-fatal: failed to register generated models for app '%s': %s\",\n                self.app_label,\n                e,\n                exc_info=True,\n            )\n\n        # Proceed with standard definition rendering\n        model_definitions = []\n        django_model_names = []\n        for carrier in self.carriers:\n            if carrier.django_model:\n                try:\n                    model_def = self.generate_model_definition(carrier)\n                    if model_def:\n                        model_definitions.append(model_def)\n                        django_model_names.append(f\"'{self._clean_generic_type(carrier.django_model.__name__)}'\")\n                except Exception as e:\n                    logger.error(\n                        f\"Error generating definition for source model {getattr(carrier.source_model, '__name__', '?')}: {e}\",\n                        exc_info=True,\n                    )\n\n        # If GFK used, append GenericEntry model definition and include in __all__\n        if gfk_used:\n            model_definitions.append(self._build_generic_entry_model_definition())\n            django_model_names.append(\"'GenericEntry'\")\n\n        unique_model_definitions = self._deduplicate_definitions(model_definitions)\n        # Ensure that every advertised model name in __all__ has a corresponding class definition.\n        try:\n            import re as _re\n\n            joined_defs = \"\\n\".join(unique_model_definitions)\n            existing_names = {m.group(1) for m in _re.finditer(r\"^\\s*class\\s+(\\w+)\\(\", joined_defs, _re.MULTILINE)}\n            advertised_names = [\n                self._clean_generic_type(getattr(c.django_model, \"__name__\", \"\"))\n                for c in self.carriers\n                if c.django_model is not None\n            ]\n            for name in advertised_names:\n                if name and name not in existing_names:\n                    minimal_def = (\n                        f'\"\"\"\\nDjango model for {name}.\\n\"\"\"\\n\\n'\n                        f\"class {name}({self.base_model_class.__name__}):\\n    # No fields defined for this model.\\n    pass\\n\\n    class Meta:\\n        app_label = '{self.app_label}'\\n        abstract = False\\n\\n\"\n                    )\n                    unique_model_definitions.append(minimal_def)\n        except Exception as exc:\n            logger.error(\n                \"Failed to ensure all advertised model names have definitions: %s\",\n                exc,\n                exc_info=True,\n            )\n\n        imports = self.import_handler.deduplicate_imports()\n        template_context = self._prepare_template_context(unique_model_definitions, django_model_names, imports)\n        template_context.update(\n            {\n                \"generation_timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n                \"base_model_module\": self.base_model_class.__module__,\n                \"base_model_name\": self.base_model_class.__name__,\n                \"extra_type_imports\": sorted(self.import_handler.extra_type_imports),\n            }\n        )\n        # Detect validator usage to ensure imports are present\n        used_validators: set[str] = set()\n        # Primary source: field factory tracking\n        try:\n            field_factory = getattr(self.model_factory_instance, \"field_factory\", None)\n            factory_used = getattr(field_factory, \"used_validators\", None)\n            if isinstance(factory_used, set):\n                used_validators.update(factory_used)\n        except Exception as exc:\n            logger.error(\"Failed to collect validator usage from field factory: %s\", exc, exc_info=True)\n        # Fallback: scan rendered definitions\n        try:\n            if not used_validators:\n                validator_symbols = [\n                    \"RegexValidator\",\n                    \"MinValueValidator\",\n                    \"MaxValueValidator\",\n                    \"URLValidator\",\n                    \"EmailValidator\",\n                    \"validate_slug\",\n                    \"validate_uuid4\",\n                    \"validate_email\",\n                    \"validate_ipv46_address\",\n                    \"validate_ipv4_address\",\n                    \"validate_ipv6_address\",\n                    \"validate_unicode_slug\",\n                ]\n                joined_defs = \"\\n\".join(unique_model_definitions)\n                for symbol in validator_symbols:\n                    if re.search(rf\"\\\\b{re.escape(symbol)}\\\\b\", joined_defs):\n                        used_validators.add(symbol)\n        except Exception as exc:\n            logger.error(\"Failed to scan rendered definitions for validator usage: %s\", exc, exc_info=True)\n        template_context[\"validator_imports\"] = sorted(used_validators)\n        template = self.jinja_env.get_template(\"models_file.py.j2\")\n        return template.render(**template_context)\n\n    def _build_generic_entry_model_definition(self) -&gt; str:\n        \"\"\"Build the GenericEntry model definition string for XMLSchema generation.\"\"\"\n        self.import_handler.add_import(\"django.contrib.contenttypes.fields\", \"GenericForeignKey\")\n        self.import_handler.add_import(\"django.contrib.contenttypes.fields\", \"GenericRelation\")\n        self.import_handler.add_import(\"django.contrib.contenttypes.models\", \"ContentType\")\n\n        fields: list[str] = []\n        fields.append(\"content_type = models.ForeignKey('contenttypes.ContentType', on_delete=models.CASCADE)\")\n        fields.append(\"object_id = models.PositiveIntegerField()\")\n        fields.append(\"content_object = GenericForeignKey('content_type', 'object_id')\")\n        fields.append(\"element_qname = models.CharField(max_length=255)\")\n        fields.append(\"type_qname = models.CharField(max_length=255, null=True, blank=True)\")\n        fields.append(\"attrs_json = models.JSONField(default=dict, blank=True)\")\n        if getattr(self, \"gfk_value_mode\", None) == \"typed_columns\":\n            fields.append(\"text_value = models.TextField(null=True, blank=True)\")\n            fields.append(\"num_value = models.DecimalField(max_digits=20, decimal_places=6, null=True, blank=True)\")\n            fields.append(\"time_value = models.DateTimeField(null=True, blank=True)\")\n        fields.append(\"order_index = models.IntegerField(default=0)\")\n        fields.append(\"path_hint = models.CharField(max_length=255, null=True, blank=True)\")\n\n        indexes_lines = [\"models.Index(fields=['content_type', 'object_id'])\"]\n        # Optional indexes when typed value columns are enabled\n        if getattr(self, \"gfk_value_mode\", None) == \"typed_columns\":\n            indexes_lines.append(\"models.Index(fields=['element_qname'])\")\n            indexes_lines.append(\"models.Index(fields=['type_qname'])\")\n            indexes_lines.append(\"models.Index(fields=['time_value'])\")\n            indexes_lines.append(\"models.Index(fields=['content_type', 'object_id', '-time_value'])\")\n\n        lines: list[str] = []\n        lines.append(f\"class GenericEntry({self.base_model_class.__name__}):\")\n        for f in fields:\n            lines.append(f\"    {f}\")\n        lines.append(\"\")\n        lines.append(\"    class Meta:\")\n        lines.append(f\"        app_label = '{self.app_label}'\")\n        lines.append(\"        abstract = False\")\n        lines.append(\"        indexes = [\")\n        for idx in indexes_lines:\n            lines.append(f\"            {idx},\")\n        lines.append(\"        ]\")\n        lines.append(\"\")\n        return \"\\n\".join(lines)\n\n    # Override to choose Timescale base per model and pass roles map to factory\n    def setup_django_model(self, source_model: XmlSchemaComplexType) -&gt; ConversionCarrier | None:  # type: ignore[override]\n        source_model_name = getattr(source_model, \"__name__\", getattr(source_model, \"name\", str(source_model)))\n        use_ts_base = False\n        if self.enable_timescale:\n            try:\n                use_ts_base = should_use_timescale_base(source_model_name, self._timescale_roles)\n            except Exception:\n                use_ts_base = False\n\n        base_class: type[models.Model]\n        if use_ts_base:\n            base_class = XmlTimescaleBase\n        else:\n            base_class = self.base_model_class\n\n        carrier = ConversionCarrier(\n            source_model=source_model,\n            meta_app_label=self.app_label,\n            base_django_model=base_class,\n            class_name_prefix=self.class_name_prefix,\n            strict=False,\n            enable_gfk=self.enable_gfk,\n            gfk_policy=self.gfk_policy,\n            gfk_threshold_children=self.gfk_threshold_children,\n            gfk_value_mode=self.gfk_value_mode,\n            gfk_normalize_common_attrs=self.gfk_normalize_common_attrs,\n        )\n\n        # Make roles map available to field factory for FK decisions\n        carrier.context_data[\"_timescale_roles\"] = self._timescale_roles\n\n        try:\n            self.model_factory_instance.make_django_model(carrier)\n            if carrier.django_model:\n                self.carriers.append(carrier)\n                return carrier\n            return None\n        except Exception:\n            logger.exception(\"Error creating Django model for %s\", source_model_name)\n            return None\n\n    def _write_to_file(self, content: str):\n        with open(self.output_path, \"w\") as f:\n            f.write(content)\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/#pydantic2django.xmlschema.XmlSchemaDjangoModelGenerator.__init__","title":"<code>__init__(schema_files, output_path='generated_models.py', app_label='xmlschema_app', filter_function=None, verbose=False, module_mappings=None, class_name_prefix='', nested_relationship_strategy='fk', list_relationship_style='child_fk', nesting_depth_threshold=1, base_model_class=None, enable_timescale=True, timescale_overrides=None, timescale_config=None, timescale_strict=False, auto_generate_missing_leaves=False, enable_gfk=True, gfk_policy='threshold_by_children', gfk_threshold_children=8, gfk_value_mode='typed_columns', gfk_normalize_common_attrs=False, gfk_overrides=None)</code>","text":"<p>Create a XML Schema \u2192 Django generator with optional Generic Entries mode.</p> <p>GFK (Generic Entries) flags: - enable_gfk: Enable Generic Entries mode. When True and policy matches, parents   gain <code>entries = GenericRelation('GenericEntry')</code> and concrete children are not emitted. - gfk_policy: Controls which nested elements are routed to GenericEntry.   Values: \"substitution_only\" | \"repeating_only\" | \"all_nested\" | \"threshold_by_children\". - gfk_threshold_children: Used only with \"threshold_by_children\"; minimum distinct child   complex types inside a wrapper-like container to activate GFK. - gfk_value_mode: \"json_only\" stores text under attrs_json['value']; \"typed_columns\"   extracts text into text_value, num_value, time_value when unambiguous. - gfk_normalize_common_attrs: Reserved (default False) for promoting frequently used   attributes (e.g., timestamp) into normalized columns when using typed columns. - gfk_overrides: Optional per-element overrides by local element name;   True forces GFK, False disables it even if the policy would apply.</p> Source code in <code>src/pydantic2django/xmlschema/generator.py</code> <pre><code>def __init__(\n    self,\n    schema_files: list[str | Path],\n    output_path: str = \"generated_models.py\",\n    app_label: str = \"xmlschema_app\",\n    filter_function: Callable[[XmlSchemaComplexType], bool] | None = None,\n    verbose: bool = False,\n    module_mappings: dict[str, str] | None = None,\n    class_name_prefix: str = \"\",\n    # Relationship handling for nested complex types\n    nested_relationship_strategy: str = \"fk\",  # one of: \"fk\", \"json\", \"auto\"\n    list_relationship_style: str = \"child_fk\",  # one of: \"child_fk\", \"m2m\", \"json\"\n    nesting_depth_threshold: int = 1,\n    # Optional override for the Django base model class\n    base_model_class: type[models.Model] | None = None,\n    # Feature flags\n    enable_timescale: bool = True,\n    # Timescale configuration\n    timescale_overrides: dict[str, TimescaleRole] | None = None,\n    timescale_config: TimescaleConfig | None = None,\n    timescale_strict: bool = False,\n    # Control behavior for missing leaf/child targets in finalize pass\n    auto_generate_missing_leaves: bool = False,\n    # --- GFK flags ---\n    enable_gfk: bool = True,\n    gfk_policy: str | None = \"threshold_by_children\",\n    gfk_threshold_children: int | None = 8,\n    gfk_value_mode: str | None = \"typed_columns\",\n    gfk_normalize_common_attrs: bool = False,\n    gfk_overrides: dict[str, bool] | None = None,\n):\n    \"\"\"Create a XML Schema \u2192 Django generator with optional Generic Entries mode.\n\n    GFK (Generic Entries) flags:\n    - enable_gfk: Enable Generic Entries mode. When True and policy matches, parents\n      gain `entries = GenericRelation('GenericEntry')` and concrete children are not emitted.\n    - gfk_policy: Controls which nested elements are routed to GenericEntry.\n      Values: \"substitution_only\" | \"repeating_only\" | \"all_nested\" | \"threshold_by_children\".\n    - gfk_threshold_children: Used only with \"threshold_by_children\"; minimum distinct child\n      complex types inside a wrapper-like container to activate GFK.\n    - gfk_value_mode: \"json_only\" stores text under attrs_json['value']; \"typed_columns\"\n      extracts text into text_value, num_value, time_value when unambiguous.\n    - gfk_normalize_common_attrs: Reserved (default False) for promoting frequently used\n      attributes (e.g., timestamp) into normalized columns when using typed columns.\n    - gfk_overrides: Optional per-element overrides by local element name;\n      True forces GFK, False disables it even if the policy would apply.\n    \"\"\"\n    discovery = XmlSchemaDiscovery()\n    model_factory = XmlSchemaModelFactory(\n        app_label=app_label,\n        nested_relationship_strategy=nested_relationship_strategy,\n        list_relationship_style=list_relationship_style,\n        nesting_depth_threshold=nesting_depth_threshold,\n        enable_gfk=enable_gfk,\n        gfk_policy=gfk_policy,\n        gfk_threshold_children=gfk_threshold_children,\n        gfk_overrides=gfk_overrides,\n    )\n\n    super().__init__(\n        output_path=output_path,\n        packages=[str(f) for f in schema_files],\n        app_label=app_label,\n        discovery_instance=discovery,\n        model_factory_instance=model_factory,\n        base_model_class=base_model_class or self._get_default_base_model_class(),\n        class_name_prefix=class_name_prefix,\n        module_mappings=module_mappings,\n        verbose=verbose,\n        filter_function=filter_function,\n        enable_timescale=enable_timescale,\n        enable_gfk=enable_gfk,\n        gfk_policy=gfk_policy,\n        gfk_threshold_children=gfk_threshold_children,\n        gfk_value_mode=gfk_value_mode,\n        gfk_normalize_common_attrs=gfk_normalize_common_attrs,\n    )\n    # Timescale classification results cached per run\n    self._timescale_roles: dict[str, TimescaleRole] = {}\n    self._timescale_overrides: dict[str, TimescaleRole] | None = timescale_overrides\n    self._timescale_config: TimescaleConfig | None = timescale_config\n    self._timescale_strict: bool = timescale_strict\n    self._auto_generate_missing_leaves: bool = auto_generate_missing_leaves\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/#pydantic2django.xmlschema.XmlSchemaDjangoModelGenerator.from_schema_files","title":"<code>from_schema_files(schema_files, **kwargs)</code>  <code>classmethod</code>","text":"<p>Convenience class method to create generator from schema files.</p> <p>Parameters:</p> Name Type Description Default <code>schema_files</code> <code>list[str | Path]</code> <p>List of XSD file paths</p> required <code>**kwargs</code> <p>Additional arguments passed to init</p> <code>{}</code> <p>Returns:</p> Type Description <code>XmlSchemaDjangoModelGenerator</code> <p>Configured XmlSchemaDjangoModelGenerator instance</p> Source code in <code>src/pydantic2django/xmlschema/generator.py</code> <pre><code>@classmethod\ndef from_schema_files(cls, schema_files: list[str | Path], **kwargs) -&gt; \"XmlSchemaDjangoModelGenerator\":\n    \"\"\"\n    Convenience class method to create generator from schema files.\n\n    Args:\n        schema_files: List of XSD file paths\n        **kwargs: Additional arguments passed to __init__\n\n    Returns:\n        Configured XmlSchemaDjangoModelGenerator instance\n    \"\"\"\n    return cls(schema_files=schema_files, **kwargs)\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/#pydantic2django.xmlschema.XmlSchemaDjangoModelGenerator.generate","title":"<code>generate()</code>","text":"<p>Main method to generate the Django models file.</p> Source code in <code>src/pydantic2django/xmlschema/generator.py</code> <pre><code>def generate(self):\n    \"\"\"\n    Main method to generate the Django models file.\n    \"\"\"\n    logger.info(f\"Starting Django model generation to {self.output_path}\")\n\n    # The base class now handles the full generation pipeline\n    super().generate()\n\n    logger.info(f\"Successfully generated Django models in {self.output_path}\")\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/#pydantic2django.xmlschema.XmlSchemaDjangoModelGenerator.generate_models_file","title":"<code>generate_models_file()</code>","text":"<p>Override to allow relationship finalization after carriers are built but before rendering templates.</p> Source code in <code>src/pydantic2django/xmlschema/generator.py</code> <pre><code>def generate_models_file(self) -&gt; str:\n    \"\"\"\n    Override to allow relationship finalization after carriers are built\n    but before rendering templates.\n    \"\"\"\n    # Discover and create carriers first via base implementation pieces\n    self.discover_models()\n    # --- GFK Prepass: when enabled, identify wrapper owners and exclude their polymorphic children ---\n    if getattr(self, \"enable_gfk\", False) and getattr(self, \"gfk_policy\", None):\n        try:\n            excluded_child_types: set[str] = set()\n            gfk_owner_wrappers: set[str] = set()\n            policy = str(getattr(self, \"gfk_policy\", \"\"))\n            threshold_val = int(getattr(self, \"gfk_threshold_children\", 0) or 0)\n            # Walk parsed schemas to infer wrapper owners and their child types\n            for schema_def in getattr(self.discovery_instance, \"parsed_schemas\", []) or []:\n                try:\n                    for ct in schema_def.get_all_complex_types():\n                        # Iterate elements of each complex type\n                        for el in ct.elements:\n                            # Only interested in complex targets\n                            if not el.type_name or el.base_type is not None:\n                                continue\n                            target_type = el.type_name.split(\":\")[-1]\n                            target_ct = schema_def.complex_types.get(target_type)\n                            if not target_ct:\n                                continue\n                            # Wrapper-like heuristic (may be used by providers; structural checks take precedence)\n                            _is_wrapper_like = bool(el.name[:1].isupper()) or str(target_type).endswith(\n                                \"WrapperType\"\n                            )\n                            # Collect distinct child complex types under target wrapper\n                            distinct_child_types: set[str] = set()\n                            has_substitution_members = False\n                            repeating_leaf_child: str | None = None\n                            for child_el in target_ct.elements:\n                                if getattr(child_el, \"substitution_group\", None):\n                                    has_substitution_members = True\n                                if getattr(child_el, \"type_name\", None):\n                                    cand = child_el.type_name.split(\":\")[-1]\n                                    if cand:\n                                        distinct_child_types.add(cand)\n                                if getattr(child_el, \"is_list\", False) and getattr(child_el, \"type_name\", None):\n                                    repeating_leaf_child = child_el.type_name.split(\":\")[-1]\n\n                            # Decide based on policy (favor generic structural checks over name heuristics)\n                            route_by_subst = policy == \"substitution_only\" and has_substitution_members\n                            route_by_all_nested = policy == \"all_nested\"\n                            route_by_threshold = policy == \"threshold_by_children\" and (\n                                threshold_val &lt;= 0 or len(distinct_child_types) &gt;= threshold_val\n                            )\n                            route_by_repeating = policy == \"repeating_only\" and bool(el.is_list)\n\n                            if route_by_subst or route_by_all_nested or route_by_threshold or route_by_repeating:\n                                # Mark the CURRENT complex type (container) as the owner so placeholders on it are suppressed\n                                try:\n                                    owner_name = getattr(ct, \"name\", None) or getattr(ct, \"__name__\", str(ct))\n                                except Exception:\n                                    owner_name = target_type\n                                gfk_owner_wrappers.add(owner_name)\n                                if getattr(self, \"verbose\", False):\n                                    reason = (\n                                        \"substitution_only\"\n                                        if route_by_subst\n                                        else (\n                                            \"all_nested\"\n                                            if route_by_all_nested\n                                            else (\n                                                \"threshold_by_children\" if route_by_threshold else \"repeating_only\"\n                                            )\n                                        )\n                                    )\n                                    logger.info(\n                                        \"[GFK][prepass] owner=%s element=%s policy=%s distinct_children=%d\",\n                                        owner_name,\n                                        el.name,\n                                        reason,\n                                        len(distinct_child_types),\n                                    )\n                                # Exclude polymorphic children under this wrapper from concrete generation\n                                if el.is_list and repeating_leaf_child:\n                                    excluded_child_types.add(repeating_leaf_child)\n                                # Also exclude distinct child complex types observed under the wrapper\n                                excluded_child_types.update({t for t in distinct_child_types if t})\n                except Exception:\n                    logger.error(\n                        \"[GFK][prepass] Failed while processing schema '%s'\",\n                        getattr(schema_def, \"schema_location\", \"?\"),\n                        exc_info=True,\n                    )\n                    continue  # best-effort per schema\n\n            # Filter discovery output to remove excluded children, keeping wrappers/parents\n            try:\n                # filtered_models is a mapping of qualname -&gt; complex_type\n                fm = dict(getattr(self.discovery_instance, \"filtered_models\", {}) or {})\n                if fm and excluded_child_types:\n                    new_fm = {k: v for k, v in fm.items() if getattr(v, \"name\", None) not in excluded_child_types}\n                    self.discovery_instance.filtered_models = new_fm  # type: ignore[attr-defined]\n                    if getattr(self, \"verbose\", False):\n                        logger.info(\n                            \"[GFK][prepass] excluded_children_count=%d (examples: %s)\",\n                            len(excluded_child_types),\n                            \", \".join(sorted(excluded_child_types)[:10]),\n                        )\n            except Exception as exc:\n                logger.error(\"[GFK][prepass] Failed to filter excluded children: %s\", exc, exc_info=True)\n\n            # Share exclusions and owner marks with the field factory for intra-model decisions\n            try:\n                ff = getattr(self.model_factory_instance, \"field_factory\", None)\n                if ff is not None:\n                    ff._gfk_excluded_child_types = excluded_child_types\n                    # Seed owner names to suppress JSON placeholders for wrapper children\n                    owner_names = set(getattr(ff, \"_gfk_owner_names\", set()))\n                    owner_names.update(gfk_owner_wrappers)\n                    ff._gfk_owner_names = owner_names\n            except Exception as exc:\n                logger.error(\"[GFK][prepass] Failed to share state with field factory: %s\", exc, exc_info=True)\n        except Exception as exc:\n            # Prepass is best-effort and should never crash generation\n            logger.error(\"[GFK][prepass] Unexpected error: %s\", exc, exc_info=True)\n    # Conflict guard: if GFK owners detected but JSON relationship styles are requested, fail fast.\n    if getattr(self, \"enable_gfk\", False):\n        ff = getattr(self.model_factory_instance, \"field_factory\", None)\n        owners = set(getattr(ff, \"_gfk_owner_names\", set()) or set())\n        list_style = getattr(ff, \"list_relationship_style\", \"child_fk\")\n        nested_style = getattr(ff, \"nested_relationship_strategy\", \"fk\")\n        if owners and (list_style == \"json\" or nested_style == \"json\"):\n            raise ValueError(\n                \"GFK mode active with owners detected, but JSON relationship strategy was requested. \"\n                \"This configuration would produce dual emission. Adjust flags or disable JSON strategy.\"\n            )\n    # Inform the field factory which models are actually included so it can\n    # avoid generating relations to filtered-out models (fallback to JSON).\n    try:\n        # Prefer local (unqualified) complex type names to match factory lookups\n        # filtered_models keys may be qualified as \"{namespace}.{name}\", so derive plain names\n        included_local_names = {\n            getattr(m, \"name\", str(m))\n            for m in self.discovery_instance.filtered_models.values()  # type: ignore[attr-defined]\n        }\n        # Also include any already-qualified keys for maximum compatibility\n        included_qualified_keys = set(self.discovery_instance.filtered_models.keys())  # type: ignore[attr-defined]\n        included_names = included_local_names | included_qualified_keys\n        # type: ignore[attr-defined]\n        self.model_factory_instance.field_factory.included_model_names = included_names  # noqa: E501\n        # Also pass through any GFK prepass exclusions to the field factory if not already set\n        ff = self.model_factory_instance.field_factory\n        if not getattr(ff, \"_gfk_excluded_child_types\", None):\n            ff._gfk_excluded_child_types = set()\n    except Exception as exc:\n        logger.error(\n            \"Failed to inform field factory about included models or defaults: %s\",\n            exc,\n            exc_info=True,\n        )\n    models_to_process = self._get_models_in_processing_order()\n\n    # Classify models for Timescale usage (hypertable vs dimension)\n    if self.enable_timescale:\n        try:\n            self._timescale_roles = classify_xml_complex_types(\n                models_to_process,\n                overrides=self._timescale_overrides,\n                config=self._timescale_config,\n            )\n        except Exception as exc:\n            logger.error(\"Timescale classification failed; proceeding without roles: %s\", exc, exc_info=True)\n            self._timescale_roles = {}\n    else:\n        self._timescale_roles = {}\n\n    # Strict validation: if any hypertable lacks a direct time-like field, fail fast\n    if self.enable_timescale and self._timescale_strict:\n        for m in models_to_process:\n            name = getattr(m, \"name\", getattr(m, \"__name__\", str(m)))\n            if self._timescale_roles.get(name) == TimescaleRole.HYPERTABLE and not has_direct_time_feature(m):\n                raise ValueError(\n                    f\"Timescale strict mode: '{name}' classified as hypertable but has no direct time-like field. \"\n                    f\"Add a time/timestamp-like attribute/element or demote via overrides.\"\n                )\n\n    # Reset state for this run (mirror BaseStaticGenerator)\n    self.carriers = []\n    self.import_handler.extra_type_imports.clear()\n    self.import_handler.pydantic_imports.clear()\n    self.import_handler.context_class_imports.clear()\n    self.import_handler.imported_names.clear()\n    self.import_handler.processed_field_types.clear()\n    self.import_handler._add_type_import(self.base_model_class)\n\n    for source_model in models_to_process:\n        self.setup_django_model(source_model)\n\n    # Give the factory a chance to add cross-model relationship fields (e.g., child FKs)\n    carriers_by_name = {\n        (getattr(c.source_model, \"name\", None) or getattr(c.source_model, \"__name__\", \"\")): c\n        for c in self.carriers\n        if c.django_model is not None\n    }\n    if hasattr(self.model_factory_instance, \"finalize_relationships\"):\n        try:\n            # type: ignore[attr-defined]\n            self.model_factory_instance.finalize_relationships(carriers_by_name, self.app_label)  # noqa: E501\n        except Exception as e:\n            logger.error(\"Error finalizing XML relationships: %s\", e, exc_info=True)\n\n    # Inject GenericRelation on parents when GFK is enabled and pending children exist\n    gfk_used = False\n    try:\n        for carrier in self.carriers:\n            if getattr(carrier, \"enable_gfk\", False) and getattr(carrier, \"pending_gfk_children\", None):\n                # Ensure contenttypes imports\n                self.import_handler.add_import(\"django.contrib.contenttypes.fields\", \"GenericRelation\")\n                self.import_handler.add_import(\"django.contrib.contenttypes.fields\", \"GenericForeignKey\")\n                self.import_handler.add_import(\"django.contrib.contenttypes.models\", \"ContentType\")\n                # Add reverse relation field on parent model\n                carrier.django_field_definitions[\n                    \"entries\"\n                ] = \"GenericRelation('GenericEntry', related_query_name='entries')\"\n                gfk_used = True\n    except Exception as exc:\n        logger.error(\"Failed while injecting GenericRelation/GenericEntry fields: %s\", exc, exc_info=True)\n\n    # Optional diagnostics: summarize GFK routing per parent\n    if gfk_used and getattr(self, \"verbose\", False):\n        try:\n            summary: dict[str, int] = {}\n            for carrier in self.carriers:\n                for entry in getattr(carrier, \"pending_gfk_children\", []) or []:\n                    owner = str(entry.get(\"owner\") or getattr(carrier.source_model, \"name\", \"\"))\n                    summary[owner] = summary.get(owner, 0) + 1\n            if summary:\n                details = \", \".join(f\"{k}:{v}\" for k, v in sorted(summary.items()))\n                logger.info(f\"[GFK] Routed children counts by owner: {details}\")\n        except Exception as exc:\n            logger.error(\"Failed to summarize GFK routing: %s\", exc, exc_info=True)\n\n    # Register generated model classes for in-memory lookup by ingestors\n    try:\n        for carrier in self.carriers:\n            if carrier.django_model is not None:\n                register_generated_model(self.app_label, carrier.django_model)\n                logger.info(\n                    \"Registered generated model %s (abstract=%s) for app '%s'\",\n                    carrier.django_model.__name__,\n                    getattr(getattr(carrier.django_model, \"_meta\", None), \"abstract\", None),\n                    self.app_label,\n                )\n                # Prevent dynamic classes from polluting Django's global app registry\n                try:\n                    from django.apps import (\n                        apps as django_apps,  # Local import to avoid hard dependency\n                    )\n\n                    model_lower = getattr(getattr(carrier.django_model, \"_meta\", None), \"model_name\", None)\n                    if model_lower:\n                        django_apps.all_models.get(self.app_label, {}).pop(model_lower, None)\n                except Exception as exc:\n                    # Best-effort cleanup only, but log for visibility\n                    logger.error(\"Failed to clean up Django global app registry: %s\", exc, exc_info=True)\n    except Exception as e:\n        logger.error(\n            \"Non-fatal: failed to register generated models for app '%s': %s\",\n            self.app_label,\n            e,\n            exc_info=True,\n        )\n\n    # Proceed with standard definition rendering\n    model_definitions = []\n    django_model_names = []\n    for carrier in self.carriers:\n        if carrier.django_model:\n            try:\n                model_def = self.generate_model_definition(carrier)\n                if model_def:\n                    model_definitions.append(model_def)\n                    django_model_names.append(f\"'{self._clean_generic_type(carrier.django_model.__name__)}'\")\n            except Exception as e:\n                logger.error(\n                    f\"Error generating definition for source model {getattr(carrier.source_model, '__name__', '?')}: {e}\",\n                    exc_info=True,\n                )\n\n    # If GFK used, append GenericEntry model definition and include in __all__\n    if gfk_used:\n        model_definitions.append(self._build_generic_entry_model_definition())\n        django_model_names.append(\"'GenericEntry'\")\n\n    unique_model_definitions = self._deduplicate_definitions(model_definitions)\n    # Ensure that every advertised model name in __all__ has a corresponding class definition.\n    try:\n        import re as _re\n\n        joined_defs = \"\\n\".join(unique_model_definitions)\n        existing_names = {m.group(1) for m in _re.finditer(r\"^\\s*class\\s+(\\w+)\\(\", joined_defs, _re.MULTILINE)}\n        advertised_names = [\n            self._clean_generic_type(getattr(c.django_model, \"__name__\", \"\"))\n            for c in self.carriers\n            if c.django_model is not None\n        ]\n        for name in advertised_names:\n            if name and name not in existing_names:\n                minimal_def = (\n                    f'\"\"\"\\nDjango model for {name}.\\n\"\"\"\\n\\n'\n                    f\"class {name}({self.base_model_class.__name__}):\\n    # No fields defined for this model.\\n    pass\\n\\n    class Meta:\\n        app_label = '{self.app_label}'\\n        abstract = False\\n\\n\"\n                )\n                unique_model_definitions.append(minimal_def)\n    except Exception as exc:\n        logger.error(\n            \"Failed to ensure all advertised model names have definitions: %s\",\n            exc,\n            exc_info=True,\n        )\n\n    imports = self.import_handler.deduplicate_imports()\n    template_context = self._prepare_template_context(unique_model_definitions, django_model_names, imports)\n    template_context.update(\n        {\n            \"generation_timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n            \"base_model_module\": self.base_model_class.__module__,\n            \"base_model_name\": self.base_model_class.__name__,\n            \"extra_type_imports\": sorted(self.import_handler.extra_type_imports),\n        }\n    )\n    # Detect validator usage to ensure imports are present\n    used_validators: set[str] = set()\n    # Primary source: field factory tracking\n    try:\n        field_factory = getattr(self.model_factory_instance, \"field_factory\", None)\n        factory_used = getattr(field_factory, \"used_validators\", None)\n        if isinstance(factory_used, set):\n            used_validators.update(factory_used)\n    except Exception as exc:\n        logger.error(\"Failed to collect validator usage from field factory: %s\", exc, exc_info=True)\n    # Fallback: scan rendered definitions\n    try:\n        if not used_validators:\n            validator_symbols = [\n                \"RegexValidator\",\n                \"MinValueValidator\",\n                \"MaxValueValidator\",\n                \"URLValidator\",\n                \"EmailValidator\",\n                \"validate_slug\",\n                \"validate_uuid4\",\n                \"validate_email\",\n                \"validate_ipv46_address\",\n                \"validate_ipv4_address\",\n                \"validate_ipv6_address\",\n                \"validate_unicode_slug\",\n            ]\n            joined_defs = \"\\n\".join(unique_model_definitions)\n            for symbol in validator_symbols:\n                if re.search(rf\"\\\\b{re.escape(symbol)}\\\\b\", joined_defs):\n                    used_validators.add(symbol)\n    except Exception as exc:\n        logger.error(\"Failed to scan rendered definitions for validator usage: %s\", exc, exc_info=True)\n    template_context[\"validator_imports\"] = sorted(used_validators)\n    template = self.jinja_env.get_template(\"models_file.py.j2\")\n    return template.render(**template_context)\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/#pydantic2django.xmlschema.XmlSchemaDjangoModelGenerator.generate_models_with_xml_metadata","title":"<code>generate_models_with_xml_metadata()</code>","text":"<p>Generate Django models with additional XML metadata.</p> <p>This method extends the base generate() to add XML-specific comments and metadata to the generated models.</p> Source code in <code>src/pydantic2django/xmlschema/generator.py</code> <pre><code>def generate_models_with_xml_metadata(self) -&gt; str:\n    \"\"\"\n    Generate Django models with additional XML metadata.\n\n    This method extends the base generate() to add XML-specific\n    comments and metadata to the generated models.\n    \"\"\"\n    content = self.generate_models_file()\n\n    # Add XML Schema file references as comments at the top\n    schema_files_comment = \"\\n\".join(\n        [f\"# Generated from XML Schema: {schema_file}\" for schema_file in self.schema_files]\n    )\n\n    # Insert after the initial comments\n    lines = content.split(\"\\n\")\n    insert_index = 0\n    for i, line in enumerate(lines):\n        if line.startswith('\"\"\"') and '\"\"\"' in line[3:]:  # Single line docstring\n            insert_index = i + 1\n            break\n        elif line.startswith('\"\"\"'):  # Multi-line docstring start\n            for j in range(i + 1, len(lines)):\n                if '\"\"\"' in lines[j]:\n                    insert_index = j + 1\n                    break\n            break\n\n    lines.insert(insert_index, schema_files_comment)\n    lines.insert(insert_index + 1, \"\")\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/#pydantic2django.xmlschema.XmlSchemaDjangoModelGenerator.get_schema_statistics","title":"<code>get_schema_statistics()</code>","text":"<p>Get statistics about the parsed schemas.</p> Source code in <code>src/pydantic2django/xmlschema/generator.py</code> <pre><code>def get_schema_statistics(self) -&gt; dict:\n    \"\"\"Get statistics about the parsed schemas.\"\"\"\n    stats = {\n        \"total_schemas\": len(self.discovery_instance.parsed_schemas),\n        \"total_complex_types\": len(self.discovery_instance.all_models),\n        \"filtered_complex_types\": len(self.discovery_instance.filtered_models),\n        \"generated_models\": len(self.carriers),\n    }\n\n    # Add per-schema breakdown\n    schema_breakdown = []\n    for schema_def in self.discovery_instance.parsed_schemas:\n        schema_breakdown.append(\n            {\n                \"schema_location\": schema_def.schema_location,\n                \"target_namespace\": schema_def.target_namespace,\n                \"complex_types\": len(schema_def.complex_types),\n                \"simple_types\": len(schema_def.simple_types),\n                \"elements\": len(schema_def.elements),\n            }\n        )\n\n    stats[\"schema_breakdown\"] = schema_breakdown\n    return stats\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/#pydantic2django.xmlschema.XmlSchemaDjangoModelGenerator.validate_schemas","title":"<code>validate_schemas()</code>","text":"<p>Validate the parsed schemas and return any warnings or errors.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of validation messages</p> Source code in <code>src/pydantic2django/xmlschema/generator.py</code> <pre><code>def validate_schemas(self) -&gt; list[str]:\n    \"\"\"\n    Validate the parsed schemas and return any warnings or errors.\n\n    Returns:\n        List of validation messages\n    \"\"\"\n    messages = []\n\n    for schema_def in self.discovery_instance.parsed_schemas:\n        # Check for common issues\n        if not schema_def.target_namespace:\n            messages.append(f\"Schema {schema_def.schema_location} has no target namespace\")\n\n        # Check for name conflicts\n        all_names = set()\n        for complex_type in schema_def.complex_types.values():\n            if complex_type.name in all_names:\n                messages.append(f\"Duplicate type name: {complex_type.name}\")\n            all_names.add(complex_type.name)\n\n    return messages\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/#pydantic2django.xmlschema.get_generated_model","title":"<code>get_generated_model(app_label, model_name)</code>","text":"<p>Retrieve a previously registered model class, if available.</p> Source code in <code>src/pydantic2django/xmlschema/__init__.py</code> <pre><code>def get_generated_model(app_label: str, model_name: str):\n    \"\"\"Retrieve a previously registered model class, if available.\"\"\"\n    return _GENERATED_MODELS_REGISTRY.get(app_label, {}).get(model_name)\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/#pydantic2django.xmlschema.register_generated_model","title":"<code>register_generated_model(app_label, model_class)</code>","text":"<p>Register a dynamically created Django model class for later lookup.</p> <p>This does not touch Django's app registry; it merely stores a reference so that tools can resolve classes by (app_label, model_name) when needed.</p> Source code in <code>src/pydantic2django/xmlschema/__init__.py</code> <pre><code>def register_generated_model(app_label: str, model_class: type) -&gt; None:\n    \"\"\"Register a dynamically created Django model class for later lookup.\n\n    This does not touch Django's app registry; it merely stores a reference\n    so that tools can resolve classes by (app_label, model_name) when needed.\n    \"\"\"\n    try:\n        name = getattr(model_class, \"__name__\", None) or str(model_class)\n    except Exception:\n        name = str(model_class)\n    bucket = _GENERATED_MODELS_REGISTRY.setdefault(app_label, {})\n    bucket[name] = model_class\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/discovery/","title":"pydantic2django.xmlschema.discovery","text":"<p>XML Schema discovery module. Discovers XML Schema complex types from XSD files and prepares them for Django model generation.</p>"},{"location":"reference/pydantic2django/xmlschema/discovery/#pydantic2django.xmlschema.discovery.XmlSchemaDiscovery","title":"<code>XmlSchemaDiscovery</code>","text":"<p>               Bases: <code>BaseDiscovery[XmlSchemaComplexType]</code></p> <p>Discovers XML Schema complex types from XSD files.</p> Source code in <code>src/pydantic2django/xmlschema/discovery.py</code> <pre><code>class XmlSchemaDiscovery(BaseDiscovery[XmlSchemaComplexType]):\n    \"\"\"Discovers XML Schema complex types from XSD files.\"\"\"\n\n    def __init__(self, schema_files: list[str | Path] | None = None):\n        super().__init__()\n        self.schema_files = [Path(f) for f in schema_files] if schema_files else []\n        self.schema_parser = XmlSchemaParser()\n        self.parsed_schemas: list[XmlSchemaDefinition] = []\n\n    def _is_target_model(self, obj: Any) -&gt; bool:\n        \"\"\"Check if object is a suitable XmlSchemaComplexType for conversion.\"\"\"\n        return (\n            isinstance(obj, XmlSchemaComplexType)\n            and not obj.abstract\n            and (len(obj.elements) &gt; 0 or len(obj.attributes) &gt; 0)\n        )\n\n    def _default_eligibility_filter(self, model: XmlSchemaComplexType) -&gt; bool:\n        \"\"\"Apply default filtering logic for XML Schema complex types.\"\"\"\n        if model.abstract:\n            return False\n\n        if len(model.elements) == 0 and len(model.attributes) == 0:\n            return False\n\n        return True\n\n    def discover_models(\n        self,\n        packages: list[str],  # Schema file paths\n        app_label: str,\n        user_filters: Callable[[XmlSchemaComplexType], bool]\n        | list[Callable[[XmlSchemaComplexType], bool]]\n        | None = None,\n    ) -&gt; None:\n        \"\"\"Discover XML Schema complex types from XSD files.\"\"\"\n        logger.info(f\"Starting XML Schema discovery for files: {packages}\")\n\n        self.all_models = {}\n        self.filtered_models = {}\n        self.dependencies = {}\n        self.parsed_schemas = []\n\n        schema_files = packages if packages else [str(f) for f in self.schema_files]\n\n        # Parse all schema files\n        for schema_file in schema_files:\n            try:\n                schema_def = self.schema_parser.parse_schema_file(schema_file)\n                self.parsed_schemas.append(schema_def)\n\n                for complex_type in schema_def.get_all_complex_types():\n                    namespace = complex_type.namespace or \"default\"\n                    qualname = f\"{namespace}.{complex_type.name}\"\n                    self.all_models[qualname] = complex_type\n\n            except XmlSchemaParseError as e:\n                logger.error(f\"Failed to parse schema file {schema_file}: {e}\")\n                continue\n\n        logger.info(f\"Discovered {len(self.all_models)} total complex types\")\n\n        # Apply filters\n        eligible_models = {}\n        for qualname, model in self.all_models.items():\n            if self._is_target_model(model) and self._default_eligibility_filter(model):\n                eligible_models[qualname] = model\n\n        # Apply user filters\n        if user_filters:\n            filters = user_filters if isinstance(user_filters, list) else [user_filters]\n            for filter_func in filters:\n                filtered_models = {}\n                for qualname, model in eligible_models.items():\n                    try:\n                        if filter_func(model):\n                            filtered_models[qualname] = model\n                    except Exception as e:\n                        logger.error(f\"Error applying filter to {qualname}: {e}\")\n                        filtered_models[qualname] = model\n                eligible_models = filtered_models\n\n        self.filtered_models = eligible_models\n        logger.info(f\"XML Schema discovery complete. {len(self.filtered_models)} models after filtering\")\n\n    def analyze_dependencies(self) -&gt; None:\n        \"\"\"Analyze dependencies between XML Schema complex types.\"\"\"\n        logger.info(\"Analyzing XML Schema dependencies...\")\n\n        self.dependencies = {}\n        for model in self.filtered_models.values():\n            self.dependencies[model] = set()\n\n        # Build type name mapping\n        type_name_to_model = {}\n        for model in self.filtered_models.values():\n            type_name_to_model[model.name] = model\n            if model.namespace:\n                type_name_to_model[f\"{model.namespace}.{model.name}\"] = model\n\n        # Find dependencies\n        for model in self.filtered_models.values():\n            dependencies = set()\n\n            # Base type inheritance\n            if model.base_type:\n                base_model = type_name_to_model.get(model.base_type)\n                if base_model and base_model is not model:\n                    dependencies.add(base_model)\n\n            # Element type references\n            for element in model.elements:\n                if element.type_name:\n                    type_name = element.type_name.split(\":\")[-1]  # Remove namespace\n                    referenced_model = type_name_to_model.get(type_name)\n                    if referenced_model and referenced_model is not model:\n                        dependencies.add(referenced_model)\n\n            # Attribute type references\n            for attribute in model.attributes.values():\n                if attribute.type_name:\n                    type_name = attribute.type_name.split(\":\")[-1]\n                    referenced_model = type_name_to_model.get(type_name)\n                    if referenced_model and referenced_model is not model:\n                        dependencies.add(referenced_model)\n\n            self.dependencies[model] = dependencies\n\n        logger.info(\"XML Schema dependency analysis complete\")\n\n    def get_models_in_registration_order(self) -&gt; list[XmlSchemaComplexType]:\n        \"\"\"Return complex types sorted topologically based on dependencies.\"\"\"\n        if not self.dependencies:\n            return list(self.filtered_models.values())\n\n        sorted_models = []\n        visited = set()\n        visiting = set()\n\n        def visit(model: XmlSchemaComplexType):\n            if model in visited:\n                return\n            if model in visiting:\n                logger.error(f\"Circular dependency detected: {model.name}\")\n                return\n\n            visiting.add(model)\n            for dep in self.dependencies.get(model, set()):\n                if dep in self.filtered_models.values():\n                    visit(dep)\n\n            visiting.remove(model)\n            visited.add(model)\n            sorted_models.append(model)\n\n        for model in self.filtered_models.values():\n            if model not in visited:\n                visit(model)\n\n        logger.info(f\"Models sorted for registration: {[m.name for m in sorted_models]}\")\n        return sorted_models\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/discovery/#pydantic2django.xmlschema.discovery.XmlSchemaDiscovery.analyze_dependencies","title":"<code>analyze_dependencies()</code>","text":"<p>Analyze dependencies between XML Schema complex types.</p> Source code in <code>src/pydantic2django/xmlschema/discovery.py</code> <pre><code>def analyze_dependencies(self) -&gt; None:\n    \"\"\"Analyze dependencies between XML Schema complex types.\"\"\"\n    logger.info(\"Analyzing XML Schema dependencies...\")\n\n    self.dependencies = {}\n    for model in self.filtered_models.values():\n        self.dependencies[model] = set()\n\n    # Build type name mapping\n    type_name_to_model = {}\n    for model in self.filtered_models.values():\n        type_name_to_model[model.name] = model\n        if model.namespace:\n            type_name_to_model[f\"{model.namespace}.{model.name}\"] = model\n\n    # Find dependencies\n    for model in self.filtered_models.values():\n        dependencies = set()\n\n        # Base type inheritance\n        if model.base_type:\n            base_model = type_name_to_model.get(model.base_type)\n            if base_model and base_model is not model:\n                dependencies.add(base_model)\n\n        # Element type references\n        for element in model.elements:\n            if element.type_name:\n                type_name = element.type_name.split(\":\")[-1]  # Remove namespace\n                referenced_model = type_name_to_model.get(type_name)\n                if referenced_model and referenced_model is not model:\n                    dependencies.add(referenced_model)\n\n        # Attribute type references\n        for attribute in model.attributes.values():\n            if attribute.type_name:\n                type_name = attribute.type_name.split(\":\")[-1]\n                referenced_model = type_name_to_model.get(type_name)\n                if referenced_model and referenced_model is not model:\n                    dependencies.add(referenced_model)\n\n        self.dependencies[model] = dependencies\n\n    logger.info(\"XML Schema dependency analysis complete\")\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/discovery/#pydantic2django.xmlschema.discovery.XmlSchemaDiscovery.discover_models","title":"<code>discover_models(packages, app_label, user_filters=None)</code>","text":"<p>Discover XML Schema complex types from XSD files.</p> Source code in <code>src/pydantic2django/xmlschema/discovery.py</code> <pre><code>def discover_models(\n    self,\n    packages: list[str],  # Schema file paths\n    app_label: str,\n    user_filters: Callable[[XmlSchemaComplexType], bool]\n    | list[Callable[[XmlSchemaComplexType], bool]]\n    | None = None,\n) -&gt; None:\n    \"\"\"Discover XML Schema complex types from XSD files.\"\"\"\n    logger.info(f\"Starting XML Schema discovery for files: {packages}\")\n\n    self.all_models = {}\n    self.filtered_models = {}\n    self.dependencies = {}\n    self.parsed_schemas = []\n\n    schema_files = packages if packages else [str(f) for f in self.schema_files]\n\n    # Parse all schema files\n    for schema_file in schema_files:\n        try:\n            schema_def = self.schema_parser.parse_schema_file(schema_file)\n            self.parsed_schemas.append(schema_def)\n\n            for complex_type in schema_def.get_all_complex_types():\n                namespace = complex_type.namespace or \"default\"\n                qualname = f\"{namespace}.{complex_type.name}\"\n                self.all_models[qualname] = complex_type\n\n        except XmlSchemaParseError as e:\n            logger.error(f\"Failed to parse schema file {schema_file}: {e}\")\n            continue\n\n    logger.info(f\"Discovered {len(self.all_models)} total complex types\")\n\n    # Apply filters\n    eligible_models = {}\n    for qualname, model in self.all_models.items():\n        if self._is_target_model(model) and self._default_eligibility_filter(model):\n            eligible_models[qualname] = model\n\n    # Apply user filters\n    if user_filters:\n        filters = user_filters if isinstance(user_filters, list) else [user_filters]\n        for filter_func in filters:\n            filtered_models = {}\n            for qualname, model in eligible_models.items():\n                try:\n                    if filter_func(model):\n                        filtered_models[qualname] = model\n                except Exception as e:\n                    logger.error(f\"Error applying filter to {qualname}: {e}\")\n                    filtered_models[qualname] = model\n            eligible_models = filtered_models\n\n    self.filtered_models = eligible_models\n    logger.info(f\"XML Schema discovery complete. {len(self.filtered_models)} models after filtering\")\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/discovery/#pydantic2django.xmlschema.discovery.XmlSchemaDiscovery.get_models_in_registration_order","title":"<code>get_models_in_registration_order()</code>","text":"<p>Return complex types sorted topologically based on dependencies.</p> Source code in <code>src/pydantic2django/xmlschema/discovery.py</code> <pre><code>def get_models_in_registration_order(self) -&gt; list[XmlSchemaComplexType]:\n    \"\"\"Return complex types sorted topologically based on dependencies.\"\"\"\n    if not self.dependencies:\n        return list(self.filtered_models.values())\n\n    sorted_models = []\n    visited = set()\n    visiting = set()\n\n    def visit(model: XmlSchemaComplexType):\n        if model in visited:\n            return\n        if model in visiting:\n            logger.error(f\"Circular dependency detected: {model.name}\")\n            return\n\n        visiting.add(model)\n        for dep in self.dependencies.get(model, set()):\n            if dep in self.filtered_models.values():\n                visit(dep)\n\n        visiting.remove(model)\n        visited.add(model)\n        sorted_models.append(model)\n\n    for model in self.filtered_models.values():\n        if model not in visited:\n            visit(model)\n\n    logger.info(f\"Models sorted for registration: {[m.name for m in sorted_models]}\")\n    return sorted_models\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/factory/","title":"pydantic2django.xmlschema.factory","text":"<p>XML Schema factory module. Creates Django fields and models from XML Schema definitions.</p>"},{"location":"reference/pydantic2django/xmlschema/factory/#pydantic2django.xmlschema.factory.XmlSchemaFieldFactory","title":"<code>XmlSchemaFieldFactory</code>","text":"<p>               Bases: <code>BaseFieldFactory[XmlSchemaFieldInfo]</code></p> <p>Creates Django fields from XML Schema elements and attributes.</p> Source code in <code>src/pydantic2django/xmlschema/factory.py</code> <pre><code>class XmlSchemaFieldFactory(BaseFieldFactory[XmlSchemaFieldInfo]):\n    \"\"\"Creates Django fields from XML Schema elements and attributes.\"\"\"\n\n    FIELD_TYPE_MAP = {\n        XmlSchemaType.STRING: models.CharField,\n        XmlSchemaType.INTEGER: models.IntegerField,\n        XmlSchemaType.LONG: models.BigIntegerField,\n        XmlSchemaType.SHORT: models.SmallIntegerField,\n        XmlSchemaType.BYTE: models.SmallIntegerField,\n        XmlSchemaType.UNSIGNEDINT: models.PositiveIntegerField,\n        XmlSchemaType.UNSIGNEDLONG: models.PositiveBigIntegerField,\n        XmlSchemaType.POSITIVEINTEGER: models.PositiveIntegerField,\n        XmlSchemaType.DECIMAL: models.DecimalField,\n        XmlSchemaType.BOOLEAN: models.BooleanField,\n        XmlSchemaType.DATE: models.DateField,\n        XmlSchemaType.DATETIME: models.DateTimeField,\n        XmlSchemaType.TIME: models.TimeField,\n        XmlSchemaType.GYEAR: models.IntegerField,\n        XmlSchemaType.ID: models.CharField,  # Often used as PK\n        XmlSchemaType.IDREF: models.ForeignKey,\n        XmlSchemaType.HEXBINARY: models.BinaryField,\n    }\n\n    def __init__(\n        self,\n        *,\n        nested_relationship_strategy: str = \"auto\",\n        list_relationship_style: str = \"child_fk\",\n        nesting_depth_threshold: int = 1,\n        included_model_names: set[str] | None = None,\n        invert_fk_to_timescale: bool = True,\n        # --- GFK flags ---\n        enable_gfk: bool = False,\n        gfk_policy: str | None = None,\n        gfk_threshold_children: int | None = None,\n        gfk_overrides: dict[str, bool] | None = None,\n    ):\n        \"\"\"Create a field factory for XML Schema with optional GFK routing.\n\n        Args:\n            nested_relationship_strategy: \"fk\" | \"json\" | \"auto\"; controls nested mapping.\n            list_relationship_style: \"child_fk\" | \"m2m\" | \"json\" for repeating nested.\n            nesting_depth_threshold: When strategy is \"auto\", use FK up to this depth.\n            included_model_names: Allowed target complex types for relation emission.\n            invert_fk_to_timescale: If True, prefer FKs from hypertable \u2192 dimension.\n\n            enable_gfk: Enable Generic Entries mode.\n            gfk_policy: \"substitution_only\" | \"repeating_only\" | \"all_nested\" | \"threshold_by_children\".\n            gfk_threshold_children: Threshold for wrappers when policy is \"threshold_by_children\".\n            gfk_overrides: Per-element overrides; True forces GFK, False disables it.\n\n        Notes:\n            - When GFK applies for a wrapper owner, this factory records children in\n              `carrier.pending_gfk_children` and suppresses JSON placeholders under that owner.\n            - The prepass may populate `_gfk_excluded_child_types` and `_gfk_owner_names` so\n              we avoid emitting relations to excluded children and suppress placeholders.\n        \"\"\"\n        super().__init__()\n        self.nested_relationship_strategy = nested_relationship_strategy\n        self.list_relationship_style = list_relationship_style\n        self.nesting_depth_threshold = max(0, int(nesting_depth_threshold))\n        # Track which Django validators are needed based on restrictions encountered\n        self.used_validators: set[str] = set()\n        # Limit FK generation only to models that will actually be generated\n        self.included_model_names: set[str] | None = included_model_names\n        # Control whether to invert FKs targeting hypertables (hypertable -&gt; dimension)\n        self.invert_fk_to_timescale = invert_fk_to_timescale\n        # Track per-target related_names to avoid collisions for keyref-based FKs\n        self._target_related_names: dict[str, set[str]] = {}\n        # GFK flags\n        self.enable_gfk = bool(enable_gfk)\n        self.gfk_policy = gfk_policy\n        self.gfk_threshold_children = gfk_threshold_children\n        self.gfk_overrides = gfk_overrides or {}\n        # Track wrapper-like owner types that should route children via GFK\n        self._gfk_owner_names: set[str] = set()\n        # Track child complex types that should be excluded from concrete generation when routed via GFK\n        self._gfk_excluded_child_types: set[str] = set()\n\n    def _gfk_override_for(self, element_name: str) -&gt; bool | None:\n        \"\"\"Return override decision for an element if provided.\n\n        True = force GFK, False = disable GFK, None = no override.\n        \"\"\"\n        try:\n            return self.gfk_overrides.get(str(element_name))\n        except Exception:\n            return None\n\n    def create_field(\n        self, field_info: XmlSchemaFieldInfo, model_name: str, carrier: ConversionCarrier[XmlSchemaComplexType]\n    ) -&gt; FieldConversionResult:\n        \"\"\"Convert XML Schema element/attribute to Django field.\"\"\"\n\n        result = FieldConversionResult(field_info=field_info, field_name=field_info.name)\n        schema_def = carrier.source_model.schema_def\n\n        source_field = field_info.element if field_info.element else field_info.attribute\n        if not source_field:\n            return result\n\n        field_type_name = source_field.type_name\n        if field_type_name and \":\" in field_type_name:\n            field_type_name = field_type_name.split(\":\", 1)[1]\n\n        field_class, kwargs = None, {}\n\n        # Check for keyref first to determine if this is a ForeignKey\n        keyref = next(\n            (kr for kr in schema_def.keyrefs if field_info.name in kr.fields),\n            None,\n        )\n\n        if keyref:\n            field_class, kwargs = self._create_foreign_key_field(field_info, model_name, carrier)\n        elif source_field and source_field.base_type == XmlSchemaType.IDREF:\n            # Fallback for IDREFs not part of an explicit keyref\n            field_class, kwargs = self._create_foreign_key_field(field_info, model_name, carrier)\n        else:\n            # Resolve simple types\n            simple_type = self._resolve_simple_type(source_field.type_name, schema_def)\n            if simple_type:\n                restr = getattr(simple_type, \"restrictions\", None)\n                if restr and getattr(restr, \"enumeration\", None):\n                    field_class, kwargs = self._create_enum_field(simple_type, field_info, model_name, carrier)\n                else:\n                    # Always apply base-type mapping and any available restrictions\n                    field_class, kwargs = self._apply_simple_type_restrictions(simple_type, field_info)\n            # Inline restrictions on element/attribute (no named simpleType)\n            elif getattr(source_field, \"restrictions\", None):\n                # Build a temporary simple type holder to reuse restriction logic\n                tmp_simple = XmlSchemaSimpleType(\n                    name=field_info.name,\n                    base_type=getattr(source_field, \"base_type\", None),\n                    restrictions=getattr(source_field, \"restrictions\", None),\n                )\n                restr = tmp_simple.restrictions\n                if restr and getattr(restr, \"enumeration\", None):\n                    field_class, kwargs = self._create_enum_field(tmp_simple, field_info, model_name, carrier)\n                else:\n                    field_class, kwargs = self._apply_simple_type_restrictions(tmp_simple, field_info)\n            elif field_info.element:\n                field_class, kwargs = self._create_element_field(field_info.element, model_name, carrier)\n            elif field_info.attribute:\n                field_class, kwargs = self._create_attribute_field(field_info.attribute, model_name)\n\n        if field_class:\n            try:\n                result.django_field = field_class(**kwargs)\n                result.field_kwargs = kwargs\n                result.field_definition_str = self._generate_field_def_string(result, carrier.meta_app_label)\n            except Exception as e:\n                result.error_str = f\"Failed to instantiate {field_class.__name__}: {e}\"\n        else:\n            result.context_field = field_info\n\n        return result\n\n    def _create_enum_field(\n        self,\n        simple_type: XmlSchemaSimpleType,\n        field_info: XmlSchemaFieldInfo,\n        model_name: str,\n        carrier: ConversionCarrier,\n    ):\n        \"\"\"Create a CharField with choices for an enumeration.\"\"\"\n        restr = getattr(simple_type, \"restrictions\", None)\n        max_length = 255\n        if restr and getattr(restr, \"enumeration\", None):\n            try:\n                max_length = max(len(val) for val, _ in restr.enumeration)\n            except Exception:\n                max_length = 255\n\n        # Get or create a shared enum class for this simpleType\n        enum_class_name, is_new = self._get_or_create_enum_class(simple_type, field_info, carrier)\n\n        # Use RawCode so choices/default emit without quotes\n        from ..django.utils.serialization import RawCode\n\n        kwargs = {\n            \"max_length\": max_length,\n            \"choices\": RawCode(f\"{enum_class_name}.choices\"),\n        }\n\n        default_val = None\n        if field_info.element:\n            default_val = field_info.element.default_value\n        elif field_info.attribute:\n            default_val = field_info.attribute.default_value\n\n        if default_val:\n            kwargs[\"default\"] = RawCode(f\"{enum_class_name}.{default_val.upper()}\")\n\n        return models.CharField, kwargs\n\n    def _apply_simple_type_restrictions(\n        self,\n        simple_type: XmlSchemaSimpleType,\n        field_info: XmlSchemaFieldInfo,\n    ):\n        \"\"\"Apply validators and other constraints from simpleType restrictions.\"\"\"\n        # Import RawCode for proper validator serialization\n        from ..django.utils.serialization import RawCode\n\n        kwargs = {\"max_length\": 255}  # Default for string-like; will be dropped for numeric fields\n        restr = getattr(simple_type, \"restrictions\", None)\n        if restr:\n            if getattr(restr, \"pattern\", None):\n                # Use RawCode to ensure validator is not quoted as string\n                kwargs[\"validators\"] = [RawCode(f\"RegexValidator(r'{restr.pattern}')\")]\n                # Note usage for later import emission\n                try:\n                    self.used_validators.add(\"RegexValidator\")\n                except Exception:\n                    pass\n            if getattr(restr, \"max_length\", None):\n                kwargs[\"max_length\"] = int(restr.max_length)\n            # Numeric bounds -&gt; validators\n            min_val = getattr(restr, \"min_inclusive\", None)\n            max_val = getattr(restr, \"max_inclusive\", None)\n            min_ex = getattr(restr, \"min_exclusive\", None)\n            max_ex = getattr(restr, \"max_exclusive\", None)\n            validators_list = kwargs.setdefault(\"validators\", [])\n            if min_val is not None:\n                validators_list.append(\n                    RawCode(f\"MinValueValidator({int(min_val) if isinstance(min_val, int) else min_val})\")\n                )\n                try:\n                    self.used_validators.add(\"MinValueValidator\")\n                except Exception:\n                    pass\n            if max_val is not None:\n                validators_list.append(\n                    RawCode(f\"MaxValueValidator({int(max_val) if isinstance(max_val, int) else max_val})\")\n                )\n                try:\n                    self.used_validators.add(\"MaxValueValidator\")\n                except Exception:\n                    pass\n            if min_ex is not None:\n                adj = int(min_ex) + 1 if isinstance(min_ex, int) else min_ex\n                validators_list.append(RawCode(f\"MinValueValidator({adj})\"))\n                try:\n                    self.used_validators.add(\"MinValueValidator\")\n                except Exception:\n                    pass\n            if max_ex is not None:\n                adj = int(max_ex) - 1 if isinstance(max_ex, int) else max_ex\n                validators_list.append(RawCode(f\"MaxValueValidator({adj})\"))\n                try:\n                    self.used_validators.add(\"MaxValueValidator\")\n                except Exception:\n                    pass\n\n        # Determine the base Django field type\n        base_field_class = self.FIELD_TYPE_MAP.get(simple_type.base_type, models.CharField)\n\n        if simple_type.base_type == XmlSchemaType.STRING and (not restr or not getattr(restr, \"max_length\", None)):\n            # If a pattern is present, it's likely a constrained string that should be a CharField\n            if not (restr and getattr(restr, \"pattern\", None)):\n                base_field_class = models.TextField\n                # TextField doesn't accept max_length, so remove it if it was defaulted\n                kwargs.pop(\"max_length\", None)\n\n        # Remove max_length for numeric fields and optionally map precision\n        if issubclass(base_field_class, (models.IntegerField, models.BigIntegerField, models.SmallIntegerField)):\n            kwargs.pop(\"max_length\", None)\n        if issubclass(base_field_class, models.DecimalField):\n            kwargs.pop(\"max_length\", None)\n            # Map precision if available\n            total = getattr(restr, \"total_digits\", None)\n            frac = getattr(restr, \"fraction_digits\", None)\n            if total is not None:\n                kwargs[\"max_digits\"] = int(total)\n            if frac is not None:\n                kwargs[\"decimal_places\"] = int(frac)\n\n        if field_info.element and (field_info.element.nillable or field_info.element.min_occurs == 0):\n            kwargs[\"null\"] = True\n            kwargs[\"blank\"] = True\n        elif field_info.attribute and field_info.attribute.use == \"optional\":\n            kwargs[\"null\"] = True\n            kwargs[\"blank\"] = True\n\n        return base_field_class, kwargs\n\n    def _create_element_field(\n        self,\n        element: XmlSchemaElement,\n        model_name: str,\n        carrier: ConversionCarrier[XmlSchemaComplexType],\n    ):\n        \"\"\"Creates a Django field from an XmlSchemaElement.\n\n        Honors GFK flags to potentially route nested complex children through\n        `GenericEntry` instead of emitting concrete fields/relations:\n        - `gfk_policy=\"repeating_only\"`: repeated complex leaves -&gt; GFK.\n        - `gfk_policy=\"all_nested\"`: all eligible nested complex children -&gt; GFK.\n        - `gfk_policy=\"threshold_by_children\"`: wrapper-like single-nested elements\n           route to GFK when the wrapper contains \u2265 `gfk_threshold_children` distinct\n           child complex types.\n        - `gfk_policy=\"substitution_only\"`: wrapper-like owners with substitution-group\n           members route those members to GFK regardless of distinct child type count.\n        \"\"\"\n        schema_def = carrier.source_model.schema_def\n\n        # Handle references to complex types (nested objects)\n        if element.base_type is None and element.type_name and schema_def:\n            target_type_name = element.type_name.split(\":\")[-1]\n            # Only create relations to complex types that are slated for generation\n            allowed = True\n            if self.included_model_names is not None and target_type_name not in self.included_model_names:\n                allowed = False\n            # If prepass marked this child type as GFK-excluded, disallow relations to it\n            try:\n                if target_type_name in getattr(self, \"_gfk_excluded_child_types\", set()):\n                    allowed = False\n            except Exception:\n                pass\n\n            if allowed:\n                strategy = self._decide_nested_strategy(current_depth=1)\n                # Repeating complex elements\n                if element.is_list:\n                    # Attempt to resolve leaf repeated complex type inside the wrapper\n                    leaf_child: str | None = None\n                    if schema_def and target_type_name in schema_def.complex_types:\n                        target_ct = schema_def.complex_types[target_type_name]\n                        for child_el in target_ct.elements:\n                            if getattr(child_el, \"is_list\", False) and child_el.type_name:\n                                cand = child_el.type_name.split(\":\")[-1]\n                                if cand:\n                                    leaf_child = cand\n                                    break\n                    # If GFK policy applies, prefer GFK routing even when JSON strategy or child type not included\n                    if self.enable_gfk and (\n                        self.gfk_policy\n                        in {\"repeating_only\", \"all_nested\", \"threshold_by_children\", \"substitution_only\"}\n                    ):\n                        ovr = self._gfk_override_for(element.name)\n                        if ovr is not False:\n                            owner_name = getattr(carrier.source_model, \"name\", None) or getattr(\n                                carrier.source_model, \"__name__\", model_name\n                            )\n                            try:\n                                logger.debug(\n                                    \"[GFK] routing repeating complex child via entries owner=%s child=%s element=%s\",\n                                    owner_name,\n                                    leaf_child or target_type_name,\n                                    element.name,\n                                )\n                            except Exception:\n                                pass\n                            carrier.pending_gfk_children.append(\n                                {\n                                    \"child\": (leaf_child or target_type_name),\n                                    \"owner\": owner_name,\n                                    \"element_name\": element.name,\n                                }\n                            )\n                            # Ensure this child complex type is excluded from concrete generation\n                            try:\n                                if leaf_child:\n                                    self._gfk_excluded_child_types.add(leaf_child)\n                                else:\n                                    self._gfk_excluded_child_types.add(target_type_name)\n                                # Also mark current model as owner to suppress all element fields under it\n                                current_name = getattr(carrier.source_model, \"name\", None) or getattr(\n                                    carrier.source_model, \"__name__\", model_name\n                                )\n                                if isinstance(current_name, str) and current_name:\n                                    self._gfk_owner_names.add(current_name)\n                            except Exception:\n                                pass\n                            return None, {}\n                    # Otherwise fall back to JSON when configured or child not allowed\n                    if strategy == \"json\" or self.list_relationship_style == \"json\" or not allowed:\n                        try:\n                            logger.debug(\n                                \"[JSON] emitting JSON placeholder for repeating element model=%s element=%s allowed=%s\",\n                                model_name,\n                                element.name,\n                                allowed,\n                            )\n                        except Exception:\n                            pass\n                        return self._make_json_field_kwargs(element)\n                    if self.list_relationship_style == \"m2m\" and leaf_child:\n                        kwargs = {\"to\": f\"{carrier.meta_app_label}.{leaf_child}\", \"blank\": True}\n                        return models.ManyToManyField, kwargs\n                    # Default: child_fk -&gt; defer actual FK creation to child model via finalize step\n                    pending = carrier.context_data.setdefault(\"_pending_child_fk\", [])\n                    pending.append(\n                        {\n                            \"child\": (leaf_child or target_type_name),\n                            \"parent\": (\n                                getattr(carrier.source_model, \"name\", None)\n                                or getattr(carrier.source_model, \"__name__\", model_name)\n                            ),\n                            \"element_name\": element.name,\n                            \"allow_collapse\": False,\n                        }\n                    )\n                    # Represent on parent as a reverse accessor only; no concrete field needed\n                    # Use a JSONField as placeholder if desired by strategy\n                    return self._make_json_field_kwargs(element) if strategy == \"json\" else (None, {})\n\n                # Single nested complex element\n                # Inspect the target complex type for repeating children and total distinct child complex types\n                repeating_child_type: str | None = None\n                distinct_child_types: set[str] = set()\n                has_substitution_members: bool = False\n                if schema_def and target_type_name in schema_def.complex_types:\n                    target_ct = schema_def.complex_types[target_type_name]\n                    for child_el in target_ct.elements:\n                        if getattr(child_el, \"substitution_group\", None):\n                            has_substitution_members = True\n                        if child_el.type_name:\n                            cand = child_el.type_name.split(\":\")[-1]\n                            if cand:\n                                distinct_child_types.add(cand)\n                        if getattr(child_el, \"is_list\", False) and child_el.type_name:\n                            cand = child_el.type_name.split(\":\")[-1]\n                            if cand and schema_def.complex_types.get(cand):\n                                repeating_child_type = cand\n                                # Do not break here to continue counting distinct types\n\n                # If configured for M2M and wrapper contains repeating complex children, emit M2M to the leaf\n                if self.list_relationship_style == \"m2m\" and repeating_child_type:\n                    kwargs = {\"to\": f\"{carrier.meta_app_label}.{repeating_child_type}\", \"blank\": True}\n                    return models.ManyToManyField, kwargs\n\n                # Heuristic: Treat wrapper-like single nested elements as containers and prefer child_fk\n                # Conditions:\n                #  - Element name is TitleCase (e.g., 'Samples', 'Events'), OR\n                #  - Target type name ends with 'WrapperType'\n                is_wrapper_like = bool(element.name[:1].isupper()) or str(target_type_name).endswith(\"WrapperType\")\n                # If GFK policy applies, handle before JSON fallback and child inclusion checks\n                if self.enable_gfk and (\n                    self.gfk_policy in {\"repeating_only\", \"all_nested\", \"threshold_by_children\", \"substitution_only\"}\n                ):\n                    ovr = self._gfk_override_for(element.name)\n                    # threshold_by_children gate unless override forces True\n                    meets_threshold = True\n                    if self.gfk_policy == \"threshold_by_children\" and ovr is not True:\n                        try:\n                            threshold = int(self.gfk_threshold_children or 0)\n                        except Exception:\n                            threshold = 0\n                        meets_threshold = threshold &lt;= 0 or len(distinct_child_types) &gt;= threshold\n                    # substitution_only: require wrapper-like and presence of substitution members\n                    policy_allows = False\n                    if self.gfk_policy == \"all_nested\":\n                        policy_allows = True if is_wrapper_like else False\n                    elif self.gfk_policy == \"substitution_only\":\n                        policy_allows = bool(is_wrapper_like and has_substitution_members)\n                    elif self.gfk_policy == \"threshold_by_children\":\n                        policy_allows = bool(is_wrapper_like and meets_threshold)\n                    elif self.gfk_policy == \"repeating_only\":\n                        policy_allows = False  # handled in repeating branch\n\n                    if ovr is not False and policy_allows:\n                        owner_name = getattr(carrier.source_model, \"name\", None) or getattr(\n                            carrier.source_model, \"__name__\", model_name\n                        )\n                        try:\n                            logger.debug(\n                                \"[GFK] marking wrapper as owner owner=%s element=%s children=%d\",\n                                target_type_name,\n                                element.name,\n                                len(distinct_child_types),\n                            )\n                        except Exception:\n                            pass\n                        if repeating_child_type:\n                            carrier.pending_gfk_children.append(\n                                {\"child\": repeating_child_type, \"owner\": owner_name, \"element_name\": element.name}\n                            )\n                        carrier.pending_gfk_children.append(\n                            {\"child\": target_type_name, \"owner\": owner_name, \"element_name\": element.name}\n                        )\n                        # Mark the target wrapper as a GFK owner so its children route to entries\n                        try:\n                            # Mark the CURRENT model as owner (not the child target type)\n                            current_name = getattr(carrier.source_model, \"name\", None) or getattr(\n                                carrier.source_model, \"__name__\", model_name\n                            )\n                            if isinstance(current_name, str) and current_name:\n                                self._gfk_owner_names.add(current_name)\n                                # Exclude observed child complex types from concrete generation\n                                for t in distinct_child_types:\n                                    if t:\n                                        self._gfk_excluded_child_types.add(t)\n                        except Exception:\n                            pass\n                        return None, {}\n\n                # If current model is a known GFK owner wrapper, suppress JSON placeholders for its child elements\n                try:\n                    current_name = getattr(carrier.source_model, \"name\", None) or getattr(\n                        carrier.source_model, \"__name__\", model_name\n                    )\n                except Exception:\n                    current_name = model_name\n\n                if current_name in getattr(self, \"_gfk_owner_names\", set()):\n                    try:\n                        logger.debug(\n                            \"[GFK] suppressing element under owner model=%s element=%s\",\n                            current_name,\n                            element.name,\n                        )\n                    except Exception:\n                        pass\n                    return None, {}\n\n                if strategy == \"json\" or not allowed:\n                    try:\n                        logger.debug(\n                            \"[JSON] emitting JSON placeholder model=%s element=%s allowed=%s\",\n                            model_name,\n                            element.name,\n                            allowed,\n                        )\n                    except Exception:\n                        pass\n                    return self._make_json_field_kwargs(element)\n\n                if self.list_relationship_style == \"child_fk\" and is_wrapper_like:\n                    # If GFK enabled and policy applies to wrappers, record GFK entries\n                    if self.enable_gfk and (\n                        self.gfk_policy\n                        in {\"repeating_only\", \"all_nested\", \"threshold_by_children\", \"substitution_only\"}\n                    ):\n                        ovr = self._gfk_override_for(element.name)\n                        # threshold_by_children: only apply if wrapper has enough distinct child complex types (unless override True)\n                        if self.gfk_policy == \"threshold_by_children\" and ovr is not True:\n                            try:\n                                threshold = int(self.gfk_threshold_children or 0)\n                            except Exception:\n                                threshold = 0\n                            meets_threshold = threshold &lt;= 0 or len(distinct_child_types) &gt;= threshold\n                            if not meets_threshold:\n                                # Fall back to default behavior when threshold not met\n                                pending = carrier.context_data.setdefault(\"_pending_child_fk\", [])\n                                if repeating_child_type:\n                                    pending.append(\n                                        {\n                                            \"child\": repeating_child_type,\n                                            \"parent\": (\n                                                getattr(carrier.source_model, \"name\", None)\n                                                or getattr(carrier.source_model, \"__name__\", model_name)\n                                            ),\n                                            \"element_name\": element.name,\n                                            \"allow_collapse\": False,\n                                        }\n                                    )\n                                pending.append(\n                                    {\n                                        \"child\": target_type_name,\n                                        \"parent\": (\n                                            getattr(carrier.source_model, \"name\", None)\n                                            or getattr(carrier.source_model, \"__name__\", model_name)\n                                        ),\n                                        \"element_name\": element.name,\n                                        \"allow_collapse\": False,\n                                    }\n                                )\n                                return None, {}\n                        owner_name = getattr(carrier.source_model, \"name\", None) or getattr(\n                            carrier.source_model, \"__name__\", model_name\n                        )\n                        if repeating_child_type:\n                            carrier.pending_gfk_children.append(\n                                {\"child\": repeating_child_type, \"owner\": owner_name, \"element_name\": element.name}\n                            )\n                        # Also record the wrapper itself so ingestor can route single-nested wrapper to entries when needed\n                        carrier.pending_gfk_children.append(\n                            {\"child\": target_type_name, \"owner\": owner_name, \"element_name\": element.name}\n                        )\n                        # Exclude children for this wrapper from concrete generation\n                        try:\n                            if repeating_child_type:\n                                self._gfk_excluded_child_types.add(repeating_child_type)\n                            for t in distinct_child_types:\n                                if t:\n                                    self._gfk_excluded_child_types.add(t)\n                        except Exception:\n                            pass\n                        return None, {}\n                    pending = carrier.context_data.setdefault(\"_pending_child_fk\", [])\n                    # If wrapper contains repeating complex children, also link the leaf child directly to this parent\n                    if repeating_child_type:\n                        pending.append(\n                            {\n                                \"child\": repeating_child_type,\n                                \"parent\": (\n                                    getattr(carrier.source_model, \"name\", None)\n                                    or getattr(carrier.source_model, \"__name__\", model_name)\n                                ),\n                                \"element_name\": element.name,\n                                \"allow_collapse\": False,\n                            }\n                        )\n                    pending.append(\n                        {\n                            \"child\": target_type_name,\n                            \"parent\": (\n                                getattr(carrier.source_model, \"name\", None)\n                                or getattr(carrier.source_model, \"__name__\", model_name)\n                            ),\n                            \"element_name\": element.name,\n                            \"allow_collapse\": False,\n                        }\n                    )\n                    # No concrete field on parent\n                    return None, {}\n\n                # If gfk_policy == 'all_nested' and not wrapper-like, route single nested through GFK as well\n                if self.enable_gfk and self.gfk_policy == \"all_nested\":\n                    ovr = self._gfk_override_for(element.name)\n                    if ovr is not False:\n                        owner_name = getattr(carrier.source_model, \"name\", None) or getattr(\n                            carrier.source_model, \"__name__\", model_name\n                        )\n                        try:\n                            logger.debug(\n                                \"[GFK] all_nested routing element to entries owner=%s child=%s\",\n                                owner_name,\n                                target_type_name,\n                            )\n                        except Exception:\n                            pass\n                        carrier.pending_gfk_children.append(\n                            {\"child\": target_type_name, \"owner\": owner_name, \"element_name\": element.name}\n                        )\n                        try:\n                            self._gfk_excluded_child_types.add(target_type_name)\n                        except Exception:\n                            pass\n                        return None, {}\n\n                # FK on parent to child, unless hypertable -&gt; hypertable\n                roles = carrier.context_data.get(\"_timescale_roles\", {})\n                src_name = str(getattr(carrier.source_model, \"__name__\", model_name))\n                # Hypertable-&gt;Hypertable =&gt; soft reference\n                if should_soft_reference(src_name, str(target_type_name), roles):\n                    # Soft reference field with index; default to UUID representation\n                    soft_kwargs = {\"null\": True, \"blank\": True}\n                    try:\n                        soft_kwargs[\"db_index\"] = True\n                    except Exception:\n                        pass\n                    return models.UUIDField, soft_kwargs\n                # Dimension-&gt;Hypertable =&gt; optionally invert FK to hypertable\n                if self.invert_fk_to_timescale and should_invert_fk(src_name, str(target_type_name), roles):\n                    # Record pending inverted FK to inject later on the hypertable model\n                    pending_inv = carrier.context_data.setdefault(\"_pending_inverted_fk\", [])\n                    pending_inv.append(\n                        {\n                            \"hypertable\": str(target_type_name),\n                            \"dimension\": src_name,\n                            \"field_name\": src_name.lower(),\n                        }\n                    )\n                    # Do not create a FK on the dimension side\n                    return None, {}\n                kwargs = {\"to\": f\"{carrier.meta_app_label}.{target_type_name}\", \"on_delete\": models.SET_NULL}\n                kwargs[\"null\"] = True\n                kwargs[\"blank\"] = True\n                # Ensure reverse accessor uniqueness on target by including source model in related_name\n                try:\n                    rn = str(element.name).lower()\n                    if not rn.endswith(\"s\"):\n                        rn = rn + \"s\"\n                except Exception:\n                    rn = \"items\"\n                # Include both source and target model names to avoid collisions across siblings\n                kwargs[\"related_name\"] = f\"{rn}_{src_name.lower()}_{str(target_type_name).lower()}\"\n                return models.ForeignKey, kwargs\n\n        # Default simple-type mapping\n        field_class = self.FIELD_TYPE_MAP.get(element.base_type, models.CharField)\n        kwargs = {}\n\n        # Treat element-based xs:ID as a primary key using CharField\n        if element.base_type == XmlSchemaType.ID:\n            field_class = models.CharField\n            kwargs[\"max_length\"] = 255\n            kwargs[\"primary_key\"] = True\n            # ID fields should not be nullable\n            kwargs.pop(\"null\", None)\n            kwargs.pop(\"blank\", None)\n            return field_class, kwargs\n\n        if element.nillable or element.min_occurs == 0:\n            kwargs[\"null\"] = True\n            kwargs[\"blank\"] = True\n\n        # Apply inline restrictions if present (including enums, patterns, bounds)\n        if element.restrictions:\n            tmp_simple = XmlSchemaSimpleType(\n                name=element.name,\n                base_type=element.base_type,\n                restrictions=element.restrictions,\n            )\n            restr = element.restrictions\n            if getattr(restr, \"enumeration\", None):\n                return self._create_enum_field(\n                    tmp_simple, XmlSchemaFieldInfo(name=element.name, element=element), model_name, carrier\n                )\n            # Otherwise apply restriction mapping (validators, lengths, numeric bounds)\n            return self._apply_simple_type_restrictions(\n                tmp_simple, XmlSchemaFieldInfo(name=element.name, element=element)\n            )\n\n        if field_class == models.CharField:\n            if element.nillable:\n                field_class = models.TextField\n                kwargs.pop(\"max_length\", None)\n            else:\n                kwargs.setdefault(\"max_length\", 255)\n\n        if element.default_value:\n            kwargs[\"default\"] = element.default_value\n\n        if element.base_type is None and element.type_name:\n            # If we reach here, the referenced type couldn't be resolved to a complex/simple mapping\n            # Fallback to JSON for safety\n            json_field, json_kwargs = self._make_json_field_kwargs(element)\n            return json_field, json_kwargs\n\n        return field_class, kwargs\n\n    def _decide_nested_strategy(self, current_depth: int) -&gt; str:\n        \"\"\"Decide how to represent nested complex types based on configuration.\"\"\"\n        if self.nested_relationship_strategy in {\"fk\", \"json\"}:\n            return self.nested_relationship_strategy\n        # auto: depth-based\n        return \"fk\" if current_depth &lt;= self.nesting_depth_threshold else \"json\"\n\n    def _make_json_field_kwargs(self, element: XmlSchemaElement):\n        kwargs: dict = {}\n        if element.nillable or element.min_occurs == 0:\n            kwargs[\"null\"] = True\n            kwargs[\"blank\"] = True\n        return models.JSONField, kwargs\n\n    def _create_attribute_field(self, attribute: XmlSchemaAttribute, model_name: str):\n        \"\"\"Creates a Django field from an XmlSchemaAttribute.\"\"\"\n        field_class = self.FIELD_TYPE_MAP.get(attribute.base_type, models.CharField)\n        kwargs = {}\n\n        if attribute.use == \"optional\":\n            kwargs[\"null\"] = True\n            kwargs[\"blank\"] = True\n\n        if field_class == models.CharField:\n            kwargs.setdefault(\"max_length\", 255)\n\n        if attribute.default_value:\n            kwargs[\"default\"] = attribute.default_value\n\n        if attribute.base_type == XmlSchemaType.ID:\n            kwargs[\"primary_key\"] = True\n\n        return field_class, kwargs\n\n    def _create_foreign_key_field(\n        self,\n        field_info: XmlSchemaFieldInfo,\n        model_name: str,\n        carrier: ConversionCarrier[XmlSchemaComplexType],\n    ) -&gt; tuple[type[models.Field], dict]:\n        \"\"\"Creates a ForeignKey field from an IDREF attribute or element.\"\"\"\n        schema_def = carrier.source_model.schema_def\n        # Default to CASCADE; may be overridden to SET_NULL if optional\n        kwargs = {\"on_delete\": models.CASCADE}\n\n        # Find the keyref that applies to this field\n        # Handle namespace prefixes in keyref fields (e.g., 'tns:author_ref' matches 'author_ref')\n        keyref = next(\n            (\n                kr\n                for kr in schema_def.keyrefs\n                if any(field_info.name == field_path.split(\":\")[-1] for field_path in kr.fields)\n            ),\n            None,\n        )\n\n        if keyref:\n            # Find the corresponding key\n            refer_name = keyref.refer.split(\":\")[-1]\n            key = next((k for k in schema_def.keys if k.name == refer_name), None)\n            if key:\n                # Determine the target model by resolving the selector xpath\n                # key.selector is like \".//tns:Author\", extract \"Author\"\n                selector_target = key.selector.split(\":\")[-1]\n\n                # The selector typically refers to elements of a specific type\n                # Try multiple resolution strategies:\n\n                # Strategy 1: Direct complex type match (Author -&gt; AuthorType)\n                if f\"{selector_target}Type\" in schema_def.complex_types:\n                    target_model_name = f\"{selector_target}Type\"\n                # Strategy 2: Look for global element with that name\n                elif selector_target in schema_def.elements:\n                    target_element = schema_def.elements[selector_target]\n                    if target_element.type_name:\n                        target_model_name = target_element.type_name.split(\":\")[-1]\n                    else:\n                        target_model_name = f\"{selector_target}Type\"\n                # Strategy 3: Check if selector_target itself is a complex type\n                elif selector_target in schema_def.complex_types:\n                    target_model_name = selector_target\n                else:\n                    # Final fallback - use the selector target name + \"Type\"\n                    target_model_name = f\"{selector_target}Type\"\n\n                # Decide on FK vs soft reference using Timescale classification\n                roles = carrier.context_data.get(\"_timescale_roles\", {})\n                src_name = str(getattr(carrier.source_model, \"__name__\", model_name))\n                if should_soft_reference(src_name, str(target_model_name), roles):\n                    # Emit a soft reference field (UUID) instead of FK\n                    soft_kwargs = {\"null\": True, \"blank\": True, \"db_index\": True}\n                    # Return UUIDField tuple indicator by piggy-backing the return contract\n                    return models.UUIDField, soft_kwargs\n                kwargs[\"to\"] = f\"{carrier.meta_app_label}.{target_model_name}\"\n\n                # Use the keyref selector to generate a concise related_name\n                if keyref.selector:\n                    related_name_base = keyref.selector.split(\":\")[-1].replace(\".//\", \"\")\n                    rn = related_name_base.lower()\n                    if not rn.endswith(\"s\"):\n                        rn = rn + \"s\"\n                    # Deduplicate per target to avoid reverse accessor collisions\n                    target = f\"{carrier.meta_app_label}.{target_model_name}\"\n                    used = self._target_related_names.setdefault(target, set())\n                    final_rn = rn if rn not in used else f\"{rn}_{model_name.lower()}\"\n                    used.add(final_rn)\n                    kwargs[\"related_name\"] = final_rn\n                else:\n                    # Fallback: pluralize the current model name (e.g., Book -&gt; books)\n                    rn = model_name.lower()\n                    if not rn.endswith(\"s\"):\n                        rn = rn + \"s\"\n                    target = f\"{carrier.meta_app_label}.{target_model_name}\"\n                    used = self._target_related_names.setdefault(target, set())\n                    final_rn = rn if rn not in used else f\"{rn}_{model_name.lower()}\"\n                    used.add(final_rn)\n                    kwargs[\"related_name\"] = final_rn\n\n        # Determine optionality to adjust on_delete/null/blank\n        is_optional = False\n        if field_info.attribute is not None:\n            is_optional = field_info.attribute.use == \"optional\"\n        elif field_info.element is not None:\n            is_optional = field_info.element.nillable or field_info.element.min_occurs == 0\n\n        if is_optional:\n            kwargs[\"on_delete\"] = models.SET_NULL\n            kwargs[\"null\"] = True\n            kwargs[\"blank\"] = True\n\n        # Fallback if keyref resolution fails\n        if \"to\" not in kwargs:\n            kwargs[\"to\"] = f\"'{carrier.meta_app_label}.OtherModel'\"\n            logger.warning(\n                \"Could not fully resolve keyref for field '%s' in model '%s'. Using placeholder.\",\n                field_info.name,\n                model_name,\n            )\n\n        return models.ForeignKey, kwargs\n\n    def _resolve_simple_type(\n        self, type_name: str | None, schema_def: XmlSchemaDefinition\n    ) -&gt; XmlSchemaSimpleType | None:\n        \"\"\"Looks up a simple type by its name in the schema definition.\"\"\"\n        if not type_name:\n            return None\n        local_name = type_name.split(\":\")[-1]\n        return schema_def.simple_types.get(local_name)\n\n    def _get_or_create_enum_class(\n        self, simple_type: XmlSchemaSimpleType, field_info: XmlSchemaFieldInfo, carrier: ConversionCarrier\n    ) -&gt; tuple[str, bool]:\n        \"\"\"\n        Get or create a TextChoices enum class for a simpleType with enumeration.\n        Returns the class name and a boolean indicating if it was newly created.\n        \"\"\"\n        # Derive enum class name from the field name via core naming utilities\n        try:\n            from ..core.utils.naming import enum_class_name_from_field\n\n            enum_name_base = enum_class_name_from_field(field_info.name)\n        except Exception:\n            enum_name_base = str(field_info.name).replace(\"_\", \" \").title().replace(\" \", \"\")\n\n        # Add a suffix to avoid clashes with model names\n        enum_class_name = f\"{enum_name_base}\"\n\n        # Store enums in the context_data of the carrier to share them across the generation process\n        if \"enums\" not in carrier.context_data:\n            carrier.context_data[\"enums\"] = {}\n\n        if enum_class_name in carrier.context_data[\"enums\"]:\n            return enum_class_name, False\n\n        choices = []\n        restr = getattr(simple_type, \"restrictions\", None)\n        if restr and getattr(restr, \"enumeration\", None):\n            for value, label in restr.enumeration:\n                # Use the value to form the enum member name, label for human-readable\n                enum_member_name = value.replace(\"-\", \" \").upper().replace(\" \", \"_\")\n                choices.append({\"name\": enum_member_name, \"value\": value, \"label\": label})\n\n        carrier.context_data[\"enums\"][enum_class_name] = {\"name\": enum_class_name, \"choices\": choices}\n\n        return enum_class_name, True\n\n    def _generate_field_def_string(self, result: FieldConversionResult, app_label: str) -&gt; str:\n        # Avoid circular import\n        from ..django.utils.serialization import generate_field_definition_string\n\n        return generate_field_definition_string(\n            field_class=result.django_field.__class__,\n            field_kwargs=result.field_kwargs,\n            app_label=app_label,\n        )\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/factory/#pydantic2django.xmlschema.factory.XmlSchemaFieldFactory.__init__","title":"<code>__init__(*, nested_relationship_strategy='auto', list_relationship_style='child_fk', nesting_depth_threshold=1, included_model_names=None, invert_fk_to_timescale=True, enable_gfk=False, gfk_policy=None, gfk_threshold_children=None, gfk_overrides=None)</code>","text":"<p>Create a field factory for XML Schema with optional GFK routing.</p> <p>Parameters:</p> Name Type Description Default <code>nested_relationship_strategy</code> <code>str</code> <p>\"fk\" | \"json\" | \"auto\"; controls nested mapping.</p> <code>'auto'</code> <code>list_relationship_style</code> <code>str</code> <p>\"child_fk\" | \"m2m\" | \"json\" for repeating nested.</p> <code>'child_fk'</code> <code>nesting_depth_threshold</code> <code>int</code> <p>When strategy is \"auto\", use FK up to this depth.</p> <code>1</code> <code>included_model_names</code> <code>set[str] | None</code> <p>Allowed target complex types for relation emission.</p> <code>None</code> <code>invert_fk_to_timescale</code> <code>bool</code> <p>If True, prefer FKs from hypertable \u2192 dimension.</p> <code>True</code> <code>enable_gfk</code> <code>bool</code> <p>Enable Generic Entries mode.</p> <code>False</code> <code>gfk_policy</code> <code>str | None</code> <p>\"substitution_only\" | \"repeating_only\" | \"all_nested\" | \"threshold_by_children\".</p> <code>None</code> <code>gfk_threshold_children</code> <code>int | None</code> <p>Threshold for wrappers when policy is \"threshold_by_children\".</p> <code>None</code> <code>gfk_overrides</code> <code>dict[str, bool] | None</code> <p>Per-element overrides; True forces GFK, False disables it.</p> <code>None</code> Notes <ul> <li>When GFK applies for a wrapper owner, this factory records children in   <code>carrier.pending_gfk_children</code> and suppresses JSON placeholders under that owner.</li> <li>The prepass may populate <code>_gfk_excluded_child_types</code> and <code>_gfk_owner_names</code> so   we avoid emitting relations to excluded children and suppress placeholders.</li> </ul> Source code in <code>src/pydantic2django/xmlschema/factory.py</code> <pre><code>def __init__(\n    self,\n    *,\n    nested_relationship_strategy: str = \"auto\",\n    list_relationship_style: str = \"child_fk\",\n    nesting_depth_threshold: int = 1,\n    included_model_names: set[str] | None = None,\n    invert_fk_to_timescale: bool = True,\n    # --- GFK flags ---\n    enable_gfk: bool = False,\n    gfk_policy: str | None = None,\n    gfk_threshold_children: int | None = None,\n    gfk_overrides: dict[str, bool] | None = None,\n):\n    \"\"\"Create a field factory for XML Schema with optional GFK routing.\n\n    Args:\n        nested_relationship_strategy: \"fk\" | \"json\" | \"auto\"; controls nested mapping.\n        list_relationship_style: \"child_fk\" | \"m2m\" | \"json\" for repeating nested.\n        nesting_depth_threshold: When strategy is \"auto\", use FK up to this depth.\n        included_model_names: Allowed target complex types for relation emission.\n        invert_fk_to_timescale: If True, prefer FKs from hypertable \u2192 dimension.\n\n        enable_gfk: Enable Generic Entries mode.\n        gfk_policy: \"substitution_only\" | \"repeating_only\" | \"all_nested\" | \"threshold_by_children\".\n        gfk_threshold_children: Threshold for wrappers when policy is \"threshold_by_children\".\n        gfk_overrides: Per-element overrides; True forces GFK, False disables it.\n\n    Notes:\n        - When GFK applies for a wrapper owner, this factory records children in\n          `carrier.pending_gfk_children` and suppresses JSON placeholders under that owner.\n        - The prepass may populate `_gfk_excluded_child_types` and `_gfk_owner_names` so\n          we avoid emitting relations to excluded children and suppress placeholders.\n    \"\"\"\n    super().__init__()\n    self.nested_relationship_strategy = nested_relationship_strategy\n    self.list_relationship_style = list_relationship_style\n    self.nesting_depth_threshold = max(0, int(nesting_depth_threshold))\n    # Track which Django validators are needed based on restrictions encountered\n    self.used_validators: set[str] = set()\n    # Limit FK generation only to models that will actually be generated\n    self.included_model_names: set[str] | None = included_model_names\n    # Control whether to invert FKs targeting hypertables (hypertable -&gt; dimension)\n    self.invert_fk_to_timescale = invert_fk_to_timescale\n    # Track per-target related_names to avoid collisions for keyref-based FKs\n    self._target_related_names: dict[str, set[str]] = {}\n    # GFK flags\n    self.enable_gfk = bool(enable_gfk)\n    self.gfk_policy = gfk_policy\n    self.gfk_threshold_children = gfk_threshold_children\n    self.gfk_overrides = gfk_overrides or {}\n    # Track wrapper-like owner types that should route children via GFK\n    self._gfk_owner_names: set[str] = set()\n    # Track child complex types that should be excluded from concrete generation when routed via GFK\n    self._gfk_excluded_child_types: set[str] = set()\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/factory/#pydantic2django.xmlschema.factory.XmlSchemaFieldFactory.create_field","title":"<code>create_field(field_info, model_name, carrier)</code>","text":"<p>Convert XML Schema element/attribute to Django field.</p> Source code in <code>src/pydantic2django/xmlschema/factory.py</code> <pre><code>def create_field(\n    self, field_info: XmlSchemaFieldInfo, model_name: str, carrier: ConversionCarrier[XmlSchemaComplexType]\n) -&gt; FieldConversionResult:\n    \"\"\"Convert XML Schema element/attribute to Django field.\"\"\"\n\n    result = FieldConversionResult(field_info=field_info, field_name=field_info.name)\n    schema_def = carrier.source_model.schema_def\n\n    source_field = field_info.element if field_info.element else field_info.attribute\n    if not source_field:\n        return result\n\n    field_type_name = source_field.type_name\n    if field_type_name and \":\" in field_type_name:\n        field_type_name = field_type_name.split(\":\", 1)[1]\n\n    field_class, kwargs = None, {}\n\n    # Check for keyref first to determine if this is a ForeignKey\n    keyref = next(\n        (kr for kr in schema_def.keyrefs if field_info.name in kr.fields),\n        None,\n    )\n\n    if keyref:\n        field_class, kwargs = self._create_foreign_key_field(field_info, model_name, carrier)\n    elif source_field and source_field.base_type == XmlSchemaType.IDREF:\n        # Fallback for IDREFs not part of an explicit keyref\n        field_class, kwargs = self._create_foreign_key_field(field_info, model_name, carrier)\n    else:\n        # Resolve simple types\n        simple_type = self._resolve_simple_type(source_field.type_name, schema_def)\n        if simple_type:\n            restr = getattr(simple_type, \"restrictions\", None)\n            if restr and getattr(restr, \"enumeration\", None):\n                field_class, kwargs = self._create_enum_field(simple_type, field_info, model_name, carrier)\n            else:\n                # Always apply base-type mapping and any available restrictions\n                field_class, kwargs = self._apply_simple_type_restrictions(simple_type, field_info)\n        # Inline restrictions on element/attribute (no named simpleType)\n        elif getattr(source_field, \"restrictions\", None):\n            # Build a temporary simple type holder to reuse restriction logic\n            tmp_simple = XmlSchemaSimpleType(\n                name=field_info.name,\n                base_type=getattr(source_field, \"base_type\", None),\n                restrictions=getattr(source_field, \"restrictions\", None),\n            )\n            restr = tmp_simple.restrictions\n            if restr and getattr(restr, \"enumeration\", None):\n                field_class, kwargs = self._create_enum_field(tmp_simple, field_info, model_name, carrier)\n            else:\n                field_class, kwargs = self._apply_simple_type_restrictions(tmp_simple, field_info)\n        elif field_info.element:\n            field_class, kwargs = self._create_element_field(field_info.element, model_name, carrier)\n        elif field_info.attribute:\n            field_class, kwargs = self._create_attribute_field(field_info.attribute, model_name)\n\n    if field_class:\n        try:\n            result.django_field = field_class(**kwargs)\n            result.field_kwargs = kwargs\n            result.field_definition_str = self._generate_field_def_string(result, carrier.meta_app_label)\n        except Exception as e:\n            result.error_str = f\"Failed to instantiate {field_class.__name__}: {e}\"\n    else:\n        result.context_field = field_info\n\n    return result\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/factory/#pydantic2django.xmlschema.factory.XmlSchemaFieldInfo","title":"<code>XmlSchemaFieldInfo</code>  <code>dataclass</code>","text":"<p>Holds information about an XML Schema field (element or attribute).</p> Source code in <code>src/pydantic2django/xmlschema/factory.py</code> <pre><code>@dataclass\nclass XmlSchemaFieldInfo:\n    \"\"\"Holds information about an XML Schema field (element or attribute).\"\"\"\n\n    name: str\n    element: XmlSchemaElement | None = None\n    attribute: XmlSchemaAttribute | None = None\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/factory/#pydantic2django.xmlschema.factory.XmlSchemaModelFactory","title":"<code>XmlSchemaModelFactory</code>","text":"<p>               Bases: <code>BaseModelFactory[XmlSchemaComplexType, XmlSchemaFieldInfo]</code></p> <p>Creates Django <code>Model</code> instances from <code>XmlSchemaComplexType</code> definitions.</p> Source code in <code>src/pydantic2django/xmlschema/factory.py</code> <pre><code>class XmlSchemaModelFactory(BaseModelFactory[XmlSchemaComplexType, XmlSchemaFieldInfo]):\n    \"\"\"Creates Django `Model` instances from `XmlSchemaComplexType` definitions.\"\"\"\n\n    def __init__(\n        self,\n        app_label: str,\n        *,\n        nested_relationship_strategy: str = \"auto\",\n        list_relationship_style: str = \"child_fk\",\n        nesting_depth_threshold: int = 1,\n        # --- GFK flags ---\n        enable_gfk: bool = False,\n        gfk_policy: str | None = None,\n        gfk_threshold_children: int | None = None,\n        gfk_overrides: dict[str, bool] | None = None,\n    ):\n        self.app_label = app_label\n        self.field_factory = XmlSchemaFieldFactory(\n            nested_relationship_strategy=nested_relationship_strategy,\n            list_relationship_style=list_relationship_style,\n            nesting_depth_threshold=nesting_depth_threshold,\n            enable_gfk=enable_gfk,\n            gfk_policy=gfk_policy,\n            gfk_threshold_children=gfk_threshold_children,\n            gfk_overrides=gfk_overrides,\n        )\n        # Track pending child FK relations to inject after all models exist\n        self._pending_child_fk: list[dict] = []\n\n    def _handle_field_result(self, result: FieldConversionResult, carrier: ConversionCarrier[XmlSchemaComplexType]):\n        \"\"\"Handle the result of field conversion and add to appropriate carrier containers.\"\"\"\n        if result.django_field:\n            # Normalize XML names to valid Django-style field names\n            try:\n                from ..core.utils.naming import sanitize_field_identifier\n\n                out_field_name = sanitize_field_identifier(result.field_name)\n            except Exception:\n                # Fallback: basic lowering\n                out_field_name = str(result.field_name).lower()\n            # Avoid generating a plain 'id' field unless it's a primary key; rename to prevent Django E004\n            try:\n                if out_field_name.lower() == \"id\" and not getattr(result.django_field, \"primary_key\", False):\n                    out_field_name = \"xml_id\"\n            except Exception:\n                pass\n\n            carrier.django_fields[out_field_name] = result.django_field\n            if result.field_definition_str:\n                carrier.django_field_definitions[out_field_name] = result.field_definition_str\n        elif result.context_field:\n            carrier.context_fields[result.field_name] = result.context_field\n        elif result.error_str:\n            carrier.invalid_fields.append((result.field_name, result.error_str))\n\n    def _process_source_fields(self, carrier: ConversionCarrier[XmlSchemaComplexType]):\n        \"\"\"Processes elements and attributes to create Django fields.\"\"\"\n        complex_type = carrier.source_model\n\n        # Get the model name from the source model\n        model_name = getattr(carrier.source_model, \"__name__\", \"UnknownModel\")\n\n        # Either/Or enforcement: if this model is marked as a GFK owner, suppress all element fields.\n        try:\n            owner_names = getattr(self.field_factory, \"_gfk_owner_names\", set())\n            if model_name in owner_names:\n                # Still process attributes; skip elements entirely\n                try:\n                    logger.info(\"[GFK] owner=%s: suppressing all child element fields (either/or)\", model_name)\n                except Exception:\n                    pass\n                # Ensure GenericRelation('entries') and GenericEntry model are emitted by signaling pending_gfk_children\n                try:\n                    owner_marker = {\"child\": model_name, \"owner\": model_name, \"element_name\": \"*\"}\n                    carrier.pending_gfk_children.append(owner_marker)\n                except Exception:\n                    pass\n                for attr_name, attribute in complex_type.attributes.items():\n                    field_info = XmlSchemaFieldInfo(name=attr_name, attribute=attribute)\n                    result = self.field_factory.create_field(field_info, model_name, carrier)\n                    self._handle_field_result(result, carrier)\n                return\n        except Exception:\n            pass\n\n        # Process attributes\n        for attr_name, attribute in complex_type.attributes.items():\n            field_info = XmlSchemaFieldInfo(name=attr_name, attribute=attribute)\n            result = self.field_factory.create_field(field_info, model_name, carrier)\n            self._handle_field_result(result, carrier)\n\n        # Process elements\n        for element in complex_type.elements:\n            field_info = XmlSchemaFieldInfo(name=element.name, element=element)\n            result = self.field_factory.create_field(field_info, model_name, carrier)\n            self._handle_field_result(result, carrier)\n            # Collect pending child FK relations from this carrier's context\n            pending = carrier.context_data.pop(\"_pending_child_fk\", [])\n            if pending:\n                self._pending_child_fk.extend(pending)\n\n        # Ensure a concrete model exists even if no fields were produced, to satisfy relations.\n        if not carrier.django_fields and not carrier.relationship_fields:\n            try:\n                # Add a primary key to register a concrete model\n                carrier.django_fields[\"id\"] = models.AutoField(primary_key=True)\n                # Also register a definition string for static emission\n                from ..django.utils.serialization import generate_field_definition_string\n\n                pk_def = generate_field_definition_string(models.AutoField, {\"primary_key\": True}, self.app_label)\n                carrier.django_field_definitions[\"id\"] = pk_def\n            except Exception:\n                # As a fallback, keep it context-only\n                pass\n\n    def _build_model_context(self, carrier: ConversionCarrier[XmlSchemaComplexType]):\n        \"\"\"Builds the final ModelContext for the Django model.\"\"\"\n        if not carrier.django_model:\n            logger.debug(\"Skipping context build: missing django model.\")\n            return\n\n        # Create ModelContext with correct parameters\n        carrier.model_context = ModelContext(\n            django_model=carrier.django_model,\n            source_class=carrier.source_model,\n            context_fields={},  # Will be populated if needed\n            context_data=carrier.context_data,  # Pass through any context data like enums\n        )\n\n    # --- Post-processing for cross-model relationships ---\n    def finalize_relationships(\n        self, carriers_by_name: dict[str, ConversionCarrier[XmlSchemaComplexType]], app_label: str\n    ):\n        \"\"\"\n        After all models are built, inject ForeignKey fields into child models\n        for repeating nested complex elements (one-to-many parent-&gt;child).\n        \"\"\"\n        # Precompute multiplicity across parents and per child-parent pair\n        parent_sets: dict[tuple[str, str], set[str]] = {}\n        pair_counts: dict[tuple[str, str], int] = {}\n        # Build a parent chain map to allow collapsing wrapper chains: child -&gt; {parents}\n        parent_chain: dict[str, set[str]] = {}\n        for rel in self._pending_child_fk:\n            child = str(rel.get(\"child\"))\n            parent = str(rel.get(\"parent\"))\n            elem = str(rel.get(\"element_name\", \"items\"))\n            parent_sets.setdefault((child, elem), set()).add(parent)\n            pair_counts[(child, parent)] = pair_counts.get((child, parent), 0) + 1\n            parent_chain.setdefault(child, set()).add(parent)\n\n        def _collapse_parent(child_name: str, orig_parent: str, allow_collapse: bool) -&gt; str:\n            \"\"\"If a child has exactly one parent repeatedly, climb to the top-most single parent.\n            If multiple parents exist at any step, keep the original parent unchanged.\n            \"\"\"\n            if not allow_collapse:\n                return orig_parent\n            seen = set()\n            current_child = child_name\n            current_parent = orig_parent\n            while True:\n                parents = parent_chain.get(current_child)\n                if not parents or len(parents) != 1:\n                    return current_parent\n                parent = next(iter(parents))\n                if parent in seen:\n                    return current_parent\n                seen.add(parent)\n                current_parent = parent\n                current_child = parent\n\n        # Prepass: compute collisions of reverse accessors per (parent, rn_base)\n        parent_rn_to_children: dict[tuple[str, str], set[str]] = {}\n        for rel in self._pending_child_fk:\n            child_name = str(rel.get(\"child\"))\n            orig_parent_name = str(rel.get(\"parent\"))\n            allow_c = bool(rel.get(\"allow_collapse\", True))\n            parent_name = _collapse_parent(child_name, orig_parent_name, allow_c)\n            if not allow_c:\n                probe = parent_name\n                visited: set[str] = set()\n                while (\n                    probe in parent_chain\n                    and len(parent_chain.get(probe, set())) == 1\n                    and str(probe).endswith(\"WrapperType\")\n                    and probe not in visited\n                ):\n                    visited.add(probe)\n                    probe = next(iter(parent_chain[probe]))\n                parent_name = probe\n            element_name = str(rel.get(\"element_name\", \"items\"))\n            rn_base = element_name.lower()\n            if not rn_base.endswith(\"s\"):\n                rn_base = rn_base + \"s\"\n            key = (str(parent_name), rn_base)\n            s = parent_rn_to_children.setdefault(key, set())\n            s.add(child_name)\n\n        # Process pending child FKs first (if any)\n        for rel in self._pending_child_fk:\n            child_name = rel.get(\"child\")\n            # Collapse wrapper chains only when single-parent chains exist\n            orig_parent_name = str(rel.get(\"parent\"))\n            allow_c = bool(rel.get(\"allow_collapse\", True))\n            parent_name = _collapse_parent(str(child_name), orig_parent_name, allow_c)\n            if not allow_c:\n                # If parent is a wrapper type with a single parent chain, climb until a non-wrapper ancestor\n                probe = parent_name\n                visited: set[str] = set()\n                while (\n                    probe in parent_chain\n                    and len(parent_chain.get(probe, set())) == 1\n                    and str(probe).endswith(\"WrapperType\")\n                    and probe not in visited\n                ):\n                    visited.add(probe)\n                    probe = next(iter(parent_chain[probe]))\n                parent_name = probe\n            element_name = rel.get(\"element_name\", \"items\")\n            child_carrier = carriers_by_name.get(child_name)\n            if not child_carrier or not child_carrier.django_model:\n                logger.info(\n                    \"Skipping FK injection: missing or filtered child '%s' for parent '%s' (element '%s')\",\n                    child_name,\n                    parent_name,\n                    element_name,\n                )\n                continue\n            # Build kwargs for FK on child -&gt; parent\n            rn_base = element_name.lower()\n            if not rn_base.endswith(\"s\"):\n                rn_base = rn_base + \"s\"\n            # Choose related_name with minimal verbosity while avoiding collisions\n            parents_for_key = parent_sets.get((str(child_name), element_name), set())\n            has_multiple_parents = len(parents_for_key) &gt; 1\n            # Collision across different children targeting same (parent, rn_base)?\n            parent_rn_children = parent_rn_to_children.get((str(parent_name), rn_base), set())\n            has_cross_child_collision = len(parent_rn_children) &gt; 1\n            if has_cross_child_collision:\n                related_name = f\"{rn_base}_{str(child_name).lower()}\"\n            elif has_multiple_parents:\n                related_name = f\"{rn_base}_{str(parent_name).lower()}\"\n            else:\n                related_name = rn_base\n            # Use Timescale roles to decide FK vs soft reference\n            roles = child_carrier.context_data.get(\"_timescale_roles\", {})\n            child_name_str = str(child_name) if child_name else \"\"\n            parent_name_str = str(parent_name) if parent_name else \"\"\n            if should_soft_reference(child_name_str, parent_name_str, roles):\n                # Soft reference: UUID field with index\n                soft_kwargs = {\"db_index\": True}\n                # Soft refs on child-&gt;parent are often required; keep nullable off by default? Use safe default True.\n                soft_kwargs[\"null\"] = True\n                soft_kwargs[\"blank\"] = True\n                from ..django.utils.serialization import generate_field_definition_string\n\n                soft_def = generate_field_definition_string(models.UUIDField, soft_kwargs, app_label)\n                field_name = f\"{parent_name.lower()}\"\n                child_carrier.django_field_definitions[field_name] = soft_def\n            else:\n                # If multiple FKs to same parent for this child, disambiguate field name with element\n                need_elem_suffix = pair_counts.get((str(child_name), str(parent_name)), 0) &gt; 1\n                suffix = f\"_{rn_base}\" if need_elem_suffix else \"\"\n                field_name = f\"{(parent_name or '').lower()}{suffix}\"\n\n                fk_kwargs = {\n                    \"to\": f\"{app_label}.{parent_name}\",\n                    \"on_delete\": \"models.CASCADE\",\n                    \"related_name\": related_name,\n                }\n\n                # For Timescale inverted relationship (hypertable -&gt; dimension), prefer SET_NULL\n                try:\n                    from ..django.utils.serialization import RawCode\n\n                    if is_hypertable(child_name_str, roles) and not is_hypertable(parent_name_str, roles):\n                        fk_kwargs[\"on_delete\"] = RawCode(\"models.SET_NULL\")\n                        fk_kwargs[\"null\"] = True\n                        fk_kwargs[\"blank\"] = True\n                        # Suggest useful indexes on hypertable\n                        idx_list: list[str] = child_carrier.context_data.setdefault(\"meta_indexes\", [])\n                        idx_list.append(f\"models.Index(fields=['{field_name}'])\")\n                        # Composite with time if defined on the hypertable\n                        has_time = (\n                            \"time\" in child_carrier.django_field_definitions or \"time\" in child_carrier.django_fields\n                        )\n                        if has_time:\n                            idx_list.append(f\"models.Index(fields=['{field_name}', '-time'])\")\n                except Exception:\n                    pass\n\n                from ..django.utils.serialization import generate_field_definition_string\n\n                fk_def = generate_field_definition_string(models.ForeignKey, fk_kwargs, app_label)\n                child_carrier.django_field_definitions[field_name] = fk_def\n\n        # Handle inverted Timescale relationships: add FK on hypertable -&gt; dimension\n        # and emit recommended indexes\n        for carrier in carriers_by_name.values():\n            pending_inverted = carrier.context_data.get(\"_pending_inverted_fk\", [])\n            for inv in pending_inverted:\n                hyper = inv.get(\"hypertable\")\n                dim = inv.get(\"dimension\")\n                field_name = inv.get(\"field_name\", (dim or \"\").lower())\n                hyper_carrier = carriers_by_name.get(str(hyper))\n                if not hyper_carrier or not hyper_carrier.django_model:\n                    logger.info(\"Skipping inverted FK: missing hypertable carrier for %s\", hyper)\n                    continue\n                # Build FK kwargs: SET_NULL safe policy\n                from ..django.utils.serialization import RawCode\n\n                fk_kwargs = {\n                    \"to\": f\"{app_label}.{dim}\",\n                    \"on_delete\": RawCode(\"models.SET_NULL\"),\n                    \"null\": True,\n                    \"blank\": True,\n                }\n                from ..django.utils.serialization import generate_field_definition_string\n\n                fk_def = generate_field_definition_string(models.ForeignKey, fk_kwargs, app_label)\n                hyper_carrier.django_field_definitions[field_name] = fk_def\n\n                # Auto-indexes: (dimension_id) and (dimension_id, -time) if time field exists\n                idx_list: list[str] = hyper_carrier.context_data.setdefault(\"meta_indexes\", [])\n                # Single-column index on FK\n                idx_list.append(f\"models.Index(fields=['{field_name}'])\")\n                # Composite with time if defined on the hypertable\n                has_time = \"time\" in hyper_carrier.django_field_definitions or \"time\" in hyper_carrier.django_fields\n                if has_time:\n                    idx_list.append(f\"models.Index(fields=['{field_name}', '-time'])\")\n\n    # --- Override meta class creation to keep dynamic models abstract (avoid registry conflicts) ---\n    def _create_django_meta(self, carrier: ConversionCarrier[XmlSchemaComplexType]):\n        \"\"\"Create the Meta class for the generated Django model (abstract for dynamic xmlschema models).\"\"\"\n        source_name = getattr(carrier.source_model, \"__name__\", \"UnknownSourceModel\")\n        source_model_name_cleaned = source_name.replace(\"_\", \" \")\n        meta_attrs = {\n            \"app_label\": self.app_label,\n            \"db_table\": f\"{self.app_label}_{source_name.lower()}\",\n            # Keep abstract to avoid polluting Django's app registry during generation\n            \"abstract\": True,\n            \"managed\": True,\n            \"verbose_name\": source_model_name_cleaned,\n            \"verbose_name_plural\": source_model_name_cleaned + \"s\",\n            \"ordering\": [\"pk\"],\n        }\n\n        logger.debug(\"Creating Meta class for xmlschema model (abstract=True)\")\n        carrier.django_meta_class = type(\"Meta\", (), meta_attrs)\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/factory/#pydantic2django.xmlschema.factory.XmlSchemaModelFactory.finalize_relationships","title":"<code>finalize_relationships(carriers_by_name, app_label)</code>","text":"<p>After all models are built, inject ForeignKey fields into child models for repeating nested complex elements (one-to-many parent-&gt;child).</p> Source code in <code>src/pydantic2django/xmlschema/factory.py</code> <pre><code>def finalize_relationships(\n    self, carriers_by_name: dict[str, ConversionCarrier[XmlSchemaComplexType]], app_label: str\n):\n    \"\"\"\n    After all models are built, inject ForeignKey fields into child models\n    for repeating nested complex elements (one-to-many parent-&gt;child).\n    \"\"\"\n    # Precompute multiplicity across parents and per child-parent pair\n    parent_sets: dict[tuple[str, str], set[str]] = {}\n    pair_counts: dict[tuple[str, str], int] = {}\n    # Build a parent chain map to allow collapsing wrapper chains: child -&gt; {parents}\n    parent_chain: dict[str, set[str]] = {}\n    for rel in self._pending_child_fk:\n        child = str(rel.get(\"child\"))\n        parent = str(rel.get(\"parent\"))\n        elem = str(rel.get(\"element_name\", \"items\"))\n        parent_sets.setdefault((child, elem), set()).add(parent)\n        pair_counts[(child, parent)] = pair_counts.get((child, parent), 0) + 1\n        parent_chain.setdefault(child, set()).add(parent)\n\n    def _collapse_parent(child_name: str, orig_parent: str, allow_collapse: bool) -&gt; str:\n        \"\"\"If a child has exactly one parent repeatedly, climb to the top-most single parent.\n        If multiple parents exist at any step, keep the original parent unchanged.\n        \"\"\"\n        if not allow_collapse:\n            return orig_parent\n        seen = set()\n        current_child = child_name\n        current_parent = orig_parent\n        while True:\n            parents = parent_chain.get(current_child)\n            if not parents or len(parents) != 1:\n                return current_parent\n            parent = next(iter(parents))\n            if parent in seen:\n                return current_parent\n            seen.add(parent)\n            current_parent = parent\n            current_child = parent\n\n    # Prepass: compute collisions of reverse accessors per (parent, rn_base)\n    parent_rn_to_children: dict[tuple[str, str], set[str]] = {}\n    for rel in self._pending_child_fk:\n        child_name = str(rel.get(\"child\"))\n        orig_parent_name = str(rel.get(\"parent\"))\n        allow_c = bool(rel.get(\"allow_collapse\", True))\n        parent_name = _collapse_parent(child_name, orig_parent_name, allow_c)\n        if not allow_c:\n            probe = parent_name\n            visited: set[str] = set()\n            while (\n                probe in parent_chain\n                and len(parent_chain.get(probe, set())) == 1\n                and str(probe).endswith(\"WrapperType\")\n                and probe not in visited\n            ):\n                visited.add(probe)\n                probe = next(iter(parent_chain[probe]))\n            parent_name = probe\n        element_name = str(rel.get(\"element_name\", \"items\"))\n        rn_base = element_name.lower()\n        if not rn_base.endswith(\"s\"):\n            rn_base = rn_base + \"s\"\n        key = (str(parent_name), rn_base)\n        s = parent_rn_to_children.setdefault(key, set())\n        s.add(child_name)\n\n    # Process pending child FKs first (if any)\n    for rel in self._pending_child_fk:\n        child_name = rel.get(\"child\")\n        # Collapse wrapper chains only when single-parent chains exist\n        orig_parent_name = str(rel.get(\"parent\"))\n        allow_c = bool(rel.get(\"allow_collapse\", True))\n        parent_name = _collapse_parent(str(child_name), orig_parent_name, allow_c)\n        if not allow_c:\n            # If parent is a wrapper type with a single parent chain, climb until a non-wrapper ancestor\n            probe = parent_name\n            visited: set[str] = set()\n            while (\n                probe in parent_chain\n                and len(parent_chain.get(probe, set())) == 1\n                and str(probe).endswith(\"WrapperType\")\n                and probe not in visited\n            ):\n                visited.add(probe)\n                probe = next(iter(parent_chain[probe]))\n            parent_name = probe\n        element_name = rel.get(\"element_name\", \"items\")\n        child_carrier = carriers_by_name.get(child_name)\n        if not child_carrier or not child_carrier.django_model:\n            logger.info(\n                \"Skipping FK injection: missing or filtered child '%s' for parent '%s' (element '%s')\",\n                child_name,\n                parent_name,\n                element_name,\n            )\n            continue\n        # Build kwargs for FK on child -&gt; parent\n        rn_base = element_name.lower()\n        if not rn_base.endswith(\"s\"):\n            rn_base = rn_base + \"s\"\n        # Choose related_name with minimal verbosity while avoiding collisions\n        parents_for_key = parent_sets.get((str(child_name), element_name), set())\n        has_multiple_parents = len(parents_for_key) &gt; 1\n        # Collision across different children targeting same (parent, rn_base)?\n        parent_rn_children = parent_rn_to_children.get((str(parent_name), rn_base), set())\n        has_cross_child_collision = len(parent_rn_children) &gt; 1\n        if has_cross_child_collision:\n            related_name = f\"{rn_base}_{str(child_name).lower()}\"\n        elif has_multiple_parents:\n            related_name = f\"{rn_base}_{str(parent_name).lower()}\"\n        else:\n            related_name = rn_base\n        # Use Timescale roles to decide FK vs soft reference\n        roles = child_carrier.context_data.get(\"_timescale_roles\", {})\n        child_name_str = str(child_name) if child_name else \"\"\n        parent_name_str = str(parent_name) if parent_name else \"\"\n        if should_soft_reference(child_name_str, parent_name_str, roles):\n            # Soft reference: UUID field with index\n            soft_kwargs = {\"db_index\": True}\n            # Soft refs on child-&gt;parent are often required; keep nullable off by default? Use safe default True.\n            soft_kwargs[\"null\"] = True\n            soft_kwargs[\"blank\"] = True\n            from ..django.utils.serialization import generate_field_definition_string\n\n            soft_def = generate_field_definition_string(models.UUIDField, soft_kwargs, app_label)\n            field_name = f\"{parent_name.lower()}\"\n            child_carrier.django_field_definitions[field_name] = soft_def\n        else:\n            # If multiple FKs to same parent for this child, disambiguate field name with element\n            need_elem_suffix = pair_counts.get((str(child_name), str(parent_name)), 0) &gt; 1\n            suffix = f\"_{rn_base}\" if need_elem_suffix else \"\"\n            field_name = f\"{(parent_name or '').lower()}{suffix}\"\n\n            fk_kwargs = {\n                \"to\": f\"{app_label}.{parent_name}\",\n                \"on_delete\": \"models.CASCADE\",\n                \"related_name\": related_name,\n            }\n\n            # For Timescale inverted relationship (hypertable -&gt; dimension), prefer SET_NULL\n            try:\n                from ..django.utils.serialization import RawCode\n\n                if is_hypertable(child_name_str, roles) and not is_hypertable(parent_name_str, roles):\n                    fk_kwargs[\"on_delete\"] = RawCode(\"models.SET_NULL\")\n                    fk_kwargs[\"null\"] = True\n                    fk_kwargs[\"blank\"] = True\n                    # Suggest useful indexes on hypertable\n                    idx_list: list[str] = child_carrier.context_data.setdefault(\"meta_indexes\", [])\n                    idx_list.append(f\"models.Index(fields=['{field_name}'])\")\n                    # Composite with time if defined on the hypertable\n                    has_time = (\n                        \"time\" in child_carrier.django_field_definitions or \"time\" in child_carrier.django_fields\n                    )\n                    if has_time:\n                        idx_list.append(f\"models.Index(fields=['{field_name}', '-time'])\")\n            except Exception:\n                pass\n\n            from ..django.utils.serialization import generate_field_definition_string\n\n            fk_def = generate_field_definition_string(models.ForeignKey, fk_kwargs, app_label)\n            child_carrier.django_field_definitions[field_name] = fk_def\n\n    # Handle inverted Timescale relationships: add FK on hypertable -&gt; dimension\n    # and emit recommended indexes\n    for carrier in carriers_by_name.values():\n        pending_inverted = carrier.context_data.get(\"_pending_inverted_fk\", [])\n        for inv in pending_inverted:\n            hyper = inv.get(\"hypertable\")\n            dim = inv.get(\"dimension\")\n            field_name = inv.get(\"field_name\", (dim or \"\").lower())\n            hyper_carrier = carriers_by_name.get(str(hyper))\n            if not hyper_carrier or not hyper_carrier.django_model:\n                logger.info(\"Skipping inverted FK: missing hypertable carrier for %s\", hyper)\n                continue\n            # Build FK kwargs: SET_NULL safe policy\n            from ..django.utils.serialization import RawCode\n\n            fk_kwargs = {\n                \"to\": f\"{app_label}.{dim}\",\n                \"on_delete\": RawCode(\"models.SET_NULL\"),\n                \"null\": True,\n                \"blank\": True,\n            }\n            from ..django.utils.serialization import generate_field_definition_string\n\n            fk_def = generate_field_definition_string(models.ForeignKey, fk_kwargs, app_label)\n            hyper_carrier.django_field_definitions[field_name] = fk_def\n\n            # Auto-indexes: (dimension_id) and (dimension_id, -time) if time field exists\n            idx_list: list[str] = hyper_carrier.context_data.setdefault(\"meta_indexes\", [])\n            # Single-column index on FK\n            idx_list.append(f\"models.Index(fields=['{field_name}'])\")\n            # Composite with time if defined on the hypertable\n            has_time = \"time\" in hyper_carrier.django_field_definitions or \"time\" in hyper_carrier.django_fields\n            if has_time:\n                idx_list.append(f\"models.Index(fields=['{field_name}', '-time'])\")\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/generator/","title":"pydantic2django.xmlschema.generator","text":"<p>XML Schema Django model generator. Main entry point for generating Django models from XML Schema files.</p>"},{"location":"reference/pydantic2django/xmlschema/generator/#pydantic2django.xmlschema.generator.XmlSchemaDjangoModelGenerator","title":"<code>XmlSchemaDjangoModelGenerator</code>","text":"<p>               Bases: <code>BaseStaticGenerator[XmlSchemaComplexType, XmlSchemaFieldInfo]</code></p> <p>Main class to orchestrate the generation of Django models from XML Schemas.</p> Source code in <code>src/pydantic2django/xmlschema/generator.py</code> <pre><code>class XmlSchemaDjangoModelGenerator(BaseStaticGenerator[XmlSchemaComplexType, XmlSchemaFieldInfo]):\n    \"\"\"\n    Main class to orchestrate the generation of Django models from XML Schemas.\n    \"\"\"\n\n    def __init__(\n        self,\n        schema_files: list[str | Path],\n        output_path: str = \"generated_models.py\",\n        app_label: str = \"xmlschema_app\",\n        filter_function: Callable[[XmlSchemaComplexType], bool] | None = None,\n        verbose: bool = False,\n        module_mappings: dict[str, str] | None = None,\n        class_name_prefix: str = \"\",\n        # Relationship handling for nested complex types\n        nested_relationship_strategy: str = \"fk\",  # one of: \"fk\", \"json\", \"auto\"\n        list_relationship_style: str = \"child_fk\",  # one of: \"child_fk\", \"m2m\", \"json\"\n        nesting_depth_threshold: int = 1,\n        # Optional override for the Django base model class\n        base_model_class: type[models.Model] | None = None,\n        # Feature flags\n        enable_timescale: bool = True,\n        # Timescale configuration\n        timescale_overrides: dict[str, TimescaleRole] | None = None,\n        timescale_config: TimescaleConfig | None = None,\n        timescale_strict: bool = False,\n        # Control behavior for missing leaf/child targets in finalize pass\n        auto_generate_missing_leaves: bool = False,\n        # --- GFK flags ---\n        enable_gfk: bool = True,\n        gfk_policy: str | None = \"threshold_by_children\",\n        gfk_threshold_children: int | None = 8,\n        gfk_value_mode: str | None = \"typed_columns\",\n        gfk_normalize_common_attrs: bool = False,\n        gfk_overrides: dict[str, bool] | None = None,\n    ):\n        \"\"\"Create a XML Schema \u2192 Django generator with optional Generic Entries mode.\n\n        GFK (Generic Entries) flags:\n        - enable_gfk: Enable Generic Entries mode. When True and policy matches, parents\n          gain `entries = GenericRelation('GenericEntry')` and concrete children are not emitted.\n        - gfk_policy: Controls which nested elements are routed to GenericEntry.\n          Values: \"substitution_only\" | \"repeating_only\" | \"all_nested\" | \"threshold_by_children\".\n        - gfk_threshold_children: Used only with \"threshold_by_children\"; minimum distinct child\n          complex types inside a wrapper-like container to activate GFK.\n        - gfk_value_mode: \"json_only\" stores text under attrs_json['value']; \"typed_columns\"\n          extracts text into text_value, num_value, time_value when unambiguous.\n        - gfk_normalize_common_attrs: Reserved (default False) for promoting frequently used\n          attributes (e.g., timestamp) into normalized columns when using typed columns.\n        - gfk_overrides: Optional per-element overrides by local element name;\n          True forces GFK, False disables it even if the policy would apply.\n        \"\"\"\n        discovery = XmlSchemaDiscovery()\n        model_factory = XmlSchemaModelFactory(\n            app_label=app_label,\n            nested_relationship_strategy=nested_relationship_strategy,\n            list_relationship_style=list_relationship_style,\n            nesting_depth_threshold=nesting_depth_threshold,\n            enable_gfk=enable_gfk,\n            gfk_policy=gfk_policy,\n            gfk_threshold_children=gfk_threshold_children,\n            gfk_overrides=gfk_overrides,\n        )\n\n        super().__init__(\n            output_path=output_path,\n            packages=[str(f) for f in schema_files],\n            app_label=app_label,\n            discovery_instance=discovery,\n            model_factory_instance=model_factory,\n            base_model_class=base_model_class or self._get_default_base_model_class(),\n            class_name_prefix=class_name_prefix,\n            module_mappings=module_mappings,\n            verbose=verbose,\n            filter_function=filter_function,\n            enable_timescale=enable_timescale,\n            enable_gfk=enable_gfk,\n            gfk_policy=gfk_policy,\n            gfk_threshold_children=gfk_threshold_children,\n            gfk_value_mode=gfk_value_mode,\n            gfk_normalize_common_attrs=gfk_normalize_common_attrs,\n        )\n        # Timescale classification results cached per run\n        self._timescale_roles: dict[str, TimescaleRole] = {}\n        self._timescale_overrides: dict[str, TimescaleRole] | None = timescale_overrides\n        self._timescale_config: TimescaleConfig | None = timescale_config\n        self._timescale_strict: bool = timescale_strict\n        self._auto_generate_missing_leaves: bool = auto_generate_missing_leaves\n\n    def _get_model_definition_extra_context(self, carrier: ConversionCarrier) -&gt; dict:\n        \"\"\"\n        Extracts additional context required for rendering the Django model,\n        including field definitions and enum classes.\n        \"\"\"\n        field_definitions = carrier.django_field_definitions\n\n        enum_classes = carrier.context_data.get(\"enums\", {})\n\n        return {\n            \"field_definitions\": field_definitions,\n            \"enum_classes\": enum_classes.values(),\n            # Extra Meta emission (e.g., indexes)\n            \"meta_indexes\": carrier.context_data.get(\"meta_indexes\", []),\n        }\n\n    # All rendering logic is now handled by the BaseStaticGenerator using the implemented abstract methods.\n    # The custom generate and _generate_file_content methods are no longer needed.\n\n    # --- Implement abstract methods from BaseStaticGenerator ---\n\n    def _get_source_model_name(self, carrier: ConversionCarrier[XmlSchemaComplexType]) -&gt; str:\n        \"\"\"Get the name of the original source model from the carrier.\"\"\"\n        return carrier.source_model.name\n\n    def _add_source_model_import(self, carrier: ConversionCarrier[XmlSchemaComplexType]):\n        \"\"\"Add the necessary import for the original source model.\"\"\"\n        # For XML Schema, the models are generated, not imported\n        pass\n\n    def _prepare_template_context(\n        self, unique_model_definitions: list[str], django_model_names: list[str], imports: dict\n    ) -&gt; dict:\n        \"\"\"Prepare the subclass-specific context for the main models_file.py.j2 template.\"\"\"\n        return {\n            \"model_definitions\": unique_model_definitions,\n            \"django_model_names\": django_model_names,\n            # Flattened import categories for templates\n            \"pydantic_imports\": sorted(imports.get(\"pydantic\", [])),\n            \"context_imports\": sorted(imports.get(\"context\", [])),\n            \"django_imports\": sorted(imports.get(\"django\", [])),\n            \"general_imports\": sorted(imports.get(\"general\", [])),\n            # Also pass the structured imports dict if templates need it\n            \"imports\": imports,\n            \"generation_timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n            \"app_label\": self.app_label,\n            \"generation_source_type\": \"xmlschema\",\n        }\n\n    def _get_default_base_model_class(self) -&gt; type[models.Model]:\n        \"\"\"Return the default Django base model for XML Schema conversion.\"\"\"\n        # Prefer local pydantic2django base; fall back to typed2django if present\n        try:\n            from pydantic2django.django.models import Xml2DjangoBaseClass as _Base\n\n            return _Base\n        except Exception as exc:\n            logger.debug(\n                \"Default base model import failed; will try typed2django fallback: %s\",\n                exc,\n                exc_info=True,\n            )\n        try:\n            from typed2django.django.models import (\n                Xml2DjangoBaseClass as _Base,  # type: ignore[import-not-found]\n            )\n\n            return _Base\n        except Exception as exc:  # pragma: no cover - defensive\n            raise ImportError(\n                \"pydantic2django.django.models.Xml2DjangoBaseClass (or typed2django equivalent) is required for XML Schema generation.\"\n            ) from exc\n\n    def _get_models_in_processing_order(self) -&gt; list[XmlSchemaComplexType]:\n        \"\"\"Return source models in the correct processing (dependency) order.\"\"\"\n        return self.discovery_instance.get_models_in_registration_order()\n\n    # --- Additional XML Schema specific methods ---\n\n    def generate_models_with_xml_metadata(self) -&gt; str:\n        \"\"\"\n        Generate Django models with additional XML metadata.\n\n        This method extends the base generate() to add XML-specific\n        comments and metadata to the generated models.\n        \"\"\"\n        content = self.generate_models_file()\n\n        # Add XML Schema file references as comments at the top\n        schema_files_comment = \"\\n\".join(\n            [f\"# Generated from XML Schema: {schema_file}\" for schema_file in self.schema_files]\n        )\n\n        # Insert after the initial comments\n        lines = content.split(\"\\n\")\n        insert_index = 0\n        for i, line in enumerate(lines):\n            if line.startswith('\"\"\"') and '\"\"\"' in line[3:]:  # Single line docstring\n                insert_index = i + 1\n                break\n            elif line.startswith('\"\"\"'):  # Multi-line docstring start\n                for j in range(i + 1, len(lines)):\n                    if '\"\"\"' in lines[j]:\n                        insert_index = j + 1\n                        break\n                break\n\n        lines.insert(insert_index, schema_files_comment)\n        lines.insert(insert_index + 1, \"\")\n\n        return \"\\n\".join(lines)\n\n    def get_schema_statistics(self) -&gt; dict:\n        \"\"\"Get statistics about the parsed schemas.\"\"\"\n        stats = {\n            \"total_schemas\": len(self.discovery_instance.parsed_schemas),\n            \"total_complex_types\": len(self.discovery_instance.all_models),\n            \"filtered_complex_types\": len(self.discovery_instance.filtered_models),\n            \"generated_models\": len(self.carriers),\n        }\n\n        # Add per-schema breakdown\n        schema_breakdown = []\n        for schema_def in self.discovery_instance.parsed_schemas:\n            schema_breakdown.append(\n                {\n                    \"schema_location\": schema_def.schema_location,\n                    \"target_namespace\": schema_def.target_namespace,\n                    \"complex_types\": len(schema_def.complex_types),\n                    \"simple_types\": len(schema_def.simple_types),\n                    \"elements\": len(schema_def.elements),\n                }\n            )\n\n        stats[\"schema_breakdown\"] = schema_breakdown\n        return stats\n\n    def validate_schemas(self) -&gt; list[str]:\n        \"\"\"\n        Validate the parsed schemas and return any warnings or errors.\n\n        Returns:\n            List of validation messages\n        \"\"\"\n        messages = []\n\n        for schema_def in self.discovery_instance.parsed_schemas:\n            # Check for common issues\n            if not schema_def.target_namespace:\n                messages.append(f\"Schema {schema_def.schema_location} has no target namespace\")\n\n            # Check for name conflicts\n            all_names = set()\n            for complex_type in schema_def.complex_types.values():\n                if complex_type.name in all_names:\n                    messages.append(f\"Duplicate type name: {complex_type.name}\")\n                all_names.add(complex_type.name)\n\n        return messages\n\n    @classmethod\n    def from_schema_files(cls, schema_files: list[str | Path], **kwargs) -&gt; \"XmlSchemaDjangoModelGenerator\":\n        \"\"\"\n        Convenience class method to create generator from schema files.\n\n        Args:\n            schema_files: List of XSD file paths\n            **kwargs: Additional arguments passed to __init__\n\n        Returns:\n            Configured XmlSchemaDjangoModelGenerator instance\n        \"\"\"\n        return cls(schema_files=schema_files, **kwargs)\n\n    def _render_choices_class(self, choices_info: dict) -&gt; str:\n        \"\"\"Render a single TextChoices class.\"\"\"\n        class_name = choices_info[\"name\"]\n        choices = choices_info[\"choices\"]\n        lines = [f\"class {class_name}(models.TextChoices):\"]\n        for value, label in choices:\n            # Attempt to create a valid Python identifier for the member name\n            member_name = re.sub(r\"[^a-zA-Z0-9_]\", \"_\", label.upper())\n            if not member_name or not member_name[0].isalpha():\n                member_name = f\"CHOICE_{member_name}\"\n            lines.append(f'    {member_name} = \"{value}\", \"{label}\"')\n        return \"\\\\n\".join(lines)\n\n    def generate(self):\n        \"\"\"\n        Main method to generate the Django models file.\n        \"\"\"\n        logger.info(f\"Starting Django model generation to {self.output_path}\")\n\n        # The base class now handles the full generation pipeline\n        super().generate()\n\n        logger.info(f\"Successfully generated Django models in {self.output_path}\")\n\n    def generate_models_file(self) -&gt; str:\n        \"\"\"\n        Override to allow relationship finalization after carriers are built\n        but before rendering templates.\n        \"\"\"\n        # Discover and create carriers first via base implementation pieces\n        self.discover_models()\n        # --- GFK Prepass: when enabled, identify wrapper owners and exclude their polymorphic children ---\n        if getattr(self, \"enable_gfk\", False) and getattr(self, \"gfk_policy\", None):\n            try:\n                excluded_child_types: set[str] = set()\n                gfk_owner_wrappers: set[str] = set()\n                policy = str(getattr(self, \"gfk_policy\", \"\"))\n                threshold_val = int(getattr(self, \"gfk_threshold_children\", 0) or 0)\n                # Walk parsed schemas to infer wrapper owners and their child types\n                for schema_def in getattr(self.discovery_instance, \"parsed_schemas\", []) or []:\n                    try:\n                        for ct in schema_def.get_all_complex_types():\n                            # Iterate elements of each complex type\n                            for el in ct.elements:\n                                # Only interested in complex targets\n                                if not el.type_name or el.base_type is not None:\n                                    continue\n                                target_type = el.type_name.split(\":\")[-1]\n                                target_ct = schema_def.complex_types.get(target_type)\n                                if not target_ct:\n                                    continue\n                                # Wrapper-like heuristic (may be used by providers; structural checks take precedence)\n                                _is_wrapper_like = bool(el.name[:1].isupper()) or str(target_type).endswith(\n                                    \"WrapperType\"\n                                )\n                                # Collect distinct child complex types under target wrapper\n                                distinct_child_types: set[str] = set()\n                                has_substitution_members = False\n                                repeating_leaf_child: str | None = None\n                                for child_el in target_ct.elements:\n                                    if getattr(child_el, \"substitution_group\", None):\n                                        has_substitution_members = True\n                                    if getattr(child_el, \"type_name\", None):\n                                        cand = child_el.type_name.split(\":\")[-1]\n                                        if cand:\n                                            distinct_child_types.add(cand)\n                                    if getattr(child_el, \"is_list\", False) and getattr(child_el, \"type_name\", None):\n                                        repeating_leaf_child = child_el.type_name.split(\":\")[-1]\n\n                                # Decide based on policy (favor generic structural checks over name heuristics)\n                                route_by_subst = policy == \"substitution_only\" and has_substitution_members\n                                route_by_all_nested = policy == \"all_nested\"\n                                route_by_threshold = policy == \"threshold_by_children\" and (\n                                    threshold_val &lt;= 0 or len(distinct_child_types) &gt;= threshold_val\n                                )\n                                route_by_repeating = policy == \"repeating_only\" and bool(el.is_list)\n\n                                if route_by_subst or route_by_all_nested or route_by_threshold or route_by_repeating:\n                                    # Mark the CURRENT complex type (container) as the owner so placeholders on it are suppressed\n                                    try:\n                                        owner_name = getattr(ct, \"name\", None) or getattr(ct, \"__name__\", str(ct))\n                                    except Exception:\n                                        owner_name = target_type\n                                    gfk_owner_wrappers.add(owner_name)\n                                    if getattr(self, \"verbose\", False):\n                                        reason = (\n                                            \"substitution_only\"\n                                            if route_by_subst\n                                            else (\n                                                \"all_nested\"\n                                                if route_by_all_nested\n                                                else (\n                                                    \"threshold_by_children\" if route_by_threshold else \"repeating_only\"\n                                                )\n                                            )\n                                        )\n                                        logger.info(\n                                            \"[GFK][prepass] owner=%s element=%s policy=%s distinct_children=%d\",\n                                            owner_name,\n                                            el.name,\n                                            reason,\n                                            len(distinct_child_types),\n                                        )\n                                    # Exclude polymorphic children under this wrapper from concrete generation\n                                    if el.is_list and repeating_leaf_child:\n                                        excluded_child_types.add(repeating_leaf_child)\n                                    # Also exclude distinct child complex types observed under the wrapper\n                                    excluded_child_types.update({t for t in distinct_child_types if t})\n                    except Exception:\n                        logger.error(\n                            \"[GFK][prepass] Failed while processing schema '%s'\",\n                            getattr(schema_def, \"schema_location\", \"?\"),\n                            exc_info=True,\n                        )\n                        continue  # best-effort per schema\n\n                # Filter discovery output to remove excluded children, keeping wrappers/parents\n                try:\n                    # filtered_models is a mapping of qualname -&gt; complex_type\n                    fm = dict(getattr(self.discovery_instance, \"filtered_models\", {}) or {})\n                    if fm and excluded_child_types:\n                        new_fm = {k: v for k, v in fm.items() if getattr(v, \"name\", None) not in excluded_child_types}\n                        self.discovery_instance.filtered_models = new_fm  # type: ignore[attr-defined]\n                        if getattr(self, \"verbose\", False):\n                            logger.info(\n                                \"[GFK][prepass] excluded_children_count=%d (examples: %s)\",\n                                len(excluded_child_types),\n                                \", \".join(sorted(excluded_child_types)[:10]),\n                            )\n                except Exception as exc:\n                    logger.error(\"[GFK][prepass] Failed to filter excluded children: %s\", exc, exc_info=True)\n\n                # Share exclusions and owner marks with the field factory for intra-model decisions\n                try:\n                    ff = getattr(self.model_factory_instance, \"field_factory\", None)\n                    if ff is not None:\n                        ff._gfk_excluded_child_types = excluded_child_types\n                        # Seed owner names to suppress JSON placeholders for wrapper children\n                        owner_names = set(getattr(ff, \"_gfk_owner_names\", set()))\n                        owner_names.update(gfk_owner_wrappers)\n                        ff._gfk_owner_names = owner_names\n                except Exception as exc:\n                    logger.error(\"[GFK][prepass] Failed to share state with field factory: %s\", exc, exc_info=True)\n            except Exception as exc:\n                # Prepass is best-effort and should never crash generation\n                logger.error(\"[GFK][prepass] Unexpected error: %s\", exc, exc_info=True)\n        # Conflict guard: if GFK owners detected but JSON relationship styles are requested, fail fast.\n        if getattr(self, \"enable_gfk\", False):\n            ff = getattr(self.model_factory_instance, \"field_factory\", None)\n            owners = set(getattr(ff, \"_gfk_owner_names\", set()) or set())\n            list_style = getattr(ff, \"list_relationship_style\", \"child_fk\")\n            nested_style = getattr(ff, \"nested_relationship_strategy\", \"fk\")\n            if owners and (list_style == \"json\" or nested_style == \"json\"):\n                raise ValueError(\n                    \"GFK mode active with owners detected, but JSON relationship strategy was requested. \"\n                    \"This configuration would produce dual emission. Adjust flags or disable JSON strategy.\"\n                )\n        # Inform the field factory which models are actually included so it can\n        # avoid generating relations to filtered-out models (fallback to JSON).\n        try:\n            # Prefer local (unqualified) complex type names to match factory lookups\n            # filtered_models keys may be qualified as \"{namespace}.{name}\", so derive plain names\n            included_local_names = {\n                getattr(m, \"name\", str(m))\n                for m in self.discovery_instance.filtered_models.values()  # type: ignore[attr-defined]\n            }\n            # Also include any already-qualified keys for maximum compatibility\n            included_qualified_keys = set(self.discovery_instance.filtered_models.keys())  # type: ignore[attr-defined]\n            included_names = included_local_names | included_qualified_keys\n            # type: ignore[attr-defined]\n            self.model_factory_instance.field_factory.included_model_names = included_names  # noqa: E501\n            # Also pass through any GFK prepass exclusions to the field factory if not already set\n            ff = self.model_factory_instance.field_factory\n            if not getattr(ff, \"_gfk_excluded_child_types\", None):\n                ff._gfk_excluded_child_types = set()\n        except Exception as exc:\n            logger.error(\n                \"Failed to inform field factory about included models or defaults: %s\",\n                exc,\n                exc_info=True,\n            )\n        models_to_process = self._get_models_in_processing_order()\n\n        # Classify models for Timescale usage (hypertable vs dimension)\n        if self.enable_timescale:\n            try:\n                self._timescale_roles = classify_xml_complex_types(\n                    models_to_process,\n                    overrides=self._timescale_overrides,\n                    config=self._timescale_config,\n                )\n            except Exception as exc:\n                logger.error(\"Timescale classification failed; proceeding without roles: %s\", exc, exc_info=True)\n                self._timescale_roles = {}\n        else:\n            self._timescale_roles = {}\n\n        # Strict validation: if any hypertable lacks a direct time-like field, fail fast\n        if self.enable_timescale and self._timescale_strict:\n            for m in models_to_process:\n                name = getattr(m, \"name\", getattr(m, \"__name__\", str(m)))\n                if self._timescale_roles.get(name) == TimescaleRole.HYPERTABLE and not has_direct_time_feature(m):\n                    raise ValueError(\n                        f\"Timescale strict mode: '{name}' classified as hypertable but has no direct time-like field. \"\n                        f\"Add a time/timestamp-like attribute/element or demote via overrides.\"\n                    )\n\n        # Reset state for this run (mirror BaseStaticGenerator)\n        self.carriers = []\n        self.import_handler.extra_type_imports.clear()\n        self.import_handler.pydantic_imports.clear()\n        self.import_handler.context_class_imports.clear()\n        self.import_handler.imported_names.clear()\n        self.import_handler.processed_field_types.clear()\n        self.import_handler._add_type_import(self.base_model_class)\n\n        for source_model in models_to_process:\n            self.setup_django_model(source_model)\n\n        # Give the factory a chance to add cross-model relationship fields (e.g., child FKs)\n        carriers_by_name = {\n            (getattr(c.source_model, \"name\", None) or getattr(c.source_model, \"__name__\", \"\")): c\n            for c in self.carriers\n            if c.django_model is not None\n        }\n        if hasattr(self.model_factory_instance, \"finalize_relationships\"):\n            try:\n                # type: ignore[attr-defined]\n                self.model_factory_instance.finalize_relationships(carriers_by_name, self.app_label)  # noqa: E501\n            except Exception as e:\n                logger.error(\"Error finalizing XML relationships: %s\", e, exc_info=True)\n\n        # Inject GenericRelation on parents when GFK is enabled and pending children exist\n        gfk_used = False\n        try:\n            for carrier in self.carriers:\n                if getattr(carrier, \"enable_gfk\", False) and getattr(carrier, \"pending_gfk_children\", None):\n                    # Ensure contenttypes imports\n                    self.import_handler.add_import(\"django.contrib.contenttypes.fields\", \"GenericRelation\")\n                    self.import_handler.add_import(\"django.contrib.contenttypes.fields\", \"GenericForeignKey\")\n                    self.import_handler.add_import(\"django.contrib.contenttypes.models\", \"ContentType\")\n                    # Add reverse relation field on parent model\n                    carrier.django_field_definitions[\n                        \"entries\"\n                    ] = \"GenericRelation('GenericEntry', related_query_name='entries')\"\n                    gfk_used = True\n        except Exception as exc:\n            logger.error(\"Failed while injecting GenericRelation/GenericEntry fields: %s\", exc, exc_info=True)\n\n        # Optional diagnostics: summarize GFK routing per parent\n        if gfk_used and getattr(self, \"verbose\", False):\n            try:\n                summary: dict[str, int] = {}\n                for carrier in self.carriers:\n                    for entry in getattr(carrier, \"pending_gfk_children\", []) or []:\n                        owner = str(entry.get(\"owner\") or getattr(carrier.source_model, \"name\", \"\"))\n                        summary[owner] = summary.get(owner, 0) + 1\n                if summary:\n                    details = \", \".join(f\"{k}:{v}\" for k, v in sorted(summary.items()))\n                    logger.info(f\"[GFK] Routed children counts by owner: {details}\")\n            except Exception as exc:\n                logger.error(\"Failed to summarize GFK routing: %s\", exc, exc_info=True)\n\n        # Register generated model classes for in-memory lookup by ingestors\n        try:\n            for carrier in self.carriers:\n                if carrier.django_model is not None:\n                    register_generated_model(self.app_label, carrier.django_model)\n                    logger.info(\n                        \"Registered generated model %s (abstract=%s) for app '%s'\",\n                        carrier.django_model.__name__,\n                        getattr(getattr(carrier.django_model, \"_meta\", None), \"abstract\", None),\n                        self.app_label,\n                    )\n                    # Prevent dynamic classes from polluting Django's global app registry\n                    try:\n                        from django.apps import (\n                            apps as django_apps,  # Local import to avoid hard dependency\n                        )\n\n                        model_lower = getattr(getattr(carrier.django_model, \"_meta\", None), \"model_name\", None)\n                        if model_lower:\n                            django_apps.all_models.get(self.app_label, {}).pop(model_lower, None)\n                    except Exception as exc:\n                        # Best-effort cleanup only, but log for visibility\n                        logger.error(\"Failed to clean up Django global app registry: %s\", exc, exc_info=True)\n        except Exception as e:\n            logger.error(\n                \"Non-fatal: failed to register generated models for app '%s': %s\",\n                self.app_label,\n                e,\n                exc_info=True,\n            )\n\n        # Proceed with standard definition rendering\n        model_definitions = []\n        django_model_names = []\n        for carrier in self.carriers:\n            if carrier.django_model:\n                try:\n                    model_def = self.generate_model_definition(carrier)\n                    if model_def:\n                        model_definitions.append(model_def)\n                        django_model_names.append(f\"'{self._clean_generic_type(carrier.django_model.__name__)}'\")\n                except Exception as e:\n                    logger.error(\n                        f\"Error generating definition for source model {getattr(carrier.source_model, '__name__', '?')}: {e}\",\n                        exc_info=True,\n                    )\n\n        # If GFK used, append GenericEntry model definition and include in __all__\n        if gfk_used:\n            model_definitions.append(self._build_generic_entry_model_definition())\n            django_model_names.append(\"'GenericEntry'\")\n\n        unique_model_definitions = self._deduplicate_definitions(model_definitions)\n        # Ensure that every advertised model name in __all__ has a corresponding class definition.\n        try:\n            import re as _re\n\n            joined_defs = \"\\n\".join(unique_model_definitions)\n            existing_names = {m.group(1) for m in _re.finditer(r\"^\\s*class\\s+(\\w+)\\(\", joined_defs, _re.MULTILINE)}\n            advertised_names = [\n                self._clean_generic_type(getattr(c.django_model, \"__name__\", \"\"))\n                for c in self.carriers\n                if c.django_model is not None\n            ]\n            for name in advertised_names:\n                if name and name not in existing_names:\n                    minimal_def = (\n                        f'\"\"\"\\nDjango model for {name}.\\n\"\"\"\\n\\n'\n                        f\"class {name}({self.base_model_class.__name__}):\\n    # No fields defined for this model.\\n    pass\\n\\n    class Meta:\\n        app_label = '{self.app_label}'\\n        abstract = False\\n\\n\"\n                    )\n                    unique_model_definitions.append(minimal_def)\n        except Exception as exc:\n            logger.error(\n                \"Failed to ensure all advertised model names have definitions: %s\",\n                exc,\n                exc_info=True,\n            )\n\n        imports = self.import_handler.deduplicate_imports()\n        template_context = self._prepare_template_context(unique_model_definitions, django_model_names, imports)\n        template_context.update(\n            {\n                \"generation_timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n                \"base_model_module\": self.base_model_class.__module__,\n                \"base_model_name\": self.base_model_class.__name__,\n                \"extra_type_imports\": sorted(self.import_handler.extra_type_imports),\n            }\n        )\n        # Detect validator usage to ensure imports are present\n        used_validators: set[str] = set()\n        # Primary source: field factory tracking\n        try:\n            field_factory = getattr(self.model_factory_instance, \"field_factory\", None)\n            factory_used = getattr(field_factory, \"used_validators\", None)\n            if isinstance(factory_used, set):\n                used_validators.update(factory_used)\n        except Exception as exc:\n            logger.error(\"Failed to collect validator usage from field factory: %s\", exc, exc_info=True)\n        # Fallback: scan rendered definitions\n        try:\n            if not used_validators:\n                validator_symbols = [\n                    \"RegexValidator\",\n                    \"MinValueValidator\",\n                    \"MaxValueValidator\",\n                    \"URLValidator\",\n                    \"EmailValidator\",\n                    \"validate_slug\",\n                    \"validate_uuid4\",\n                    \"validate_email\",\n                    \"validate_ipv46_address\",\n                    \"validate_ipv4_address\",\n                    \"validate_ipv6_address\",\n                    \"validate_unicode_slug\",\n                ]\n                joined_defs = \"\\n\".join(unique_model_definitions)\n                for symbol in validator_symbols:\n                    if re.search(rf\"\\\\b{re.escape(symbol)}\\\\b\", joined_defs):\n                        used_validators.add(symbol)\n        except Exception as exc:\n            logger.error(\"Failed to scan rendered definitions for validator usage: %s\", exc, exc_info=True)\n        template_context[\"validator_imports\"] = sorted(used_validators)\n        template = self.jinja_env.get_template(\"models_file.py.j2\")\n        return template.render(**template_context)\n\n    def _build_generic_entry_model_definition(self) -&gt; str:\n        \"\"\"Build the GenericEntry model definition string for XMLSchema generation.\"\"\"\n        self.import_handler.add_import(\"django.contrib.contenttypes.fields\", \"GenericForeignKey\")\n        self.import_handler.add_import(\"django.contrib.contenttypes.fields\", \"GenericRelation\")\n        self.import_handler.add_import(\"django.contrib.contenttypes.models\", \"ContentType\")\n\n        fields: list[str] = []\n        fields.append(\"content_type = models.ForeignKey('contenttypes.ContentType', on_delete=models.CASCADE)\")\n        fields.append(\"object_id = models.PositiveIntegerField()\")\n        fields.append(\"content_object = GenericForeignKey('content_type', 'object_id')\")\n        fields.append(\"element_qname = models.CharField(max_length=255)\")\n        fields.append(\"type_qname = models.CharField(max_length=255, null=True, blank=True)\")\n        fields.append(\"attrs_json = models.JSONField(default=dict, blank=True)\")\n        if getattr(self, \"gfk_value_mode\", None) == \"typed_columns\":\n            fields.append(\"text_value = models.TextField(null=True, blank=True)\")\n            fields.append(\"num_value = models.DecimalField(max_digits=20, decimal_places=6, null=True, blank=True)\")\n            fields.append(\"time_value = models.DateTimeField(null=True, blank=True)\")\n        fields.append(\"order_index = models.IntegerField(default=0)\")\n        fields.append(\"path_hint = models.CharField(max_length=255, null=True, blank=True)\")\n\n        indexes_lines = [\"models.Index(fields=['content_type', 'object_id'])\"]\n        # Optional indexes when typed value columns are enabled\n        if getattr(self, \"gfk_value_mode\", None) == \"typed_columns\":\n            indexes_lines.append(\"models.Index(fields=['element_qname'])\")\n            indexes_lines.append(\"models.Index(fields=['type_qname'])\")\n            indexes_lines.append(\"models.Index(fields=['time_value'])\")\n            indexes_lines.append(\"models.Index(fields=['content_type', 'object_id', '-time_value'])\")\n\n        lines: list[str] = []\n        lines.append(f\"class GenericEntry({self.base_model_class.__name__}):\")\n        for f in fields:\n            lines.append(f\"    {f}\")\n        lines.append(\"\")\n        lines.append(\"    class Meta:\")\n        lines.append(f\"        app_label = '{self.app_label}'\")\n        lines.append(\"        abstract = False\")\n        lines.append(\"        indexes = [\")\n        for idx in indexes_lines:\n            lines.append(f\"            {idx},\")\n        lines.append(\"        ]\")\n        lines.append(\"\")\n        return \"\\n\".join(lines)\n\n    # Override to choose Timescale base per model and pass roles map to factory\n    def setup_django_model(self, source_model: XmlSchemaComplexType) -&gt; ConversionCarrier | None:  # type: ignore[override]\n        source_model_name = getattr(source_model, \"__name__\", getattr(source_model, \"name\", str(source_model)))\n        use_ts_base = False\n        if self.enable_timescale:\n            try:\n                use_ts_base = should_use_timescale_base(source_model_name, self._timescale_roles)\n            except Exception:\n                use_ts_base = False\n\n        base_class: type[models.Model]\n        if use_ts_base:\n            base_class = XmlTimescaleBase\n        else:\n            base_class = self.base_model_class\n\n        carrier = ConversionCarrier(\n            source_model=source_model,\n            meta_app_label=self.app_label,\n            base_django_model=base_class,\n            class_name_prefix=self.class_name_prefix,\n            strict=False,\n            enable_gfk=self.enable_gfk,\n            gfk_policy=self.gfk_policy,\n            gfk_threshold_children=self.gfk_threshold_children,\n            gfk_value_mode=self.gfk_value_mode,\n            gfk_normalize_common_attrs=self.gfk_normalize_common_attrs,\n        )\n\n        # Make roles map available to field factory for FK decisions\n        carrier.context_data[\"_timescale_roles\"] = self._timescale_roles\n\n        try:\n            self.model_factory_instance.make_django_model(carrier)\n            if carrier.django_model:\n                self.carriers.append(carrier)\n                return carrier\n            return None\n        except Exception:\n            logger.exception(\"Error creating Django model for %s\", source_model_name)\n            return None\n\n    def _write_to_file(self, content: str):\n        with open(self.output_path, \"w\") as f:\n            f.write(content)\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/generator/#pydantic2django.xmlschema.generator.XmlSchemaDjangoModelGenerator.__init__","title":"<code>__init__(schema_files, output_path='generated_models.py', app_label='xmlschema_app', filter_function=None, verbose=False, module_mappings=None, class_name_prefix='', nested_relationship_strategy='fk', list_relationship_style='child_fk', nesting_depth_threshold=1, base_model_class=None, enable_timescale=True, timescale_overrides=None, timescale_config=None, timescale_strict=False, auto_generate_missing_leaves=False, enable_gfk=True, gfk_policy='threshold_by_children', gfk_threshold_children=8, gfk_value_mode='typed_columns', gfk_normalize_common_attrs=False, gfk_overrides=None)</code>","text":"<p>Create a XML Schema \u2192 Django generator with optional Generic Entries mode.</p> <p>GFK (Generic Entries) flags: - enable_gfk: Enable Generic Entries mode. When True and policy matches, parents   gain <code>entries = GenericRelation('GenericEntry')</code> and concrete children are not emitted. - gfk_policy: Controls which nested elements are routed to GenericEntry.   Values: \"substitution_only\" | \"repeating_only\" | \"all_nested\" | \"threshold_by_children\". - gfk_threshold_children: Used only with \"threshold_by_children\"; minimum distinct child   complex types inside a wrapper-like container to activate GFK. - gfk_value_mode: \"json_only\" stores text under attrs_json['value']; \"typed_columns\"   extracts text into text_value, num_value, time_value when unambiguous. - gfk_normalize_common_attrs: Reserved (default False) for promoting frequently used   attributes (e.g., timestamp) into normalized columns when using typed columns. - gfk_overrides: Optional per-element overrides by local element name;   True forces GFK, False disables it even if the policy would apply.</p> Source code in <code>src/pydantic2django/xmlschema/generator.py</code> <pre><code>def __init__(\n    self,\n    schema_files: list[str | Path],\n    output_path: str = \"generated_models.py\",\n    app_label: str = \"xmlschema_app\",\n    filter_function: Callable[[XmlSchemaComplexType], bool] | None = None,\n    verbose: bool = False,\n    module_mappings: dict[str, str] | None = None,\n    class_name_prefix: str = \"\",\n    # Relationship handling for nested complex types\n    nested_relationship_strategy: str = \"fk\",  # one of: \"fk\", \"json\", \"auto\"\n    list_relationship_style: str = \"child_fk\",  # one of: \"child_fk\", \"m2m\", \"json\"\n    nesting_depth_threshold: int = 1,\n    # Optional override for the Django base model class\n    base_model_class: type[models.Model] | None = None,\n    # Feature flags\n    enable_timescale: bool = True,\n    # Timescale configuration\n    timescale_overrides: dict[str, TimescaleRole] | None = None,\n    timescale_config: TimescaleConfig | None = None,\n    timescale_strict: bool = False,\n    # Control behavior for missing leaf/child targets in finalize pass\n    auto_generate_missing_leaves: bool = False,\n    # --- GFK flags ---\n    enable_gfk: bool = True,\n    gfk_policy: str | None = \"threshold_by_children\",\n    gfk_threshold_children: int | None = 8,\n    gfk_value_mode: str | None = \"typed_columns\",\n    gfk_normalize_common_attrs: bool = False,\n    gfk_overrides: dict[str, bool] | None = None,\n):\n    \"\"\"Create a XML Schema \u2192 Django generator with optional Generic Entries mode.\n\n    GFK (Generic Entries) flags:\n    - enable_gfk: Enable Generic Entries mode. When True and policy matches, parents\n      gain `entries = GenericRelation('GenericEntry')` and concrete children are not emitted.\n    - gfk_policy: Controls which nested elements are routed to GenericEntry.\n      Values: \"substitution_only\" | \"repeating_only\" | \"all_nested\" | \"threshold_by_children\".\n    - gfk_threshold_children: Used only with \"threshold_by_children\"; minimum distinct child\n      complex types inside a wrapper-like container to activate GFK.\n    - gfk_value_mode: \"json_only\" stores text under attrs_json['value']; \"typed_columns\"\n      extracts text into text_value, num_value, time_value when unambiguous.\n    - gfk_normalize_common_attrs: Reserved (default False) for promoting frequently used\n      attributes (e.g., timestamp) into normalized columns when using typed columns.\n    - gfk_overrides: Optional per-element overrides by local element name;\n      True forces GFK, False disables it even if the policy would apply.\n    \"\"\"\n    discovery = XmlSchemaDiscovery()\n    model_factory = XmlSchemaModelFactory(\n        app_label=app_label,\n        nested_relationship_strategy=nested_relationship_strategy,\n        list_relationship_style=list_relationship_style,\n        nesting_depth_threshold=nesting_depth_threshold,\n        enable_gfk=enable_gfk,\n        gfk_policy=gfk_policy,\n        gfk_threshold_children=gfk_threshold_children,\n        gfk_overrides=gfk_overrides,\n    )\n\n    super().__init__(\n        output_path=output_path,\n        packages=[str(f) for f in schema_files],\n        app_label=app_label,\n        discovery_instance=discovery,\n        model_factory_instance=model_factory,\n        base_model_class=base_model_class or self._get_default_base_model_class(),\n        class_name_prefix=class_name_prefix,\n        module_mappings=module_mappings,\n        verbose=verbose,\n        filter_function=filter_function,\n        enable_timescale=enable_timescale,\n        enable_gfk=enable_gfk,\n        gfk_policy=gfk_policy,\n        gfk_threshold_children=gfk_threshold_children,\n        gfk_value_mode=gfk_value_mode,\n        gfk_normalize_common_attrs=gfk_normalize_common_attrs,\n    )\n    # Timescale classification results cached per run\n    self._timescale_roles: dict[str, TimescaleRole] = {}\n    self._timescale_overrides: dict[str, TimescaleRole] | None = timescale_overrides\n    self._timescale_config: TimescaleConfig | None = timescale_config\n    self._timescale_strict: bool = timescale_strict\n    self._auto_generate_missing_leaves: bool = auto_generate_missing_leaves\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/generator/#pydantic2django.xmlschema.generator.XmlSchemaDjangoModelGenerator.from_schema_files","title":"<code>from_schema_files(schema_files, **kwargs)</code>  <code>classmethod</code>","text":"<p>Convenience class method to create generator from schema files.</p> <p>Parameters:</p> Name Type Description Default <code>schema_files</code> <code>list[str | Path]</code> <p>List of XSD file paths</p> required <code>**kwargs</code> <p>Additional arguments passed to init</p> <code>{}</code> <p>Returns:</p> Type Description <code>XmlSchemaDjangoModelGenerator</code> <p>Configured XmlSchemaDjangoModelGenerator instance</p> Source code in <code>src/pydantic2django/xmlschema/generator.py</code> <pre><code>@classmethod\ndef from_schema_files(cls, schema_files: list[str | Path], **kwargs) -&gt; \"XmlSchemaDjangoModelGenerator\":\n    \"\"\"\n    Convenience class method to create generator from schema files.\n\n    Args:\n        schema_files: List of XSD file paths\n        **kwargs: Additional arguments passed to __init__\n\n    Returns:\n        Configured XmlSchemaDjangoModelGenerator instance\n    \"\"\"\n    return cls(schema_files=schema_files, **kwargs)\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/generator/#pydantic2django.xmlschema.generator.XmlSchemaDjangoModelGenerator.generate","title":"<code>generate()</code>","text":"<p>Main method to generate the Django models file.</p> Source code in <code>src/pydantic2django/xmlschema/generator.py</code> <pre><code>def generate(self):\n    \"\"\"\n    Main method to generate the Django models file.\n    \"\"\"\n    logger.info(f\"Starting Django model generation to {self.output_path}\")\n\n    # The base class now handles the full generation pipeline\n    super().generate()\n\n    logger.info(f\"Successfully generated Django models in {self.output_path}\")\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/generator/#pydantic2django.xmlschema.generator.XmlSchemaDjangoModelGenerator.generate_models_file","title":"<code>generate_models_file()</code>","text":"<p>Override to allow relationship finalization after carriers are built but before rendering templates.</p> Source code in <code>src/pydantic2django/xmlschema/generator.py</code> <pre><code>def generate_models_file(self) -&gt; str:\n    \"\"\"\n    Override to allow relationship finalization after carriers are built\n    but before rendering templates.\n    \"\"\"\n    # Discover and create carriers first via base implementation pieces\n    self.discover_models()\n    # --- GFK Prepass: when enabled, identify wrapper owners and exclude their polymorphic children ---\n    if getattr(self, \"enable_gfk\", False) and getattr(self, \"gfk_policy\", None):\n        try:\n            excluded_child_types: set[str] = set()\n            gfk_owner_wrappers: set[str] = set()\n            policy = str(getattr(self, \"gfk_policy\", \"\"))\n            threshold_val = int(getattr(self, \"gfk_threshold_children\", 0) or 0)\n            # Walk parsed schemas to infer wrapper owners and their child types\n            for schema_def in getattr(self.discovery_instance, \"parsed_schemas\", []) or []:\n                try:\n                    for ct in schema_def.get_all_complex_types():\n                        # Iterate elements of each complex type\n                        for el in ct.elements:\n                            # Only interested in complex targets\n                            if not el.type_name or el.base_type is not None:\n                                continue\n                            target_type = el.type_name.split(\":\")[-1]\n                            target_ct = schema_def.complex_types.get(target_type)\n                            if not target_ct:\n                                continue\n                            # Wrapper-like heuristic (may be used by providers; structural checks take precedence)\n                            _is_wrapper_like = bool(el.name[:1].isupper()) or str(target_type).endswith(\n                                \"WrapperType\"\n                            )\n                            # Collect distinct child complex types under target wrapper\n                            distinct_child_types: set[str] = set()\n                            has_substitution_members = False\n                            repeating_leaf_child: str | None = None\n                            for child_el in target_ct.elements:\n                                if getattr(child_el, \"substitution_group\", None):\n                                    has_substitution_members = True\n                                if getattr(child_el, \"type_name\", None):\n                                    cand = child_el.type_name.split(\":\")[-1]\n                                    if cand:\n                                        distinct_child_types.add(cand)\n                                if getattr(child_el, \"is_list\", False) and getattr(child_el, \"type_name\", None):\n                                    repeating_leaf_child = child_el.type_name.split(\":\")[-1]\n\n                            # Decide based on policy (favor generic structural checks over name heuristics)\n                            route_by_subst = policy == \"substitution_only\" and has_substitution_members\n                            route_by_all_nested = policy == \"all_nested\"\n                            route_by_threshold = policy == \"threshold_by_children\" and (\n                                threshold_val &lt;= 0 or len(distinct_child_types) &gt;= threshold_val\n                            )\n                            route_by_repeating = policy == \"repeating_only\" and bool(el.is_list)\n\n                            if route_by_subst or route_by_all_nested or route_by_threshold or route_by_repeating:\n                                # Mark the CURRENT complex type (container) as the owner so placeholders on it are suppressed\n                                try:\n                                    owner_name = getattr(ct, \"name\", None) or getattr(ct, \"__name__\", str(ct))\n                                except Exception:\n                                    owner_name = target_type\n                                gfk_owner_wrappers.add(owner_name)\n                                if getattr(self, \"verbose\", False):\n                                    reason = (\n                                        \"substitution_only\"\n                                        if route_by_subst\n                                        else (\n                                            \"all_nested\"\n                                            if route_by_all_nested\n                                            else (\n                                                \"threshold_by_children\" if route_by_threshold else \"repeating_only\"\n                                            )\n                                        )\n                                    )\n                                    logger.info(\n                                        \"[GFK][prepass] owner=%s element=%s policy=%s distinct_children=%d\",\n                                        owner_name,\n                                        el.name,\n                                        reason,\n                                        len(distinct_child_types),\n                                    )\n                                # Exclude polymorphic children under this wrapper from concrete generation\n                                if el.is_list and repeating_leaf_child:\n                                    excluded_child_types.add(repeating_leaf_child)\n                                # Also exclude distinct child complex types observed under the wrapper\n                                excluded_child_types.update({t for t in distinct_child_types if t})\n                except Exception:\n                    logger.error(\n                        \"[GFK][prepass] Failed while processing schema '%s'\",\n                        getattr(schema_def, \"schema_location\", \"?\"),\n                        exc_info=True,\n                    )\n                    continue  # best-effort per schema\n\n            # Filter discovery output to remove excluded children, keeping wrappers/parents\n            try:\n                # filtered_models is a mapping of qualname -&gt; complex_type\n                fm = dict(getattr(self.discovery_instance, \"filtered_models\", {}) or {})\n                if fm and excluded_child_types:\n                    new_fm = {k: v for k, v in fm.items() if getattr(v, \"name\", None) not in excluded_child_types}\n                    self.discovery_instance.filtered_models = new_fm  # type: ignore[attr-defined]\n                    if getattr(self, \"verbose\", False):\n                        logger.info(\n                            \"[GFK][prepass] excluded_children_count=%d (examples: %s)\",\n                            len(excluded_child_types),\n                            \", \".join(sorted(excluded_child_types)[:10]),\n                        )\n            except Exception as exc:\n                logger.error(\"[GFK][prepass] Failed to filter excluded children: %s\", exc, exc_info=True)\n\n            # Share exclusions and owner marks with the field factory for intra-model decisions\n            try:\n                ff = getattr(self.model_factory_instance, \"field_factory\", None)\n                if ff is not None:\n                    ff._gfk_excluded_child_types = excluded_child_types\n                    # Seed owner names to suppress JSON placeholders for wrapper children\n                    owner_names = set(getattr(ff, \"_gfk_owner_names\", set()))\n                    owner_names.update(gfk_owner_wrappers)\n                    ff._gfk_owner_names = owner_names\n            except Exception as exc:\n                logger.error(\"[GFK][prepass] Failed to share state with field factory: %s\", exc, exc_info=True)\n        except Exception as exc:\n            # Prepass is best-effort and should never crash generation\n            logger.error(\"[GFK][prepass] Unexpected error: %s\", exc, exc_info=True)\n    # Conflict guard: if GFK owners detected but JSON relationship styles are requested, fail fast.\n    if getattr(self, \"enable_gfk\", False):\n        ff = getattr(self.model_factory_instance, \"field_factory\", None)\n        owners = set(getattr(ff, \"_gfk_owner_names\", set()) or set())\n        list_style = getattr(ff, \"list_relationship_style\", \"child_fk\")\n        nested_style = getattr(ff, \"nested_relationship_strategy\", \"fk\")\n        if owners and (list_style == \"json\" or nested_style == \"json\"):\n            raise ValueError(\n                \"GFK mode active with owners detected, but JSON relationship strategy was requested. \"\n                \"This configuration would produce dual emission. Adjust flags or disable JSON strategy.\"\n            )\n    # Inform the field factory which models are actually included so it can\n    # avoid generating relations to filtered-out models (fallback to JSON).\n    try:\n        # Prefer local (unqualified) complex type names to match factory lookups\n        # filtered_models keys may be qualified as \"{namespace}.{name}\", so derive plain names\n        included_local_names = {\n            getattr(m, \"name\", str(m))\n            for m in self.discovery_instance.filtered_models.values()  # type: ignore[attr-defined]\n        }\n        # Also include any already-qualified keys for maximum compatibility\n        included_qualified_keys = set(self.discovery_instance.filtered_models.keys())  # type: ignore[attr-defined]\n        included_names = included_local_names | included_qualified_keys\n        # type: ignore[attr-defined]\n        self.model_factory_instance.field_factory.included_model_names = included_names  # noqa: E501\n        # Also pass through any GFK prepass exclusions to the field factory if not already set\n        ff = self.model_factory_instance.field_factory\n        if not getattr(ff, \"_gfk_excluded_child_types\", None):\n            ff._gfk_excluded_child_types = set()\n    except Exception as exc:\n        logger.error(\n            \"Failed to inform field factory about included models or defaults: %s\",\n            exc,\n            exc_info=True,\n        )\n    models_to_process = self._get_models_in_processing_order()\n\n    # Classify models for Timescale usage (hypertable vs dimension)\n    if self.enable_timescale:\n        try:\n            self._timescale_roles = classify_xml_complex_types(\n                models_to_process,\n                overrides=self._timescale_overrides,\n                config=self._timescale_config,\n            )\n        except Exception as exc:\n            logger.error(\"Timescale classification failed; proceeding without roles: %s\", exc, exc_info=True)\n            self._timescale_roles = {}\n    else:\n        self._timescale_roles = {}\n\n    # Strict validation: if any hypertable lacks a direct time-like field, fail fast\n    if self.enable_timescale and self._timescale_strict:\n        for m in models_to_process:\n            name = getattr(m, \"name\", getattr(m, \"__name__\", str(m)))\n            if self._timescale_roles.get(name) == TimescaleRole.HYPERTABLE and not has_direct_time_feature(m):\n                raise ValueError(\n                    f\"Timescale strict mode: '{name}' classified as hypertable but has no direct time-like field. \"\n                    f\"Add a time/timestamp-like attribute/element or demote via overrides.\"\n                )\n\n    # Reset state for this run (mirror BaseStaticGenerator)\n    self.carriers = []\n    self.import_handler.extra_type_imports.clear()\n    self.import_handler.pydantic_imports.clear()\n    self.import_handler.context_class_imports.clear()\n    self.import_handler.imported_names.clear()\n    self.import_handler.processed_field_types.clear()\n    self.import_handler._add_type_import(self.base_model_class)\n\n    for source_model in models_to_process:\n        self.setup_django_model(source_model)\n\n    # Give the factory a chance to add cross-model relationship fields (e.g., child FKs)\n    carriers_by_name = {\n        (getattr(c.source_model, \"name\", None) or getattr(c.source_model, \"__name__\", \"\")): c\n        for c in self.carriers\n        if c.django_model is not None\n    }\n    if hasattr(self.model_factory_instance, \"finalize_relationships\"):\n        try:\n            # type: ignore[attr-defined]\n            self.model_factory_instance.finalize_relationships(carriers_by_name, self.app_label)  # noqa: E501\n        except Exception as e:\n            logger.error(\"Error finalizing XML relationships: %s\", e, exc_info=True)\n\n    # Inject GenericRelation on parents when GFK is enabled and pending children exist\n    gfk_used = False\n    try:\n        for carrier in self.carriers:\n            if getattr(carrier, \"enable_gfk\", False) and getattr(carrier, \"pending_gfk_children\", None):\n                # Ensure contenttypes imports\n                self.import_handler.add_import(\"django.contrib.contenttypes.fields\", \"GenericRelation\")\n                self.import_handler.add_import(\"django.contrib.contenttypes.fields\", \"GenericForeignKey\")\n                self.import_handler.add_import(\"django.contrib.contenttypes.models\", \"ContentType\")\n                # Add reverse relation field on parent model\n                carrier.django_field_definitions[\n                    \"entries\"\n                ] = \"GenericRelation('GenericEntry', related_query_name='entries')\"\n                gfk_used = True\n    except Exception as exc:\n        logger.error(\"Failed while injecting GenericRelation/GenericEntry fields: %s\", exc, exc_info=True)\n\n    # Optional diagnostics: summarize GFK routing per parent\n    if gfk_used and getattr(self, \"verbose\", False):\n        try:\n            summary: dict[str, int] = {}\n            for carrier in self.carriers:\n                for entry in getattr(carrier, \"pending_gfk_children\", []) or []:\n                    owner = str(entry.get(\"owner\") or getattr(carrier.source_model, \"name\", \"\"))\n                    summary[owner] = summary.get(owner, 0) + 1\n            if summary:\n                details = \", \".join(f\"{k}:{v}\" for k, v in sorted(summary.items()))\n                logger.info(f\"[GFK] Routed children counts by owner: {details}\")\n        except Exception as exc:\n            logger.error(\"Failed to summarize GFK routing: %s\", exc, exc_info=True)\n\n    # Register generated model classes for in-memory lookup by ingestors\n    try:\n        for carrier in self.carriers:\n            if carrier.django_model is not None:\n                register_generated_model(self.app_label, carrier.django_model)\n                logger.info(\n                    \"Registered generated model %s (abstract=%s) for app '%s'\",\n                    carrier.django_model.__name__,\n                    getattr(getattr(carrier.django_model, \"_meta\", None), \"abstract\", None),\n                    self.app_label,\n                )\n                # Prevent dynamic classes from polluting Django's global app registry\n                try:\n                    from django.apps import (\n                        apps as django_apps,  # Local import to avoid hard dependency\n                    )\n\n                    model_lower = getattr(getattr(carrier.django_model, \"_meta\", None), \"model_name\", None)\n                    if model_lower:\n                        django_apps.all_models.get(self.app_label, {}).pop(model_lower, None)\n                except Exception as exc:\n                    # Best-effort cleanup only, but log for visibility\n                    logger.error(\"Failed to clean up Django global app registry: %s\", exc, exc_info=True)\n    except Exception as e:\n        logger.error(\n            \"Non-fatal: failed to register generated models for app '%s': %s\",\n            self.app_label,\n            e,\n            exc_info=True,\n        )\n\n    # Proceed with standard definition rendering\n    model_definitions = []\n    django_model_names = []\n    for carrier in self.carriers:\n        if carrier.django_model:\n            try:\n                model_def = self.generate_model_definition(carrier)\n                if model_def:\n                    model_definitions.append(model_def)\n                    django_model_names.append(f\"'{self._clean_generic_type(carrier.django_model.__name__)}'\")\n            except Exception as e:\n                logger.error(\n                    f\"Error generating definition for source model {getattr(carrier.source_model, '__name__', '?')}: {e}\",\n                    exc_info=True,\n                )\n\n    # If GFK used, append GenericEntry model definition and include in __all__\n    if gfk_used:\n        model_definitions.append(self._build_generic_entry_model_definition())\n        django_model_names.append(\"'GenericEntry'\")\n\n    unique_model_definitions = self._deduplicate_definitions(model_definitions)\n    # Ensure that every advertised model name in __all__ has a corresponding class definition.\n    try:\n        import re as _re\n\n        joined_defs = \"\\n\".join(unique_model_definitions)\n        existing_names = {m.group(1) for m in _re.finditer(r\"^\\s*class\\s+(\\w+)\\(\", joined_defs, _re.MULTILINE)}\n        advertised_names = [\n            self._clean_generic_type(getattr(c.django_model, \"__name__\", \"\"))\n            for c in self.carriers\n            if c.django_model is not None\n        ]\n        for name in advertised_names:\n            if name and name not in existing_names:\n                minimal_def = (\n                    f'\"\"\"\\nDjango model for {name}.\\n\"\"\"\\n\\n'\n                    f\"class {name}({self.base_model_class.__name__}):\\n    # No fields defined for this model.\\n    pass\\n\\n    class Meta:\\n        app_label = '{self.app_label}'\\n        abstract = False\\n\\n\"\n                )\n                unique_model_definitions.append(minimal_def)\n    except Exception as exc:\n        logger.error(\n            \"Failed to ensure all advertised model names have definitions: %s\",\n            exc,\n            exc_info=True,\n        )\n\n    imports = self.import_handler.deduplicate_imports()\n    template_context = self._prepare_template_context(unique_model_definitions, django_model_names, imports)\n    template_context.update(\n        {\n            \"generation_timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n            \"base_model_module\": self.base_model_class.__module__,\n            \"base_model_name\": self.base_model_class.__name__,\n            \"extra_type_imports\": sorted(self.import_handler.extra_type_imports),\n        }\n    )\n    # Detect validator usage to ensure imports are present\n    used_validators: set[str] = set()\n    # Primary source: field factory tracking\n    try:\n        field_factory = getattr(self.model_factory_instance, \"field_factory\", None)\n        factory_used = getattr(field_factory, \"used_validators\", None)\n        if isinstance(factory_used, set):\n            used_validators.update(factory_used)\n    except Exception as exc:\n        logger.error(\"Failed to collect validator usage from field factory: %s\", exc, exc_info=True)\n    # Fallback: scan rendered definitions\n    try:\n        if not used_validators:\n            validator_symbols = [\n                \"RegexValidator\",\n                \"MinValueValidator\",\n                \"MaxValueValidator\",\n                \"URLValidator\",\n                \"EmailValidator\",\n                \"validate_slug\",\n                \"validate_uuid4\",\n                \"validate_email\",\n                \"validate_ipv46_address\",\n                \"validate_ipv4_address\",\n                \"validate_ipv6_address\",\n                \"validate_unicode_slug\",\n            ]\n            joined_defs = \"\\n\".join(unique_model_definitions)\n            for symbol in validator_symbols:\n                if re.search(rf\"\\\\b{re.escape(symbol)}\\\\b\", joined_defs):\n                    used_validators.add(symbol)\n    except Exception as exc:\n        logger.error(\"Failed to scan rendered definitions for validator usage: %s\", exc, exc_info=True)\n    template_context[\"validator_imports\"] = sorted(used_validators)\n    template = self.jinja_env.get_template(\"models_file.py.j2\")\n    return template.render(**template_context)\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/generator/#pydantic2django.xmlschema.generator.XmlSchemaDjangoModelGenerator.generate_models_with_xml_metadata","title":"<code>generate_models_with_xml_metadata()</code>","text":"<p>Generate Django models with additional XML metadata.</p> <p>This method extends the base generate() to add XML-specific comments and metadata to the generated models.</p> Source code in <code>src/pydantic2django/xmlschema/generator.py</code> <pre><code>def generate_models_with_xml_metadata(self) -&gt; str:\n    \"\"\"\n    Generate Django models with additional XML metadata.\n\n    This method extends the base generate() to add XML-specific\n    comments and metadata to the generated models.\n    \"\"\"\n    content = self.generate_models_file()\n\n    # Add XML Schema file references as comments at the top\n    schema_files_comment = \"\\n\".join(\n        [f\"# Generated from XML Schema: {schema_file}\" for schema_file in self.schema_files]\n    )\n\n    # Insert after the initial comments\n    lines = content.split(\"\\n\")\n    insert_index = 0\n    for i, line in enumerate(lines):\n        if line.startswith('\"\"\"') and '\"\"\"' in line[3:]:  # Single line docstring\n            insert_index = i + 1\n            break\n        elif line.startswith('\"\"\"'):  # Multi-line docstring start\n            for j in range(i + 1, len(lines)):\n                if '\"\"\"' in lines[j]:\n                    insert_index = j + 1\n                    break\n            break\n\n    lines.insert(insert_index, schema_files_comment)\n    lines.insert(insert_index + 1, \"\")\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/generator/#pydantic2django.xmlschema.generator.XmlSchemaDjangoModelGenerator.get_schema_statistics","title":"<code>get_schema_statistics()</code>","text":"<p>Get statistics about the parsed schemas.</p> Source code in <code>src/pydantic2django/xmlschema/generator.py</code> <pre><code>def get_schema_statistics(self) -&gt; dict:\n    \"\"\"Get statistics about the parsed schemas.\"\"\"\n    stats = {\n        \"total_schemas\": len(self.discovery_instance.parsed_schemas),\n        \"total_complex_types\": len(self.discovery_instance.all_models),\n        \"filtered_complex_types\": len(self.discovery_instance.filtered_models),\n        \"generated_models\": len(self.carriers),\n    }\n\n    # Add per-schema breakdown\n    schema_breakdown = []\n    for schema_def in self.discovery_instance.parsed_schemas:\n        schema_breakdown.append(\n            {\n                \"schema_location\": schema_def.schema_location,\n                \"target_namespace\": schema_def.target_namespace,\n                \"complex_types\": len(schema_def.complex_types),\n                \"simple_types\": len(schema_def.simple_types),\n                \"elements\": len(schema_def.elements),\n            }\n        )\n\n    stats[\"schema_breakdown\"] = schema_breakdown\n    return stats\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/generator/#pydantic2django.xmlschema.generator.XmlSchemaDjangoModelGenerator.validate_schemas","title":"<code>validate_schemas()</code>","text":"<p>Validate the parsed schemas and return any warnings or errors.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of validation messages</p> Source code in <code>src/pydantic2django/xmlschema/generator.py</code> <pre><code>def validate_schemas(self) -&gt; list[str]:\n    \"\"\"\n    Validate the parsed schemas and return any warnings or errors.\n\n    Returns:\n        List of validation messages\n    \"\"\"\n    messages = []\n\n    for schema_def in self.discovery_instance.parsed_schemas:\n        # Check for common issues\n        if not schema_def.target_namespace:\n            messages.append(f\"Schema {schema_def.schema_location} has no target namespace\")\n\n        # Check for name conflicts\n        all_names = set()\n        for complex_type in schema_def.complex_types.values():\n            if complex_type.name in all_names:\n                messages.append(f\"Duplicate type name: {complex_type.name}\")\n            all_names.add(complex_type.name)\n\n    return messages\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/ingestor/","title":"pydantic2django.xmlschema.ingestor","text":"<p>XML instance ingestion utilities.</p> <p>Provides a schema-aware ingestor that can parse an XML document and create corresponding Django model instances generated from an XML Schema.</p> <p>Scope (initial): - Supports nested complex types (single and repeated) following the same   relationship strategies used during generation (default child_fk for lists,   FK on parent for single nested elements). - Maps simple elements and attributes to Django fields with basic name   conversion (camelCase \u2192 snake_case), mirroring Xml2DjangoBaseClass helpers. - Minimal namespace handling by stripping namespace URIs when matching names.</p> <p>Future extensions (not implemented here): - Robust namespace mapping and cross-schema references - key/keyref post-pass resolution using ID/IDREF values - Type coercion beyond Django's default conversion</p>"},{"location":"reference/pydantic2django/xmlschema/ingestor/#pydantic2django.xmlschema.ingestor.SchemaSyncError","title":"<code>SchemaSyncError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when schema-to-model contract validation fails (strict mode).</p> Source code in <code>src/pydantic2django/xmlschema/ingestor.py</code> <pre><code>class SchemaSyncError(Exception):\n    \"\"\"Raised when schema-to-model contract validation fails (strict mode).\"\"\"\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/ingestor/#pydantic2django.xmlschema.ingestor.XmlInstanceIngestor","title":"<code>XmlInstanceIngestor</code>","text":"<p>Schema-aware ingestor for XML instance documents.</p> <p>Given XSD schema files and an app_label where models were generated, this ingestor parses an XML instance and creates the corresponding Django model instances, wiring up relationships according to generation strategy.</p> Source code in <code>src/pydantic2django/xmlschema/ingestor.py</code> <pre><code>class XmlInstanceIngestor:\n    \"\"\"\n    Schema-aware ingestor for XML instance documents.\n\n    Given XSD schema files and an app_label where models were generated,\n    this ingestor parses an XML instance and creates the corresponding Django\n    model instances, wiring up relationships according to generation strategy.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        schema_files: list[str | Path],\n        app_label: str,\n        dynamic_model_fallback: bool = False,\n        strict: bool = False,\n    ):\n        \"\"\"\n        Initialize a schema-aware XML ingestor.\n\n        Args:\n            schema_files: Paths to XSD files used to parse/resolve types.\n            app_label: Django app label expected to contain installed models.\n            dynamic_model_fallback: Controls behavior when an installed model is missing.\n                - True: fall back to dynamically generated stand-ins from the\n                  in-memory registry, allowing unsaved/ephemeral workflows.\n                - False (default): raise ModelResolutionError when a discovered complex type cannot be\n                  resolved to an installed Django model, avoiding implicit usage of stand-ins.\n\n        Behavior:\n            - Regardless of the fallback flag, the ingestor will always prefer installed Django\n              models and pre-cache them for all discovered complex types to ensure consistent\n              resolution across root and nested elements.\n        \"\"\"\n        try:  # Validate dependency early\n            import lxml.etree  # noqa: F401\n        except ImportError as exc:  # pragma: no cover - environment dependent\n            raise ImportError(\"lxml is required for XML ingestion. Install with: pip install lxml\") from exc\n\n        self.app_label = app_label\n        self._save_objects: bool = True\n        self._dynamic_model_fallback: bool = bool(dynamic_model_fallback)\n        self._strict: bool = bool(strict)\n        self.created_instances: list[Any] = []\n\n        discovery = XmlSchemaDiscovery()\n        discovery.discover_models(packages=[str(p) for p in schema_files], app_label=app_label)\n        # Keep references for mapping\n        self._schemas: list[XmlSchemaDefinition] = list(discovery.parsed_schemas)\n\n        # Pre-resolve and cache installed Django model classes for all discovered complex types.\n        # This guarantees we consistently use concrete models (with managers) when available.\n        self._model_resolution_cache: dict[str, type] = {}\n        try:\n            discovered_names: set[str] = set()\n            for schema in self._schemas:\n                for ct in schema.get_all_complex_types():\n                    discovered_names.add(ct.name)\n            for model_name in discovered_names:\n                try:\n                    model_cls = django_apps.get_model(f\"{self.app_label}.{model_name}\")\n                except Exception:\n                    model_cls = None\n                if model_cls is not None:\n                    self._model_resolution_cache[model_name] = model_cls\n        except Exception:\n            # Cache population is best-effort; fall back to on-demand lookups.\n            self._model_resolution_cache = {}\n\n    # --- Public API ---\n    def ingest_from_string(self, xml_string: str, *, save: bool = True) -&gt; Any:\n        \"\"\"\n        Ingest an XML instance from a string, returning the created root Django instance.\n        \"\"\"\n        import lxml.etree as _etree\n\n        self._save_objects = bool(save)\n        self.created_instances = []\n        root = _etree.fromstring(xml_string)\n        return self._ingest_root_element(root)\n\n    def ingest_from_file(self, xml_path: str | Path, *, save: bool = True) -&gt; Any:\n        \"\"\"\n        Ingest an XML instance from a file path, returning the created root Django instance.\n        \"\"\"\n        xml_path = Path(xml_path)\n        import lxml.etree as _etree\n\n        self._save_objects = bool(save)\n        self.created_instances = []\n        with xml_path.open(\"rb\") as f:\n            tree = _etree.parse(f)\n        root = tree.getroot()\n        return self._ingest_root_element(root)\n\n    def validate_models(self, *, strict: bool = True) -&gt; list[ContractIssue]:\n        \"\"\"\n        Validate that discovered schema types align with installed Django models.\n\n        - Ensures expected fields from attributes, simple elements, and single nested complex\n          elements exist on the corresponding Django model.\n        - Flags Timescale-based models that are missing the canonical 'time' field.\n\n        Returns a list of ContractIssue. If strict=True and issues exist, raises SchemaSyncError\n        with a concise remediation message.\n        \"\"\"\n        issues: list[ContractIssue] = []\n        for schema in self._schemas:\n            try:\n                complex_types = list(schema.get_all_complex_types())\n            except Exception:\n                continue\n            for ct in complex_types:\n                model_cls = self._get_model_for_complex_type(ct)\n                if model_cls is None:\n                    issues.append(\n                        ContractIssue(\n                            complex_type_name=ct.name,\n                            model_name=f\"{self.app_label}.{ct.name}\",\n                            missing_fields=[\"&lt;installed model not found&gt;\"],\n                            extra_fields=[],\n                            problems=[\"No installed Django model resolved for complex type\"],\n                        )\n                    )\n                    continue\n\n                try:\n                    model_field_names = {f.name for f in model_cls._meta.fields}\n                except Exception:\n                    model_field_names = set()\n\n                expected_fields: set[str] = set()\n                # Attributes\n                for attr_name in getattr(ct, \"attributes\", {}).keys():\n                    expected_fields.add(self._xml_name_to_django_field(attr_name))\n                # Elements (simple + parent links for single nested complex)\n                for el in getattr(ct, \"elements\", []):\n                    if getattr(el, \"type_name\", None) and getattr(el, \"base_type\", None) is None:\n                        if not getattr(el, \"is_list\", False):\n                            expected_fields.add(self._xml_name_to_django_field(el.name))\n                        continue\n                    expected_fields.add(self._xml_name_to_django_field(el.name))\n\n                missing = sorted([fn for fn in expected_fields if fn not in model_field_names])\n                problems: list[str] = []\n                if self._is_timescale_model(model_cls) and \"time\" not in model_field_names:\n                    problems.append(\"Timescale model missing required 'time' field\")\n\n                if missing or problems:\n                    issues.append(\n                        ContractIssue(\n                            complex_type_name=ct.name,\n                            model_name=f\"{self.app_label}.{model_cls.__name__}\",\n                            missing_fields=missing,\n                            extra_fields=[],\n                            problems=problems,\n                        )\n                    )\n\n        if strict and issues:\n            details = \"; \".join(\n                f\"[{i.model_name}] missing={i.missing_fields or '-'} problems={i.problems or '-'}\" for i in issues[:5]\n            )\n            raise SchemaSyncError(\n                f\"Schema and static model are out of sync; verify schema and/or regenerate static Django models and re-migrate. Details: {details}\"\n            )\n        return issues\n\n    # --- Core ingestion ---\n    def _ingest_root_element(self, elem: Any) -&gt; Any:\n        local_name = self._local_name(elem.tag)\n        complex_type = self._resolve_root_complex_type(local_name)\n        if complex_type is None:\n            known_types = []\n            try:\n                for s in self._schemas:\n                    known_types.extend([ct.name for ct in s.get_all_complex_types()])\n            except Exception:\n                pass\n            raise ValueError(\n                \"Could not resolve complex type for root element \"\n                f\"'{local_name}'. Known discovered types (sample): \"\n                f\"{known_types[:10]}\"\n            )\n\n        model_cls = self._get_model_for_complex_type(complex_type)\n        if model_cls is None:\n            raise ValueError(f\"Could not find Django model for complex type '{complex_type.name}'\")\n\n        instance = self._build_instance_from_element(elem, complex_type, model_cls, parent_instance=None)\n        return instance\n\n    def _build_instance_from_element(\n        self,\n        elem: Any,\n        complex_type: XmlSchemaComplexType,\n        model_cls: type,\n        parent_instance: Any | None,\n        parent_link_field: str | None = None,\n    ) -&gt; Any:\n        \"\"\"\n        Create and save a Django model instance from an XML element according to its complex type.\n        \"\"\"\n        # Prepare field values for simple elements and attributes first\n        field_values: dict[str, Any] = {}\n\n        # Attributes on complex types\n        for attr_name, _attr in complex_type.attributes.items():\n            xml_attr_value = elem.get(attr_name)\n            if xml_attr_value is not None:\n                dj_name = self._xml_name_to_django_field(attr_name)\n                field_values[dj_name] = xml_attr_value\n\n        # Child elements\n        children_by_local: dict[str, list[Any]] = {}\n        for child in elem:\n            if not isinstance(child.tag, str):\n                continue\n            lname = self._local_name(child.tag)\n            children_by_local.setdefault(lname, []).append(child)\n\n        # Strict mode: detect unexpected XML child elements and attributes not declared in the schema\n        if self._strict:\n            expected_child_names = {e.name for e in getattr(complex_type, \"elements\", [])}\n            unexpected_children = sorted([n for n in children_by_local.keys() if n not in expected_child_names])\n            if unexpected_children:\n                raise SchemaSyncError(\n                    \"Schema and static model are out of sync; verify schema and/or regenerate static Django models and re-migrate. \"\n                    f\"Unexpected XML elements for type '{complex_type.name}': {unexpected_children}\"\n                )\n            expected_attr_names = set(getattr(complex_type, \"attributes\", {}).keys())\n            xml_attr_local = {self._local_name(k) for k in getattr(elem, \"attrib\", {}).keys()}\n            unexpected_attrs = sorted([n for n in xml_attr_local if n not in expected_attr_names])\n            if unexpected_attrs:\n                raise SchemaSyncError(\n                    \"Schema and static model are out of sync; verify schema and/or regenerate static Django models and re-migrate. \"\n                    f\"Unmapped XML attributes for type '{complex_type.name}': {unexpected_attrs}\"\n                )\n\n        # Map simple fields and collect nested complex elements to process\n        nested_to_process: list[tuple[XmlSchemaElement, list[Any]]] = []\n        for el_def in complex_type.elements:\n            name = el_def.name\n            matched_children = children_by_local.get(name, [])\n            if not matched_children:\n                continue\n\n            if el_def.type_name and el_def.base_type is None:\n                # Nested complex type\n                nested_to_process.append((el_def, matched_children))\n                continue\n\n            # Simple content\n            # Multiple occurrences of a simple element -&gt; pick first; advanced list handling can be added later\n            first = matched_children[0]\n            dj_name = self._xml_name_to_django_field(name)\n            field_values[dj_name] = first.text\n\n        # If this element is a repeated child (child_fk strategy), and we know the FK field name\n        # on the child model, include it in initial field values to satisfy NOT NULL constraints.\n        if parent_instance is not None and parent_link_field:\n            try:\n                model_field_names = {f.name for f in model_cls._meta.fields}\n                if parent_link_field in model_field_names:\n                    field_values.setdefault(parent_link_field, parent_instance)\n            except Exception:\n                pass\n\n        # Instantiate without other relationships first\n        instance: Any\n        if self._save_objects:\n            # Timescale-aware timestamp remapping: if model expects a 'time' field,\n            # remap common XML timestamp attributes (e.g., creationTime \u2192 creation_time)\n            # into 'time' when not already provided.\n            try:\n                model_field_names = {f.name for f in model_cls._meta.fields}\n            except Exception:\n                model_field_names = set()\n            if has_timescale_time_field(model_field_names) and \"time\" not in field_values:\n                # Always attempt a helpful alias remap; only enforce requiredness for Timescale models\n                map_time_alias_into_time(field_values, aliases=TIMESERIES_TIME_ALIASES)\n                if self._is_timescale_model(model_cls) and \"time\" not in field_values:\n                    raise TimeseriesTimestampMissingError(\n                        model_cls.__name__, attempted_aliases=list(TIMESERIES_TIME_ALIASES)\n                    )\n\n            instance = model_cls.objects.create(**field_values)\n        else:\n            try:\n                # Mirror the same remapping for unsaved construction flows\n                try:\n                    model_field_names = {f.name for f in model_cls._meta.fields}\n                except Exception:\n                    model_field_names = set()\n                if has_timescale_time_field(model_field_names) and \"time\" not in field_values:\n                    # For unsaved flows, remap aliases but do not enforce a hard requirement\n                    map_time_alias_into_time(field_values, aliases=TIMESERIES_TIME_ALIASES)\n                instance = model_cls(**field_values)\n            except TypeError as exc:\n                # If the dynamically generated class is abstract (not installed app),\n                # construct a lightweight proxy object for unsaved workflows.\n                is_abstract = getattr(getattr(model_cls, \"_meta\", None), \"abstract\", None)\n                if is_abstract:\n                    try:\n                        proxy_cls = type(model_cls.__name__, (), {})\n                        instance = proxy_cls()\n                        for k, v in field_values.items():\n                            setattr(instance, k, v)\n                    except Exception:\n                        raise TypeError(f\"Cannot instantiate abstract model '{model_cls.__name__}'\") from exc\n                else:\n                    raise\n\n        # Track created/instantiated instance\n        try:\n            self.created_instances.append(instance)\n        except Exception:\n            pass\n\n        # Attach any remaining XML attributes as dynamic attributes if not mapped\n        try:\n            model_field_names = {f.name for f in model_cls._meta.fields}\n            unmapped_dynamic: list[str] = []\n            for attr_name, attr_val in getattr(elem, \"attrib\", {}).items():\n                dj_name = self._xml_name_to_django_field(attr_name)\n                if dj_name not in field_values and dj_name not in model_field_names:\n                    if self._strict:\n                        unmapped_dynamic.append(dj_name)\n                    else:\n                        setattr(instance, dj_name, attr_val)\n            if self._strict and unmapped_dynamic:\n                raise SchemaSyncError(\n                    \"Schema and static model are out of sync; verify schema and/or regenerate static Django models and re-migrate. \"\n                    f\"Unmapped XML attributes for type '{complex_type.name}': {unmapped_dynamic}\"\n                )\n        except Exception:\n            pass\n\n        # Handle nested complex elements\n        for el_def, elements in nested_to_process:\n            target_type_name = (el_def.type_name or \"\").split(\":\")[-1]\n            target_complex_type = self._find_complex_type(target_type_name)\n            if target_complex_type is None:\n                logger.warning(\"Unknown nested complex type '%s' for element '%s'\", target_type_name, el_def.name)\n                continue\n            target_model_cls = self._get_model_for_complex_type(target_complex_type)\n            if target_model_cls is None:\n                logger.warning(\n                    \"Missing Django model for nested type '%s' (element '%s')\", target_type_name, el_def.name\n                )\n                continue\n\n            if el_def.is_list:\n                # If the parent model exposes GenericRelation('entries'), persist as GenericEntry rows\n                try:\n                    has_entries = hasattr(instance, \"entries\")\n                except Exception:\n                    has_entries = False\n                if has_entries:\n                    try:\n                        GenericEntry = django_apps.get_model(f\"{self.app_label}.GenericEntry\")\n                    except Exception:\n                        GenericEntry = None\n                    if GenericEntry is not None:\n                        for idx, child_elem in enumerate(elements):\n                            entry_data_kwargs = self._build_generic_entry_kwargs(\n                                instance=instance,\n                                el_name=el_def.name,\n                                target_type_name=target_type_name,\n                                child_elem=child_elem,\n                                GenericEntry=GenericEntry,\n                                order_index=idx,\n                            )\n                            # Create entry row\n                            if self._save_objects:\n                                GenericEntry.objects.create(**entry_data_kwargs)\n                        # Skip concrete child instance creation when using GFK\n                        continue\n                # Default generation style 'child_fk': inject FK on child named after parent class in lowercase\n                parent_fk_field = instance.__class__.__name__.lower()\n                for child_elem in elements:\n                    child_instance: Any = self._build_instance_from_element(\n                        child_elem,\n                        target_complex_type,\n                        target_model_cls,\n                        parent_instance=instance,\n                        parent_link_field=parent_fk_field,\n                    )\n                    # Set parent FK on child; save update if field exists\n                    if hasattr(child_instance, parent_fk_field):\n                        setattr(child_instance, parent_fk_field, instance)\n                        if self._save_objects:\n                            child_instance.save(update_fields=[parent_fk_field])\n                    else:\n                        # If strategy was m2m/json, this will be a no-op; can extend later\n                        logger.debug(\n                            \"Child model %s lacks FK field '%s' to parent; skipping back-link\",\n                            child_instance.__class__.__name__,\n                            parent_fk_field,\n                        )\n                continue\n\n            # Single nested complex element: either FK on parent or GFK entry (all_nested policy)\n            parent_fk_name = self._xml_name_to_django_field(el_def.name)\n            # If parent exposes entries and does NOT define a concrete FK field for this element, persist as GenericEntry\n            use_gfk_single = False\n            try:\n                has_entries = hasattr(instance, \"entries\")\n            except Exception:\n                has_entries = False\n            if has_entries:\n                try:\n                    model_field_names = {f.name for f in model_cls._meta.fields}\n                except Exception:\n                    model_field_names = set()\n                if parent_fk_name not in model_field_names:\n                    use_gfk_single = True\n\n            if use_gfk_single:\n                try:\n                    GenericEntry = django_apps.get_model(f\"{self.app_label}.GenericEntry\")\n                except Exception:\n                    GenericEntry = None\n                if GenericEntry is not None:\n                    child_elem = elements[0]\n                    single_entry_kwargs = self._build_generic_entry_kwargs(\n                        instance=instance,\n                        el_name=el_def.name,\n                        target_type_name=target_type_name,\n                        child_elem=child_elem,\n                        GenericEntry=GenericEntry,\n                        order_index=0,\n                    )\n                    if self._save_objects:\n                        GenericEntry.objects.create(**single_entry_kwargs)\n                # Skip concrete child instance creation\n                continue\n\n            # Default: create child instance and set FK on parent\n            child_elem = elements[0]\n            child_instance = self._build_instance_from_element(\n                child_elem, target_complex_type, target_model_cls, parent_instance=instance\n            )\n\n            # For proxy instances (non-Django), the attribute may not exist yet; set unconditionally\n            try:\n                setattr(instance, parent_fk_name, child_instance)\n                if self._save_objects and hasattr(instance, \"save\"):\n                    instance.save(update_fields=[parent_fk_name])\n            except Exception:\n                logger.debug(\n                    \"Could not set nested field '%s' on parent %s\",\n                    parent_fk_name,\n                    instance.__class__.__name__,\n                )\n\n        return instance\n\n    @staticmethod\n    def _build_generic_entry_kwargs(\n        *,\n        instance: Any,\n        el_name: str,\n        target_type_name: str | None,\n        child_elem: Any,\n        GenericEntry: Any,\n        order_index: int,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Build keyword args for creating a `GenericEntry` row from an XML child element.\n\n        Behavior is aligned with GFK flag semantics:\n        - `gfk_value_mode=\"json_only\"`: element text is stored inside `attrs_json[\"value\"]`.\n        - `gfk_value_mode=\"typed_columns\"`: if the `GenericEntry` model declares typed\n          columns (`text_value`, `num_value`, `time_value`), element text is copied to\n          `text_value` and parsed into `num_value` and/or `time_value` when unambiguous.\n          Remaining attributes are preserved in `attrs_json`.\n\n        Args:\n            instance: The owning Django model instance for `content_object`.\n            el_name: Local name of the XML element.\n            target_type_name: Resolved type name for the element (simple or complex), if available.\n            child_elem: Parsed XML element (lxml Element or equivalent).\n            GenericEntry: The `GenericEntry` Django model class for this app.\n            order_index: Monotonic index of this entry under the owner, used to preserve order.\n\n        Returns:\n            A dict of keyword arguments suitable for `GenericEntry.objects.create(**kwargs)`.\n        \"\"\"\n        entry_attrs_json: dict[str, Any] = {}\n        try:\n            for k, v in getattr(child_elem, \"attrib\", {}).items():\n                entry_attrs_json[k] = v\n        except Exception:\n            pass\n\n        entry_kwargs: dict[str, Any] = {\n            \"content_object\": instance,\n            \"element_qname\": el_name,\n            \"type_qname\": target_type_name or None,\n            \"attrs_json\": entry_attrs_json,\n            \"order_index\": order_index,\n            \"path_hint\": el_name,\n        }\n\n        # Handle text value\n        try:\n            text_val = getattr(child_elem, \"text\", None)\n        except Exception:\n            text_val = None\n        if text_val and str(text_val).strip() != \"\":\n            raw = str(text_val).strip()\n            # If GenericEntry defines typed columns, attempt parse to numeric/time\n            has_text_col = False\n            try:\n                has_text_col = hasattr(GenericEntry, \"_meta\") and any(\n                    f.name == \"text_value\" for f in GenericEntry._meta.fields\n                )\n            except Exception:\n                has_text_col = False\n            if has_text_col:\n                entry_kwargs[\"text_value\"] = raw\n                try:\n                    entry_kwargs[\"num_value\"] = Decimal(raw)\n                except Exception:\n                    pass\n                try:\n                    iso = raw.replace(\"Z\", \"+00:00\")\n                    entry_kwargs[\"time_value\"] = _dt.fromisoformat(iso)\n                except Exception:\n                    pass\n            else:\n                entry_attrs_json[\"value\"] = raw\n\n        return entry_kwargs\n\n    # --- Helpers ---\n    def _resolve_root_complex_type(self, root_local_name: str) -&gt; XmlSchemaComplexType | None:\n        # Try global elements with explicit type\n        for schema in self._schemas:\n            element = schema.elements.get(root_local_name)\n            if element and element.type_name:\n                type_name = element.type_name.split(\":\")[-1]\n                ct = schema.find_complex_type(type_name, namespace=schema.target_namespace)\n                if ct:\n                    return ct\n        # Try complex type named exactly as root\n        for schema in self._schemas:\n            ct = schema.find_complex_type(root_local_name, namespace=schema.target_namespace)\n            if ct:\n                return ct\n        return None\n\n    def _find_complex_type(self, type_name: str) -&gt; XmlSchemaComplexType | None:\n        for schema in self._schemas:\n            ct = schema.find_complex_type(type_name, namespace=schema.target_namespace)\n            if ct:\n                return ct\n        return None\n\n    def _get_model_for_complex_type(self, complex_type: XmlSchemaComplexType) -&gt; type | None:\n        model_name = complex_type.name\n        # Prefer cache of installed models first for determinism across root and nested types\n        cached = getattr(self, \"_model_resolution_cache\", {}).get(model_name)\n        if cached is not None:\n            return cached\n\n        # Otherwise, attempt to resolve from app registry, then fall back to generated stand-ins\n        try:\n            model_cls = django_apps.get_model(f\"{self.app_label}.{model_name}\")\n            # Populate cache for subsequent lookups\n            try:\n                self._model_resolution_cache[model_name] = model_cls\n            except Exception:\n                pass\n            return model_cls\n        except Exception as exc:\n            # Optionally fallback to in-memory registry for dynamically generated classes\n            if not getattr(self, \"_dynamic_model_fallback\", True):\n                # Provide a detailed error to aid configuration\n                raise ModelResolutionError(\n                    app_label=self.app_label,\n                    model_name=model_name,\n                    message=(\n                        \"Installed Django model not found and dynamic model fallback is disabled. \"\n                        \"Ensure your concrete models are installed under the correct app label, or enable \"\n                        \"dynamic fallback explicitly.\"\n                    ),\n                ) from exc\n            try:\n                return get_generated_model(self.app_label, model_name)\n            except Exception:\n                return None\n\n    @staticmethod\n    def _is_timescale_model(model_cls: type) -&gt; bool:\n        try:\n            from pydantic2django.django.models import TimescaleModel as _TsModel\n        except Exception:\n            return False\n        try:\n            return issubclass(model_cls, _TsModel)\n        except Exception:\n            return False\n\n    @staticmethod\n    def _local_name(qname: str) -&gt; str:\n        if \"}\" in qname:\n            return qname.split(\"}\", 1)[1]\n        if \":\" in qname:\n            return qname.split(\":\", 1)[1]\n        return qname\n\n    @staticmethod\n    def _xml_name_to_django_field(xml_name: str) -&gt; str:\n        # Use shared normalization so all ingestion paths stay consistent\n        return sanitize_field_identifier(xml_name)\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/ingestor/#pydantic2django.xmlschema.ingestor.XmlInstanceIngestor.__init__","title":"<code>__init__(*, schema_files, app_label, dynamic_model_fallback=False, strict=False)</code>","text":"<p>Initialize a schema-aware XML ingestor.</p> <p>Parameters:</p> Name Type Description Default <code>schema_files</code> <code>list[str | Path]</code> <p>Paths to XSD files used to parse/resolve types.</p> required <code>app_label</code> <code>str</code> <p>Django app label expected to contain installed models.</p> required <code>dynamic_model_fallback</code> <code>bool</code> <p>Controls behavior when an installed model is missing. - True: fall back to dynamically generated stand-ins from the   in-memory registry, allowing unsaved/ephemeral workflows. - False (default): raise ModelResolutionError when a discovered complex type cannot be   resolved to an installed Django model, avoiding implicit usage of stand-ins.</p> <code>False</code> Behavior <ul> <li>Regardless of the fallback flag, the ingestor will always prefer installed Django   models and pre-cache them for all discovered complex types to ensure consistent   resolution across root and nested elements.</li> </ul> Source code in <code>src/pydantic2django/xmlschema/ingestor.py</code> <pre><code>def __init__(\n    self,\n    *,\n    schema_files: list[str | Path],\n    app_label: str,\n    dynamic_model_fallback: bool = False,\n    strict: bool = False,\n):\n    \"\"\"\n    Initialize a schema-aware XML ingestor.\n\n    Args:\n        schema_files: Paths to XSD files used to parse/resolve types.\n        app_label: Django app label expected to contain installed models.\n        dynamic_model_fallback: Controls behavior when an installed model is missing.\n            - True: fall back to dynamically generated stand-ins from the\n              in-memory registry, allowing unsaved/ephemeral workflows.\n            - False (default): raise ModelResolutionError when a discovered complex type cannot be\n              resolved to an installed Django model, avoiding implicit usage of stand-ins.\n\n    Behavior:\n        - Regardless of the fallback flag, the ingestor will always prefer installed Django\n          models and pre-cache them for all discovered complex types to ensure consistent\n          resolution across root and nested elements.\n    \"\"\"\n    try:  # Validate dependency early\n        import lxml.etree  # noqa: F401\n    except ImportError as exc:  # pragma: no cover - environment dependent\n        raise ImportError(\"lxml is required for XML ingestion. Install with: pip install lxml\") from exc\n\n    self.app_label = app_label\n    self._save_objects: bool = True\n    self._dynamic_model_fallback: bool = bool(dynamic_model_fallback)\n    self._strict: bool = bool(strict)\n    self.created_instances: list[Any] = []\n\n    discovery = XmlSchemaDiscovery()\n    discovery.discover_models(packages=[str(p) for p in schema_files], app_label=app_label)\n    # Keep references for mapping\n    self._schemas: list[XmlSchemaDefinition] = list(discovery.parsed_schemas)\n\n    # Pre-resolve and cache installed Django model classes for all discovered complex types.\n    # This guarantees we consistently use concrete models (with managers) when available.\n    self._model_resolution_cache: dict[str, type] = {}\n    try:\n        discovered_names: set[str] = set()\n        for schema in self._schemas:\n            for ct in schema.get_all_complex_types():\n                discovered_names.add(ct.name)\n        for model_name in discovered_names:\n            try:\n                model_cls = django_apps.get_model(f\"{self.app_label}.{model_name}\")\n            except Exception:\n                model_cls = None\n            if model_cls is not None:\n                self._model_resolution_cache[model_name] = model_cls\n    except Exception:\n        # Cache population is best-effort; fall back to on-demand lookups.\n        self._model_resolution_cache = {}\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/ingestor/#pydantic2django.xmlschema.ingestor.XmlInstanceIngestor.ingest_from_file","title":"<code>ingest_from_file(xml_path, *, save=True)</code>","text":"<p>Ingest an XML instance from a file path, returning the created root Django instance.</p> Source code in <code>src/pydantic2django/xmlschema/ingestor.py</code> <pre><code>def ingest_from_file(self, xml_path: str | Path, *, save: bool = True) -&gt; Any:\n    \"\"\"\n    Ingest an XML instance from a file path, returning the created root Django instance.\n    \"\"\"\n    xml_path = Path(xml_path)\n    import lxml.etree as _etree\n\n    self._save_objects = bool(save)\n    self.created_instances = []\n    with xml_path.open(\"rb\") as f:\n        tree = _etree.parse(f)\n    root = tree.getroot()\n    return self._ingest_root_element(root)\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/ingestor/#pydantic2django.xmlschema.ingestor.XmlInstanceIngestor.ingest_from_string","title":"<code>ingest_from_string(xml_string, *, save=True)</code>","text":"<p>Ingest an XML instance from a string, returning the created root Django instance.</p> Source code in <code>src/pydantic2django/xmlschema/ingestor.py</code> <pre><code>def ingest_from_string(self, xml_string: str, *, save: bool = True) -&gt; Any:\n    \"\"\"\n    Ingest an XML instance from a string, returning the created root Django instance.\n    \"\"\"\n    import lxml.etree as _etree\n\n    self._save_objects = bool(save)\n    self.created_instances = []\n    root = _etree.fromstring(xml_string)\n    return self._ingest_root_element(root)\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/ingestor/#pydantic2django.xmlschema.ingestor.XmlInstanceIngestor.validate_models","title":"<code>validate_models(*, strict=True)</code>","text":"<p>Validate that discovered schema types align with installed Django models.</p> <ul> <li>Ensures expected fields from attributes, simple elements, and single nested complex   elements exist on the corresponding Django model.</li> <li>Flags Timescale-based models that are missing the canonical 'time' field.</li> </ul> <p>Returns a list of ContractIssue. If strict=True and issues exist, raises SchemaSyncError with a concise remediation message.</p> Source code in <code>src/pydantic2django/xmlschema/ingestor.py</code> <pre><code>def validate_models(self, *, strict: bool = True) -&gt; list[ContractIssue]:\n    \"\"\"\n    Validate that discovered schema types align with installed Django models.\n\n    - Ensures expected fields from attributes, simple elements, and single nested complex\n      elements exist on the corresponding Django model.\n    - Flags Timescale-based models that are missing the canonical 'time' field.\n\n    Returns a list of ContractIssue. If strict=True and issues exist, raises SchemaSyncError\n    with a concise remediation message.\n    \"\"\"\n    issues: list[ContractIssue] = []\n    for schema in self._schemas:\n        try:\n            complex_types = list(schema.get_all_complex_types())\n        except Exception:\n            continue\n        for ct in complex_types:\n            model_cls = self._get_model_for_complex_type(ct)\n            if model_cls is None:\n                issues.append(\n                    ContractIssue(\n                        complex_type_name=ct.name,\n                        model_name=f\"{self.app_label}.{ct.name}\",\n                        missing_fields=[\"&lt;installed model not found&gt;\"],\n                        extra_fields=[],\n                        problems=[\"No installed Django model resolved for complex type\"],\n                    )\n                )\n                continue\n\n            try:\n                model_field_names = {f.name for f in model_cls._meta.fields}\n            except Exception:\n                model_field_names = set()\n\n            expected_fields: set[str] = set()\n            # Attributes\n            for attr_name in getattr(ct, \"attributes\", {}).keys():\n                expected_fields.add(self._xml_name_to_django_field(attr_name))\n            # Elements (simple + parent links for single nested complex)\n            for el in getattr(ct, \"elements\", []):\n                if getattr(el, \"type_name\", None) and getattr(el, \"base_type\", None) is None:\n                    if not getattr(el, \"is_list\", False):\n                        expected_fields.add(self._xml_name_to_django_field(el.name))\n                    continue\n                expected_fields.add(self._xml_name_to_django_field(el.name))\n\n            missing = sorted([fn for fn in expected_fields if fn not in model_field_names])\n            problems: list[str] = []\n            if self._is_timescale_model(model_cls) and \"time\" not in model_field_names:\n                problems.append(\"Timescale model missing required 'time' field\")\n\n            if missing or problems:\n                issues.append(\n                    ContractIssue(\n                        complex_type_name=ct.name,\n                        model_name=f\"{self.app_label}.{model_cls.__name__}\",\n                        missing_fields=missing,\n                        extra_fields=[],\n                        problems=problems,\n                    )\n                )\n\n    if strict and issues:\n        details = \"; \".join(\n            f\"[{i.model_name}] missing={i.missing_fields or '-'} problems={i.problems or '-'}\" for i in issues[:5]\n        )\n        raise SchemaSyncError(\n            f\"Schema and static model are out of sync; verify schema and/or regenerate static Django models and re-migrate. Details: {details}\"\n        )\n    return issues\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/ingestor/#pydantic2django.xmlschema.ingestor.clear_ingestor_cache","title":"<code>clear_ingestor_cache()</code>","text":"<p>Clear all cached ingestors (helpful for tests).</p> Source code in <code>src/pydantic2django/xmlschema/ingestor.py</code> <pre><code>def clear_ingestor_cache() -&gt; None:\n    \"\"\"Clear all cached ingestors (helpful for tests).\"\"\"\n    _INGESTOR_CACHE.clear()\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/ingestor/#pydantic2django.xmlschema.ingestor.get_shared_ingestor","title":"<code>get_shared_ingestor(*, schema_files, app_label, dynamic_model_fallback=False, strict=False)</code>","text":"<p>Get or create a shared XmlInstanceIngestor for a given (app_label, schema set).</p> <p>Reuses the same ingestor instance across calls if the schema files (paths and mtimes) are unchanged, eliminating repeated discovery and setup work.</p> <p>Parameters:</p> Name Type Description Default <code>schema_files</code> <code>list[str | Path]</code> <p>Absolute or relative paths to XSD files. Paths are normalized.</p> required <code>app_label</code> <code>str</code> <p>Django app label that contains the installed models for the discovered types.</p> required <code>dynamic_model_fallback</code> <code>bool</code> <p>If True, when an installed model is not found in the app registry, the ingestor will fall back to dynamically generated stand-in classes registered in the in-memory registry. If False, the ingestor will raise a ModelResolutionError instead of falling back. This is useful to guarantee that only concrete, installed Django models (with managers) are used during ingestion.</p> <code>False</code> Notes <ul> <li>The shared-ingestor cache key includes the value of <code>dynamic_model_fallback</code> to avoid reusing   an instance created with a different fallback policy.</li> </ul> Source code in <code>src/pydantic2django/xmlschema/ingestor.py</code> <pre><code>def get_shared_ingestor(\n    *, schema_files: list[str | Path], app_label: str, dynamic_model_fallback: bool = False, strict: bool = False\n) -&gt; XmlInstanceIngestor:\n    \"\"\"\n    Get or create a shared XmlInstanceIngestor for a given (app_label, schema set).\n\n    Reuses the same ingestor instance across calls if the schema files (paths and mtimes)\n    are unchanged, eliminating repeated discovery and setup work.\n\n    Args:\n        schema_files: Absolute or relative paths to XSD files. Paths are normalized.\n        app_label: Django app label that contains the installed models for the discovered types.\n        dynamic_model_fallback: If True, when an installed model is not found in the app\n            registry, the ingestor will fall back to dynamically generated stand-in classes registered\n            in the in-memory registry. If False, the ingestor will raise a ModelResolutionError instead\n            of falling back. This is useful to guarantee that only concrete, installed Django models\n            (with managers) are used during ingestion.\n\n    Notes:\n        - The shared-ingestor cache key includes the value of `dynamic_model_fallback` to avoid reusing\n          an instance created with a different fallback policy.\n    \"\"\"\n    # Include fallback and strict flags in the cache key (without changing CacheKey type)\n    cache_label = f\"{app_label}|dyn={1 if dynamic_model_fallback else 0}|strict={1 if strict else 0}\"\n    key = _schema_key(cache_label, schema_files)\n    cached = _INGESTOR_CACHE.get(key)\n    if cached is not None:\n        return cached\n\n    # Optional pre-warm to ensure classes are registered\n    warmup_xmlschema_models(schema_files, app_label=app_label)\n\n    inst = XmlInstanceIngestor(\n        schema_files=[Path(p) for p in _normalize_schema_files(schema_files)],\n        app_label=app_label,\n        dynamic_model_fallback=dynamic_model_fallback,\n        strict=strict,\n    )\n    _INGESTOR_CACHE.put(key, inst)\n    return inst\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/ingestor/#pydantic2django.xmlschema.ingestor.ingestor_cache_stats","title":"<code>ingestor_cache_stats()</code>","text":"<p>Return internal cache stats for diagnostics.</p> Source code in <code>src/pydantic2django/xmlschema/ingestor.py</code> <pre><code>def ingestor_cache_stats() -&gt; dict[str, Any]:\n    \"\"\"Return internal cache stats for diagnostics.\"\"\"\n    return _INGESTOR_CACHE.stats()\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/ingestor/#pydantic2django.xmlschema.ingestor.set_ingestor_cache","title":"<code>set_ingestor_cache(maxsize=None, ttl_seconds=None)</code>","text":"<p>Configure global shared-ingestor cache (LRU size and TTL).</p> Source code in <code>src/pydantic2django/xmlschema/ingestor.py</code> <pre><code>def set_ingestor_cache(maxsize: int | None = None, ttl_seconds: float | None = None) -&gt; None:\n    \"\"\"Configure global shared-ingestor cache (LRU size and TTL).\"\"\"\n    _INGESTOR_CACHE.set_params(maxsize=maxsize, ttl_seconds=ttl_seconds)\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/ingestor/#pydantic2django.xmlschema.ingestor.warmup_xmlschema_models","title":"<code>warmup_xmlschema_models(schema_files, *, app_label)</code>","text":"<p>Pre-generate and register XML-derived Django models once per (app_label, schema set).</p> <p>This runs the generator to populate the in-memory registry used by the ingestor, avoiding repeated generation when used in long-lived processes (e.g., workers).</p> Source code in <code>src/pydantic2django/xmlschema/ingestor.py</code> <pre><code>def warmup_xmlschema_models(schema_files: list[str | Path], *, app_label: str) -&gt; None:\n    \"\"\"\n    Pre-generate and register XML-derived Django models once per (app_label, schema set).\n\n    This runs the generator to populate the in-memory registry used by the ingestor,\n    avoiding repeated generation when used in long-lived processes (e.g., workers).\n    \"\"\"\n    try:\n        from .generator import XmlSchemaDjangoModelGenerator\n    except Exception:\n        # Generator not available or optional deps missing; skip warmup.\n        return\n\n    key = _schema_key(app_label, schema_files)\n    if key in _WARMED_KEYS:\n        return\n\n    try:\n        gen = XmlSchemaDjangoModelGenerator(\n            schema_files=[Path(p) for p in _normalize_schema_files(schema_files)],\n            app_label=app_label,\n            output_path=\"__discard__.py\",\n            verbose=False,\n        )\n        _ = gen.generate_models_file()\n        _WARMED_KEYS.add(key)\n        logger.info(\"Warmed XML models for app '%s' with %d schema files\", app_label, len(schema_files))\n    except Exception as exc:\n        logger.debug(\"Warmup failed for app '%s': %s\", app_label, exc)\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/models/","title":"pydantic2django.xmlschema.models","text":"<p>XML Schema data models for representing parsed XSD structures. These models represent the parsed XML Schema definitions before Django model conversion.</p>"},{"location":"reference/pydantic2django/xmlschema/models/#pydantic2django.xmlschema.models.XmlSchemaAttribute","title":"<code>XmlSchemaAttribute</code>  <code>dataclass</code>","text":"<p>Represents an XML Schema attribute.</p> Source code in <code>src/pydantic2django/xmlschema/models.py</code> <pre><code>@dataclass\nclass XmlSchemaAttribute:\n    \"\"\"Represents an XML Schema attribute.\"\"\"\n\n    name: str\n    base_type: XmlSchemaType | None = None\n    type_name: str | None = None  # For references to simple types\n    use: str = \"optional\"  # required, optional, prohibited\n    default_value: Any | None = None\n    fixed_value: Any | None = None\n    restrictions: XmlSchemaRestriction | None = None\n    documentation: str | None = None\n    namespace: str | None = None\n\n    @property\n    def is_required(self) -&gt; bool:\n        \"\"\"Check if attribute is required.\"\"\"\n        return self.use == \"required\"\n\n    @property\n    def is_optional(self) -&gt; bool:\n        \"\"\"Check if attribute is optional.\"\"\"\n        return self.use == \"optional\"\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/models/#pydantic2django.xmlschema.models.XmlSchemaAttribute.is_optional","title":"<code>is_optional</code>  <code>property</code>","text":"<p>Check if attribute is optional.</p>"},{"location":"reference/pydantic2django/xmlschema/models/#pydantic2django.xmlschema.models.XmlSchemaAttribute.is_required","title":"<code>is_required</code>  <code>property</code>","text":"<p>Check if attribute is required.</p>"},{"location":"reference/pydantic2django/xmlschema/models/#pydantic2django.xmlschema.models.XmlSchemaComplexType","title":"<code>XmlSchemaComplexType</code>  <code>dataclass</code>","text":"<p>Represents an XML Schema complex type.</p> Source code in <code>src/pydantic2django/xmlschema/models.py</code> <pre><code>@dataclass\nclass XmlSchemaComplexType:\n    \"\"\"Represents an XML Schema complex type.\"\"\"\n\n    name: str\n    namespace: str | None = None\n    elements: list[XmlSchemaElement] = field(default_factory=list)\n    attributes: dict[str, XmlSchemaAttribute] = field(default_factory=dict)\n    base_type: str | None = None  # For inheritance\n    mixed: bool = False  # Mixed content model\n    abstract: bool = False\n    sequence: bool = False\n    choice: bool = False\n    all_elements: bool = False\n    documentation: str | None = None\n    schema_location: str | None = None\n    schema_def: \"XmlSchemaDefinition | None\" = field(default=None, repr=False)\n\n    @property\n    def __name__(self) -&gt; str:\n        return self.name\n\n    # Schema linking fields\n    conversion_session_id: str | None = field(default_factory=lambda: str(uuid.uuid4()))\n    schema_source_file: str | None = None\n\n    def add_element(self, element: XmlSchemaElement) -&gt; None:\n        \"\"\"Add an element to this complex type.\"\"\"\n        self.elements.append(element)\n\n    def add_attribute(self, attribute: XmlSchemaAttribute) -&gt; None:\n        \"\"\"Add an attribute to this complex type.\"\"\"\n        self.attributes[attribute.name] = attribute\n\n    def get_required_elements(self) -&gt; list[XmlSchemaElement]:\n        \"\"\"Get all required elements (min_occurs &gt; 0).\"\"\"\n        return [elem for elem in self.elements if elem.is_required]\n\n    def get_optional_elements(self) -&gt; list[XmlSchemaElement]:\n        \"\"\"Get all optional elements (min_occurs = 0).\"\"\"\n        return [elem for elem in self.elements if elem.is_optional]\n\n    def get_required_attributes(self) -&gt; list[XmlSchemaAttribute]:\n        \"\"\"Get all required attributes.\"\"\"\n        return [attr for attr in self.attributes.values() if attr.is_required]\n\n    def has_list_elements(self) -&gt; bool:\n        \"\"\"Check if any elements can have multiple occurrences.\"\"\"\n        return any(elem.is_list for elem in self.elements)\n\n    def __hash__(self):\n        return hash((self.name, self.namespace, self.schema_location))\n\n    def __eq__(self, other):\n        if not isinstance(other, XmlSchemaComplexType):\n            return NotImplemented\n        return (\n            self.name == other.name\n            and self.namespace == other.namespace\n            and self.schema_location == other.schema_location\n        )\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/models/#pydantic2django.xmlschema.models.XmlSchemaComplexType.add_attribute","title":"<code>add_attribute(attribute)</code>","text":"<p>Add an attribute to this complex type.</p> Source code in <code>src/pydantic2django/xmlschema/models.py</code> <pre><code>def add_attribute(self, attribute: XmlSchemaAttribute) -&gt; None:\n    \"\"\"Add an attribute to this complex type.\"\"\"\n    self.attributes[attribute.name] = attribute\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/models/#pydantic2django.xmlschema.models.XmlSchemaComplexType.add_element","title":"<code>add_element(element)</code>","text":"<p>Add an element to this complex type.</p> Source code in <code>src/pydantic2django/xmlschema/models.py</code> <pre><code>def add_element(self, element: XmlSchemaElement) -&gt; None:\n    \"\"\"Add an element to this complex type.\"\"\"\n    self.elements.append(element)\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/models/#pydantic2django.xmlschema.models.XmlSchemaComplexType.get_optional_elements","title":"<code>get_optional_elements()</code>","text":"<p>Get all optional elements (min_occurs = 0).</p> Source code in <code>src/pydantic2django/xmlschema/models.py</code> <pre><code>def get_optional_elements(self) -&gt; list[XmlSchemaElement]:\n    \"\"\"Get all optional elements (min_occurs = 0).\"\"\"\n    return [elem for elem in self.elements if elem.is_optional]\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/models/#pydantic2django.xmlschema.models.XmlSchemaComplexType.get_required_attributes","title":"<code>get_required_attributes()</code>","text":"<p>Get all required attributes.</p> Source code in <code>src/pydantic2django/xmlschema/models.py</code> <pre><code>def get_required_attributes(self) -&gt; list[XmlSchemaAttribute]:\n    \"\"\"Get all required attributes.\"\"\"\n    return [attr for attr in self.attributes.values() if attr.is_required]\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/models/#pydantic2django.xmlschema.models.XmlSchemaComplexType.get_required_elements","title":"<code>get_required_elements()</code>","text":"<p>Get all required elements (min_occurs &gt; 0).</p> Source code in <code>src/pydantic2django/xmlschema/models.py</code> <pre><code>def get_required_elements(self) -&gt; list[XmlSchemaElement]:\n    \"\"\"Get all required elements (min_occurs &gt; 0).\"\"\"\n    return [elem for elem in self.elements if elem.is_required]\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/models/#pydantic2django.xmlschema.models.XmlSchemaComplexType.has_list_elements","title":"<code>has_list_elements()</code>","text":"<p>Check if any elements can have multiple occurrences.</p> Source code in <code>src/pydantic2django/xmlschema/models.py</code> <pre><code>def has_list_elements(self) -&gt; bool:\n    \"\"\"Check if any elements can have multiple occurrences.\"\"\"\n    return any(elem.is_list for elem in self.elements)\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/models/#pydantic2django.xmlschema.models.XmlSchemaConversionSession","title":"<code>XmlSchemaConversionSession</code>  <code>dataclass</code>","text":"<p>Represents a complete conversion session linking all schema elements together. This provides the mechanism to reference all schema elements for a particular conversion.</p> Source code in <code>src/pydantic2django/xmlschema/models.py</code> <pre><code>@dataclass\nclass XmlSchemaConversionSession:\n    \"\"\"\n    Represents a complete conversion session linking all schema elements together.\n    This provides the mechanism to reference all schema elements for a particular conversion.\n    \"\"\"\n\n    session_id: str = field(default_factory=lambda: str(uuid.uuid4()))\n    schema_files: list[str] = field(default_factory=list)\n    schemas: list[XmlSchemaDefinition] = field(default_factory=list)\n    target_namespace: str | None = None\n    app_label: str = \"xmlschema_app\"\n    created_at: str | None = None  # ISO timestamp\n    conversion_metadata: dict[str, Any] = field(default_factory=dict)\n\n    def add_schema(self, schema: XmlSchemaDefinition) -&gt; None:\n        \"\"\"Add a schema to this conversion session.\"\"\"\n        schema.conversion_session_id = self.session_id\n        self.schemas.append(schema)\n        if schema.schema_location not in self.schema_files:\n            self.schema_files.append(schema.schema_location)\n\n    def get_all_complex_types(self) -&gt; list[XmlSchemaComplexType]:\n        \"\"\"Get all complex types from all schemas in this session.\"\"\"\n        all_types = []\n        for schema in self.schemas:\n            all_types.extend(schema.get_all_complex_types())\n        return all_types\n\n    def find_complex_type_across_schemas(self, name: str, namespace: str | None = None) -&gt; XmlSchemaComplexType | None:\n        \"\"\"Find a complex type across all schemas in this session.\"\"\"\n        for schema in self.schemas:\n            complex_type = schema.find_complex_type(name, namespace)\n            if complex_type:\n                return complex_type\n        return None\n\n    def get_conversion_statistics(self) -&gt; dict[str, Any]:\n        \"\"\"Get statistics about this conversion session.\"\"\"\n        total_complex_types = sum(len(schema.complex_types) for schema in self.schemas)\n        total_simple_types = sum(len(schema.simple_types) for schema in self.schemas)\n        total_elements = sum(len(schema.elements) for schema in self.schemas)\n\n        return {\n            \"session_id\": self.session_id,\n            \"total_schemas\": len(self.schemas),\n            \"total_complex_types\": total_complex_types,\n            \"total_simple_types\": total_simple_types,\n            \"total_elements\": total_elements,\n            \"schema_files\": self.schema_files,\n            \"target_namespace\": self.target_namespace,\n            \"app_label\": self.app_label,\n        }\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/models/#pydantic2django.xmlschema.models.XmlSchemaConversionSession.add_schema","title":"<code>add_schema(schema)</code>","text":"<p>Add a schema to this conversion session.</p> Source code in <code>src/pydantic2django/xmlschema/models.py</code> <pre><code>def add_schema(self, schema: XmlSchemaDefinition) -&gt; None:\n    \"\"\"Add a schema to this conversion session.\"\"\"\n    schema.conversion_session_id = self.session_id\n    self.schemas.append(schema)\n    if schema.schema_location not in self.schema_files:\n        self.schema_files.append(schema.schema_location)\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/models/#pydantic2django.xmlschema.models.XmlSchemaConversionSession.find_complex_type_across_schemas","title":"<code>find_complex_type_across_schemas(name, namespace=None)</code>","text":"<p>Find a complex type across all schemas in this session.</p> Source code in <code>src/pydantic2django/xmlschema/models.py</code> <pre><code>def find_complex_type_across_schemas(self, name: str, namespace: str | None = None) -&gt; XmlSchemaComplexType | None:\n    \"\"\"Find a complex type across all schemas in this session.\"\"\"\n    for schema in self.schemas:\n        complex_type = schema.find_complex_type(name, namespace)\n        if complex_type:\n            return complex_type\n    return None\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/models/#pydantic2django.xmlschema.models.XmlSchemaConversionSession.get_all_complex_types","title":"<code>get_all_complex_types()</code>","text":"<p>Get all complex types from all schemas in this session.</p> Source code in <code>src/pydantic2django/xmlschema/models.py</code> <pre><code>def get_all_complex_types(self) -&gt; list[XmlSchemaComplexType]:\n    \"\"\"Get all complex types from all schemas in this session.\"\"\"\n    all_types = []\n    for schema in self.schemas:\n        all_types.extend(schema.get_all_complex_types())\n    return all_types\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/models/#pydantic2django.xmlschema.models.XmlSchemaConversionSession.get_conversion_statistics","title":"<code>get_conversion_statistics()</code>","text":"<p>Get statistics about this conversion session.</p> Source code in <code>src/pydantic2django/xmlschema/models.py</code> <pre><code>def get_conversion_statistics(self) -&gt; dict[str, Any]:\n    \"\"\"Get statistics about this conversion session.\"\"\"\n    total_complex_types = sum(len(schema.complex_types) for schema in self.schemas)\n    total_simple_types = sum(len(schema.simple_types) for schema in self.schemas)\n    total_elements = sum(len(schema.elements) for schema in self.schemas)\n\n    return {\n        \"session_id\": self.session_id,\n        \"total_schemas\": len(self.schemas),\n        \"total_complex_types\": total_complex_types,\n        \"total_simple_types\": total_simple_types,\n        \"total_elements\": total_elements,\n        \"schema_files\": self.schema_files,\n        \"target_namespace\": self.target_namespace,\n        \"app_label\": self.app_label,\n    }\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/models/#pydantic2django.xmlschema.models.XmlSchemaDefinition","title":"<code>XmlSchemaDefinition</code>  <code>dataclass</code>","text":"<p>Represents a complete XML Schema definition.</p> Source code in <code>src/pydantic2django/xmlschema/models.py</code> <pre><code>@dataclass\nclass XmlSchemaDefinition:\n    \"\"\"Represents a complete XML Schema definition.\"\"\"\n\n    schema_location: str\n    target_namespace: str | None = None\n    element_form_default: str = \"unqualified\"\n    attribute_form_default: str = \"unqualified\"\n    complex_types: dict[str, XmlSchemaComplexType] = field(default_factory=dict)\n    simple_types: dict[str, XmlSchemaSimpleType] = field(default_factory=dict)\n    elements: dict[str, XmlSchemaElement] = field(default_factory=dict)\n    attributes: dict[str, XmlSchemaAttribute] = field(default_factory=dict)\n    # Attribute groups (name -&gt; attributes mapping)\n    attribute_groups: dict[str, dict[str, XmlSchemaAttribute]] = field(default_factory=dict)\n    # Substitution group membership (head local name -&gt; list of member elements)\n    element_substitutions: dict[str, list[XmlSchemaElement]] = field(default_factory=dict)\n    imports: list[XmlSchemaImport] = field(default_factory=list)\n    includes: list[str] = field(default_factory=list)\n    keys: list[XmlSchemaKey] = field(default_factory=list)\n    keyrefs: list[XmlSchemaKeyRef] = field(default_factory=list)\n\n    # Schema linking fields\n    conversion_session_id: str | None = field(default_factory=lambda: str(uuid.uuid4()))\n\n    def add_complex_type(self, complex_type: XmlSchemaComplexType) -&gt; None:\n        \"\"\"Add a complex type to this schema.\"\"\"\n        complex_type.schema_location = self.schema_location\n        complex_type.namespace = complex_type.namespace or self.target_namespace\n        complex_type.conversion_session_id = self.conversion_session_id\n        complex_type.schema_source_file = self.schema_location\n        complex_type.schema_def = self\n        self.complex_types[complex_type.name] = complex_type\n\n    def add_simple_type(self, simple_type: XmlSchemaSimpleType) -&gt; None:\n        \"\"\"Add a simple type to this schema.\"\"\"\n        simple_type.schema_location = self.schema_location\n        simple_type.namespace = simple_type.namespace or self.target_namespace\n        self.simple_types[simple_type.name] = simple_type\n\n    def add_element(self, element: XmlSchemaElement) -&gt; None:\n        \"\"\"Add a global element to this schema.\"\"\"\n        element.namespace = element.namespace or self.target_namespace\n        self.elements[element.name] = element\n        # Track substitution group membership for later expansion\n        if element.substitution_group:\n            head = element.substitution_group\n            self.element_substitutions.setdefault(head, []).append(element)\n\n    def add_attribute_group(self, name: str, attributes: dict[str, XmlSchemaAttribute]) -&gt; None:\n        \"\"\"Register a named attribute group (flattened attributes).\"\"\"\n        self.attribute_groups[name] = attributes\n\n    def get_attribute_group(self, name: str) -&gt; dict[str, XmlSchemaAttribute] | None:\n        return self.attribute_groups.get(name)\n\n    def get_substitution_members(self, head_local_name: str) -&gt; list[XmlSchemaElement]:\n        return self.element_substitutions.get(head_local_name, [])\n\n    def get_all_complex_types(self) -&gt; list[XmlSchemaComplexType]:\n        \"\"\"Get all complex types in this schema.\"\"\"\n        return list(self.complex_types.values())\n\n    def get_all_simple_types(self) -&gt; list[XmlSchemaSimpleType]:\n        \"\"\"Get all simple types in this schema.\"\"\"\n        return list(self.simple_types.values())\n\n    def find_complex_type(self, name: str, namespace: str | None = None) -&gt; XmlSchemaComplexType | None:\n        \"\"\"Find a complex type by name and optional namespace.\"\"\"\n        # Try exact match first\n        if name in self.complex_types:\n            complex_type = self.complex_types[name]\n            if namespace is None or complex_type.namespace == namespace:\n                return complex_type\n\n        # Try namespace-qualified lookup\n        for complex_type in self.complex_types.values():\n            if complex_type.name == name and (namespace is None or complex_type.namespace == namespace):\n                return complex_type\n\n        return None\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/models/#pydantic2django.xmlschema.models.XmlSchemaDefinition.add_attribute_group","title":"<code>add_attribute_group(name, attributes)</code>","text":"<p>Register a named attribute group (flattened attributes).</p> Source code in <code>src/pydantic2django/xmlschema/models.py</code> <pre><code>def add_attribute_group(self, name: str, attributes: dict[str, XmlSchemaAttribute]) -&gt; None:\n    \"\"\"Register a named attribute group (flattened attributes).\"\"\"\n    self.attribute_groups[name] = attributes\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/models/#pydantic2django.xmlschema.models.XmlSchemaDefinition.add_complex_type","title":"<code>add_complex_type(complex_type)</code>","text":"<p>Add a complex type to this schema.</p> Source code in <code>src/pydantic2django/xmlschema/models.py</code> <pre><code>def add_complex_type(self, complex_type: XmlSchemaComplexType) -&gt; None:\n    \"\"\"Add a complex type to this schema.\"\"\"\n    complex_type.schema_location = self.schema_location\n    complex_type.namespace = complex_type.namespace or self.target_namespace\n    complex_type.conversion_session_id = self.conversion_session_id\n    complex_type.schema_source_file = self.schema_location\n    complex_type.schema_def = self\n    self.complex_types[complex_type.name] = complex_type\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/models/#pydantic2django.xmlschema.models.XmlSchemaDefinition.add_element","title":"<code>add_element(element)</code>","text":"<p>Add a global element to this schema.</p> Source code in <code>src/pydantic2django/xmlschema/models.py</code> <pre><code>def add_element(self, element: XmlSchemaElement) -&gt; None:\n    \"\"\"Add a global element to this schema.\"\"\"\n    element.namespace = element.namespace or self.target_namespace\n    self.elements[element.name] = element\n    # Track substitution group membership for later expansion\n    if element.substitution_group:\n        head = element.substitution_group\n        self.element_substitutions.setdefault(head, []).append(element)\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/models/#pydantic2django.xmlschema.models.XmlSchemaDefinition.add_simple_type","title":"<code>add_simple_type(simple_type)</code>","text":"<p>Add a simple type to this schema.</p> Source code in <code>src/pydantic2django/xmlschema/models.py</code> <pre><code>def add_simple_type(self, simple_type: XmlSchemaSimpleType) -&gt; None:\n    \"\"\"Add a simple type to this schema.\"\"\"\n    simple_type.schema_location = self.schema_location\n    simple_type.namespace = simple_type.namespace or self.target_namespace\n    self.simple_types[simple_type.name] = simple_type\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/models/#pydantic2django.xmlschema.models.XmlSchemaDefinition.find_complex_type","title":"<code>find_complex_type(name, namespace=None)</code>","text":"<p>Find a complex type by name and optional namespace.</p> Source code in <code>src/pydantic2django/xmlschema/models.py</code> <pre><code>def find_complex_type(self, name: str, namespace: str | None = None) -&gt; XmlSchemaComplexType | None:\n    \"\"\"Find a complex type by name and optional namespace.\"\"\"\n    # Try exact match first\n    if name in self.complex_types:\n        complex_type = self.complex_types[name]\n        if namespace is None or complex_type.namespace == namespace:\n            return complex_type\n\n    # Try namespace-qualified lookup\n    for complex_type in self.complex_types.values():\n        if complex_type.name == name and (namespace is None or complex_type.namespace == namespace):\n            return complex_type\n\n    return None\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/models/#pydantic2django.xmlschema.models.XmlSchemaDefinition.get_all_complex_types","title":"<code>get_all_complex_types()</code>","text":"<p>Get all complex types in this schema.</p> Source code in <code>src/pydantic2django/xmlschema/models.py</code> <pre><code>def get_all_complex_types(self) -&gt; list[XmlSchemaComplexType]:\n    \"\"\"Get all complex types in this schema.\"\"\"\n    return list(self.complex_types.values())\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/models/#pydantic2django.xmlschema.models.XmlSchemaDefinition.get_all_simple_types","title":"<code>get_all_simple_types()</code>","text":"<p>Get all simple types in this schema.</p> Source code in <code>src/pydantic2django/xmlschema/models.py</code> <pre><code>def get_all_simple_types(self) -&gt; list[XmlSchemaSimpleType]:\n    \"\"\"Get all simple types in this schema.\"\"\"\n    return list(self.simple_types.values())\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/models/#pydantic2django.xmlschema.models.XmlSchemaElement","title":"<code>XmlSchemaElement</code>  <code>dataclass</code>","text":"<p>Represents an XML Schema element.</p> Source code in <code>src/pydantic2django/xmlschema/models.py</code> <pre><code>@dataclass\nclass XmlSchemaElement:\n    \"\"\"Represents an XML Schema element.\"\"\"\n\n    name: str\n    base_type: XmlSchemaType | None = None\n    type_name: str | None = None  # For references to complex types\n    min_occurs: int = 1\n    max_occurs: int | str = 1  # Can be \"unbounded\"\n    is_list: bool = False\n    default_value: Any | None = None\n    fixed_value: Any | None = None\n    nillable: bool = False\n    abstract: bool = False\n    substitution_group: str | None = None\n    restrictions: XmlSchemaRestriction | None = None\n    complex_type: \"XmlSchemaComplexType | None\" = None\n    documentation: str | None = None\n    namespace: str | None = None\n    schema_location: str | None = None\n    keys: list[XmlSchemaKey] = field(default_factory=list)\n    keyrefs: list[XmlSchemaKeyRef] = field(default_factory=list)\n\n    @property\n    def is_optional(self) -&gt; bool:\n        \"\"\"Check if element is optional (min_occurs = 0).\"\"\"\n        return self.min_occurs == 0\n\n    @property\n    def is_required(self) -&gt; bool:\n        \"\"\"Check if element is required (min_occurs &gt; 0).\"\"\"\n        return self.min_occurs &gt; 0\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/models/#pydantic2django.xmlschema.models.XmlSchemaElement.is_optional","title":"<code>is_optional</code>  <code>property</code>","text":"<p>Check if element is optional (min_occurs = 0).</p>"},{"location":"reference/pydantic2django/xmlschema/models/#pydantic2django.xmlschema.models.XmlSchemaElement.is_required","title":"<code>is_required</code>  <code>property</code>","text":"<p>Check if element is required (min_occurs &gt; 0).</p>"},{"location":"reference/pydantic2django/xmlschema/models/#pydantic2django.xmlschema.models.XmlSchemaImport","title":"<code>XmlSchemaImport</code>  <code>dataclass</code>","text":"<p>Represents an XSD import statement.</p> Source code in <code>src/pydantic2django/xmlschema/models.py</code> <pre><code>@dataclass\nclass XmlSchemaImport:\n    \"\"\"Represents an XSD import statement.\"\"\"\n\n    namespace: str | None\n    schema_location: str | None\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/models/#pydantic2django.xmlschema.models.XmlSchemaKey","title":"<code>XmlSchemaKey</code>  <code>dataclass</code>","text":"<p>Represents an  definition. Source code in <code>src/pydantic2django/xmlschema/models.py</code> <pre><code>@dataclass\nclass XmlSchemaKey:\n    \"\"\"Represents an &lt;xs:key&gt; definition.\"\"\"\n\n    name: str\n    selector: str\n    fields: list[str] = field(default_factory=list)\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/models/#pydantic2django.xmlschema.models.XmlSchemaKeyRef","title":"<code>XmlSchemaKeyRef</code>  <code>dataclass</code>","text":"<p>Represents an  definition. Source code in <code>src/pydantic2django/xmlschema/models.py</code> <pre><code>@dataclass\nclass XmlSchemaKeyRef:\n    \"\"\"Represents an &lt;xs:keyref&gt; definition.\"\"\"\n\n    name: str\n    refer: str\n    selector: str\n    fields: list[str] = field(default_factory=list)\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/models/#pydantic2django.xmlschema.models.XmlSchemaRestriction","title":"<code>XmlSchemaRestriction</code>  <code>dataclass</code>","text":"<p>Represents XML Schema restrictions/facets.</p> Source code in <code>src/pydantic2django/xmlschema/models.py</code> <pre><code>@dataclass\nclass XmlSchemaRestriction:\n    \"\"\"Represents XML Schema restrictions/facets.\"\"\"\n\n    base: str | None = None\n    min_length: int | None = None\n    max_length: int | None = None\n    length: int | None = None\n    pattern: str | None = None\n    enumeration: list[tuple[str, str]] = field(default_factory=list)\n    min_inclusive: int | float | None = None\n    max_inclusive: int | float | None = None\n    min_exclusive: int | float | None = None\n    max_exclusive: int | float | None = None\n    total_digits: int | None = None\n    fraction_digits: int | None = None\n    white_space: str | None = None  # preserve, replace, collapse\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/models/#pydantic2django.xmlschema.models.XmlSchemaSimpleType","title":"<code>XmlSchemaSimpleType</code>  <code>dataclass</code>","text":"<p>Represents an XML Schema simple type.</p> Source code in <code>src/pydantic2django/xmlschema/models.py</code> <pre><code>@dataclass\nclass XmlSchemaSimpleType:\n    \"\"\"Represents an XML Schema simple type.\"\"\"\n\n    name: str\n    namespace: str | None = None\n    base_type: XmlSchemaType | None = None\n    restrictions: XmlSchemaRestriction | None = None\n    schema_location: str | None = None\n    documentation: str | None = None\n\n    @property\n    def __name__(self) -&gt; str:\n        return self.name\n\n    # --- Backward-compatibility aliases/proxies ---\n    @property\n    def restriction(self) -&gt; XmlSchemaRestriction | None:  # pragma: no cover - compatibility shim\n        return self.restrictions\n\n    @restriction.setter\n    def restriction(self, value: XmlSchemaRestriction | None) -&gt; None:  # pragma: no cover - compatibility shim\n        self.restrictions = value\n\n    @property\n    def enumeration(self) -&gt; list[tuple[str, str]]:  # pragma: no cover - compatibility shim\n        return self.restrictions.enumeration if self.restrictions and self.restrictions.enumeration else []\n\n    @enumeration.setter\n    def enumeration(self, values: list[tuple[str, str]]) -&gt; None:  # pragma: no cover - compatibility shim\n        if self.restrictions is None:\n            self.restrictions = XmlSchemaRestriction()\n        self.restrictions.enumeration = list(values)\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/models/#pydantic2django.xmlschema.models.XmlSchemaType","title":"<code>XmlSchemaType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>XML Schema built-in data types.</p> Source code in <code>src/pydantic2django/xmlschema/models.py</code> <pre><code>class XmlSchemaType(Enum):\n    \"\"\"XML Schema built-in data types.\"\"\"\n\n    # String types\n    STRING = \"xs:string\"\n    NORMALIZEDSTRING = \"xs:normalizedString\"\n    TOKEN = \"xs:token\"\n\n    # Numeric types\n    INTEGER = \"xs:integer\"\n    LONG = \"xs:long\"\n    INT = \"xs:int\"\n    SHORT = \"xs:short\"\n    BYTE = \"xs:byte\"\n    DECIMAL = \"xs:decimal\"\n    FLOAT = \"xs:float\"\n    DOUBLE = \"xs:double\"\n\n    # Unsigned numeric types\n    UNSIGNEDLONG = \"xs:unsignedLong\"\n    UNSIGNEDINT = \"xs:unsignedInt\"\n    UNSIGNEDSHORT = \"xs:unsignedShort\"\n    UNSIGNEDBYTE = \"xs:unsignedByte\"\n\n    # Specialized numeric types\n    POSITIVEINTEGER = \"xs:positiveInteger\"\n    NONNEGATIVEINTEGER = \"xs:nonNegativeInteger\"\n    NEGATIVEINTEGER = \"xs:negativeInteger\"\n    NONPOSITIVEINTEGER = \"xs:nonPositiveInteger\"\n\n    # Boolean\n    BOOLEAN = \"xs:boolean\"\n\n    # Date and time types\n    DATE = \"xs:date\"\n    DATETIME = \"xs:dateTime\"\n    TIME = \"xs:time\"\n    DURATION = \"xs:duration\"\n    GYEAR = \"xs:gYear\"\n    GMONTH = \"xs:gMonth\"\n    GDAY = \"xs:gDay\"\n    GYEARMONTH = \"xs:gYearMonth\"\n    GMONTHDAY = \"xs:gMonthDay\"\n\n    # Binary types\n    BASE64BINARY = \"xs:base64Binary\"\n    HEXBINARY = \"xs:hexBinary\"\n\n    # URI types\n    ANYURI = \"xs:anyURI\"\n\n    # Other types\n    QNAME = \"xs:QName\"\n    NOTATION = \"xs:NOTATION\"\n    ID = \"xs:ID\"\n    IDREF = \"xs:IDREF\"\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/parser/","title":"pydantic2django.xmlschema.parser","text":"<p>XML Schema parser that converts XSD files into internal representation models. Uses lxml for XML parsing and provides the foundation for the discovery pipeline.</p>"},{"location":"reference/pydantic2django/xmlschema/parser/#pydantic2django.xmlschema.parser.XmlSchemaParseError","title":"<code>XmlSchemaParseError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when XML Schema parsing fails</p> Source code in <code>src/pydantic2django/xmlschema/parser.py</code> <pre><code>class XmlSchemaParseError(Exception):\n    \"\"\"Exception raised when XML Schema parsing fails\"\"\"\n\n    pass\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/parser/#pydantic2django.xmlschema.parser.XmlSchemaParser","title":"<code>XmlSchemaParser</code>","text":"<p>Parses XML Schema (XSD) files into internal representation models. Supports local files and basic schema imports/includes.</p> Source code in <code>src/pydantic2django/xmlschema/parser.py</code> <pre><code>class XmlSchemaParser:\n    \"\"\"\n    Parses XML Schema (XSD) files into internal representation models.\n    Supports local files and basic schema imports/includes.\n    \"\"\"\n\n    # XML Schema namespace\n    XS_NAMESPACE = \"http://www.w3.org/2001/XMLSchema\"\n\n    def __init__(self):\n        if etree is None:\n            raise ImportError(\"lxml is required for XML Schema parsing. Install with: pip install lxml\")\n        self.parsed_schemas: dict[str, XmlSchemaDefinition] = {}\n        self.type_mappings = self._build_type_mappings()\n\n    def _build_type_mappings(self) -&gt; dict[str, XmlSchemaType]:\n        \"\"\"Build mapping from XSD type names to our enum values\"\"\"\n        return {\n            \"string\": XmlSchemaType.STRING,\n            \"normalizedString\": XmlSchemaType.NORMALIZEDSTRING,\n            \"token\": XmlSchemaType.TOKEN,\n            \"int\": XmlSchemaType.INTEGER,\n            \"integer\": XmlSchemaType.INTEGER,\n            \"long\": XmlSchemaType.LONG,\n            \"short\": XmlSchemaType.SHORT,\n            \"byte\": XmlSchemaType.BYTE,\n            \"unsignedInt\": XmlSchemaType.UNSIGNEDINT,\n            \"unsignedLong\": XmlSchemaType.UNSIGNEDLONG,\n            \"positiveInteger\": XmlSchemaType.POSITIVEINTEGER,\n            \"nonNegativeInteger\": XmlSchemaType.NONNEGATIVEINTEGER,\n            \"decimal\": XmlSchemaType.DECIMAL,\n            \"float\": XmlSchemaType.FLOAT,\n            \"double\": XmlSchemaType.DOUBLE,\n            \"boolean\": XmlSchemaType.BOOLEAN,\n            \"date\": XmlSchemaType.DATE,\n            \"dateTime\": XmlSchemaType.DATETIME,\n            \"time\": XmlSchemaType.TIME,\n            \"duration\": XmlSchemaType.DURATION,\n            \"gYear\": XmlSchemaType.GYEAR,\n            \"anyURI\": XmlSchemaType.ANYURI,\n            \"base64Binary\": XmlSchemaType.BASE64BINARY,\n            \"hexBinary\": XmlSchemaType.HEXBINARY,\n            \"QName\": XmlSchemaType.QNAME,\n            \"ID\": XmlSchemaType.ID,\n            \"IDREF\": XmlSchemaType.IDREF,\n        }\n\n    def parse_schema_file(self, schema_path: str | Path) -&gt; XmlSchemaDefinition:\n        \"\"\"\n        Parse a single XSD file into an XmlSchemaDefinition.\n\n        Args:\n            schema_path: Path to the XSD file\n\n        Returns:\n            Parsed schema definition\n\n        Raises:\n            XmlSchemaParseError: If parsing fails\n        \"\"\"\n        schema_path = Path(schema_path)\n\n        if not schema_path.exists():\n            raise XmlSchemaParseError(f\"Schema file not found: {schema_path}\")\n\n        try:\n            logger.info(f\"Parsing XML Schema: {schema_path}\")\n\n            # Parse the XML document\n            with open(schema_path, \"rb\") as f:\n                tree = etree.parse(f)\n            root = tree.getroot()\n\n            # Verify it's a schema document\n            if root.tag != f\"{{{self.XS_NAMESPACE}}}schema\":\n                raise XmlSchemaParseError(f\"Not a valid XML Schema document: {schema_path}\")\n\n            # Create schema definition\n            schema_def = XmlSchemaDefinition(\n                schema_location=str(schema_path),\n                target_namespace=root.get(\"targetNamespace\"),\n                element_form_default=root.get(\"elementFormDefault\", \"unqualified\"),\n                attribute_form_default=root.get(\"attributeFormDefault\", \"unqualified\"),\n            )\n\n            # Parse schema contents\n            self._parse_schema_contents(root, schema_def)\n\n            # Cache the parsed schema\n            self.parsed_schemas[str(schema_path)] = schema_def\n\n            logger.info(f\"Successfully parsed schema with {len(schema_def.complex_types)} complex types\")\n            return schema_def\n\n        except etree.XMLSyntaxError as e:\n            raise XmlSchemaParseError(f\"XML syntax error in {schema_path}: {e}\") from e\n        except Exception as e:\n            raise XmlSchemaParseError(f\"Failed to parse schema {schema_path}: {e}\") from e\n\n    def _parse_schema_contents(self, schema_root: Any, schema_def: XmlSchemaDefinition):\n        \"\"\"Parse the contents of a schema element\"\"\"\n        logger.info(f\"Parsing contents of schema: {schema_def.schema_location}\")\n        logger.debug(f\"Root element: {schema_root.tag}\")\n        for child in schema_root:\n            if not isinstance(child.tag, str):\n                logger.debug(f\"Skipping non-string tag: {child.tag}\")\n                continue\n            tag_name = self._get_local_name(child.tag)\n            logger.debug(f\"Found child tag: {tag_name}\")\n\n            if tag_name == \"complexType\":\n                complex_type = self._parse_complex_type(child, schema_def)\n                if complex_type:\n                    schema_def.add_complex_type(complex_type)\n            elif tag_name == \"simpleType\":\n                simple_type = self._parse_simple_type(child, schema_def)\n                if simple_type:\n                    schema_def.add_simple_type(simple_type)\n            elif tag_name == \"element\":\n                element = self._parse_element(child, schema_def)\n                if element:\n                    schema_def.add_element(element)\n            elif tag_name == \"attributeGroup\":\n                # Parse named attributeGroup definitions at the schema root\n                name = child.get(\"name\")\n                if name:\n                    attrs: dict[str, XmlSchemaAttribute] = {}\n                    for attr_elem in child.findall(f\"{{{self.XS_NAMESPACE}}}attribute\"):\n                        attr = self._parse_attribute(attr_elem, schema_def)\n                        if attr:\n                            attrs[attr.name] = attr\n                    if attrs:\n                        schema_def.add_attribute_group(name, attrs)\n            elif tag_name == \"key\":\n                key = self._parse_key(child)\n                if key:\n                    schema_def.keys.append(key)\n            elif tag_name == \"keyref\":\n                keyref = self._parse_keyref(child)\n                if keyref:\n                    schema_def.keyrefs.append(keyref)\n\n    def _parse_complex_type(\n        self, element: Any, schema_def: XmlSchemaDefinition, name_override: str | None = None\n    ) -&gt; XmlSchemaComplexType | None:\n        \"\"\"Parse a complexType element\"\"\"\n        name = name_override or element.get(\"name\")\n        if not name:\n            logger.warning(\"Skipping anonymous complex type\")\n            return None\n\n        complex_type = XmlSchemaComplexType(\n            name=name,\n            abstract=element.get(\"abstract\", \"false\").lower() == \"true\",\n            mixed=element.get(\"mixed\", \"false\").lower() == \"true\",\n            namespace=schema_def.target_namespace,\n            schema_location=schema_def.schema_location,\n        )\n\n        # Parse documentation\n        doc_elem = element.find(f\"{{{self.XS_NAMESPACE}}}annotation/{{{self.XS_NAMESPACE}}}documentation\")\n        if doc_elem is not None and doc_elem.text:\n            complex_type.documentation = doc_elem.text.strip()\n\n        logger.debug(f\"Parsing content for complexType: {name}\")\n        # Parse content model (sequence, choice, all)\n        for child in element:\n            tag_name = self._get_local_name(child.tag)\n            logger.debug(f\"  - Found tag in {name}: {tag_name}\")\n\n            if tag_name == \"sequence\":\n                complex_type.sequence = True\n                complex_type.choice = False\n                self._parse_particle_content(child, complex_type, schema_def)\n\n            elif tag_name == \"choice\":\n                complex_type.choice = True\n                complex_type.sequence = False\n                self._parse_particle_content(child, complex_type, schema_def)\n\n            elif tag_name == \"all\":\n                complex_type.all_elements = True\n                complex_type.sequence = False\n                self._parse_particle_content(child, complex_type, schema_def)\n\n            elif tag_name == \"attribute\":\n                attribute = self._parse_attribute(child, schema_def)\n                if attribute:\n                    complex_type.attributes[attribute.name] = attribute\n\n            elif tag_name == \"attributeGroup\":\n                # Expand referenced attribute group into this complex type\n                ref = child.get(\"ref\")\n                if ref:\n                    ref_local = self._get_local_name(ref)\n                    group = schema_def.get_attribute_group(ref_local)\n                    if group:\n                        # Shallow copy attributes to avoid shared mutations\n                        for aname, attr in group.items():\n                            complex_type.attributes.setdefault(aname, copy.deepcopy(attr))\n\n            elif tag_name == \"complexContent\":\n                # Handle inheritance/extension\n                self._parse_complex_content(child, complex_type, schema_def)\n\n            elif tag_name == \"simpleContent\":\n                # Handle simple content with attributes\n                self._parse_simple_content(child, complex_type, schema_def)\n\n        return complex_type\n\n    def _parse_particle_content(\n        self, particle: Any, complex_type: XmlSchemaComplexType, schema_def: XmlSchemaDefinition\n    ):\n        \"\"\"Parse sequence, choice, or all content\"\"\"\n        logger.debug(f\"Parsing particle content for {complex_type.name}\")\n        for child in particle:\n            # Some lxml nodes (comments/PIs) expose non-string tag objects; skip them safely\n            if not isinstance(child.tag, str):\n                logger.debug(\"Skipping non-string particle tag under %s: %r\", complex_type.name, child.tag)\n                continue\n            tag_name = self._get_local_name(child.tag)\n            logger.debug(f\"  - Found particle child in {complex_type.name}: {tag_name}\")\n\n            if tag_name == \"element\":\n                element = self._parse_element(child, schema_def)\n                if element:\n                    complex_type.elements.append(element)\n                    # Expand substitution group members if this element is a head\n                    try:\n                        head_local = element.name\n                        members = schema_def.get_substitution_members(head_local)\n                        if members:\n                            existing_names = {e.name for e in complex_type.elements}\n                            for mem in members:\n                                if mem.name in existing_names:\n                                    continue\n                                clone = copy.deepcopy(element)\n                                clone.name = mem.name\n                                clone.type_name = mem.type_name or clone.type_name\n                                # Clear base_type so factory resolves complex type\n                                clone.base_type = None\n                                complex_type.elements.append(clone)\n                    except Exception:\n                        # Best-effort expansion only\n                        pass\n\n            elif tag_name in (\"sequence\", \"choice\", \"all\"):\n                # Nested groups\n                self._parse_particle_content(child, complex_type, schema_def)\n\n        logger.debug(f\"Finished particle content for {complex_type.name}. Total elements: {len(complex_type.elements)}\")\n\n    def _parse_element(self, element: Any, schema_def: XmlSchemaDefinition) -&gt; XmlSchemaElement | None:\n        \"\"\"Parse an element definition\"\"\"\n        name = element.get(\"name\")\n        ref = element.get(\"ref\")\n\n        if not name and not ref:\n            logger.warning(\"Element without name or ref, skipping\")\n            return None\n\n        # Keys and keyrefs can be defined within an element\n        for key_elem in element.findall(f\"{{{self.XS_NAMESPACE}}}key\"):\n            key = self._parse_key(key_elem)\n            if key:\n                schema_def.keys.append(key)\n\n        for keyref_elem in element.findall(f\"{{{self.XS_NAMESPACE}}}keyref\"):\n            keyref = self._parse_keyref(keyref_elem)\n            if keyref:\n                schema_def.keyrefs.append(keyref)\n\n        # If it's a reference, we might need to look it up later\n        if ref:\n            # For now, just store the ref. Resolution can happen in a later pass.\n            pass\n\n        xml_element = XmlSchemaElement(\n            name=name or ref,\n            type_name=element.get(\"type\"),\n            min_occurs=int(element.get(\"minOccurs\", \"1\")),\n            max_occurs=element.get(\"maxOccurs\", \"1\"),\n            nillable=element.get(\"nillable\", \"false\").lower() == \"true\",\n            default_value=element.get(\"default\"),\n            fixed_value=element.get(\"fixed\"),\n            abstract=element.get(\"abstract\", \"false\").lower() == \"true\",\n            namespace=schema_def.target_namespace,\n            schema_location=schema_def.schema_location,\n        )\n\n        # Handle maxOccurs=\"unbounded\"\n        if xml_element.max_occurs == \"unbounded\":\n            xml_element.is_list = True\n            xml_element.max_occurs = -1  # Or some indicator of unbounded\n        else:\n            xml_element.max_occurs = int(xml_element.max_occurs)\n\n        # Capture substitution group if present\n        try:\n            subst = element.get(\"substitutionGroup\")\n            if subst:\n                xml_element.substitution_group = self._get_local_name(subst)\n        except Exception:\n            pass\n\n        # Map built-in types\n        if xml_element.type_name:\n            type_local_name = self._get_local_name(xml_element.type_name)\n            if type_local_name in self.type_mappings:\n                xml_element.base_type = self.type_mappings[type_local_name]\n\n        # Look for an inline complexType and parse it\n        inline_complex_type_elem = element.find(f\"{{{self.XS_NAMESPACE}}}complexType\")\n        if inline_complex_type_elem is not None:\n            # Synthesize a name for the inline complex type\n            type_name = f\"{xml_element.name}_Type\"\n            logger.debug(f\"Found inline complexType for element '{xml_element.name}'. Creating type '{type_name}'.\")\n\n            # Parse the complex type\n            complex_type = self._parse_complex_type(inline_complex_type_elem, schema_def, name_override=type_name)\n\n            if complex_type:\n                schema_def.add_complex_type(complex_type)\n                xml_element.type_name = type_name  # Associate element with new type\n\n        # Handle inline simpleType as well\n        inline_simple_type_elem = element.find(f\"{{{self.XS_NAMESPACE}}}simpleType\")\n        if inline_simple_type_elem is not None:\n            # Synthesize a name for the inline simple type\n            type_name = f\"{xml_element.name}_SimpleType\"\n            simple_type = self._parse_simple_type(inline_simple_type_elem, schema_def, name_override=type_name)\n            if simple_type:\n                schema_def.add_simple_type(simple_type)\n                xml_element.type_name = type_name\n\n        # Parse documentation\n        doc_elem = element.find(f\"{{{self.XS_NAMESPACE}}}annotation/{{{self.XS_NAMESPACE}}}documentation\")\n        if doc_elem is not None and doc_elem.text:\n            xml_element.documentation = doc_elem.text.strip()\n\n        return xml_element\n\n    def _parse_attribute(self, element: Any, schema_def: XmlSchemaDefinition) -&gt; XmlSchemaAttribute | None:\n        \"\"\"Parse an attribute definition\"\"\"\n        name = element.get(\"name\")\n        ref = element.get(\"ref\")\n\n        if not name and not ref:\n            logger.warning(\"Attribute without name or ref, skipping\")\n            return None\n\n        attribute = XmlSchemaAttribute(\n            name=name or ref,\n            type_name=element.get(\"type\"),\n            use=element.get(\"use\", \"optional\"),\n            default_value=element.get(\"default\"),\n            fixed_value=element.get(\"fixed\"),\n            namespace=schema_def.target_namespace,\n        )\n\n        # Map built-in types\n        if attribute.type_name:\n            type_local_name = self._get_local_name(attribute.type_name)\n            if type_local_name in self.type_mappings:\n                attribute.base_type = self.type_mappings[type_local_name]\n\n        return attribute\n\n    def _parse_simple_type(\n        self, element: Any, schema_def: XmlSchemaDefinition, name_override: str | None = None\n    ) -&gt; XmlSchemaSimpleType | None:\n        \"\"\"Parse a simpleType element.\"\"\"\n        name = name_override or element.get(\"name\")\n        if not name:\n            logger.warning(\"Skipping anonymous simple type.\")\n            return None\n\n        simple_type = XmlSchemaSimpleType(name=name)\n\n        # Parse documentation\n        doc_elem = element.find(f\"{{{self.XS_NAMESPACE}}}annotation/{{{self.XS_NAMESPACE}}}documentation\")\n        if doc_elem is not None and doc_elem.text:\n            simple_type.documentation = doc_elem.text.strip()\n\n        # Parse content (restriction, list, union)\n        for child in element:\n            tag_name = self._get_local_name(child.tag)\n            if tag_name == \"restriction\":\n                simple_type.restrictions = self._parse_restriction(child)\n                if simple_type.restrictions and simple_type.restrictions.base:\n                    type_local_name = self._get_local_name(simple_type.restrictions.base)\n                    if type_local_name in self.type_mappings:\n                        simple_type.base_type = self.type_mappings[type_local_name]\n            # TODO: Add support for list and union simpleTypes\n\n        return simple_type\n\n    def _parse_restriction(self, restriction_elem: Any) -&gt; XmlSchemaRestriction:\n        \"\"\"Parse a restriction element\"\"\"\n        base_type_qname = restriction_elem.get(\"base\")\n        base_type_name = self._get_local_name(base_type_qname) if base_type_qname else None\n        base_type = self._get_base_type(base_type_name)\n\n        restriction = XmlSchemaRestriction(base=base_type.value if base_type else None)\n\n        for child in restriction_elem:\n            tag_name = self._get_local_name(child.tag)\n            value = child.get(\"value\")\n\n            if tag_name == \"enumeration\":\n                if value:\n                    # Try to read per-enumeration documentation label\n                    doc_elem = child.find(f\"{{{self.XS_NAMESPACE}}}annotation/{{{self.XS_NAMESPACE}}}documentation\")\n                    if doc_elem is not None and doc_elem.text:\n                        label = doc_elem.text.strip()\n                    else:\n                        label = value.replace(\"_\", \" \").title()\n                    restriction.enumeration.append((value, label))\n            elif tag_name == \"pattern\":\n                if value:\n                    restriction.pattern = value\n            elif tag_name == \"minLength\":\n                restriction.min_length = int(value)\n            elif tag_name == \"maxLength\":\n                restriction.max_length = int(value)\n            elif tag_name == \"length\":\n                restriction.length = int(value)\n            elif tag_name == \"minInclusive\":\n                restriction.min_inclusive = float(value)\n            elif tag_name == \"maxInclusive\":\n                restriction.max_inclusive = float(value)\n            elif tag_name == \"minExclusive\":\n                restriction.min_exclusive = float(value)\n            elif tag_name == \"maxExclusive\":\n                restriction.max_exclusive = float(value)\n            elif tag_name == \"fractionDigits\":\n                restriction.fraction_digits = int(value)\n            elif tag_name == \"totalDigits\":\n                restriction.total_digits = int(value)\n            elif tag_name == \"whiteSpace\":\n                restriction.white_space = value\n\n        return restriction\n\n    def _parse_complex_content(\n        self, complex_content: Any, complex_type: XmlSchemaComplexType, schema_def: XmlSchemaDefinition\n    ):\n        \"\"\"Parse complexContent, which typically implies extension or restriction.\n\n        Minimal support implemented:\n        - xs:extension: resolve base complex type, shallowly inherit its elements/attributes,\n          then parse additional particles/attributes declared within the extension.\n        - xs:restriction: treated similarly to extension (limited). Restriction facets are\n          not enforced; local particles/attributes are parsed if present.\n        \"\"\"\n        try:\n            # Handle &lt;xs:extension base=\"...\"&gt; ... &lt;/xs:extension&gt;\n            extension = complex_content.find(f\"{{{self.XS_NAMESPACE}}}extension\")\n            if extension is not None:\n                base_qname = extension.get(\"base\")\n                base_local = self._get_local_name(base_qname) if base_qname else None\n\n                if base_local:\n                    complex_type.base_type = base_local\n                    base_ct = schema_def.find_complex_type(base_local, namespace=schema_def.target_namespace)\n                    if base_ct:\n                        # Inherit elements and attributes from the base type\n                        if base_ct.elements:\n                            complex_type.elements.extend(copy.deepcopy(base_ct.elements))\n                        if base_ct.attributes:\n                            complex_type.attributes.update(copy.deepcopy(base_ct.attributes))\n                    else:\n                        logger.debug(\"Base complex type '%s' not found for %s\", base_local, complex_type.name)\n\n                # Parse additional content inside the extension\n                for child in extension:\n                    tag_name = self._get_local_name(child.tag)\n                    if tag_name in (\"sequence\", \"choice\", \"all\"):\n                        if tag_name == \"sequence\":\n                            complex_type.sequence = True\n                            complex_type.choice = False\n                        elif tag_name == \"choice\":\n                            complex_type.choice = True\n                            complex_type.sequence = False\n                        elif tag_name == \"all\":\n                            complex_type.all_elements = True\n                            complex_type.sequence = False\n                        self._parse_particle_content(child, complex_type, schema_def)\n                    elif tag_name == \"attribute\":\n                        attribute = self._parse_attribute(child, schema_def)\n                        if attribute:\n                            complex_type.attributes[attribute.name] = attribute\n                    elif tag_name == \"attributeGroup\":\n                        ref = child.get(\"ref\")\n                        if ref:\n                            ref_local = self._get_local_name(ref)\n                            group = schema_def.get_attribute_group(ref_local)\n                            if group:\n                                for aname, attr in group.items():\n                                    complex_type.attributes.setdefault(aname, copy.deepcopy(attr))\n                return\n\n            # Handle &lt;xs:restriction base=\"...\"&gt; ... &lt;/xs:restriction&gt;\n            restriction = complex_content.find(f\"{{{self.XS_NAMESPACE}}}restriction\")\n            if restriction is not None:\n                base_qname = restriction.get(\"base\")\n                base_local = self._get_local_name(base_qname) if base_qname else None\n\n                if base_local:\n                    complex_type.base_type = base_local\n                    base_ct = schema_def.find_complex_type(base_local, namespace=schema_def.target_namespace)\n                    if base_ct:\n                        # Limited handling: inherit, but do not enforce restriction facets\n                        if base_ct.elements:\n                            complex_type.elements.extend(copy.deepcopy(base_ct.elements))\n                        if base_ct.attributes:\n                            complex_type.attributes.update(copy.deepcopy(base_ct.attributes))\n\n                # Parse any locally declared particles/attributes\n                for child in restriction:\n                    tag_name = self._get_local_name(child.tag)\n                    if tag_name in (\"sequence\", \"choice\", \"all\"):\n                        self._parse_particle_content(child, complex_type, schema_def)\n                    elif tag_name == \"attribute\":\n                        attribute = self._parse_attribute(child, schema_def)\n                        if attribute:\n                            complex_type.attributes[attribute.name] = attribute\n\n                logger.warning(f\"complexContent restriction in {complex_type.name} is treated as extension (limited).\")\n                return\n\n            logger.warning(f\"complexContent in {complex_type.name} has no extension/restriction; limited support.\")\n        except Exception:\n            logger.warning(f\"complexContent in {complex_type.name} is not fully supported yet.\")\n\n    def _parse_simple_content(\n        self, simple_content: Any, complex_type: XmlSchemaComplexType, schema_def: XmlSchemaDefinition\n    ):\n        \"\"\"Parse simpleContent, which adds attributes to a simple type.\"\"\"\n        # Minimal support: capture attributes declared on the extension node\n        # so that types like HeaderType (attributes-only) are not filtered out.\n        try:\n            ext = simple_content.find(f\"{{{self.XS_NAMESPACE}}}extension\")\n            if ext is None:\n                logger.warning(f\"simpleContent in {complex_type.name} has no extension; limited support\")\n                return\n            for attr_elem in ext.findall(f\"{{{self.XS_NAMESPACE}}}attribute\"):\n                attribute = self._parse_attribute(attr_elem, schema_def)\n                if attribute:\n                    complex_type.attributes[attribute.name] = attribute\n            # Expand attribute groups referenced in simpleContent extension\n            for ag in ext.findall(f\"{{{self.XS_NAMESPACE}}}attributeGroup\"):\n                ref = ag.get(\"ref\")\n                if ref:\n                    ref_local = self._get_local_name(ref)\n                    group = schema_def.get_attribute_group(ref_local)\n                    if group:\n                        for aname, attr in group.items():\n                            complex_type.attributes.setdefault(aname, copy.deepcopy(attr))\n        except Exception:\n            logger.warning(f\"simpleContent in {complex_type.name} is not fully supported yet.\")\n\n    def _get_local_name(self, qname: str) -&gt; str:\n        \"\"\"Extract the local name from a qualified name (e.g., {http://...}string -&gt; string).\"\"\"\n        if \"}\" in qname:\n            return qname.split(\"}\", 1)[-1]\n        if \":\" in qname:\n            return qname.split(\":\", 1)[-1]\n        return qname\n\n    def _parse_key(self, element: Any) -&gt; XmlSchemaKey | None:\n        \"\"\"Parse a key element.\"\"\"\n        name = element.get(\"name\")\n        if not name:\n            logger.warning(\"Key without name, skipping.\")\n            return None\n\n        selector_elem = element.find(f\"{{{self.XS_NAMESPACE}}}selector\")\n        selector = selector_elem.get(\"xpath\") if selector_elem is not None else None\n\n        fields = [\n            field.get(\"xpath\") for field in element.findall(f\"{{{self.XS_NAMESPACE}}}field\") if field.get(\"xpath\")\n        ]\n\n        if not selector or not fields:\n            logger.warning(f\"Key '{name}' is missing selector or fields, skipping.\")\n            return None\n\n        return XmlSchemaKey(name=name, selector=selector, fields=fields)\n\n    def _parse_keyref(self, element: Any) -&gt; XmlSchemaKeyRef | None:\n        \"\"\"Parse a keyref element.\"\"\"\n        name = element.get(\"name\")\n        refer = element.get(\"refer\")\n\n        if not name or not refer:\n            logger.warning(\"KeyRef without name or refer, skipping.\")\n            return None\n\n        selector_elem = element.find(f\"{{{self.XS_NAMESPACE}}}selector\")\n        selector = selector_elem.get(\"xpath\") if selector_elem is not None else None\n\n        fields = [\n            field.get(\"xpath\") for field in element.findall(f\"{{{self.XS_NAMESPACE}}}field\") if field.get(\"xpath\")\n        ]\n\n        if not selector or not fields:\n            logger.warning(f\"KeyRef '{name}' is missing selector or fields, skipping.\")\n            return None\n\n        return XmlSchemaKeyRef(name=name, refer=refer, selector=selector, fields=fields)\n\n    def parse_multiple_schemas(self, schema_paths: list[str | Path]) -&gt; list[XmlSchemaDefinition]:\n        \"\"\"\n        Parse multiple XSD files and return a list of schema definitions.\n\n        Args:\n            schema_paths: List of paths to XSD files\n\n        Returns:\n            List of parsed schema definitions\n        \"\"\"\n        schemas = []\n        for path in schema_paths:\n            try:\n                schema = self.parse_schema_file(path)\n                schemas.append(schema)\n            except XmlSchemaParseError as e:\n                logger.error(f\"Failed to parse schema {path}: {e}\")\n                # Continue with other schemas\n                continue\n\n        return schemas\n\n    def _get_base_type(self, type_name: str | None) -&gt; XmlSchemaType:\n        \"\"\"Resolve XML Schema built-in type to an XmlSchemaType enum.\"\"\"\n        if not type_name:\n            return XmlSchemaType.STRING  # Default\n\n        logger.debug(f\"Attempting to resolve base type for: {type_name}\")\n        # Remove namespace prefix (e.g., 'xs:')\n        local_name = type_name.split(\":\")[-1].upper()\n\n        try:\n            resolved_type = XmlSchemaType[local_name]\n            logger.debug(f\"Resolved '{type_name}' to '{resolved_type}'\")\n            return resolved_type\n        except KeyError:\n            # It's not a built-in, might be a custom simpleType or complexType\n            # The factory will need to resolve this later.\n            # For now, we can return a default or a special \"unresolved\" type\n            logger.debug(f\"Could not resolve '{type_name}' to a built-in type. Defaulting to STRING.\")\n            return XmlSchemaType.STRING  # Fallback\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/parser/#pydantic2django.xmlschema.parser.XmlSchemaParser.parse_multiple_schemas","title":"<code>parse_multiple_schemas(schema_paths)</code>","text":"<p>Parse multiple XSD files and return a list of schema definitions.</p> <p>Parameters:</p> Name Type Description Default <code>schema_paths</code> <code>list[str | Path]</code> <p>List of paths to XSD files</p> required <p>Returns:</p> Type Description <code>list[XmlSchemaDefinition]</code> <p>List of parsed schema definitions</p> Source code in <code>src/pydantic2django/xmlschema/parser.py</code> <pre><code>def parse_multiple_schemas(self, schema_paths: list[str | Path]) -&gt; list[XmlSchemaDefinition]:\n    \"\"\"\n    Parse multiple XSD files and return a list of schema definitions.\n\n    Args:\n        schema_paths: List of paths to XSD files\n\n    Returns:\n        List of parsed schema definitions\n    \"\"\"\n    schemas = []\n    for path in schema_paths:\n        try:\n            schema = self.parse_schema_file(path)\n            schemas.append(schema)\n        except XmlSchemaParseError as e:\n            logger.error(f\"Failed to parse schema {path}: {e}\")\n            # Continue with other schemas\n            continue\n\n    return schemas\n</code></pre>"},{"location":"reference/pydantic2django/xmlschema/parser/#pydantic2django.xmlschema.parser.XmlSchemaParser.parse_schema_file","title":"<code>parse_schema_file(schema_path)</code>","text":"<p>Parse a single XSD file into an XmlSchemaDefinition.</p> <p>Parameters:</p> Name Type Description Default <code>schema_path</code> <code>str | Path</code> <p>Path to the XSD file</p> required <p>Returns:</p> Type Description <code>XmlSchemaDefinition</code> <p>Parsed schema definition</p> <p>Raises:</p> Type Description <code>XmlSchemaParseError</code> <p>If parsing fails</p> Source code in <code>src/pydantic2django/xmlschema/parser.py</code> <pre><code>def parse_schema_file(self, schema_path: str | Path) -&gt; XmlSchemaDefinition:\n    \"\"\"\n    Parse a single XSD file into an XmlSchemaDefinition.\n\n    Args:\n        schema_path: Path to the XSD file\n\n    Returns:\n        Parsed schema definition\n\n    Raises:\n        XmlSchemaParseError: If parsing fails\n    \"\"\"\n    schema_path = Path(schema_path)\n\n    if not schema_path.exists():\n        raise XmlSchemaParseError(f\"Schema file not found: {schema_path}\")\n\n    try:\n        logger.info(f\"Parsing XML Schema: {schema_path}\")\n\n        # Parse the XML document\n        with open(schema_path, \"rb\") as f:\n            tree = etree.parse(f)\n        root = tree.getroot()\n\n        # Verify it's a schema document\n        if root.tag != f\"{{{self.XS_NAMESPACE}}}schema\":\n            raise XmlSchemaParseError(f\"Not a valid XML Schema document: {schema_path}\")\n\n        # Create schema definition\n        schema_def = XmlSchemaDefinition(\n            schema_location=str(schema_path),\n            target_namespace=root.get(\"targetNamespace\"),\n            element_form_default=root.get(\"elementFormDefault\", \"unqualified\"),\n            attribute_form_default=root.get(\"attributeFormDefault\", \"unqualified\"),\n        )\n\n        # Parse schema contents\n        self._parse_schema_contents(root, schema_def)\n\n        # Cache the parsed schema\n        self.parsed_schemas[str(schema_path)] = schema_def\n\n        logger.info(f\"Successfully parsed schema with {len(schema_def.complex_types)} complex types\")\n        return schema_def\n\n    except etree.XMLSyntaxError as e:\n        raise XmlSchemaParseError(f\"XML syntax error in {schema_path}: {e}\") from e\n    except Exception as e:\n        raise XmlSchemaParseError(f\"Failed to parse schema {schema_path}: {e}\") from e\n</code></pre>"}]}